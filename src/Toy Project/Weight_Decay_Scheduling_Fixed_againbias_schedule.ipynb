{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Full Training"
      ],
      "metadata": {
        "id": "nFErLyP-vDvS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzLKpmZICaWN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c326c31-5d4c-419b-f7c5-9283fd5cf125"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.14.0\n"
          ]
        }
      ],
      "source": [
        "  #TensorFlow and tf.keras\n",
        "import tensorflow as tf\n",
        "\n",
        "# Helper libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-nlp==0.6.2\n",
        "!pip install tensorflow==2.14.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wf1llNJQvAFk",
        "outputId": "11556895-2ced-4b4e-e061-2063f2d9dbe8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-nlp==0.6.2\n",
            "  Downloading keras_nlp-0.6.2-py3-none-any.whl (590 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m590.1/590.1 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras-core (from keras-nlp==0.6.2)\n",
            "  Downloading keras_core-0.1.7-py3-none-any.whl (950 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m950.8/950.8 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras-nlp==0.6.2) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras-nlp==0.6.2) (1.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-nlp==0.6.2) (23.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from keras-nlp==0.6.2) (2023.6.3)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras-nlp==0.6.2) (13.7.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from keras-nlp==0.6.2) (0.1.8)\n",
            "Collecting tensorflow-text (from keras-nlp==0.6.2)\n",
            "  Downloading tensorflow_text-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting namex (from keras-core->keras-nlp==0.6.2)\n",
            "  Downloading namex-0.0.7-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras-core->keras-nlp==0.6.2) (3.9.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras-nlp==0.6.2) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras-nlp==0.6.2) (2.16.1)\n",
            "Requirement already satisfied: tensorflow-hub>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text->keras-nlp==0.6.2) (0.15.0)\n",
            "Collecting tensorflow<2.16,>=2.15.0 (from tensorflow-text->keras-nlp==0.6.2)\n",
            "  Downloading tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras-nlp==0.6.2) (0.1.2)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp==0.6.2) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp==0.6.2) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp==0.6.2) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp==0.6.2) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp==0.6.2) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp==0.6.2) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp==0.6.2) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp==0.6.2) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp==0.6.2) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp==0.6.2) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp==0.6.2) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp==0.6.2) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp==0.6.2) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp==0.6.2) (0.34.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp==0.6.2) (1.59.2)\n",
            "Collecting tensorboard<2.16,>=2.15 (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp==0.6.2)\n",
            "  Downloading tensorboard-2.15.1-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp==0.6.2)\n",
            "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras<2.16,>=2.15.0 (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp==0.6.2)\n",
            "  Downloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp==0.6.2) (0.41.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp==0.6.2) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp==0.6.2) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp==0.6.2) (3.5.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp==0.6.2) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp==0.6.2) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp==0.6.2) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp==0.6.2) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp==0.6.2) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp==0.6.2) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp==0.6.2) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp==0.6.2) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp==0.6.2) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp==0.6.2) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp==0.6.2) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp==0.6.2) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp==0.6.2) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp==0.6.2) (3.2.2)\n",
            "Installing collected packages: namex, tensorflow-estimator, keras, keras-core, tensorboard, tensorflow, tensorflow-text, keras-nlp\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.14.0\n",
            "    Uninstalling tensorflow-estimator-2.14.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.14.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.14.0\n",
            "    Uninstalling keras-2.14.0:\n",
            "      Successfully uninstalled keras-2.14.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.14.1\n",
            "    Uninstalling tensorboard-2.14.1:\n",
            "      Successfully uninstalled tensorboard-2.14.1\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.14.0\n",
            "    Uninstalling tensorflow-2.14.0:\n",
            "      Successfully uninstalled tensorflow-2.14.0\n",
            "Successfully installed keras-2.15.0 keras-core-0.1.7 keras-nlp-0.6.2 namex-0.0.7 tensorboard-2.15.1 tensorflow-2.15.0 tensorflow-estimator-2.15.0 tensorflow-text-2.15.0\n",
            "Collecting tensorflow==2.14.0\n",
            "  Downloading tensorflow-2.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (489.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m489.8/489.8 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (0.2.0)\n",
            "Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (0.34.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (1.59.2)\n",
            "Collecting tensorboard<2.15,>=2.14 (from tensorflow==2.14.0)\n",
            "  Downloading tensorboard-2.14.1-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m111.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-estimator<2.15,>=2.14.0 (from tensorflow==2.14.0)\n",
            "  Downloading tensorflow_estimator-2.14.0-py2.py3-none-any.whl (440 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras<2.15,>=2.14.0 (from tensorflow==2.14.0)\n",
            "  Downloading keras-2.14.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m95.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.14.0) (0.41.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0) (3.5.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (3.2.2)\n",
            "Installing collected packages: tensorflow-estimator, keras, tensorboard, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.15.0\n",
            "    Uninstalling tensorflow-estimator-2.15.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.15.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.15.0\n",
            "    Uninstalling keras-2.15.0:\n",
            "      Successfully uninstalled keras-2.15.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.15.1\n",
            "    Uninstalling tensorboard-2.15.1:\n",
            "      Successfully uninstalled tensorboard-2.15.1\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.15.0\n",
            "    Uninstalling tensorflow-2.15.0:\n",
            "      Successfully uninstalled tensorflow-2.15.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-text 2.15.0 requires tensorflow<2.16,>=2.15.0; platform_machine != \"arm64\" or platform_system != \"Darwin\", but you have tensorflow 2.14.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed keras-2.14.0 tensorboard-2.14.1 tensorflow-2.14.0 tensorflow-estimator-2.14.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name =  \"bert_tiny_en_uncased\""
      ],
      "metadata": {
        "id": "IHaq6tYcvHED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name =  \"bert_small_en_uncased\""
      ],
      "metadata": {
        "id": "NCViAWW4wSMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow import keras\n",
        "import keras_nlp\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NddXKYBgvM2b",
        "outputId": "c05fa470-3208-4605-b5f0-b4c01a17d203"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using TensorFlow backend\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pretrained classifier.\n",
        "bert_tiny = keras_nlp.models.BertClassifier.from_preset(\n",
        "    model_name,\n",
        "    preprocessor=None,\n",
        "    num_classes=2,\n",
        "    load_weights = True,\n",
        "    activation='sigmoid'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUUeD1dtvqHI",
        "outputId": "20625e44-4543-494d-81d1-910efc84e938"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/keras-nlp/models/bert_tiny_en_uncased/v1/model.h5\n",
            "17602216/17602216 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pretrained classifier.\n",
        "bert_small = keras_nlp.models.BertClassifier.from_preset(\n",
        "    model_name,\n",
        "    preprocessor=None,\n",
        "    num_classes=2,\n",
        "    load_weights = True,\n",
        "    activation='sigmoid'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "foH94SYGwU4R",
        "outputId": "ccf8c6f0-7d0f-4bca-986b-022523541d15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/keras-nlp/models/bert_small_en_uncased/v1/model.h5\n",
            "115149824/115149824 [==============================] - 1s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert_tiny.get_layer('bert_backbone').summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6KJZal5vtFf",
        "outputId": "345dfca1-8476-4b52-b04b-4af0e0ca94d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"bert_backbone\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " token_ids (InputLayer)      [(None, None)]               0         []                            \n",
            "                                                                                                  \n",
            " token_embedding (Reversibl  (None, None, 128)            3906816   ['token_ids[0][0]']           \n",
            " eEmbedding)                                                                                      \n",
            "                                                                                                  \n",
            " segment_ids (InputLayer)    [(None, None)]               0         []                            \n",
            "                                                                                                  \n",
            " position_embedding (Positi  (None, None, 128)            65536     ['token_embedding[0][0]']     \n",
            " onEmbedding)                                                                                     \n",
            "                                                                                                  \n",
            " segment_embedding (Embeddi  (None, None, 128)            256       ['segment_ids[0][0]']         \n",
            " ng)                                                                                              \n",
            "                                                                                                  \n",
            " add (Add)                   (None, None, 128)            0         ['token_embedding[0][0]',     \n",
            "                                                                     'position_embedding[0][0]',  \n",
            "                                                                     'segment_embedding[0][0]']   \n",
            "                                                                                                  \n",
            " embeddings_layer_norm (Lay  (None, None, 128)            256       ['add[0][0]']                 \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " embeddings_dropout (Dropou  (None, None, 128)            0         ['embeddings_layer_norm[0][0]'\n",
            " t)                                                                 ]                             \n",
            "                                                                                                  \n",
            " padding_mask (InputLayer)   [(None, None)]               0         []                            \n",
            "                                                                                                  \n",
            " transformer_layer_0 (Trans  (None, None, 128)            198272    ['embeddings_dropout[0][0]',  \n",
            " formerEncoder)                                                      'padding_mask[0][0]']        \n",
            "                                                                                                  \n",
            " transformer_layer_1 (Trans  (None, None, 128)            198272    ['transformer_layer_0[0][0]', \n",
            " formerEncoder)                                                      'padding_mask[0][0]']        \n",
            "                                                                                                  \n",
            " pooled_dense (Dense)        (None, None, 128)            16512     ['transformer_layer_1[0][0]'] \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (  (None, 128)                  0         ['pooled_dense[0][0]']        \n",
            " SlicingOpLambda)                                                                                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 4385920 (16.73 MB)\n",
            "Trainable params: 4385920 (16.73 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert_small.get_layer('bert_backbone_1').summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d60W68sjwamx",
        "outputId": "f8be1ccf-0ed4-44aa-b712-30e719712749"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"bert_backbone_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " token_ids (InputLayer)      [(None, None)]               0         []                            \n",
            "                                                                                                  \n",
            " token_embedding (Reversibl  (None, None, 512)            1562726   ['token_ids[0][0]']           \n",
            " eEmbedding)                                              4                                       \n",
            "                                                                                                  \n",
            " segment_ids (InputLayer)    [(None, None)]               0         []                            \n",
            "                                                                                                  \n",
            " position_embedding (Positi  (None, None, 512)            262144    ['token_embedding[0][0]']     \n",
            " onEmbedding)                                                                                     \n",
            "                                                                                                  \n",
            " segment_embedding (Embeddi  (None, None, 512)            1024      ['segment_ids[0][0]']         \n",
            " ng)                                                                                              \n",
            "                                                                                                  \n",
            " add_1 (Add)                 (None, None, 512)            0         ['token_embedding[0][0]',     \n",
            "                                                                     'position_embedding[0][0]',  \n",
            "                                                                     'segment_embedding[0][0]']   \n",
            "                                                                                                  \n",
            " embeddings_layer_norm (Lay  (None, None, 512)            1024      ['add_1[0][0]']               \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " embeddings_dropout (Dropou  (None, None, 512)            0         ['embeddings_layer_norm[0][0]'\n",
            " t)                                                                 ]                             \n",
            "                                                                                                  \n",
            " padding_mask (InputLayer)   [(None, None)]               0         []                            \n",
            "                                                                                                  \n",
            " transformer_layer_0 (Trans  (None, None, 512)            3152384   ['embeddings_dropout[0][0]',  \n",
            " formerEncoder)                                                      'padding_mask[0][0]']        \n",
            "                                                                                                  \n",
            " transformer_layer_1 (Trans  (None, None, 512)            3152384   ['transformer_layer_0[0][0]', \n",
            " formerEncoder)                                                      'padding_mask[0][0]']        \n",
            "                                                                                                  \n",
            " transformer_layer_2 (Trans  (None, None, 512)            3152384   ['transformer_layer_1[0][0]', \n",
            " formerEncoder)                                                      'padding_mask[0][0]']        \n",
            "                                                                                                  \n",
            " transformer_layer_3 (Trans  (None, None, 512)            3152384   ['transformer_layer_2[0][0]', \n",
            " formerEncoder)                                                      'padding_mask[0][0]']        \n",
            "                                                                                                  \n",
            " pooled_dense (Dense)        (None, None, 512)            262656    ['transformer_layer_3[0][0]'] \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1  (None, 512)                  0         ['pooled_dense[0][0]']        \n",
            "  (SlicingOpLambda)                                                                               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 28763648 (109.72 MB)\n",
            "Trainable params: 28763648 (109.72 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eruw652VUt_D",
        "outputId": "c68a435d-6975-4ed9-8093-d54e1bc77b35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7MqDQO0KCaWS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50771a33-f29d-4f15-d27f-7cbe4dd34171"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 1us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 2s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 1s 0us/step\n"
          ]
        }
      ],
      "source": [
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IjnLH5S2CaWx"
      },
      "outputs": [],
      "source": [
        "\n",
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bW5WzIPlCaWv"
      },
      "outputs": [],
      "source": [
        "train_images = train_images / 255.0\n",
        "\n",
        "test_images = test_images / 255.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# 학습 데이터를 학습용과 검증용으로 분리합니다. 여기서는 20%를 검증 데이터로 사용합니다.\n",
        "train_images, val_images, train_labels, val_labels = train_test_split(\n",
        "    train_images, train_labels, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "eL-MK2B3T6VR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 입력층 정의\n",
        "inputs = tf.keras.Input(shape=(28, 28))\n",
        "\n",
        "# 첫 번째 레이어: Flatten\n",
        "x = tf.keras.layers.Flatten()(inputs)\n",
        "\n",
        "# 두 번째 레이어: Dense\n",
        "x = tf.keras.layers.Dense(256, activation='relu')(x)\n",
        "\n",
        "# 세 번째 레이어: Dense\n",
        "x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
        "\n",
        "# 네 번째 레이어: Dense\n",
        "outputs = tf.keras.layers.Dense(10)(x)\n",
        "\n",
        "# 모델 생성\n",
        "model = tf.keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "metadata": {
        "id": "9rAhkfmlzJrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.utils.plot_model(model, \"my_model.png\", show_shapes=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 533
        },
        "id": "xoxKMvj70N2p",
        "outputId": "3e0e1829-233d-48fc-f853-4d2743d7cfe7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUgAAAIECAIAAAD0BCDuAAAABmJLR0QA/wD/AP+gvaeTAAAgAElEQVR4nOydeVgUR/7/qwfmYGCGQ065ZxAVNRKjUVAXjRuN8QHFI2LERI0JahJEUQliiCIeBKM8GEzW8/kqRlA0aIjorrjosh4xP2FVXBUvBBE5BIZhBhmgfn/UN/2dcAwDzEwP7ef1F11dVH+qat7d1VXd/aYwxggAAHbBYToAAAB0DwgbAFgICBsAWAgIGwBYiKn6xpUrV3bs2MFUKAAA9JhVq1b5+fnRm3+6YpeUlGRkZBg8pL7N1atXr169ynQUeqG0tBR+D32CjIyMkpIS9RTT9pmOHz9uqHjYwJw5cxBLG+3YsWNz585lZdVYBkVRbVLgHhsAWAgIGwBYCAgbAFgICBsAWAgIGwBYiB6FfebMGUtLy19++UV/h+gWra2tO3fu9Pf3ZzoQo2uZXrJ06VLqD0JDQ9V3nT9/Pjo6+sSJExKJhGRYsGCBeobJkyeLRCITE5MhQ4bcuHHDkGHHxcX5+PiIxWI+n+/l5bV27Vq5XE7v/emnn0aNGiUSidzd3RctWlReXq6nMk+fPp2QkNDS0kJny8zMpNvT1ta2h9XDaqSnp7dJ6Q1ZWVlisfj06dO6KrA33L9/f+zYsQih4cOH67bk2bNnz549u1v/YlQtowEtfw9hYWE2NjbZ2dn37t1rbGyk02NjYwMDA2UyGdmUSqX9+vVDCGVlZan/e3Z29vTp03UbuTYEBASkpKRUV1fLZLL09HQul/vee++RXWlpaQihhISE2tra/Px8iUTi6+urUqn0VGZSUlJAQEBNTQ3J2draWlpaeunSpffff79fv37a1AUhlJ6e/qcU9Q3dClvfKBQKPz8/bXIWFBTMnDkzNTXV19fXGIStb7RvGc1oL2xnZ+c2iVu3bvX29lYqlXSKVCo9cuQIh8Nxdnaura2l05kS9rRp05qbm+nNDz74ACH09OlTjPHEiRP79+/f2tpKdn3//fcIoby8PP2VGR4e7ufn1+bcsWLFih4Luw/fY+/fv7+iokKbnMOHDz9x4sT8+fP5fL6+ozIGtG8ZPfHgwYOvv/5648aNAoFAPd3f3z8iIuLZs2erV69mKjaarKwsExMTepMMehUKBUKopKTEycmJfurD1dUVIVRcXKy/Mjds2FBQUJCUlKSDiiGE9HePnZeX5+bmRlEUOTPt3r3b3NxcKBSeOnVq6tSpYrHYxcXl6NGjCKHk5GSBQGBvb7906VInJyeBQODv73/t2jWEUHh4OI/Hc3R0JGV+/vnn5ubmFEVVVVVFRERERkY+fPiQoigvLy891UIfGL5lzp49KxaLN2/ebLA6JicnY4yDgoLa74qPj/f29t63b9/58+fb78UY79ixY/DgwXw+39raesaMGXfv3kUaWwkh1NLSEhsb6+bmZmZm9sYbb5CBRnd59uyZmZmZp6cnQkgikaifGcnNsEQi0V+Z1tbWAQEBSUlJWFcfPlG/fOt2KE4eXt21axfZjImJQQjl5OTU1dVVVFSMHz/e3Ny8qakJYxwWFmZubn7nzp3GxsbCwkIywUAGMPPnz3dwcKDLTExMRAhVVlZijGfNmiWVSrsV0ujRo41hKG7glsnKyhKJRHFxcd2tWo+H4hKJxMfHp002qVT6+PFjjPHly5c5HI6Hh4dcLsd/HorHxsbyeLzDhw/X1tbevHlzxIgRtra25eXlmltp9erVfD4/IyOjpqZm3bp1HA7n+vXr3appQ0ODSCQKDw8nm7m5uVwuNzk5WSaT3b59e/DgwVOmTOlWgT0oMzo6GiGUn59Pp/Slobi/v79YLLazswsJCWloaHj69ClJNzU1JedpHx+f3bt319fXHzx40MCxMYv+WmbatGkymezrr7/WQ9Qd0NDQ8PjxY6lU2lkGPz+/lStXPnny5KuvvlJPVyqVO3bsmDlzZmhoqKWl5bBhw3788ceqqqo9e/bQedq3UmNj4+7du4ODg2fNmmVlZbV+/Xoul9vdJtqyZYuTk1N8fDzZDAgIiIqKCg8PF4vFQ4cOra+v37dvX7cK7EGZAwYMQAjdunWruwfqEMbusXk8HkJIpVK13zVy5EihUEjGYK8hfb1lKioqMMZCoVBDnvj4+IEDB6akpOTl5dGJhYWFcrl85MiRdMqoUaN4PB65+2gD3Ur37t1TKBRDhw4l6WZmZo6Ojt1qopMnTx47duzcuXMikYikxMTE7NmzJycnRy6XP3r0yN/f38/Pr83rUzovk7TYixcvtD+KBox08ozP51dWVjIdhTFi/C3T2NiIENI8TykQCA4ePEhR1OLFi5VKJUmsra1FCFlYWKjntLKyqq+v11BUQ0MDQmj9+vX02m9xcTGZr9KGtLS0bdu25ebmenh4kJTnz58nJCR89tln77zzjrm5uaen5969e8vKysjNjv7KNDMzQ3+0Xu8xRmGrVKra2loXFxemAzE6+kTLkB+o+hMXHeLn57dq1aqioqJNmzaRFCsrK4RQGxl3WV87OzuE0M6dO9XvMK9cuaJNqLt27UpNTb1w4UL//v3pxKKiopaWFvUUsVhsY2NTWFio1zKbmprQH63Xezp4H5txcnNzMcZjxoxBCJmamnY4KH096RMtY29vT1FUXV1dlzk3bdqUlZWVn5/v5uaGEBo6dKiFhcXvv/9OZ7h27VpTU9Nbb72loRBXV1eBQFBQUNCtIDHGX331VU1NTWZmpqnpn1RAziPPnz+nU+rr61++fEkWqPRXJmkxBweHblWkM4zlit3a2lpTU9Pc3Hzz5s2IiAg3N7eFCxcihLy8vF6+fJmZmalSqSorK9XXEm1sbMrKyp48eVJfX2+cP3Gd0PuWyc7ONuRyl1AolEgkpaWlXeYkA3J64VcgEERGRp48eTI1NVUmk926dWvZsmVOTk5hYWGaC1m0aNHRo0d3794tk8laWlpKS0uJhEJCQhwcHDp8UvXOnTvffvvt3r17uVwupcb27ds9PT0nTpy4d+/eS5cuKZXKkpISEsAnn3yipzIJpMWGDRvWZbtphfoARofLXbt27SKrrEKhMCgoKCUlhcwNDBgw4OHDh3v27BGLxQghd3f3+/fvh4WFcblcZ2dnU1NTsVg8Y8aMhw8fknKqq6snTpwoEAg8PT2//PLLNWvWIIS8vLyePn1648YNd3d3MzOzcePGkRWRzrhy5crYsWOdnJxIlR0dHf39/S9evKiTmnZ3ucvwLXPmzBmRSBQfH9/dqvV4uSs8PJzL5SoUCrJ58uRJMklua2v7xRdftPn3NWvW0Mtdra2tiYmJAwYM4HK51tbWwcHB9+7dwxhrbqVXr15FRUW5ubmZmpra2dnNmjWrsLAQYxwcHIwQio2NbR9zZ5PPiYmJGGPyOICXlxefz7ewsBg7duzPP/9M/lEfZRKmTZvm7OxMP5qGe7fcZRSPlJKHjQ1/XJ2g10dKmW2ZHgu7qKjI1NT08OHDegtNK1paWsaPH79//34jLxNjXFVVJRAItm/frp7Yl9axO6PLuZbXlj7RMkql8ty5c0VFRWQGyMvLKy4uLi4uTv3dJgPT0tKSmZlZX18fEhJizGUSNmzY4OvrGx4ejhDCGJeVleXl5T148KDHBRqLsHvD3bt3qc7ReR8A7Xn58uV7773n7e29ePFikhIdHT1nzpyQkBBtZtH0QW5u7okTJ7KzszWvqDNeJkJox44dBQUFZ86c4XK5CKFTp045OzuPHz/+119/7Xmh6pdvRobi0dHR5GEDDw+P48ePG/jovUd/Q3HGW6b3v4dz585FRUXpKh5WkpmZuWXLFvV3wnoAajcUp7DaQ+fkc7MY/De7A+s/Pwy/B+OHoqj09HTyliiBDUNxAADaAMIGABYCwgYAFgLCBgAWAsIGABbSwUsg7Q2+gC5hcaOxuGospgNh9+yTUa8tO3fuRAitXLmS6UB0z5UrV5KSkuD3YPzMnTu3TUoHwlZfDQO6hKxgs7XRkpKS2Fo1NtFe2HCPDQAsBIQNACwEhA0ALASEDQAsBIQNACyk28K+evXq4MGDORwORVEODg7099D1h7oJq6OjYxujVoBxwEa3N2Ual43ulClTEEK08acBkEqllpaWBjuc9hih26auABvd185GV9/Cbm8E+3oKWyeGuD0uBGx0yS6w0dUZjBvBGgk6aQfDNybY6Ha3TGO00TUGi9x//etfPj4+lpaWAoFg2LBh586dQwgtWbKE3KhIpdL8/HyE0KJFi4RCoaWl5enTpzv0Xv3222+FQqFIJKqoqIiMjHR2dr53717vmwh1bhCrfTv0IVddsNHtbpnGYqPbZihuAItczUPx48ePb9iw4eXLl9XV1WPGjKEHMLNmzTIxMXn27Bmd88MPPzx9+jTu3HuV1GXFihW7du2aOXPmf//7X81NoeVQXINBrPbtYGBXXbDR7bs2uroUNn1DlZKSghB68OABxjgsLExdkNevX0cIbdy4EetU2Ops2bIF/eH5SC4L9Lfy6+rqBgwY0NzcrFQqhUJhSEgISVcoFHw+f/ny5e3r0iXaCFuhUFhYWNCHwxj/9ttvCCGirm4JWyeNqSU9E7ZcLqcoKjAwsE02WtgY48jISIQQMQ+gha25lTr7jWnoSu2JiYnx9vam5/kwxuvXr6cvfi4uLiUlJd0qsAdlHjhwACF06NAhOsXo7rGZNYIl33Al6wfvvPOOt7f3gQMHSOXT0tJCQkJMTEx6773aLbplEKs9xumqCza6PSuzz9vo6sMI9tdff50wYYKdnR2fz1+7di2dTlHU0qVLHz16lJOTgxA6dOgQcUvqpfdqd+mZQaw2GKGrLtjovo42uro1gr106dLOnTufPn0aHBzs6Oh47dq1urq6hIQE9TwLFy4UCAT79u27d++eWCx2d3dHvfNe7QE9M4jtEuN01QUb3dfRRle3RrD/7//9P3Nz81u3bqlUquXLl5M5xjZf/LC2tp47d25aWppIJPr0009JYs+8V3uMZoPYHreDcbrqgo3u62Kjqw+LXJVK9eLFi9zcXHNzc/KzOH/+fGNjY1FRUftbsmXLlr169SorKyswMJCkaPBe1QeaDWK71Q7G76oLNrp90kb36tWrQ4YM4XA4CCFHR8fNmzfr2wj2hx9+ICasHXLy5EmMcVRUlI2NjZWV1Zw5c8gzPVKplKwDEd58883o6Gj1inTovZqQkEDGQq6urlqaRWq53NWZQaz27VBeXm5gV12w0QUb3U4xEovc999//9GjR/oo2ZDPihu4McFG1zBl4j5qo8uUESw9hr958ya5lDEShm4xTlddsNHtJWCj2w2ioqKKioru37+/aNEieuoV0Adgo9sb+p6NLrNGsDExMRwOx9XVlTxDqicMNhQ3fGOCja4BABtdIwVsdAHGARtdAHgtAGEDAAsBYQMACwFhAwAL6eBZ8WPHjhk+jr4LeRKQlY1GXqVgZdXYj/oUOfgqAkAfRdNyF8BiyFoIXH5fE+AeGwBYCAgbAFgICBsAWAgIGwBYCAgbAFgICBsAWAgIGwBYCAgbAFgICBsAWAgIGwBYCAgbAFgICBsAWAgIGwBYCAgbAFgICBsAWAgIGwBYCAgbAFgICBsAWAgIGwBYCAgbAFgICBsAWAgIGwBYCAgbAFgICBsAWAgIGwBYCAgbAFgICBsAWAgIGwBYCAgbAFgICBsAWAgIGwBYCAgbAFgICBsAWIgp0wEA+uLSpUtXrlyhN+/evYsQSkhIoFP8/Pz+8pe/MBAZoH8ojDHTMQB6IScn569//SuXy+Vw2o7LWltbVSrV+fPnJ02axEhsgL4BYbOW1tZWR0fHysrKDvfa2tqWl5ebmJgYOCrAMMA9NmvhcDjz58/n8Xjtd/F4vNDQUFA1iwFhs5l58+Y1NTW1T29qapo3b57h4wEMBgzFWY6Hh0dxcXGbRFdX1+LiYoqiGAkJMABwxWY5CxYs4HK56ilcLnfhwoWganYDV2yWc/fu3cGDB7dJvH379pAhQxiJBzAMcMVmOYMGDRoyZIj69dnHxwdUzXpA2Ozno48+oifAuVzuxx9/zGw8gAGAoTj7KSkpcXd3Jx1NUdSjR488PDyYDgrQL3DFZj+urq6jR4/mcDgcDmf06NGg6tcBEPZrwYIFCyiK4nA4CxYsYDoWwBDAUPy1oKqqytHRESFUVlZmb2/PdDiA3tGxsGF1FAB6hm6VqPvXNiMiIvz8/HReLLPMnTu3r9fr0qVLFEWNHz++TfrOnTsRQitXrmQiKAAhhK5cuZKUlKTbMnV/xU5PT//ggw90WKYxwIJ61dfXI4REIlGb9Dlz5iCEjh8/zkBMAEIIoWPHjs2dO9fYr9iAcdJe0gCLgVlxAGAhIGwAYCEgbABgISBsAGAhTAr71atXK1ascHR0FAqFf/3rX+3t7SmK+vHHHxkMSYecOXPG0tLyl19+YToQHXP+/Pno6OgTJ05IJBKKoiiKavM02+TJk0UikYmJyZAhQ27cuGHI2OLi4nx8fMRiMZ/P9/LyWrt2rVwup/f+9NNPo0aNEolE7u7uixYtKi8v11OZp0+fTkhIaGlp0XkFuwHWKQih9PR0LTNv3rzZ29u7pqbmb3/72/Hjx4uKihBCP/zwg25D0gndqhchKytLLBafPn1aTyHpitmzZ8+ePVvLzLGxsYGBgTKZjGxKpdJ+/fohhLKystSzZWdnT58+XceBakFAQEBKSkp1dbVMJktPT+dyue+99x7ZlZaWhhBKSEiora3Nz8+XSCS+vr4qlUpPZSYlJQUEBNTU1GgTdnp6uu6VqOPiuiOAUaNGffjhh/SmlsJWKBR+fn6dbeqJHghb3+iq4toLe+vWrd7e3kqlkk6RSqVHjhzhcDjOzs61tbV0OlPCnjZtWnNzM71Jnjt4+vQpxnjixIn9+/dvbW0lu77//nuEUF5env7KDA8P9/Pz0+bcoQ9hMzkULy0tbfPVHm3Yv39/RUVFZ5uvDwau+IMHD77++uuNGzcKBAL1dH9//4iIiGfPnq1evdpgwXRGVlaW+qdXbW1tEUIKhQIhVFJS4uTkRD/y7OrqihBq/zU4HZa5YcOGgoICnT9SpiXMCPsf//iHl5fX8+fP/+d//oeiKAsLi/Z5/vWvf/n4+FhaWgoEgmHDhp07dw4hFBERERkZ+fDhQ4qivLy82mwihFpaWmJjY93c3MzMzN544w1yLty9e7e5ublQKDx16tTUqVPFYrGLi8vRo0f1V8G8vDw3NzeKoshZXEMAycnJAoHA3t5+6dKlTk5OAoHA39//2rVrCKHw8HAej0de3kAIff755+bm5hRFVVVVta/42bNnxWLx5s2b9VSj5ORkjHFQUFD7XfHx8d7e3vv27Tt//nz7vRjjHTt2DB48mM/nW1tbz5gxg3iSaO6UDvuxuzx79szMzMzT0xMhJJFI1M+D5GZYIpHor0xra+uAgICkpCTMyHtWuh0AoO4MWR0cHD7++GN6s81Q/Pjx4xs2bHj58mV1dfWYMWP69etH0mfNmiWVSun/arO5evVqPp+fkZFRU1Ozbt06Dodz/fp1jHFMTAxCKCcnp66urqKiYvz48ebm5k1NTfqoF6GkpAQhtGvXLrKpIYCwsDBzc/M7d+40NjYWFhaSyRgy2Js/f76DgwNdZmJiIkKosrKyfcWzsrJEIlFcXFy3gsRaD8UlEomPj0+bRKlU+vjxY4zx5cuXORyOh4eHXC7Hfx6Kx8bG8ni8w4cP19bW3rx5c8SIEcSrQHObdNaP2tPQ0CASicLDw8lmbm4ul8tNTk6WyWS3b98ePHjwlClTulVgD8qMjo5GCOXn52sulm332JqFrc6WLVsQQhUVFVijsJVKpVAoDAkJIZsKhYLP5y9fvhz/8Rui7w9TUlIQQg8ePNBHvQgdCrvDAMLCwiwtLel/vH79OkJo48aNuDvC7jHaCFsul1MUFRgY2CadFjbGODIyEiH0xRdfYDVhKxQKCwsLukcwxr/99htCiJyAOmsTDf2oPTExMd7e3vQ8H8Z4/fr19PXMxcWlpKSkWwX2oMwDBw4ghA4dOqS5WLbdY2sPuRXvcv3g3r17CoVi6NChZNPMzMzR0ZEM/NpA/DFUKpWuI9UWDQGMHDlSKBR2GDZTkFOqUCjUkCc+Pn7gwIEpKSl5eXl0YmFhoVwuHzlyJJ0yatQoHo9H7jXaQLeJ9v3YGSdPnjx27Ni5c+foJ+RjYmL27NmTk5Mjl8sfPXrk7+/v5+dHTr76K5O02IsXL7Q/iq4wXmH/+uuvEyZMsLOz4/P5a9eu1eZfGhoaEELr16+n/qC4uJjMc/Qt+Hx+Z55bjNDY2IgQ4vP5GvIIBIKDBw9SFLV48WKlUkkSa2trEUJt5lCsrKzIq2ad0ct+TEtL27ZtW25uLv0RqOfPnyckJHz22WfvvPOOubm5p6fn3r17y8rKyAhIf2WamZmhP1rPwBipsJ8+fRocHOzo6Hjt2rW6ujp181cN2NnZIYR27typPiZRt5LtE6hUqtraWhcXF6YD+T/ID7TLEZOfn9+qVauKioo2bdpEUqysrNAfb4zSdFm73vTjrl27UlNTL1y40L9/fzqxqKiopaVFPUUsFtvY2BQWFuq1TOKvRFrPwBjpa5u3bt1SqVTLly8nc4xafpjF1dVVIBAUFBToOTr9kpubizEeM2YMQsjU1JTB+wUa8lBgXV1dlzk3bdqUlZWVn5/v5uaGEBo6dKiFhcXvv/9OZ7h27VpTU9Nbb72loZCe9SPG+KuvvqqpqcnMzDQ1/dMPm5xHnj9/TqfU19e/fPmSLFDpr0zSYg4ODt2qiE4w0is2+VmcP3++sbGxqKhI/ZbMxsamrKzsyZMn9fX1KpVKfdPExGTRokVHjx7dvXu3TCZraWkpLS1Vb3qjpbW1taamprm5+ebNmxEREW5ubgsXLkQIeXl5vXz5MjMzU6VSVVZWqq+7tmmH7Oxs/S13CYVCiURSWlraZU4yIKcXfgUCQWRk5MmTJ1NTU2Uy2a1bt5YtW+bk5BQWFqa5kM76MSQkxMHBocMnVe/cufPtt9/u3buXy+VSamzfvt3T03PixIl79+69dOmSUqksKSkhAXzyySd6KpNAWmzYsGFdtpvu0e1cHNJu9vjJkydvvvkmQsjU1HTEiBEZGRnfffcdObGZm5vPnDkTYxwVFWVjY2NlZTVnzhyyGiyVSp8+fXrjxg13d3czM7Nx48aVl5e32Xz16lVUVJSbm5upqamdnd2sWbMKCwtTUlLINMaAAQMePny4Z88esViMEHJ3d79//74O60Wza9cusv4sFAqDgoI0BxAWFsblcp2dnU1NTcVi8YwZMx4+fEjKqa6unjhxokAg8PT0/PLLL9esWYMQ8vLyat8OZ86cEYlE8fHx2gdJ0HK5Kzw8nMvlKhQKsnny5EmpVIoQsrW1JTPh6qxZs4Ze7mptbU1MTBwwYACXy7W2tg4ODr537x7GWHObdNiPGOPg4GCEUGxsbPsIb9261eEvPDExEWNMFv+9vLz4fL6FhcXYsWN//vln8o/6KJMwbdo0Z2dn+tG0zmDbclcfQq/1CgsLs7Gx0VPhXaKlsIuKikxNTQ8fPmyAkDTQ0tIyfvz4/fv3G3mZGOOqqiqBQLB9+/Yuc76+y12sh+E3gbTAy8srLi4uLi5O/d0mA9PS0pKZmVlfXx8SEmLMZRI2bNjg6+sbHh6u22K1BIQNaEt0dPScOXNCQkK0mUXTB7m5uSdOnMjOzta8os54mQihHTt2FBQUnDlzpgdvQ+gEEDbDrFu37uDBg3V1dZ6enhkZGUyH0wWbN28ODw/funUrI0efNGnSkSNH6IfnjbbMU6dOvXr1Kjc319raWofFdgsjXe56fdiyZQt5YLavMHny5MmTJzMdhVEzffr06dOnMxsDXLEBgIWAsAGAhYCwAYCFgLABgIXofvKsz710oSVsrRd57PHYsWNMB/L6oo+fFtjoAoBRoFsl6v6K3dddKTuEBW6bnQFum4xD3DZ1WybcYwMACwFhAwALAWEDAAsBYQMACwFhAwALAWEDAAsxtLDV7Vfb4OHhsX37dpaZ6b5WGLPD7oQJE9r/5OjvImvjsNvY2Dho0CDiEGAURrkaMbSwZ82a9ejRI6lUSntfNDc3KxSKFy9eCIXC1atXX7582cAhATrhm2++SU5OXrduHd3F/fr1S01N/fXXX+k8f//7348fPx4YGFhYWDhixAgGoyWMGzcOIZSenj5//vw5c+aUlpaeOnXq0qVLU6dObW5ubpM5Jibm3r175O+goCCBQDBp0iTy4XQjhPmhuImJiZmZmb29vbe3t/b/pVQq/f39O9vsQ+gkcsarv23btrS0tGPHjtEuGQih5ORkDocTFhbG1BdX1BEIBOrWPBjjsLAwYkTxt7/9rX///mvWrLG0tPT19V21alVBQUEbr5LLly/fvn1bPWXFihXDhw9///33258CjAHmhU2TmZmpfWbWmOnqJHJmq98nHHbPnj2rftIpKSm5ffv2O++8g7Rw2FUqlWvWrGlviMusUa5mjEjYHdKHzHRxJ36x2rvh9hVL3Tb0RYfdbdu2rVixgvzdpcNuTEzM559/TvxJ1GHYKFczuv3oKdLuM73q99gY45ycHPKhZmysZrra1EuDX6z2ppmGt9TV8vPDGuhzDrulpaU+Pj4tLS1kU7Mbbl5eXlBQEMaYuKnFxMSoF6WlUa5mWPX54bq6OnpyctKkSZ1lmz179jfffGNtbW1jYxMUFFRdXd2lW11jY+Pu3buDg4NnzZplZWW1fv16Lpd78OBBOoO/v79YLLazswsJCWloaHj69Gnvq6NUKnfs2DFz5szQ0FBLS8thw4b9+OOPVVVVe/bs6W5Rpqam5CLm4+Oze/fu+vp69eC1Ydq0aTKZ7Ouvv+7uoXtAQ0PD48ePiXlAh/j5+a1cufLJkydfffWVero2Lda+p7rsXG3Ytm3bl19+yeH8748/ICAgKioqPDxcLBYPHTq0vr5+3759dJARERG7d9DnDUcAACAASURBVO/urKgBAwYghDrzFWAQxoStfsX+5z//qc2/GLOZbrf8YrXHCC1129DnHHbLyspOnz5NHJQIGtxw161b99lnnzk7O3dWGoNGuZoxinvsCRMmdDa/0lfMdHvmF6sNxmap24a+5bCLEEpISPj000/peT4Nbrh5eXm3bt1asmSJhtIYNMrVjFEIuzP6kJluz/xiu8QILXXb0IccdhFC5eXlP/300/Lly+kUDW64+/fvz8nJ4XA45AxCDr1582aKomj/UAaNcjVj1MJWN9MVCATGbKar2S+2x264Rmip24ZuOewOGjQoPz+fbBrSYZcmISEhNDTUxsaGTtHghnvw4EH104f65Bl9B8GgUa5mjFrYfchMV7NfrPZuuMjoLXXb0CccdgkvXrw4cODAypUr1RO1ccPVAJNGuZrR7SQ76mpZ6N///jf9hJmjo+OkSZPU9xqtmW6X9cKd+8Xi7rjhGt5St/fLXcbvsEtYtWpVaGho+/Qu3XAJHS53aWmUqxmw0WUMg9XL8Ja6vRc2ix12NaO9Ua5mWLWODXSGMb8z1CFsddjtEmaNcjUDwgZ0ACsddjXDuFGuZkDYRkTfstRtA/scdjVgDEa5mgEbXSOiz1nqtuH1cdg1BqNczcAVGwBYCAgbAFgICBsAWAgIGwBYiO7dNseMGWPMLy30jIyMDFbWCyF09epVhBB5HB1ghNLS0qtXr+pYibotjlg3AkYI+RiAMT7VDCCEdG14qmNhA0YL8QAGg/vXBLjHBgAWAsIGABYCwgYAFgLCBgAWAsIGABYCwgYAFgLCBgAWAsIGABYCwgYAFgLCBgAWAsIGABYCwgYAFgLCBgAWAsIGABYCwgYAFgLCBgAWAsIGABYCwgYAFgLCBgAWAsIGABYCwgYAFgLCBgAWAsIGABYCwgYAFgLCBgAWAsIGABYCwgYAFgLCBgAWAsIGABYCwgYAFgLCBgAWAsIGABYCwgYAFkJhjJmOAdALhw4d2rFjR0tLC9msqqpCCNna2pJNExOTVatWffTRR4zFB+gTEDZruX///sCBAzVkuHfvnre3t8HiAQwJDMVZi7e39/DhwymKar+Loqjhw4eDqlkMCJvNfPTRRyYmJu3TTU1NP/74Y8PHAxgMGIqzmbKyMldX19bW1jbpFEWVlJQ4OzszEhVgAOCKzWb69+/v7+/P4fyplzkcztixY0HV7AaEzXIWLFjQJoWiKJgMZz0wFGc5NTU1Dg4OKpWKTjE1NS0vL+/Xrx+DUQH6Bq7YLMfa2vrdd9+lp9BMTEymTJkCqmY9IGz2ExoaSs+fYYxDQ0OZjQcwADAUZz8KhaJfv36NjY0IIYFAUFVVZW5uznRQgH6BKzb7EQqFwcHBXC6Xy+UGBweDql8HQNivBR9++KFKpVKpVB9++CHTsQCGwNTwhzx27JjhD/qa09LSIhQKMcYymQza3/B88MEHBj4iA/fYHT69DAAsxvAqY2Yonp6ejvs46enpCCGmo+gGubm5Fy9e1DIzO/rIGCC/E8PDwFAcYITx48czHQJgOEDYrwttnhgH2A10NgCwEBA2ALAQEDYAsBAQNgCwkD4j7CVLlohEIoqiCgoKmI6l55w5c8bS0vKXX35hOhBdcv78+ejo6BMnTkgkEoqiKIpq8xL45MmTRSKRiYnJkCFDbty4YcjYJkyYQLXDwsKC7P3pp59GjRolEonc3d0XLVpUXl7evoTGxsZBgwatX78eIXT69OmEhAT6w6/GTJ8R9r59+/bu3ct0FL0Fs+6Vm2+++SY5OXndunWzZs169OiRVCrt169famrqr7/+Suf5+9//fvz48cDAwMLCwhEjRjAYLWHcuHEIofT09Pnz58+ZM6e0tPTUqVOXLl2aOnVqc3Nzm8wxMTH37t0jfwcFBQkEgkmTJtXW1ho66G7SZ4TNDqZNm1ZXVxcYGKin8pVKpb+/v54Kb8+2bdvS0tKOHTsmEonoxOTkZA6HExYWVldXZ7BIOkMgEMhkMvUnRsLCwtauXYsQ+tvf/ta/f/81a9ZYWlr6+vquWrWqoKDg2rVr6v9++fLl27dvq6esWLFi+PDh77//fvtTgFHRl4QNz6J2yf79+ysqKgxzrAcPHnz99dcbN24UCATq6f7+/hEREc+ePVu9erVhItHA2bNn1U86JSUlt2/ffuedd8jfTk5O9I/K1dUVIVRcXExnViqVa9asSUpKalPmhg0bCgoK2qcbFUYtbIxxYmLiwIED+Xy+paXlmjVr6F0tLS2xsbFubm5mZmZvvPEGeXBv9+7d5ubmQqHw1KlTU6dOFYvFLi4uR48eJf9y8eLFt99+WygUisXiYcOGyWSyzsrRE3l5eW5ubhRFff/995qjTU5OFggE9vb2S5cudXJyEggE/v7+5GISHh7O4/EcHR1JmZ9//rm5uTlFUVVVVREREZGRkQ8fPqQoysvLCyF09uxZsVi8efNmfVQnOTkZYxwUFNR+V3x8vLe39759+86fP99+L8Z4x44dgwcP5vP51tbWM2bMuHv3ruYGQTrqqW3btq1YsYL8LZFI1E+C5AZbIpHQKTExMZ9//rmdnV2bQqytrQMCApKSkoz6xsrwT88irZ9DjomJoSjqu+++q6mpUSgUKSkpCKH8/HyM8erVq/l8fkZGRk1Nzbp16zgczvXr18m/IIRycnLq6uoqKirGjx9vbm7e1NQkl8vFYnFCQoJSqSwvL585c2ZlZaWGcrqkZ8+Kl5SUIIR27dpFV7DDaDHGYWFh5ubmd+7caWxsLCwsJHM8T58+xRjPnz/fwcGBLjMxMREhRKoza9YsqVRK78rKyhKJRHFxcd2NU5s+kkgkPj4+bRKlUunjx48xxpcvX+ZwOB4eHnK5HGOcnZ09ffp0kic2NpbH4x0+fLi2tvbmzZsjRoywtbUtLy/X3CA97ima0tJSHx+flpYWspmbm8vlcpOTk2Uy2e3btwcPHjxlyhQ6c15eXlBQEMa4srISIRQTE6NeVHR0NP1T1AxT7xQYr7AVCoVQKHz33XfpFHLyzs/PVyqVQqEwJCSEzsnn85cvX47/+GUolUqyi5wLHjx4QO6UsrKy1A+hoZwu0aGw20eLMQ4LC7O0tKT/8fr16wihjRs34u4Iu8d02UdyuZyiqMDAwDbptLAxxpGRkQihL774AqsJW6FQWFhY0G2OMf7tt98QQuTs01mD9KanaL744osffvhBPYXMdRNcXFxKSkro8keOHFlaWoo7EfaBAwcQQocOHeryoEwJ23iH4g8ePFAoFJMmTWq/6969ewqFYujQoWTTzMzM0dGRDOfawOPxEEIqlUoikdjb24eGhm7YsOHJkyfdLccw0NG23zVy5EihUMhgbG2oqKjAGAuFQg154uPjBw4cmJKSkpeXRycWFhbK5fKRI0fSKaNGjeLxeG1mrQh0g/S+p8rKyk6fPr1w4UI6JSYmZs+ePTk5OXK5/NGjR/7+/n5+fuTMu27dus8++0zDp9dJxV+8eKF9AAbGeIVdWlqKEGp/h4MQamhoQAitX7+eXpksLi5WKBQaSjMzM7tw4cK4ceM2b94skUhCQkKUSmUPymEQPp9Prh7GAPmCGp/P15BHIBAcPHiQoqjFixcrlUqSSBaK6JVkgpWVVX19vYaiet9TCQkJn376KT3P9/z584SEhM8+++ydd94xNzf39PTcu3dvWVlZYmJiXl7erVu3lixZoqE0MzMz9EcjGCfGK2zSB69evWq/i6h9586d6mOPK1euaC5wyJAhv/zyS1lZWVRUVHp6+vbt23tWDiOoVKra2loXFxemA/lfyC+7y0c1/Pz8Vq1aVVRUtGnTJpJiZWWFEGoj4y6r1sueKi8v/+mnn5YvX06nFBUVtbS09O/fn04Ri8U2NjaFhYX79+/PycnhcDjkDEIOvXnzZoqifv/9d5K5qamJbgTjxHiFPXToUA6Hc/Hixfa7XF1dBQJBtx5BKysru3PnDkLIzs5u69atI0aMuHPnTg/KYYrc3FyM8ZgxYxBCpqamHQ7XDYm9vT1FUdqsVG/atGnQoEH5+flkc+jQoRYWFrRCEELXrl1ramp66623NBTSy55KSEgIDQ21sbGhU8h55Pnz53RKfX39y5cvXV1dDx48qH76UL/Hpu8gSMUdHBx6Fo8BMF5h29nZzZ49OyMjY//+/TKZ7ObNm3v27CG7BALBokWLjh49unv3bplM1tLSUlpaqt5J7SkrK1u6dOndu3ebmpry8/OLi4vHjBnTg3IMSWtra01NTXNz882bNyMiItzc3MgtopeX18uXLzMzM1UqVWVlpfrSq42NTVlZ2ZMnT+rr61UqVXZ2tp6Wu4RCoUQiIbdLmiEDctqxQCAQREZGnjx5MjU1VSaT3bp1a9myZU5OTmFhYZoL6aynQkJCHBwcNDyp+uLFiwMHDqxcuVI90dPTc+LEiXv37r106ZJSqSwpKSEBfPLJJ9pUn1R82LBh2mRmBv3MyWkCab3cVV9f/+mnn/br18/CwmLcuHGxsbEIIRcXl//85z+vXr2Kiopyc3MzNTW1s7ObNWtWYWFhSkoKmdUYMGDAw4cP9+zZIxaLEULu7u7/+Mc//P39ra2tTUxM+vfvHxMT09zcjDHusBxtYuvBbOeuXbvI+rNQKAwKCtIQ7f3798PCwrhcrrOzs6mpqVgsnjFjxsOHD0k51dXVEydOFAgEnp6eX375JVne9/Lyevr06Y0bN9zd3c3MzMaNG1deXn7mzBmRSBQfH9+tOLF2fRQeHs7lchUKBdk8efKkVCpFCNna2pKZcHXWrFlDL3e1trYmJiYOGDCAy+VaW1sHBwffu3cPY6y5QTrrqeDgYIRQbGxsZ3GuWrUqNDS0fTpZ+ffy8uLz+RYWFmPHjv3555/bZ+twVnzatGnOzs6tra2amwjDclefQ98dFhYWZmNjo7/yNaNNHxUVFZmamh4+fNgwIXVGS0vL+PHj9+/fb7AjVlVVCQSC7du3a5MZlruAthj5W0ReXl5xcXFxcXFyuZypGFpaWjIzM+vr60NCQgx20A0bNvj6+oaHhxvsiD0AhA30nOjo6Dlz5oSEhDD1vkdubu6JEyeys7M1r6jrkB07dhQUFJw5c4bL5RrmiD0DhG2MrFu37uDBg3V1dZ6enhkZGUyHo4nNmzeHh4dv3bqVkaNPmjTpyJEj9JPz+ubUqVOvXr3Kzc21trY2zBF7DHyl1BjZsmXLli1bmI5CWyZPnjx58mSmozAE06dPnz59OtNRaAVcsQGAhYCwAYCFgLABgIWAsAGAhTDjtjlmzBjjeZ+hZ5SWll69enX27NlMB6IXMjIyWNBHxgD5nRheZXDFBgA2YviH3RA8Umr0sKOPjAF4pBQAAJ0BwgYAFgLCBgAWAsIGABYCwgYAFmKMwlb3bSTweDx7e/sJEyYkJibW1NQwHSDwJ4zZbTMuLs7Hx0csFvP5fC8vr7Vr19Jvj8fHx7dx4aQ/b4wQUqlUW7Zs8fLy4vF4VlZWQ4cOffLkSR9y2zTe5S6pVEq+mE8+/fXPf/5z4cKFFEU5OTl11wJCH8ByFyE2NjYwMJA2viNum6idN4O6E4ghCQgISElJqa6ulslk6enpXC73vffeI7voD6fSDBkyhP7H4ODggQMHXr16VaVSlZWVBQUF3bp1C2OclJQUEBBQU1OjZQDwaaS20MJW5/jx4xwOx97evra2Vj/RaYteO0yhUPj5+TFYiJZ9tHXrVm9vb9q4A2MslUqPHDnC4XCcnZ3V+4gpYU+bNo183I7wwQcfIISIU9KmTZs6+67T0aNHKYq6efNmh3vDw8P9/PxUKpU2AcA6tlbMnj174cKFFRUVP/74I9Ox6BGdmGbq23mzT7htZmVl0R9IRQjZ2toihLp0Gvjhhx9GjBjR2UdIwW1TL5BP8GZnZ6O+4LmJO3GW1N4002idN/ui2+azZ8/MzMw8PT015Glqarp69aqvr29nGcBts2NQL4biGGMiRVdXV8yo56aWQywNzpLae+sZ3nlTmz7qc26bDQ0NIpEoPDycbG7atMnFxcXKyorL5Xp4eEyfPv23337DGD9+/Bgh5OvrO2HCBEdHRz6fP2jQoO+//179Y8PgttnRIXsnbIwxRVFWVlbMem5q02GanSW7JWwDO2922Ud90W0zJibG29ubnucjn2Gvr69/9erVlStX3nzzTTMzs9u3b9+6dQsh9O677/773/+urq6ura396quvEEKpqal0UeC2qXsaGhowxmKx2Pg9N7vlLKk9xuC82efcNk+ePHns2LFz586JRCKS4urq+uabb1pYWPB4vDFjxhw8eFCpVKakpBCnwSFDhvj7+9vY2FhaWm7cuNHS0pI2okHgtqkP7t+/jxAaNGiQ8Xtu9sxZUhsYd97sW26baWlp27Zty83N9fDw6CzPsGHDTExM7t+/7+TkhBCqqqqid/F4PHd394cPH9Ip4Lape86ePYsQmjp1qvF7bvbMWbJLjMF5sw+5be7atSs1NfXChQvq3prtaW1tbW1tJXY/AwYMIBaONM3NzZaWlvQmuG3qmPLy8p07d7q4uCxevNj4PTc1O0v22DTTGJw3+4TbJsY4Kirq1q1bmZmZbcYICKEpU6aob5KpOD8/P4TQ3Llz8/PzHz16RHYpFIri4mL11S9w2+wVGGO5XE5mIysrK9PT08eOHWtiYpKZmSkWi43fc1Ozs6T2ppnI+Jw3+4Tb5p07d7799tu9e/dyuVz1R0e3b9+OEHr27FlaWlptba1Kpbpy5cqSJUvc3NyWLVuGEFq1apW7u/vChQufPn1aXV0dFRWlVCrJFBoB3DY7AHU143r69Ok33nhDKBTyeDwOh4MQItPgb7/9dlxcXHV1NZ2TQc9NLWc7O3OWxN0xzTS882aXfYT7gtsmmd9uT2JiIsY4MjJSKpWam5ubmpq6uLh8+umnZWVl9P+WlJTMmzfP2tqaz+e//fbb2dnZ6iWD22ZHh2TFZ3cM2WGGd97Upo/AbVObzLDcBWjCCN8oArdNgx2xB4CwgZ4DbptGCwjb2DFy501w2zROwG3T2DF+501w2zRC4IoNACwEhA0ALASEDQAsBIQNACwEhA0ALIQZG10DHxEAmMXwKmNguUuHXxQDtGfnzp0IoZUrVzIdCGAIGLhiA4xAvrx77NgxpgMBDAHcYwMACwFhAwALAWEDAAsBYQMACwFhAwALAWEDAAsBYQMACwFhAwALAWEDAAsBYQMACwFhAwALAWEDAAsBYQMACwFhAwALAWEDAAsBYQMACwFhAwALAWEDAAsBYQMACwFhAwALAWEDAAsBYQMACwFhAwALAWEDAAsBYQMACwFhAwALAWEDAAsBYQMACwFhAwALAWEDAAsBYQMACwFhAwALMWU6AEBfVFVVyWQyerOhoQEh9OjRIzpFLBbb2toyEBmgfyiMMdMxAHrh4MGDixcv1pDhwIEDixYtMlg8gCEBYbOWuro6Ozs7lUrV4V4ul1tZWWlpaWngqADDAPfYrMXS0vL99983Ne3gbsvU1HTatGmgahYDwmYzoaGhLS0t7dNbW1tDQ0MNHw9gMGAozmYaGxttbW3JtJk6QqGwqqrKzMyMkagAAwBXbDYjEAhmzpzJ5XLVE7lc7uzZs0HV7AaEzXI+/PDDNvNnKpXqww8/ZCoewDDAUJzlNDc3Ozg4vHz5kk6xsrKqrKzscFINYA1wxWY5pqam8+bNo0fjXC43NDQUVM16QNjsZ968efRoXKVSzZs3j9l4AAMAQ3H2gzF2dXV99uwZQsjJyenZs2cURTEdFKBf4IrNfiiKWrBgAY/H4/F4H3/8Maj6dQCu2K8FN2/eHD58OPlj2LBhTIcD6B09TqJcuXJlx44d+isf6BYWFhYIobi4OKYDAf6XVatW+fn56alwPQ7FS0pKMjIy9Fe+cZKRkVFaWsp0FB3g7u7u4eHRmxKuXr169epVHYXzupORkVFSUqK/8vW+7HH8+HF9H8KooChq5cqVH3zwAdOBtIW8iS2RSHpcwpw5c9Dr16F6Qt8zHbCe+brQG0kDfQ6YFQcAFgLCBgAWAsIGABYCwgYAFmJcwl6yZIlIJKIoqqCggOlYEEKotbV1586d/v7+ej3KmTNnLC0tf/nlF70exfCcP38+Ojr6xIkTEomEoijyAJx6hsmTJ4tEIhMTkyFDhty4ccOQscXFxfn4+IjFYj6f7+XltXbtWrlcTnbFx8dTf2bo0KH0P6pUqi1btnh5efF4PCsrq6FDhz558uT06dMJCQkdfqyGKYxL2Pv27du7dy/TUfwvRUVFf/nLX1atWqVQKPR6IFY+/PfNN98kJyevW7du1qxZjx49kkql/fr1S01N/fXXX+k8f//7348fPx4YGFhYWDhixAhDhnfhwoUvvvjiyZMnVVVVW7ZsSUpKIot5XTJ37txDhw4dOXJEoVD897//lUqlcrk8KChIIBBMmjSptrZW35FriXEJ23j4z3/+89VXXy1btszX11ffx5o2bVpdXV1gYKCeylcqlfoedLRh27ZtaWlpx44dE4lEdGJycjKHwwkLC6urqzNkMB1iYWERFhZmY2MjEok++OCD4ODgs2fP0k+MHD58GKtx+/Ztkp6WlpaZmXn8+PHRo0ebmpo6OTmdOnWKXM9XrFgxfPjw999/v7m5mbFaqWF0wjaSVxSGDx9+4sSJ+fPn8/l8pmPpLfv376+oqDDY4R48ePD1119v3LhRIBCop/v7+0dERDx79mz16tUGC6YzsrKyTExM6E1inNDl0OyHH34YMWJEZw/bb9iwoaCgICkpSYdx9hjmhY0xTkxMHDhwIJ/Pt7S0XLNmDb2rpaUlNjbWzc3NzMzsjTfeSE9PRwjt3r3b3NxcKBSeOnVq6tSpYrHYxcXl6NGj5F8uXrz49ttvC4VCsVg8bNgwYoXRYTlGQl5enpubG0VR33//PdJYu+TkZIFAYG9vv3TpUicnJ4FA4O/vf+3aNYRQeHg4j8dzdHQkZX7++efm5uYURVVVVUVERERGRj58+JCiKC8vL4TQ2bNnxWLx5s2b9VSj5ORkjHFQUFD7XfHx8d7e3vv27Tt//nz7vRjjHTt2DB48mM/nW1tbz5gx4+7du5rbBOmoc589e2ZmZubp6akhT1NT09WrVzWM4KytrQMCApKSkozi3grrDdLEXWaLiYmhKOq7776rqalRKBQpKSkIofz8fIzx6tWr+Xx+RkZGTU3NunXrOBzO9evXyb8ghHJycurq6ioqKsaPH29ubt7U1CSXy8VicUJCglKpLC8vnzlzZmVlpYZytGH06NHDhw/XvtYIofT0dO3zY4zJCHDXrl10g3RYO4xxWFiYubn5nTt3GhsbCwsLR40aJRKJnj59ijGeP3++g4MDXWZiYiJCiFR/1qxZUqmU3pWVlSUSieLi4roVJMZ49uzZs2fP7jKbRCLx8fFpkyiVSh8/fowxvnz5MofD8fDwkMvlGOPs7Ozp06eTPLGxsTwe7/Dhw7W1tTdv3hwxYoStrW15ebnmNulN5xIaGhpEIlF4eDjZ3LRpk4uLi5WVFZfL9fDwmD59+m+//YYxfvz4MULI19d3woQJjo6OfD5/0KBB33//fWtrK11UdHQ0/evVTA9+J92CYWErFAqhUPjuu+/SKeRMnJ+fr1QqhUJhSEgInZPP5y9fvhz/0c1KpZLsIueCBw8ekHuhrKws9UNoKEcbmBJ2+9phjMPCwiwtLel/vH79OkJo48aNuDvC7jHaCFsul1MUFRgY2CadFjbGODIyEiH0xRdfYDVhKxQKCwsLupswxr/99htCiJyAOmuTXnYuISYmxtvbWyaTkc2nT5/euHGjvr7+1atXV65cefPNN83MzG7fvn3r1i2E0Lvvvvvvf/+7urq6trb2q6++QgilpqbSRR04cAAhdOjQoS4Pqm9hMzwUf/DggUKhmDRpUvtd9+7dUygU9EqDmZmZo6MjGZu1gcfjIYRUKpVEIrG3tw8NDd2wYcOTJ0+6W45xQteu/a6RI0cKhUKjqktFRQXGWCgUasgTHx8/cODAlJSUvLw8OrGwsFAul48cOZJOGTVqFI/HI/cabaDbpPede/LkyWPHjp07d46e53N1dX3zzTctLCx4PN6YMWMOHjyoVCpTUlLIbMuQIUP8/f1tbGwsLS03btxoaWm5Z88eujRS8RcvXmgfgJ5gWNjkDUc7O7v2u8hn7tevX08vJxYXF2ue3jAzM7tw4cK4ceM2b94skUhCQkKUSmUPyulD8Pn8yspKpqP4PxobGxFCmmccBQLBwYMHKYpavHixUqkkiWShiLw0TmNlZVVfX6+hqF52blpa2rZt23JzczW80Dps2DATE5P79+87OTkhhKqqquhdPB7P3d394cOHdAr5WjtpBGZhWNhk4vTVq1ftdxG179y5U32AceXKFc0FDhky5JdffikrK4uKikpPT9++fXvPyukTqFSq2tpaFxcXpgP5P8gvu8tHNfz8/FatWlVUVLRp0yaSYmVlhRBqI+Mua9ebzt21a1dqauqFCxf69++vIVtra2trayufz7ewsBgwYMCdO3fU9zY3N6tboDU1NaE/GoFZGBb20KFDORzOxYsX2+9ydXUVCATdegStrKyMtLudnd3WrVtHjBhx586dHpTTV8jNzcUYjxkzBiFkamrambGmIbG3t6coSpuV6k2bNg0aNCg/P59sDh061MLC4vfff6czXLt2ramp6a233tJQSM86F2McFRV169atzMzMNmMEhNCUKVPUN8lUHPnUydy5c/Pz82mPcYVCUVxcrL76RSru4ODQrXj0AcPCtrOzmz17dkZGxv79+2Uy2c2bN+k7FoFAsGjRoqNHj+7evVsmk7W0tJSWlj5//lxDaWVlZUuXLr17925TU1N+fn5xcfGYMWN6UI4x09raWlNT09zcfPPmzYiICDc3t4ULFyKEvLy8Xr58mZmZqVKpV0KXoQAAGqxJREFUKisri4uL6X+xsbEpKyt78uRJfX29SqXKzs7W33KXUCiUSCTafEOGDMjpxWSBQBAZGXny5MnU1FSZTHbr1q1ly5Y5OTmFhYVpLqSzzg0JCXFwcOjwSdU7d+58++23e/fu5XK56o+Obt++HSH07NmztLS02tpalUp15cqVJUuWuLm5LVu2DCG0atUqd3f3hQsXPn36tLq6OioqSqlUkik0Aqm4UXxVTn/zcloud9XX13/66af9+vWzsLAYN25cbGwsQsjFxeU///nPq1evoqKi3NzcTE1N7ezsZs2aVVhYmJKSQqYoBgwY8PDhwz179ojFYoSQu7v7P/7xD39/f2traxMTk/79+8fExDQ3N2OMOyxHc1RXrlwZO3Ysua1CCDk6Ovr7+1+8eLHL6qBuznbu2rWLrD8LhcKgoCANtbt//35YWBiXy3V2djY1NRWLxTNmzHj48CEpp7q6euLEiQKBwNPT88svvySPA3h5eZE5Xnd3dzMzs3HjxpWXl585c0YkEsXHx2sfJEHL5a7w8HAul6tQKMjmyZMnpVIpQsjW1pbMhKuzZs0aermrtbU1MTFxwIABXC7X2to6ODj43r17GGPNbdJZ5wYHByOEYmNj20dI5rfbk5iYiDGOjIyUSqXm5uampqYuLi6ffvppWVkZ/b8lJSXz5s2ztrbm8/lvv/12dna2esnTpk1zdnZWXwDrjO7+TroL88JmGXrtMPIUpJ4K7xIthV1UVGRqatrmqUzD09LSMn78+P379xvsiFVVVQKBYPv27dpk1rewmX/yDOgWRvUKUYd4eXnFxcXFxcXR70sZnpaWlszMzPr6+pCQEIMddMOGDb6+vuHh4QY7ogZeU2HfvXuX6hxD/hpYSXR09Jw5c0JCQph63yM3N/fEiRPZ2dmaV9R1yI4dOwoKCs6cOdPGtJgpXlNhDxo0SMMwJi0tjekAO2DdunUHDx6sq6vz9PQ0/u86b968OTw8fOvWrYwcfdKkSUeOHKEfntc3p06devXqVW5urrW1tWGO2CXwldI+w5YtW7Zs2cJ0FN1g8uTJkydPZjoKQzB9+vTp06czHcWfeE2v2ADAbkDYAMBCQNgAwEJA2ADAQkDYAMBC9D4rbiTfMDMkc+fOnTt3LtNR6IvXsEP7InoXtlF9YMwAzJ07NyIiQn++xwyyc+dOhNDKlSuZDoQN6PvUr3dhG6GhrF6ZO3eun58fK2tNDHRZWTXDo29hwz02ALAQEDYAsBAQNgCwEBA2ALAQEDYAsBBjFLa68SqBx+PZ29tPmDAhMTGxpqaG6QABrTBmG11ChzbJGhx2EUI//fQTMWBxd3dftGhReXk5QsgIbXSN99NIUqmUuF6Qz/f985//XLhwIUVRTk5O3fVwMSRIz5+8YRAtP41EiI2NDQwMpO01iI0uaufTom7xY2Du378/duxYhFAbp5eAgICUlJTq6mqZTJaens7lct977z2yi7yon5CQUFtbm5+fL5FIfH19VSoVxjgpKSkgIKCmpkbLo+v7d9IHhK3O8ePHORyOvb19bW1t76LTF/rrMIVC4efnx2Ah2gt769at3t7etCMPxlgqlR45coTD4Tg7O6v3HVPCLigomDlzZmpqqq+vbxthT5s2jXwDk0DW7YlB2sSJE/v3709/q5D4KObl5ZHN8PBwPz8/ovMu0bewjXEoroHZs2cvXLiwoqLixx9/ZDoWQ6MTN1wDWOr2CRtdDTbJGhx2S0pKnJyc6IdqXV1dEUL0l57BRrdXkM9oZ2dnoz7rs4s78YvV3g3XmC11+6KNrgbUHXYlEon6aZHcYEskErIJNrpa0eFQHGNMpOjq6oqNw2e3DUiLIZYGv1jtTTMNb6nLShtdzW6qbRx2c3NzuVxucnKyTCa7ffv24MGDp0yZop4fbHS7pjNhY4wpirKysjISn902dNlhmv1iuyVsA1vqstJGV7Ow2zjsYozXr19PXxRdXFxKSkrU84ONbs9paGjAGIvF4j7qs9stv1jtMRJL3T5no6uB9g67MTExe/bsycnJkcvljx498vf39/PzI/bmBLDR7Tn3799HCA0aNKiP+uz2zC9WG4zBUrdv2ehqoL3D7vPnzxMSEj777LN33nnH3Nzc09Nz7969ZWVlZEBEABvdnnP27FmE0NSpU/uoz27P/GK7xEgsdfuQja4GOnTYLSoqamlpUU8Ri8U2NjaFhYV0Ctjo9pDy8vKdO3e6uLgsXry4j/rsavaL7bEbrpFY6vYJG10N4M4ddskpRt2ntb6+/uXLl2TRiwA2ulqBMZbL5eR5gMrKyvT09LFjx5qYmGRmZorF4j7qs6vZL1Z7N1xklJa6fcJGVwMaHHY9PT0nTpy4d+/eS5cuKZXKkpISEtsnn3xC/zvY6Gri9OnTb7zxhlAo5PF4HA4HIUSmwd9+++24uLjq6mo6J4M+u52BtJjt7MwvFnfHDdfwlrqssdHFndska3bYJc8CeHl58fl8CwuLsWPH/vzzz+rFgo0ua9F3h9EY3lIXbHQ1Aza6gG4wrteJ/gBsdA12RA2AsAHdAza6jAPC7pMYv6Uu2OgyC9jo9kn6hKUu2OgyCFyxAYCFgLABgIWAsAGAhYCwAYCF6H3y7NixY/o+hLGhvxdImIU8L/kadmifRH/PvrxuPpsA0C30+uQZhY3h+0yA/iFf24Tr7WsC3GMDAAsBYQMACwFhAwALAWEDAAsBYQMACwFhAwALAWEDAAsBYQMACwFhAwALAWEDAAsBYQMACwFhAwALAWEDAAsBYQMACwFhAwALAWEDAAsBYQMACwFhAwALAWEDAAsBYQMACwFhAwALAWEDAAsBYQMACwFhAwALAWEDAAsBYQMACwFhAwALAWEDAAsBYQMACwFhAwALAWEDAAsBYQMACwFhAwALMWU6AEBfXLp06cqVK/Tm3bt3EUIJCQl0ip+f31/+8hcGIgP0D4UxZjoGQC/k5OT89a9/5XK5HE7bcVlra6tKpTp//vykSZMYiQ3QNyBs1tLa2uro6FhZWdnhXltb2/LychMTEwNHBRgGuMdmLRwOZ/78+Twer/0uHo8XGhoKqmYxIGw2M2/evKampvbpTU1N8+bNM3w8gMGAoTjL8fDwKC4ubpPo6upaXFxMURQjIQEGAK7YLGfBggVcLlc9hcvlLly4EFTNbuCKzXLu3r07ePDgNom3b98eMmQII/EAhgGu2Cxn0KBBQ4YMUb8++/j4gKpZDwib/Xz00Uf0BDiXy/3444+ZjQcwADAUZz8lJSXu7u6koymKevTokYeHB9NBAfoFrtjsx9XVdfTo0RwOh8PhjB49GlT9OgDCfi1YsGABRVEcDmfBggVMxwIYAhiKvxZUVVU5OjoihMrKyuzt7ZkOB9A/WG+kp6czXTkAMF7S09P1pz69v7b5usl77ty5ERERfn5+TAfSlkuXLlEUNX78+B6XsHPnToTQypUrdRfU68vcuXP1Wr7ehf3BBx/o+xBGxdy5c/38/Iyw1lOnTkUIiUSiHpdw/Phx9Pp1qJ7o88IGjITeSBroc8CsOACwEBA2ALAQEDYAsBAQNgCwEOMS9pIlS0QiEUVRBQUFzEYSFxfn4+MjFov5fL6Xl9fatWvlcrmejnXmzBlLS8tffvlFT+Uzxfnz56Ojo0+cOCGRSCiKoiiqzXNvkydPFolEJiYmQ4YMuXHjhuEjbG1t3blzp7+/v3qi5q7/6aefRo0aJRKJ3N3dFy1aVF5ejhA6ffp0QkJCS0uLoSugAf0tkZMV7O7+19GjRxFC+fn5+ghJewICAlJSUqqrq2UyWXp6OpfLfe+997T5R9T9Bw+ysrLEYvHp06d7FKnhmD179uzZs7XMHBsbGxgYKJPJyKZUKu3Xrx9CKCsrSz1bdnb29OnTdRyodty/f3/s2LEIoeHDh6una+j6tLQ0hFBCQkJtbW1+fr5EIvH19VWpVBjjpKSkgICAmpoaLY/eg99JtwBhd8y0adOam5vpTbJ4+/Tp0y7/Ud8d1gMUCoWfn1/vy9Fe2Fu3bvX29lYqlXSKVCo9cuQIh8Nxdnaura2l05kSdkFBwcyZM1NTU319fdsIW0PXT5w4sX///q2trWTX999/jxDKy8sjm+Hh4X5+fkTnXaLv34lxDcURQkbyyZ6srCz1j3ja2toihBQKBXMR9Zz9+/dXVFQY7HAPHjz4+uuvN27cKBAI1NP9/f0jIiKePXu2evVqgwXTGcOHDz9x4sT8+fP5fH6bXRq6vqSkxMnJif6Jurq6IoToT8pt2LChoKAgKSnJAPF3CfPCxhgnJiYOHDiQz+dbWlquWbOG3tXS0hIbG+vm5mZmZvbGG2+QIcDu3bvNzc2FQuGpU6emTp0qFotdXFzIdR4hdPHixbffflsoFIrF4mHDhslkss7K6RbPnj0zMzPz9PTUUaX/j7y8PDc3N4qiyOlfQ+2Sk5MFAoG9vf3SpUudnJwEAoG/v/+1a9cQQuHh4Twej7zmgRD6/PPPzc3NKYqqqqqKiIiIjIx8+PAhRVFeXl4IobNnz4rF4s2bN+u8LoTk5GSMcVBQUPtd8fHx3t7e+/btO3/+fPu9GOMdO3YMHjyYz+dbW1vPmDGDuJdo7vHed65m1LteIpGonyLJDbZEIiGb1tbWAQEBSUlJ2BherNLfYEDLoXhMTAxFUd99911NTY1CoUhJSUF/DMVXr17N5/MzMjJqamrWrVvH4XCuX79O/gUhlJOTU1dXV1FRMX78eHNz86amJrlcLhaLExISlEpleXn5zJkzKysrNZSjJQ0NDSKRKDw8XJvMqPtDrJKSEoTQrl276AbpsHYY47CwMHNz8zt37jQ2NhYWFpJZHDJKnD9/voODA11mYmIiQohUf9asWVKplN6VlZUlEoni4uK6FSTWeigukUh8fHzaJEql0sePH2OML1++zOFwPDw85HI5/vNQPDY2lsfjHT58uLa29ubNmyNGjCCuBprbpJedO3r06DZDcXXadH1ubi6Xy01OTpbJZLdv3x48ePCUKVPU80dHRyPtbiR78DvpFgwLW6FQCIXCd999l06h77GVSqVQKAwJCaFz8vn85cuX4z+6mb6FI+eCBw8e3L59G7WbntFQjpbExMR4e3vT80Ca0ZWw29cOYxwWFmZpaUn/4/Xr1xFCGzduxN0Rdo/RRthyuZyiqMDAwDbptLAxxpGRkQihL774AqsJW6FQWFhY0N2EMf7tt98QQuQE1Fmb9L5zNQu7fdevX7+evii6uLiUlJSo5z9w4ABC6NChQ10eV9/CZngo/uDBA4VC0aGD1L179xQKxdChQ8mmmZmZo6MjGZu1gZhdqFQqiURib28fGhq6YcOGJ0+edLecDjl58uSxY8fOnTvH1LPWdO3a7xo5cqRQKNS+LgagoqICYywUCjXkiY+PHzhwYEpKSl5eHp1YWFgol8tHjhxJp4waNYrH45F7jTbQbdLLztVM+66PiYnZs2dPTk6OXC5/9OiRv7+/n58fOS8TSMVfvHihkwB6A8PCLi0tRQjZ2dm139XQ0IAQWr9+PfUHxcXFmqevzMzMLly4MG7cuM2bN0skkpCQEKVS2YNyaNLS0rZt25abm2u0nxPi8/mduXMxQmNjI0Ko/YyUOgKB4ODBgxRFLV68WKlUksTa2lqEkIWFhXpOKyur+vp6DUX1pnM18//bO9uQpr4/gJ/pptvUpebU+chykuRDUhBOjTRBUCEzFXzRCy1q+SIrQ2YPWmlpNVFIElHEIKMUtVnkgqBUIgsiU1NMs8xiaj7l5vPTfi8O7b//1LnN3d276/m8273Xc8+93/v13nvOueezNvRDQ0N37tw5ffr04cOHraysuFxueXm5VCqFD0cQBoMB/p0EfME5sWHD6cLCwtpVMNuLiopUHzBUvbDr4uvr+/z5c6lUKhQKq6urCwoK9CsHAFBcXFxVVfX69WsXFxd9jg17lpaW/v796+bmhndF/ge8sjcdqsHn89PT0/v6+nJzc+ESW1tbAIBaGm96dHoHVzPrhr6vr29lZUV1CYvFsre37+rqUi6BQiV4EvAF58T28/MzMzNrbm5eu8rd3Z1Op+s0BE0qlXZ3dwMA2Gx2fn7+vn37uru79ShHoVAIhcLOzk6xWKx2DyEUTU1NCoUiKCgIAEClUtd9XDcyjo6OFAplampq0y1zc3N9fHza2trgTz8/P2tr648fPyo3+PDhw+Li4v79+zUUokdwNaMh9PBfzNDQkHKJXC6fmJiAnV4QeOBOTk6Gqo/e4JzYbDY7ISGhtra2oqJCJpN1dHSUlZXBVXQ6PSUl5fHjxyUlJTKZbGVl5ffv36qndS1SqfTMmTM9PT2Li4ttbW0/f/4MCgrSo5zu7u67d++Wl5fTaDSKCgUFBYY8eL1YXV2dnJxcXl7u6Og4f/68h4dHcnIyAIDH401MTIjF4qWlpdHRUVVfl729vVQqHRgYkMvlS0tLEokEu+4uJpO5a9cu+IalGfhAruwxptPpFy9erK+vr6qqkslknZ2dqampHA5HIBBoLmSj4CYlJTk5Oek6UlVD6Llcbnh4eHl5eUtLy9zc3K9fv2DdTp48qfxzeOD+/v467RQTsGuX07K7Sy6Xnzp1aufOndbW1qGhodnZ2QAANze39vb2hYUFoVDo4eFBpVLZbHZ8fHxXV9f9+/dhE4W3t3d/f39ZWRmLxQIAeHp6vnr1Kjg42M7Oztzc3MXF5cqVK3AI0brlaKhSZ2fnuudKJBJtejhAx9bO4uJi2P/MZDKPHDmi4eh6e3sFAgGNRnN1daVSqSwW6+jRo/39/bCc8fHx8PBwOp3O5XLPnj0LhwPweLzBwcFPnz55enoyGIzQ0NDh4eHGxkYbG5ubN29qX0mIlt1daWlpNBptdnYW/qyvr/fy8gIAODg4wJZwVTIyMpTdXaurqyKRyNvbm0aj2dnZxcXFff36VaFQaD4nGwU3Li4OAJCdnb1uJVtbW0NCQjgcDoyss7NzcHBwc3Oz5tDDcQE8Hs/S0tLa2jokJOTp06eqxcbExLi6uiqHpmlA1+tEV/BPbJKBacAEAoG9vT1GhW+Klond19dHpVIfPnxohCppYGVl5eDBgxUVFUbb49jYGJ1OLygo0GZjrBMb/5FnCJ0g1idE68Hj8XJycnJycrD7Hm5TVlZWxGKxXC5PSkoy2k6vX78eGBiYlpZmtD1qYJsmdk9PD2VjjHk1kJJLly4lJiYmJSVp04qGBU1NTXV1dRKJRHOPugEpLCz8/PlzY2OjmrQYL7ZpYvv4+Gh4jIFf5xGNy5cvV1ZWTk1Ncbnc2tpavKuzCbdu3UpLS8vPz8dl7xEREY8ePVIOnseahoaGhYWFpqYmOzs74+xxU9AspSZDXl5eXl4e3rXQgcjIyMjISLxrYQxiY2NjY2PxrsX/sU3v2AgEuUGJjUCQEJTYCAQJQYmNQJAQzBvPampqsN4F0dj6RwjEBI6X3IYBNUmwG/uy3TybCIROmLZGV0GE+Z+MCIVCqa6uJqWSMjExEfxzbiK2CNaTdqJ3bASChKDERiBICEpsBIKEoMRGIEgISmwEgoSgxEYgSAgRE1tVvAqxsLBwdHQMCwsTiUSTk5N4VxChMyaq1AUAvH37NiQkhMlkcjgcoVAIZ9QlojdXDey6yLc4NZKXlxe0XsDp+968eZOcnEyhUDgcjk4OFyMDiGfbNBQ6aXRVMV2l7pcvXxgMRlZW1vT09Lt37xwcHFJSUuAqXb25amB9nRDxjq0GhUKxtbUNCwurrKysqakZGRmJiYnBa2oOHJmbm1t7P8GlEJ24ffv2kydPampqVFUq9+7dMzMzEwgERIhje3t7ZmZmampqYGCg2qrc3FxnZ+cbN25YWVnx+XyhUPjgwQNoGjl37tzevXujo6OXl5fxqPUmmEBiq5KQkJCcnPznz5/S0lK862JsDGLDRUrdtWyk1F1eXn7x4sWhQ4eUo8SioqIUCkVDQwP8SShvrhomltgAADiNtkQiAUTy7OqEYgNfrPY2XKTUNYJS9/v379PT0x4eHsolcB7ljo4O+JNY3lw1sHvKN9Q7thowFd3d3RWE8eyqArR4d9Lgi9Vemml8pa4e79gmrdSFghq1yeQZDEZERITyp/beXDW0uU62gundsW1sbCgUilwun5+fLykpiYuLi4+Pt7W1vXr1Ko1Gq6ysVG4ZHBzMYrHYbHZSUtLMzMzg4ODAwIBMJvP19aXT6U5OTnV1dQ4ODpuWY1jm5uYKCwuPHTt2/PjxHTt2+Pv7l5aWjo2NKRUo2kOlUuE9bc+ePSUlJXK5XNdqx8TEyGSyrKwsXXetDTMzMz9+/IB3uXXh8/kXLlwYGBjIzMxUXa7NKVobXIPHETaAK10lEBqNphQJAgC8vb0BABtpBnDE9BJ7ZmZGoVCwWCyCeHZ1RSdfrPYgpa7B4wjbBdTaxhYXF1Wde8Tx5qpheond29sLAPDx8SGCZ1cP9PPFagNS6gKDxhG2U8BXP8js7Oz8/LxSDASI5M1Vw/QS++XLlwCAqKgo3D27+qGfL3ZTkFLX4HHkcrk2NjaqesNv374BAAICApRLiOPNVcPEEnt4eLioqMjNze3EiRM4ena3gmZfrN42XKTUNXgcqVRqdHR0S0vL6uoqXCKRSCgUimojP3G8uWoQOrEVCsX09DR0F46OjlZXV4eEhJibm4vFYhaLhaNndyto9sVqb8MFSKm7phDDKnUBAFlZWSMjI9euXZuZmWltbRWJRMnJybt371ZuQCBvrhrYNbjr3d317NmzgIAAJpNpYWFhZmYG/g0+O3DgQE5Ozvj4uHJLvDy7GgBadGNs5ItV6GLDNb5SV4/uLpNW6sK1cOCDpaUlh8PJyMiYn59X/VvtvblqaHOdbAUiJrZJg3XAlBhfqatHYpNbqauTN1cNrK8TQj+KIzRD6K+LAABkV+oSypurBkpsBLaQValLNG+uGiixTRKk1NUeLJS6BPTmqoE0uiYJUuriCwG9uWqgOzYCQUJQYiMQJAQlNgJBQlBiIxAkBPPGM2hy21YUFRWR0lz3/v17sC0DaopQFJjN6tLa2lpYWIhR4QiEqZOens7n8zEqHMPERiAQeIHesREIEoISG4EgISixEQgSghIbgSAh/wFO/YghdIrm8gAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2I16RR9R21db",
        "outputId": "aeb6a21e-5621-4ea4-948a-cc7dfb5d4300"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 28, 28)]          0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 784)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               200960    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 128)               32896     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 10)                1290      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 235146 (918.54 KB)\n",
            "Trainable params: 235146 (918.54 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "I3i6poLY3igP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_train_images = train_images.shape[0]"
      ],
      "metadata": {
        "id": "Skuz1PH06A2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_train_images"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "suVxATCi6Jn_",
        "outputId": "cdd23017-dbb2-4d88-c879-a6ae809717a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "48000"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 30\n",
        "dataset_size = num_train_images"
      ],
      "metadata": {
        "id": "x8WGAvTm5z2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_images, train_labels, epochs=epochs, batch_size=batch_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RkbxdLEt3lXp",
        "outputId": "eda683ce-8ce6-4c63-85c6-905d9f0098af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "1600/1600 [==============================] - 12s 3ms/step - loss: 0.4953 - accuracy: 0.8197\n",
            "Epoch 2/30\n",
            "1600/1600 [==============================] - 4s 3ms/step - loss: 0.3692 - accuracy: 0.8634\n",
            "Epoch 3/30\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.3309 - accuracy: 0.8777\n",
            "Epoch 4/30\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.3075 - accuracy: 0.8857\n",
            "Epoch 5/30\n",
            "1600/1600 [==============================] - 4s 3ms/step - loss: 0.2882 - accuracy: 0.8923\n",
            "Epoch 6/30\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.2726 - accuracy: 0.8973\n",
            "Epoch 7/30\n",
            "1600/1600 [==============================] - 7s 5ms/step - loss: 0.2595 - accuracy: 0.9027\n",
            "Epoch 8/30\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.2478 - accuracy: 0.9056\n",
            "Epoch 9/30\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.2361 - accuracy: 0.9100\n",
            "Epoch 10/30\n",
            "1600/1600 [==============================] - 6s 3ms/step - loss: 0.2280 - accuracy: 0.9144\n",
            "Epoch 11/30\n",
            "1600/1600 [==============================] - 4s 3ms/step - loss: 0.2179 - accuracy: 0.9158\n",
            "Epoch 12/30\n",
            "1600/1600 [==============================] - 4s 3ms/step - loss: 0.2125 - accuracy: 0.9197\n",
            "Epoch 13/30\n",
            "1600/1600 [==============================] - 6s 4ms/step - loss: 0.2039 - accuracy: 0.9219\n",
            "Epoch 14/30\n",
            "1600/1600 [==============================] - 4s 3ms/step - loss: 0.1973 - accuracy: 0.9251\n",
            "Epoch 15/30\n",
            "1600/1600 [==============================] - 4s 3ms/step - loss: 0.1884 - accuracy: 0.9272\n",
            "Epoch 16/30\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1819 - accuracy: 0.9301\n",
            "Epoch 17/30\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1781 - accuracy: 0.9315\n",
            "Epoch 18/30\n",
            "1600/1600 [==============================] - 4s 3ms/step - loss: 0.1722 - accuracy: 0.9338\n",
            "Epoch 19/30\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1700 - accuracy: 0.9352\n",
            "Epoch 20/30\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1585 - accuracy: 0.9392\n",
            "Epoch 21/30\n",
            "1600/1600 [==============================] - 4s 3ms/step - loss: 0.1584 - accuracy: 0.9394\n",
            "Epoch 22/30\n",
            "1600/1600 [==============================] - 4s 3ms/step - loss: 0.1515 - accuracy: 0.9420\n",
            "Epoch 23/30\n",
            "1600/1600 [==============================] - 6s 4ms/step - loss: 0.1515 - accuracy: 0.9427\n",
            "Epoch 24/30\n",
            "1600/1600 [==============================] - 4s 3ms/step - loss: 0.1451 - accuracy: 0.9445\n",
            "Epoch 25/30\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1395 - accuracy: 0.9463\n",
            "Epoch 26/30\n",
            "1600/1600 [==============================] - 6s 4ms/step - loss: 0.1351 - accuracy: 0.9481\n",
            "Epoch 27/30\n",
            "1600/1600 [==============================] - 4s 3ms/step - loss: 0.1319 - accuracy: 0.9485\n",
            "Epoch 28/30\n",
            "1600/1600 [==============================] - 6s 3ms/step - loss: 0.1324 - accuracy: 0.9492\n",
            "Epoch 29/30\n",
            "1600/1600 [==============================] - 7s 4ms/step - loss: 0.1248 - accuracy: 0.9517\n",
            "Epoch 30/30\n",
            "1600/1600 [==============================] - 4s 3ms/step - loss: 0.1239 - accuracy: 0.9519\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7957931a5930>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VflXLEeECaXC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c10ae4ad-838c-4ead-ed13-6fb96556b270"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 - 1s - loss: 0.5087 - accuracy: 0.8824 - 693ms/epoch - 2ms/step\n",
            "\n",
            "Test accuracy: 0.8823999762535095\n"
          ]
        }
      ],
      "source": [
        "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
        "\n",
        "print('\\nTest accuracy:', test_acc)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 저장\n",
        "model.save(\"/content/drive/MyDrive/LoRA/original-model\")"
      ],
      "metadata": {
        "id": "dbLPojrTz1iv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LoRA\n"
      ],
      "metadata": {
        "id": "uaocZiKTvPbz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "기존 코드 Dense 에 맞게 변경"
      ],
      "metadata": {
        "id": "W879ki7__YHC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "이 코드에서 LoraLayer 클래스의 call 메소드는 이제 선택적으로 training 인자를 받고, 이 인자는 레이어가 학습 모드인지 추론 모드인지를 결정하는 데 사용됩니다. training 인자가 명시적으로 제공되지 않은 경우에는 레이어의 내부 training 플래그를 사용합니다.\n",
        "\n",
        "학습 모드에서는 self.original_layer의 출력에 decay factor를 적용하고 lora_output을 결합합니다. 추론 또는 평가 모드에서는 lora_output만을 사용합니다. 이 방식으로 fit, evaluate, predict 등 모델의 메소드들을 호출할 때 Keras가 자동으로 training 인자를 설정하므로 사용자는 수동으로 이 값을 설정할 필요가 없습니다."
      ],
      "metadata": {
        "id": "5ultecq1XN9B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from tensorflow import keras\n",
        "\n",
        "class LoraLayer(keras.layers.Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        original_layer,\n",
        "        total_iteration = 1000 ,  # Total number of iterations for the decay\n",
        "        start_percent=0.05,  # The percentage of total_iteration when decay starts\n",
        "        end_percent=0.85,  # The percentage of total_iteration when decay ends\n",
        "        min_decay_factor=0,  # The minimum value that decay factor can take\n",
        "        rank=64,\n",
        "        alpha=32,\n",
        "        trainable=True,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        original_layer_config = original_layer.get_config()\n",
        "        name = original_layer_config[\"name\"]\n",
        "        kwargs.pop(\"name\", None)\n",
        "\n",
        "        super().__init__(name=name, trainable=trainable, **kwargs)\n",
        "\n",
        "        self.rank = rank\n",
        "        self.alpha = alpha\n",
        "        self._scale = alpha / rank\n",
        "\n",
        "        self.original_layer = original_layer\n",
        "        self.original_layer.trainable = False\n",
        "\n",
        "\n",
        "        self.total_iteration = total_iteration\n",
        "        self.start_step = int(total_iteration * start_percent)\n",
        "        self.end_step = int(total_iteration * end_percent)\n",
        "        self.min_decay_factor = min_decay_factor\n",
        "\n",
        "        self.current_step = tf.Variable(0, dtype=tf.int32, trainable=False)\n",
        "        self.decay_factor = tf.Variable(1.0, dtype=tf.float32, trainable=False)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # LoRA weights.\n",
        "        self.A_weight = self.add_weight(\n",
        "            name=\"lora_A_weight\",\n",
        "            shape=(self.rank, input_shape[-1]),\n",
        "            initializer=keras.initializers.VarianceScaling(\n",
        "                scale=math.sqrt(5), mode=\"fan_in\", distribution=\"uniform\"\n",
        "            ),\n",
        "            trainable=self.trainable,\n",
        "        )\n",
        "\n",
        "        self.B_weight = self.add_weight(\n",
        "            name=\"lora_B_weight\",\n",
        "            shape=(self.original_layer.units, self.rank),\n",
        "            initializer='zeros',\n",
        "            trainable=self.trainable,\n",
        "        )\n",
        "        self.C_weight = self.add_weight(\n",
        "            name=\"lora_C_weight\",\n",
        "            shape=(self.original_layer.units,),\n",
        "            initializer='zeros',\n",
        "            trainable=self.trainable,\n",
        "        )\n",
        "\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "            if training is None:\n",
        "                training = self.trainable\n",
        "\n",
        "            # Calculate the linear decay factor\n",
        "            if self.current_step < self.start_step:\n",
        "                self.decay_factor.assign(1.0)  # Decay has not started yet\n",
        "            elif self.current_step > self.end_step:\n",
        "                self.decay_factor.assign(tf.cast(self.min_decay_factor, dtype=tf.float32))  # Ensure float32 type for consistency\n",
        "            else:\n",
        "                # Linear decay between start_step and end_step\n",
        "                self.decay_factor.assign(1.0 - ((tf.cast(self.current_step, dtype=tf.float32) - self.start_step) /\n",
        "                                        (self.end_step - self.start_step) *\n",
        "                                        (1.0 - tf.cast(self.min_decay_factor, dtype=tf.float32))))\n",
        "\n",
        "            # Matrix multiplication for A and B weights with inputs\n",
        "            lora_A_output = tf.matmul(self.A_weight, tf.transpose(inputs))  # Ax\n",
        "            lora_output = tf.transpose(tf.matmul(self.B_weight, lora_A_output) * self._scale)  + self.C_weight # BAx Transpose back to [batch_size, original_layer.units]\n",
        "\n",
        "\n",
        "            if training:\n",
        "                original_output = self.original_layer(inputs)\n",
        "                # 평균과 표준편차 계산\n",
        "                original_weight_matrix = self.original_layer.weights[0]\n",
        "                original_mean = tf.reduce_mean(original_weight_matrix, axis=0)\n",
        "                original_variance = tf.reduce_mean(tf.square(original_weight_matrix - original_mean), axis=0)\n",
        "                original_stddev = tf.sqrt(original_variance)\n",
        "\n",
        "                # decay_factor가 0.3보다 작으면 noise_mean과 noise_std를 0으로 설정\n",
        "                noise_mean = tf.where(self.decay_factor < 0.3, 0.0, original_mean * (1 - self.decay_factor))\n",
        "                noise_std = tf.where(self.decay_factor < 0.3, 0.0, original_stddev * tf.sqrt(1 - tf.square(self.decay_factor)))\n",
        "                noise = tf.random.normal(tf.shape(original_weight_matrix), mean=noise_mean, stddev=noise_std)\n",
        "\n",
        "                self.current_step.assign_add(1)\n",
        "\n",
        "                return original_output * self.decay_factor + lora_output + (inputs @ noise)\n",
        "\n",
        "            else:\n",
        "                return lora_output\n"
      ],
      "metadata": {
        "id": "2GnZ1Vfw8lsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LoRA 적용 + Weights 값들 확인\n",
        "    - W + AB (LoRA_o_model_1, LoRA_o_model_2)\n"
      ],
      "metadata": {
        "id": "xoDqEH5-1Ade"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_original = tf.keras.models.load_model(\"/content/drive/MyDrive/LoRA/original-model\")"
      ],
      "metadata": {
        "id": "OXwAhFCfz8eX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_original.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uL6-0ARgaIgb",
        "outputId": "93e4f33b-ba34-46ee-9bc8-4dab1e35d2d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 28, 28)]          0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 784)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               200960    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 128)               32896     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 10)                1290      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 235146 (918.54 KB)\n",
            "Trainable params: 235146 (918.54 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model_original.evaluate(test_images,  test_labels, verbose=2)\n",
        "\n",
        "print('\\nTest accuracy:', test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4SPhz9XJOEtL",
        "outputId": "a48eac2b-22e3-40ed-81c5-bf67e0e01c8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 - 1s - loss: 0.3787 - accuracy: 0.8779 - 611ms/epoch - 2ms/step\n",
            "\n",
            "Test accuracy: 0.8779000043869019\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "기존 것에 그냥 오버라이딩이 안됨\n",
        "rank = 64"
      ],
      "metadata": {
        "id": "bnPHMs0wH4KO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 30"
      ],
      "metadata": {
        "id": "jc0nYbw6Wzzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_iteration= int(epochs * (dataset_size / batch_size))  # Total number of iterations for the decay"
      ],
      "metadata": {
        "id": "NdTdpstYuAkd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the original layers by name that you want to replace with LoraLayer\n",
        "original_layers_to_replace = ['dense', 'dense_1']\n",
        "\n",
        "# Create a dictionary to hold the LoraLayers corresponding to the original layers\n",
        "lora_layers = {layer_name: LoraLayer(original_layer=model_original.get_layer(layer_name),\n",
        "                                      rank=64,\n",
        "                                      alpha=32,\n",
        "                                      total_iteration = total_iteration,\n",
        "                                      trainable=True)\n",
        "               for layer_name in original_layers_to_replace}\n",
        "\n",
        "# Create the new model\n",
        "inputs = model_original.input\n",
        "x = inputs\n",
        "\n",
        "# Sequentially get layers from the original model and connect them.\n",
        "# Replace with LoraLayer when we encounter the specified layers.\n",
        "for layer in model_original.layers:\n",
        "    if layer.name in original_layers_to_replace:\n",
        "        # Use the corresponding LoraLayer\n",
        "        x = lora_layers[layer.name](x)\n",
        "    else:\n",
        "        # If the layer is not meant to be replaced, use the original layer\n",
        "        layer.trainable = False  # Make sure the original layer is not trainable\n",
        "        x = layer(x)\n",
        "\n",
        "# Define the new model with the LoRA layers applied\n",
        "LoRA_modified_model = keras.Model(inputs=inputs, outputs=x)\n",
        "\n",
        "# Check the model structure\n",
        "LoRA_modified_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_OcczRq0kGW",
        "outputId": "0348c217-ff94-46ac-fd72-c4fcc6453190"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_9\" was not an Input tensor, it was generated by layer \"input_1\".\n",
            "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
            "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 28, 28), dtype=tf.float32, name='input_1'), name='input_1', description=\"created by layer 'input_1'\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_9\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        multiple                     0         ['input_1[0][0]']             \n",
            "                                                                                                  \n",
            " flatten (Flatten)           (None, 784)                  0         ['input_1[3][0]']             \n",
            "                                                                                                  \n",
            " dense (LoraLayer)           (None, 256)                  267778    ['flatten[3][0]']             \n",
            "                                                                                                  \n",
            " dense_1 (LoraLayer)         (None, 128)                  57602     ['dense[0][0]']               \n",
            "                                                                                                  \n",
            " dense_2 (Dense)             (None, 10)                   1290      ['dense_1[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 326670 (1.25 MB)\n",
            "Trainable params: 91520 (357.50 KB)\n",
            "Non-trainable params: 235150 (918.55 KB)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#tf.keras.utils.plot_model(LoRA_modified_model, \"my_model.png\", show_shapes=True)"
      ],
      "metadata": {
        "id": "Xtmv-CXJwuBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PrintCurrentStepCallback(keras.callbacks.Callback):\n",
        "    def __init__(self, lora_layers):\n",
        "        super().__init__()\n",
        "        self.lora_layers = lora_layers\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        print()\n",
        "        for i, lora_layer in enumerate(self.lora_layers):\n",
        "            current_step = lora_layer.current_step.value()\n",
        "            decay_factor = lora_layer.decay_factor.value()\n",
        "            print(f\"End of epoch {epoch + 1}, LoraLayer {i}: {current_step} Step\")\n",
        "            print(f\"End of epoch {epoch + 1}, LoraLayer {i}: Decay factor: {decay_factor}\")"
      ],
      "metadata": {
        "id": "BijApgiEJ5_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 콜백 생성 시 lora_layers 딕셔너리의 값만 사용\n",
        "print_step_callback = PrintCurrentStepCallback(list(lora_layers.values()))"
      ],
      "metadata": {
        "id": "JPaL5jj-J6Bf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TestCallback(keras.callbacks.Callback):\n",
        "    def __init__(self, test_data):\n",
        "        self.test_data = test_data\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        x, y = self.test_data\n",
        "        loss, acc = self.model.evaluate(x, y, verbose=0)\n",
        "        print('\\nTesting loss: {}, acc: {}\\n'.format(loss, acc))"
      ],
      "metadata": {
        "id": "gYWO7vLEJ6FV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NoiseMeanCallback(keras.callbacks.Callback):\n",
        "    def __init__(self, lora_layers):\n",
        "        super().__init__()\n",
        "        self.lora_layers = lora_layers\n",
        "        self.noise_means = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        epoch_noise_means = []\n",
        "        for layer in self.lora_layers:\n",
        "            if isinstance(layer, LoraLayer):\n",
        "                # LoraLayer의 가중치에 대한 평균 계산\n",
        "                original_weight_matrix = layer.original_layer.weights[0]\n",
        "                noise_mean = tf.reduce_mean(original_weight_matrix, axis=0).numpy()\n",
        "                epoch_noise_means.append(noise_mean)\n",
        "        self.noise_means.append(epoch_noise_means)\n"
      ],
      "metadata": {
        "id": "V7zpRtwkKA7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LoRA_modified_model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "JCkKygQRcAvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "noise_mean_callback = NoiseMeanCallback(lora_layers)\n",
        "\n",
        "history = LoRA_modified_model.fit(train_images, train_labels,\n",
        "                                  epochs=epochs,\n",
        "                                  batch_size=batch_size,\n",
        "                                  validation_data=(val_images, val_labels),\n",
        "                                  callbacks=[print_step_callback, TestCallback((val_images, val_labels)), noise_mean_callback])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZg2sirDKW30",
        "outputId": "17b056ce-4fac-4458-a5e7-9ca5e39d6cee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "1595/1600 [============================>.] - ETA: 0s - loss: 0.2101 - accuracy: 0.9193\n",
            "End of epoch 1, LoraLayer 0: 1600 Step\n",
            "End of epoch 1, LoraLayer 0: Decay factor: 1.0\n",
            "End of epoch 1, LoraLayer 1: 1600 Step\n",
            "End of epoch 1, LoraLayer 1: Decay factor: 1.0\n",
            "\n",
            "Testing loss: 2.369999408721924, acc: 0.10566666722297668\n",
            "\n",
            "1600/1600 [==============================] - 13s 8ms/step - loss: 0.2101 - accuracy: 0.9193 - val_loss: 2.3700 - val_accuracy: 0.1057\n",
            "Epoch 2/30\n",
            "1600/1600 [==============================] - ETA: 0s - loss: 0.1995 - accuracy: 0.9243\n",
            "End of epoch 2, LoraLayer 0: 3200 Step\n",
            "End of epoch 2, LoraLayer 0: Decay factor: 0.9791666865348816\n",
            "End of epoch 2, LoraLayer 1: 3200 Step\n",
            "End of epoch 2, LoraLayer 1: Decay factor: 0.9791666865348816\n",
            "\n",
            "Testing loss: 2.4589195251464844, acc: 0.07616666704416275\n",
            "\n",
            "1600/1600 [==============================] - 8s 5ms/step - loss: 0.1995 - accuracy: 0.9243 - val_loss: 2.4589 - val_accuracy: 0.0762\n",
            "Epoch 3/30\n",
            "1589/1600 [============================>.] - ETA: 0s - loss: 0.1942 - accuracy: 0.9251\n",
            "End of epoch 3, LoraLayer 0: 4800 Step\n",
            "End of epoch 3, LoraLayer 0: Decay factor: 0.9375\n",
            "End of epoch 3, LoraLayer 1: 4800 Step\n",
            "End of epoch 3, LoraLayer 1: Decay factor: 0.9375\n",
            "\n",
            "Testing loss: 2.450533390045166, acc: 0.11033333092927933\n",
            "\n",
            "1600/1600 [==============================] - 12s 7ms/step - loss: 0.1940 - accuracy: 0.9252 - val_loss: 2.4505 - val_accuracy: 0.1103\n",
            "Epoch 4/30\n",
            "1599/1600 [============================>.] - ETA: 0s - loss: 0.1926 - accuracy: 0.9263\n",
            "End of epoch 4, LoraLayer 0: 6400 Step\n",
            "End of epoch 4, LoraLayer 0: Decay factor: 0.8958333134651184\n",
            "End of epoch 4, LoraLayer 1: 6400 Step\n",
            "End of epoch 4, LoraLayer 1: Decay factor: 0.8958333134651184\n",
            "\n",
            "Testing loss: 2.4522292613983154, acc: 0.10516666620969772\n",
            "\n",
            "1600/1600 [==============================] - 12s 7ms/step - loss: 0.1925 - accuracy: 0.9264 - val_loss: 2.4522 - val_accuracy: 0.1052\n",
            "Epoch 5/30\n",
            "1600/1600 [==============================] - ETA: 0s - loss: 0.1916 - accuracy: 0.9256\n",
            "End of epoch 5, LoraLayer 0: 8000 Step\n",
            "End of epoch 5, LoraLayer 0: Decay factor: 0.8541666269302368\n",
            "End of epoch 5, LoraLayer 1: 8000 Step\n",
            "End of epoch 5, LoraLayer 1: Decay factor: 0.8541666269302368\n",
            "\n",
            "Testing loss: 2.7213594913482666, acc: 0.045249998569488525\n",
            "\n",
            "1600/1600 [==============================] - 9s 6ms/step - loss: 0.1916 - accuracy: 0.9256 - val_loss: 2.7214 - val_accuracy: 0.0452\n",
            "Epoch 6/30\n",
            "1599/1600 [============================>.] - ETA: 0s - loss: 0.1908 - accuracy: 0.9273\n",
            "End of epoch 6, LoraLayer 0: 9600 Step\n",
            "End of epoch 6, LoraLayer 0: Decay factor: 0.8125\n",
            "End of epoch 6, LoraLayer 1: 9600 Step\n",
            "End of epoch 6, LoraLayer 1: Decay factor: 0.8125\n",
            "\n",
            "Testing loss: 2.8066565990448, acc: 0.046166665852069855\n",
            "\n",
            "1600/1600 [==============================] - 11s 7ms/step - loss: 0.1907 - accuracy: 0.9273 - val_loss: 2.8067 - val_accuracy: 0.0462\n",
            "Epoch 7/30\n",
            "1596/1600 [============================>.] - ETA: 0s - loss: 0.1914 - accuracy: 0.9277\n",
            "End of epoch 7, LoraLayer 0: 11200 Step\n",
            "End of epoch 7, LoraLayer 0: Decay factor: 0.7708333134651184\n",
            "End of epoch 7, LoraLayer 1: 11200 Step\n",
            "End of epoch 7, LoraLayer 1: Decay factor: 0.7708333134651184\n",
            "\n",
            "Testing loss: 2.6034982204437256, acc: 0.07349999994039536\n",
            "\n",
            "1600/1600 [==============================] - 12s 7ms/step - loss: 0.1916 - accuracy: 0.9276 - val_loss: 2.6035 - val_accuracy: 0.0735\n",
            "Epoch 8/30\n",
            "1594/1600 [============================>.] - ETA: 0s - loss: 0.1939 - accuracy: 0.9276\n",
            "End of epoch 8, LoraLayer 0: 12800 Step\n",
            "End of epoch 8, LoraLayer 0: Decay factor: 0.7291666269302368\n",
            "End of epoch 8, LoraLayer 1: 12800 Step\n",
            "End of epoch 8, LoraLayer 1: Decay factor: 0.7291666269302368\n",
            "\n",
            "Testing loss: 2.891423225402832, acc: 0.08224999904632568\n",
            "\n",
            "1600/1600 [==============================] - 9s 6ms/step - loss: 0.1939 - accuracy: 0.9276 - val_loss: 2.8914 - val_accuracy: 0.0822\n",
            "Epoch 9/30\n",
            "1598/1600 [============================>.] - ETA: 0s - loss: 0.1937 - accuracy: 0.9279\n",
            "End of epoch 9, LoraLayer 0: 14400 Step\n",
            "End of epoch 9, LoraLayer 0: Decay factor: 0.6875\n",
            "End of epoch 9, LoraLayer 1: 14400 Step\n",
            "End of epoch 9, LoraLayer 1: Decay factor: 0.6875\n",
            "\n",
            "Testing loss: 2.7636523246765137, acc: 0.04508333280682564\n",
            "\n",
            "1600/1600 [==============================] - 12s 7ms/step - loss: 0.1938 - accuracy: 0.9278 - val_loss: 2.7637 - val_accuracy: 0.0451\n",
            "Epoch 10/30\n",
            "1600/1600 [==============================] - ETA: 0s - loss: 0.1968 - accuracy: 0.9262\n",
            "End of epoch 10, LoraLayer 0: 16000 Step\n",
            "End of epoch 10, LoraLayer 0: Decay factor: 0.6458333134651184\n",
            "End of epoch 10, LoraLayer 1: 16000 Step\n",
            "End of epoch 10, LoraLayer 1: Decay factor: 0.6458333134651184\n",
            "\n",
            "Testing loss: 2.824404001235962, acc: 0.06108333170413971\n",
            "\n",
            "1600/1600 [==============================] - 10s 6ms/step - loss: 0.1968 - accuracy: 0.9262 - val_loss: 2.8244 - val_accuracy: 0.0611\n",
            "Epoch 11/30\n",
            "1597/1600 [============================>.] - ETA: 0s - loss: 0.1997 - accuracy: 0.9268\n",
            "End of epoch 11, LoraLayer 0: 17600 Step\n",
            "End of epoch 11, LoraLayer 0: Decay factor: 0.6041666269302368\n",
            "End of epoch 11, LoraLayer 1: 17600 Step\n",
            "End of epoch 11, LoraLayer 1: Decay factor: 0.6041666269302368\n",
            "\n",
            "Testing loss: 3.2860729694366455, acc: 0.037416666746139526\n",
            "\n",
            "1600/1600 [==============================] - 9s 6ms/step - loss: 0.1995 - accuracy: 0.9269 - val_loss: 3.2861 - val_accuracy: 0.0374\n",
            "Epoch 12/30\n",
            "1597/1600 [============================>.] - ETA: 0s - loss: 0.2015 - accuracy: 0.9247\n",
            "End of epoch 12, LoraLayer 0: 19200 Step\n",
            "End of epoch 12, LoraLayer 0: Decay factor: 0.5625\n",
            "End of epoch 12, LoraLayer 1: 19200 Step\n",
            "End of epoch 12, LoraLayer 1: Decay factor: 0.5625\n",
            "\n",
            "Testing loss: 3.3358466625213623, acc: 0.09916666895151138\n",
            "\n",
            "1600/1600 [==============================] - 12s 8ms/step - loss: 0.2016 - accuracy: 0.9247 - val_loss: 3.3358 - val_accuracy: 0.0992\n",
            "Epoch 13/30\n",
            "1598/1600 [============================>.] - ETA: 0s - loss: 0.2043 - accuracy: 0.9246\n",
            "End of epoch 13, LoraLayer 0: 20800 Step\n",
            "End of epoch 13, LoraLayer 0: Decay factor: 0.5208333134651184\n",
            "End of epoch 13, LoraLayer 1: 20800 Step\n",
            "End of epoch 13, LoraLayer 1: Decay factor: 0.5208333134651184\n",
            "\n",
            "Testing loss: 3.041663408279419, acc: 0.08033332973718643\n",
            "\n",
            "1600/1600 [==============================] - 9s 6ms/step - loss: 0.2043 - accuracy: 0.9245 - val_loss: 3.0417 - val_accuracy: 0.0803\n",
            "Epoch 14/30\n",
            "1594/1600 [============================>.] - ETA: 0s - loss: 0.2079 - accuracy: 0.9229\n",
            "End of epoch 14, LoraLayer 0: 22400 Step\n",
            "End of epoch 14, LoraLayer 0: Decay factor: 0.4791666269302368\n",
            "End of epoch 14, LoraLayer 1: 22400 Step\n",
            "End of epoch 14, LoraLayer 1: Decay factor: 0.4791666269302368\n",
            "\n",
            "Testing loss: 3.4371063709259033, acc: 0.0650833323597908\n",
            "\n",
            "1600/1600 [==============================] - 11s 7ms/step - loss: 0.2080 - accuracy: 0.9229 - val_loss: 3.4371 - val_accuracy: 0.0651\n",
            "Epoch 15/30\n",
            "1593/1600 [============================>.] - ETA: 0s - loss: 0.2129 - accuracy: 0.9215\n",
            "End of epoch 15, LoraLayer 0: 24000 Step\n",
            "End of epoch 15, LoraLayer 0: Decay factor: 0.4375\n",
            "End of epoch 15, LoraLayer 1: 24000 Step\n",
            "End of epoch 15, LoraLayer 1: Decay factor: 0.4375\n",
            "\n",
            "Testing loss: 3.6518566608428955, acc: 0.11150000244379044\n",
            "\n",
            "1600/1600 [==============================] - 13s 8ms/step - loss: 0.2128 - accuracy: 0.9216 - val_loss: 3.6519 - val_accuracy: 0.1115\n",
            "Epoch 16/30\n",
            "1600/1600 [==============================] - ETA: 0s - loss: 0.2178 - accuracy: 0.9192\n",
            "End of epoch 16, LoraLayer 0: 25600 Step\n",
            "End of epoch 16, LoraLayer 0: Decay factor: 0.3958333134651184\n",
            "End of epoch 16, LoraLayer 1: 25600 Step\n",
            "End of epoch 16, LoraLayer 1: Decay factor: 0.3958333134651184\n",
            "\n",
            "Testing loss: 4.123335838317871, acc: 0.12475000321865082\n",
            "\n",
            "1600/1600 [==============================] - 10s 6ms/step - loss: 0.2178 - accuracy: 0.9192 - val_loss: 4.1233 - val_accuracy: 0.1248\n",
            "Epoch 17/30\n",
            "1596/1600 [============================>.] - ETA: 0s - loss: 0.2238 - accuracy: 0.9178\n",
            "End of epoch 17, LoraLayer 0: 27200 Step\n",
            "End of epoch 17, LoraLayer 0: Decay factor: 0.3541666269302368\n",
            "End of epoch 17, LoraLayer 1: 27200 Step\n",
            "End of epoch 17, LoraLayer 1: Decay factor: 0.3541666269302368\n",
            "\n",
            "Testing loss: 3.009767532348633, acc: 0.10824999958276749\n",
            "\n",
            "1600/1600 [==============================] - 11s 7ms/step - loss: 0.2237 - accuracy: 0.9179 - val_loss: 3.0098 - val_accuracy: 0.1082\n",
            "Epoch 18/30\n",
            "1593/1600 [============================>.] - ETA: 0s - loss: 0.2291 - accuracy: 0.9159\n",
            "End of epoch 18, LoraLayer 0: 28800 Step\n",
            "End of epoch 18, LoraLayer 0: Decay factor: 0.3125\n",
            "End of epoch 18, LoraLayer 1: 28800 Step\n",
            "End of epoch 18, LoraLayer 1: Decay factor: 0.3125\n",
            "\n",
            "Testing loss: 3.671337842941284, acc: 0.10641666501760483\n",
            "\n",
            "1600/1600 [==============================] - 11s 7ms/step - loss: 0.2292 - accuracy: 0.9159 - val_loss: 3.6713 - val_accuracy: 0.1064\n",
            "Epoch 19/30\n",
            "1589/1600 [============================>.] - ETA: 0s - loss: 0.2361 - accuracy: 0.9123\n",
            "End of epoch 19, LoraLayer 0: 30400 Step\n",
            "End of epoch 19, LoraLayer 0: Decay factor: 0.2708333134651184\n",
            "End of epoch 19, LoraLayer 1: 30400 Step\n",
            "End of epoch 19, LoraLayer 1: Decay factor: 0.2708333134651184\n",
            "\n",
            "Testing loss: 3.0228772163391113, acc: 0.12808333337306976\n",
            "\n",
            "1600/1600 [==============================] - 9s 6ms/step - loss: 0.2360 - accuracy: 0.9123 - val_loss: 3.0229 - val_accuracy: 0.1281\n",
            "Epoch 20/30\n",
            "1595/1600 [============================>.] - ETA: 0s - loss: 0.2453 - accuracy: 0.9105\n",
            "End of epoch 20, LoraLayer 0: 32000 Step\n",
            "End of epoch 20, LoraLayer 0: Decay factor: 0.22916662693023682\n",
            "End of epoch 20, LoraLayer 1: 32000 Step\n",
            "End of epoch 20, LoraLayer 1: Decay factor: 0.22916662693023682\n",
            "\n",
            "Testing loss: 3.018733501434326, acc: 0.15324999392032623\n",
            "\n",
            "1600/1600 [==============================] - 11s 7ms/step - loss: 0.2453 - accuracy: 0.9105 - val_loss: 3.0187 - val_accuracy: 0.1532\n",
            "Epoch 21/30\n",
            "1598/1600 [============================>.] - ETA: 0s - loss: 0.2573 - accuracy: 0.9062\n",
            "End of epoch 21, LoraLayer 0: 33600 Step\n",
            "End of epoch 21, LoraLayer 0: Decay factor: 0.1875\n",
            "End of epoch 21, LoraLayer 1: 33600 Step\n",
            "End of epoch 21, LoraLayer 1: Decay factor: 0.1875\n",
            "\n",
            "Testing loss: 2.943803310394287, acc: 0.11741666495800018\n",
            "\n",
            "1600/1600 [==============================] - 10s 6ms/step - loss: 0.2575 - accuracy: 0.9061 - val_loss: 2.9438 - val_accuracy: 0.1174\n",
            "Epoch 22/30\n",
            "1593/1600 [============================>.] - ETA: 0s - loss: 0.2698 - accuracy: 0.9023\n",
            "End of epoch 22, LoraLayer 0: 35200 Step\n",
            "End of epoch 22, LoraLayer 0: Decay factor: 0.1458333134651184\n",
            "End of epoch 22, LoraLayer 1: 35200 Step\n",
            "End of epoch 22, LoraLayer 1: Decay factor: 0.1458333134651184\n",
            "\n",
            "Testing loss: 2.4245212078094482, acc: 0.2529166638851166\n",
            "\n",
            "1600/1600 [==============================] - 10s 6ms/step - loss: 0.2699 - accuracy: 0.9023 - val_loss: 2.4245 - val_accuracy: 0.2529\n",
            "Epoch 23/30\n",
            "1592/1600 [============================>.] - ETA: 0s - loss: 0.2930 - accuracy: 0.8934\n",
            "End of epoch 23, LoraLayer 0: 36800 Step\n",
            "End of epoch 23, LoraLayer 0: Decay factor: 0.10416662693023682\n",
            "End of epoch 23, LoraLayer 1: 36800 Step\n",
            "End of epoch 23, LoraLayer 1: Decay factor: 0.10416662693023682\n",
            "\n",
            "Testing loss: 2.4145779609680176, acc: 0.21549999713897705\n",
            "\n",
            "1600/1600 [==============================] - 12s 7ms/step - loss: 0.2930 - accuracy: 0.8934 - val_loss: 2.4146 - val_accuracy: 0.2155\n",
            "Epoch 24/30\n",
            "1592/1600 [============================>.] - ETA: 0s - loss: 0.3307 - accuracy: 0.8805\n",
            "End of epoch 24, LoraLayer 0: 38400 Step\n",
            "End of epoch 24, LoraLayer 0: Decay factor: 0.0625\n",
            "End of epoch 24, LoraLayer 1: 38400 Step\n",
            "End of epoch 24, LoraLayer 1: Decay factor: 0.0625\n",
            "\n",
            "Testing loss: 1.37623131275177, acc: 0.48266667127609253\n",
            "\n",
            "1600/1600 [==============================] - 9s 6ms/step - loss: 0.3308 - accuracy: 0.8805 - val_loss: 1.3762 - val_accuracy: 0.4827\n",
            "Epoch 25/30\n",
            "1589/1600 [============================>.] - ETA: 0s - loss: 0.4172 - accuracy: 0.8521\n",
            "End of epoch 25, LoraLayer 0: 40000 Step\n",
            "End of epoch 25, LoraLayer 0: Decay factor: 0.020833313465118408\n",
            "End of epoch 25, LoraLayer 1: 40000 Step\n",
            "End of epoch 25, LoraLayer 1: Decay factor: 0.020833313465118408\n",
            "\n",
            "Testing loss: 0.6846304535865784, acc: 0.7555833458900452\n",
            "\n",
            "1600/1600 [==============================] - 11s 7ms/step - loss: 0.4177 - accuracy: 0.8518 - val_loss: 0.6846 - val_accuracy: 0.7556\n",
            "Epoch 26/30\n",
            "1595/1600 [============================>.] - ETA: 0s - loss: 0.4882 - accuracy: 0.8315\n",
            "End of epoch 26, LoraLayer 0: 41600 Step\n",
            "End of epoch 26, LoraLayer 0: Decay factor: 0.0\n",
            "End of epoch 26, LoraLayer 1: 41600 Step\n",
            "End of epoch 26, LoraLayer 1: Decay factor: 0.0\n",
            "\n",
            "Testing loss: 0.4917931854724884, acc: 0.828166663646698\n",
            "\n",
            "1600/1600 [==============================] - 11s 7ms/step - loss: 0.4882 - accuracy: 0.8315 - val_loss: 0.4918 - val_accuracy: 0.8282\n",
            "Epoch 27/30\n",
            "1594/1600 [============================>.] - ETA: 0s - loss: 0.4582 - accuracy: 0.8392\n",
            "End of epoch 27, LoraLayer 0: 43200 Step\n",
            "End of epoch 27, LoraLayer 0: Decay factor: 0.0\n",
            "End of epoch 27, LoraLayer 1: 43200 Step\n",
            "End of epoch 27, LoraLayer 1: Decay factor: 0.0\n",
            "\n",
            "Testing loss: 0.48919031023979187, acc: 0.8328333497047424\n",
            "\n",
            "1600/1600 [==============================] - 9s 5ms/step - loss: 0.4581 - accuracy: 0.8393 - val_loss: 0.4892 - val_accuracy: 0.8328\n",
            "Epoch 28/30\n",
            "1600/1600 [==============================] - ETA: 0s - loss: 0.4440 - accuracy: 0.8443\n",
            "End of epoch 28, LoraLayer 0: 44800 Step\n",
            "End of epoch 28, LoraLayer 0: Decay factor: 0.0\n",
            "End of epoch 28, LoraLayer 1: 44800 Step\n",
            "End of epoch 28, LoraLayer 1: Decay factor: 0.0\n",
            "\n",
            "Testing loss: 0.517317533493042, acc: 0.8190833330154419\n",
            "\n",
            "1600/1600 [==============================] - 12s 7ms/step - loss: 0.4440 - accuracy: 0.8443 - val_loss: 0.5173 - val_accuracy: 0.8191\n",
            "Epoch 29/30\n",
            "1600/1600 [==============================] - ETA: 0s - loss: 0.4371 - accuracy: 0.8479\n",
            "End of epoch 29, LoraLayer 0: 46400 Step\n",
            "End of epoch 29, LoraLayer 0: Decay factor: 0.0\n",
            "End of epoch 29, LoraLayer 1: 46400 Step\n",
            "End of epoch 29, LoraLayer 1: Decay factor: 0.0\n",
            "\n",
            "Testing loss: 0.4924200773239136, acc: 0.8354166746139526\n",
            "\n",
            "1600/1600 [==============================] - 11s 7ms/step - loss: 0.4371 - accuracy: 0.8479 - val_loss: 0.4924 - val_accuracy: 0.8354\n",
            "Epoch 30/30\n",
            "1591/1600 [============================>.] - ETA: 0s - loss: 0.4301 - accuracy: 0.8489\n",
            "End of epoch 30, LoraLayer 0: 48000 Step\n",
            "End of epoch 30, LoraLayer 0: Decay factor: 0.0\n",
            "End of epoch 30, LoraLayer 1: 48000 Step\n",
            "End of epoch 30, LoraLayer 1: Decay factor: 0.0\n",
            "\n",
            "Testing loss: 0.47154539823532104, acc: 0.8368333578109741\n",
            "\n",
            "1600/1600 [==============================] - 9s 5ms/step - loss: 0.4300 - accuracy: 0.8488 - val_loss: 0.4715 - val_accuracy: 0.8368\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 손실 그래프\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Epoch vs Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# 정확도 그래프\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Epoch vs Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "tps9ex9N4MyY",
        "outputId": "f8f9fdc4-b501-431f-da0c-170986375137"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# noise_mean_callback.noise_means 에는 각 LoraLayer의 noise_mean 값이 에폭 별로 저장되어 있습니다.\n",
        "# 각 LoraLayer의 noise_mean 시각화\n",
        "for layer_index in range(len(noise_mean_callback.noise_means[0])):\n",
        "    # 각 LoraLayer에 대한 noise_mean 값 추출\n",
        "    layer_noise_means = [epoch[layer_index] for epoch in noise_mean_callback.noise_means]\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(layer_noise_means)\n",
        "    plt.title(f'Epoch vs Noise Mean for LoraLayer {layer_index}')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Noise Mean')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "kqROVFxt8lk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 각 LoraLayer의 decay factor 시각화\n",
        "for i, layer_decay_factors in enumerate(zip(*decay_factor_callback.decay_factors)):\n",
        "    plt.plot(layer_decay_factors, label=f'LoraLayer {i}')\n",
        "\n",
        "plt.title('Epoch vs Decay Factor')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Decay Factor')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "id": "aeL0hLOGPDzj",
        "outputId": "13034c57-9efc-4a8e-d098-baa5d8c4942c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-c7fce5bdfdfd>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# 각 LoraLayer의 decay factor 시각화\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_decay_factors\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdecay_factor_callback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecay_factors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_decay_factors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf'LoraLayer {i}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'decay_factor_callback' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 학습을 시작합니다. 이제 검증 데이터로 val_images와 val_labels를 사용합니다.\n",
        "LoRA_modified_model.fit(train_images, train_labels,\n",
        "                        epochs=epochs,\n",
        "                        batch_size=batch_size,\n",
        "                        validation_data=(val_images, val_labels),\n",
        "                        callbacks=[print_step_callback, TestCallback((val_images,val_labels))])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wlbdj8GW2ZmO",
        "outputId": "faa1e482-21a7-49e8-fc3d-9a3208663ebe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "1596/1600 [============================>.] - ETA: 0s - loss: 1.5739 - accuracy: 0.5965\n",
            "End of epoch 1, LoraLayer 0: 7600 Step\n",
            "End of epoch 1, LoraLayer 0: Decay factor: 0.9666666984558105\n",
            "End of epoch 1, LoraLayer 1: 7600 Step\n",
            "End of epoch 1, LoraLayer 1: Decay factor: 0.9666666984558105\n",
            "End of epoch 1, LoraLayer 2: 7600 Step\n",
            "End of epoch 1, LoraLayer 2: Decay factor: 0.9666666984558105\n",
            "\n",
            "Testing loss: 2.2994821071624756, acc: 0.10441666841506958\n",
            "\n",
            "1600/1600 [==============================] - 16s 9ms/step - loss: 1.5724 - accuracy: 0.5967 - val_loss: 2.2995 - val_accuracy: 0.1044\n",
            "Epoch 2/30\n",
            "1591/1600 [============================>.] - ETA: 0s - loss: 0.7698 - accuracy: 0.7397\n",
            "End of epoch 2, LoraLayer 0: 9200 Step\n",
            "End of epoch 2, LoraLayer 0: Decay factor: 0.9333333373069763\n",
            "End of epoch 2, LoraLayer 1: 9200 Step\n",
            "End of epoch 2, LoraLayer 1: Decay factor: 0.9333333373069763\n",
            "End of epoch 2, LoraLayer 2: 9200 Step\n",
            "End of epoch 2, LoraLayer 2: Decay factor: 0.9333333373069763\n",
            "\n",
            "Testing loss: 2.2520015239715576, acc: 0.15216666460037231\n",
            "\n",
            "1600/1600 [==============================] - 9s 6ms/step - loss: 0.7697 - accuracy: 0.7396 - val_loss: 2.2520 - val_accuracy: 0.1522\n",
            "Epoch 3/30\n",
            "1590/1600 [============================>.] - ETA: 0s - loss: 0.6583 - accuracy: 0.7641\n",
            "End of epoch 3, LoraLayer 0: 10800 Step\n",
            "End of epoch 3, LoraLayer 0: Decay factor: 0.8999999761581421\n",
            "End of epoch 3, LoraLayer 1: 10800 Step\n",
            "End of epoch 3, LoraLayer 1: Decay factor: 0.8999999761581421\n",
            "End of epoch 3, LoraLayer 2: 10800 Step\n",
            "End of epoch 3, LoraLayer 2: Decay factor: 0.8999999761581421\n",
            "\n",
            "Testing loss: 2.199087142944336, acc: 0.12383333593606949\n",
            "\n",
            "1600/1600 [==============================] - 11s 7ms/step - loss: 0.6580 - accuracy: 0.7642 - val_loss: 2.1991 - val_accuracy: 0.1238\n",
            "Epoch 4/30\n",
            "1597/1600 [============================>.] - ETA: 0s - loss: 0.6176 - accuracy: 0.7778\n",
            "End of epoch 4, LoraLayer 0: 12400 Step\n",
            "End of epoch 4, LoraLayer 0: Decay factor: 0.8666666746139526\n",
            "End of epoch 4, LoraLayer 1: 12400 Step\n",
            "End of epoch 4, LoraLayer 1: Decay factor: 0.8666666746139526\n",
            "End of epoch 4, LoraLayer 2: 12400 Step\n",
            "End of epoch 4, LoraLayer 2: Decay factor: 0.8666666746139526\n",
            "\n",
            "Testing loss: 2.148958921432495, acc: 0.2213333398103714\n",
            "\n",
            "1600/1600 [==============================] - 12s 7ms/step - loss: 0.6176 - accuracy: 0.7778 - val_loss: 2.1490 - val_accuracy: 0.2213\n",
            "Epoch 5/30\n",
            "1593/1600 [============================>.] - ETA: 0s - loss: 0.5952 - accuracy: 0.7859\n",
            "End of epoch 5, LoraLayer 0: 14000 Step\n",
            "End of epoch 5, LoraLayer 0: Decay factor: 0.8333333134651184\n",
            "End of epoch 5, LoraLayer 1: 14000 Step\n",
            "End of epoch 5, LoraLayer 1: Decay factor: 0.8333333134651184\n",
            "End of epoch 5, LoraLayer 2: 14000 Step\n",
            "End of epoch 5, LoraLayer 2: Decay factor: 0.8333333134651184\n",
            "\n",
            "Testing loss: 2.1084327697753906, acc: 0.22808332741260529\n",
            "\n",
            "1600/1600 [==============================] - 11s 7ms/step - loss: 0.5955 - accuracy: 0.7859 - val_loss: 2.1084 - val_accuracy: 0.2281\n",
            "Epoch 6/30\n",
            "1590/1600 [============================>.] - ETA: 0s - loss: 0.5801 - accuracy: 0.7940\n",
            "End of epoch 6, LoraLayer 0: 15600 Step\n",
            "End of epoch 6, LoraLayer 0: Decay factor: 0.800000011920929\n",
            "End of epoch 6, LoraLayer 1: 15600 Step\n",
            "End of epoch 6, LoraLayer 1: Decay factor: 0.800000011920929\n",
            "End of epoch 6, LoraLayer 2: 15600 Step\n",
            "End of epoch 6, LoraLayer 2: Decay factor: 0.800000011920929\n",
            "\n",
            "Testing loss: 2.0509936809539795, acc: 0.23849999904632568\n",
            "\n",
            "1600/1600 [==============================] - 10s 6ms/step - loss: 0.5792 - accuracy: 0.7942 - val_loss: 2.0510 - val_accuracy: 0.2385\n",
            "Epoch 7/30\n",
            "1591/1600 [============================>.] - ETA: 0s - loss: 0.5652 - accuracy: 0.7995\n",
            "End of epoch 7, LoraLayer 0: 17200 Step\n",
            "End of epoch 7, LoraLayer 0: Decay factor: 0.7666666507720947\n",
            "End of epoch 7, LoraLayer 1: 17200 Step\n",
            "End of epoch 7, LoraLayer 1: Decay factor: 0.7666666507720947\n",
            "End of epoch 7, LoraLayer 2: 17200 Step\n",
            "End of epoch 7, LoraLayer 2: Decay factor: 0.7666666507720947\n",
            "\n",
            "Testing loss: 2.013362169265747, acc: 0.24591666460037231\n",
            "\n",
            "1600/1600 [==============================] - 12s 7ms/step - loss: 0.5651 - accuracy: 0.7995 - val_loss: 2.0134 - val_accuracy: 0.2459\n",
            "Epoch 8/30\n",
            "1594/1600 [============================>.] - ETA: 0s - loss: 0.5526 - accuracy: 0.8047\n",
            "End of epoch 8, LoraLayer 0: 18800 Step\n",
            "End of epoch 8, LoraLayer 0: Decay factor: 0.7333333492279053\n",
            "End of epoch 8, LoraLayer 1: 18800 Step\n",
            "End of epoch 8, LoraLayer 1: Decay factor: 0.7333333492279053\n",
            "End of epoch 8, LoraLayer 2: 18800 Step\n",
            "End of epoch 8, LoraLayer 2: Decay factor: 0.7333333492279053\n",
            "\n",
            "Testing loss: 1.972048282623291, acc: 0.24791666865348816\n",
            "\n",
            "1600/1600 [==============================] - 11s 7ms/step - loss: 0.5529 - accuracy: 0.8046 - val_loss: 1.9720 - val_accuracy: 0.2479\n",
            "Epoch 9/30\n",
            "1597/1600 [============================>.] - ETA: 0s - loss: 0.5420 - accuracy: 0.8097\n",
            "End of epoch 9, LoraLayer 0: 20400 Step\n",
            "End of epoch 9, LoraLayer 0: Decay factor: 0.7000000476837158\n",
            "End of epoch 9, LoraLayer 1: 20400 Step\n",
            "End of epoch 9, LoraLayer 1: Decay factor: 0.7000000476837158\n",
            "End of epoch 9, LoraLayer 2: 20400 Step\n",
            "End of epoch 9, LoraLayer 2: Decay factor: 0.7000000476837158\n",
            "\n",
            "Testing loss: 1.898695468902588, acc: 0.2775000035762787\n",
            "\n",
            "1600/1600 [==============================] - 11s 7ms/step - loss: 0.5423 - accuracy: 0.8097 - val_loss: 1.8987 - val_accuracy: 0.2775\n",
            "Epoch 10/30\n",
            "1597/1600 [============================>.] - ETA: 0s - loss: 0.5327 - accuracy: 0.8125\n",
            "End of epoch 10, LoraLayer 0: 22000 Step\n",
            "End of epoch 10, LoraLayer 0: Decay factor: 0.6666666269302368\n",
            "End of epoch 10, LoraLayer 1: 22000 Step\n",
            "End of epoch 10, LoraLayer 1: Decay factor: 0.6666666269302368\n",
            "End of epoch 10, LoraLayer 2: 22000 Step\n",
            "End of epoch 10, LoraLayer 2: Decay factor: 0.6666666269302368\n",
            "\n",
            "Testing loss: 1.81147301197052, acc: 0.31200000643730164\n",
            "\n",
            "1600/1600 [==============================] - 14s 9ms/step - loss: 0.5327 - accuracy: 0.8125 - val_loss: 1.8115 - val_accuracy: 0.3120\n",
            "Epoch 11/30\n",
            "1599/1600 [============================>.] - ETA: 0s - loss: 0.5244 - accuracy: 0.8154\n",
            "End of epoch 11, LoraLayer 0: 23600 Step\n",
            "End of epoch 11, LoraLayer 0: Decay factor: 0.6333333253860474\n",
            "End of epoch 11, LoraLayer 1: 23600 Step\n",
            "End of epoch 11, LoraLayer 1: Decay factor: 0.6333333253860474\n",
            "End of epoch 11, LoraLayer 2: 23600 Step\n",
            "End of epoch 11, LoraLayer 2: Decay factor: 0.6333333253860474\n",
            "\n",
            "Testing loss: 1.7425532341003418, acc: 0.3785833418369293\n",
            "\n",
            "1600/1600 [==============================] - 11s 7ms/step - loss: 0.5243 - accuracy: 0.8154 - val_loss: 1.7426 - val_accuracy: 0.3786\n",
            "Epoch 12/30\n",
            "1597/1600 [============================>.] - ETA: 0s - loss: 0.5165 - accuracy: 0.8189\n",
            "End of epoch 12, LoraLayer 0: 25200 Step\n",
            "End of epoch 12, LoraLayer 0: Decay factor: 0.6000000238418579\n",
            "End of epoch 12, LoraLayer 1: 25200 Step\n",
            "End of epoch 12, LoraLayer 1: Decay factor: 0.6000000238418579\n",
            "End of epoch 12, LoraLayer 2: 25200 Step\n",
            "End of epoch 12, LoraLayer 2: Decay factor: 0.6000000238418579\n",
            "\n",
            "Testing loss: 1.6471558809280396, acc: 0.437583327293396\n",
            "\n",
            "1600/1600 [==============================] - 11s 7ms/step - loss: 0.5163 - accuracy: 0.8189 - val_loss: 1.6472 - val_accuracy: 0.4376\n",
            "Epoch 13/30\n",
            "1597/1600 [============================>.] - ETA: 0s - loss: 0.5084 - accuracy: 0.8207\n",
            "End of epoch 13, LoraLayer 0: 26800 Step\n",
            "End of epoch 13, LoraLayer 0: Decay factor: 0.5666666626930237\n",
            "End of epoch 13, LoraLayer 1: 26800 Step\n",
            "End of epoch 13, LoraLayer 1: Decay factor: 0.5666666626930237\n",
            "End of epoch 13, LoraLayer 2: 26800 Step\n",
            "End of epoch 13, LoraLayer 2: Decay factor: 0.5666666626930237\n",
            "\n",
            "Testing loss: 1.5682746171951294, acc: 0.47058331966400146\n",
            "\n",
            "1600/1600 [==============================] - 11s 7ms/step - loss: 0.5081 - accuracy: 0.8209 - val_loss: 1.5683 - val_accuracy: 0.4706\n",
            "Epoch 14/30\n",
            "1600/1600 [==============================] - ETA: 0s - loss: 0.5010 - accuracy: 0.8249\n",
            "End of epoch 14, LoraLayer 0: 28400 Step\n",
            "End of epoch 14, LoraLayer 0: Decay factor: 0.5333333015441895\n",
            "End of epoch 14, LoraLayer 1: 28400 Step\n",
            "End of epoch 14, LoraLayer 1: Decay factor: 0.5333333015441895\n",
            "End of epoch 14, LoraLayer 2: 28400 Step\n",
            "End of epoch 14, LoraLayer 2: Decay factor: 0.5333333015441895\n",
            "\n",
            "Testing loss: 1.5215967893600464, acc: 0.476749986410141\n",
            "\n",
            "1600/1600 [==============================] - 10s 6ms/step - loss: 0.5010 - accuracy: 0.8249 - val_loss: 1.5216 - val_accuracy: 0.4767\n",
            "Epoch 15/30\n",
            "1597/1600 [============================>.] - ETA: 0s - loss: 0.4950 - accuracy: 0.8265\n",
            "End of epoch 15, LoraLayer 0: 30000 Step\n",
            "End of epoch 15, LoraLayer 0: Decay factor: 0.5\n",
            "End of epoch 15, LoraLayer 1: 30000 Step\n",
            "End of epoch 15, LoraLayer 1: Decay factor: 0.5\n",
            "End of epoch 15, LoraLayer 2: 30000 Step\n",
            "End of epoch 15, LoraLayer 2: Decay factor: 0.5\n",
            "\n",
            "Testing loss: 1.424595832824707, acc: 0.5094166398048401\n",
            "\n",
            "1600/1600 [==============================] - 11s 7ms/step - loss: 0.4949 - accuracy: 0.8265 - val_loss: 1.4246 - val_accuracy: 0.5094\n",
            "Epoch 16/30\n",
            "1599/1600 [============================>.] - ETA: 0s - loss: 0.4883 - accuracy: 0.8288\n",
            "End of epoch 16, LoraLayer 0: 31600 Step\n",
            "End of epoch 16, LoraLayer 0: Decay factor: 0.46666669845581055\n",
            "End of epoch 16, LoraLayer 1: 31600 Step\n",
            "End of epoch 16, LoraLayer 1: Decay factor: 0.46666669845581055\n",
            "End of epoch 16, LoraLayer 2: 31600 Step\n",
            "End of epoch 16, LoraLayer 2: Decay factor: 0.46666669845581055\n",
            "\n",
            "Testing loss: 1.3758189678192139, acc: 0.5286666750907898\n",
            "\n",
            "1600/1600 [==============================] - 13s 8ms/step - loss: 0.4884 - accuracy: 0.8287 - val_loss: 1.3758 - val_accuracy: 0.5287\n",
            "Epoch 17/30\n",
            "1597/1600 [============================>.] - ETA: 0s - loss: 0.4836 - accuracy: 0.8305\n",
            "End of epoch 17, LoraLayer 0: 33200 Step\n",
            "End of epoch 17, LoraLayer 0: Decay factor: 0.4333333373069763\n",
            "End of epoch 17, LoraLayer 1: 33200 Step\n",
            "End of epoch 17, LoraLayer 1: Decay factor: 0.4333333373069763\n",
            "End of epoch 17, LoraLayer 2: 33200 Step\n",
            "End of epoch 17, LoraLayer 2: Decay factor: 0.4333333373069763\n",
            "\n",
            "Testing loss: 1.310917615890503, acc: 0.5412499904632568\n",
            "\n",
            "1600/1600 [==============================] - 10s 6ms/step - loss: 0.4836 - accuracy: 0.8305 - val_loss: 1.3109 - val_accuracy: 0.5412\n",
            "Epoch 18/30\n",
            "1595/1600 [============================>.] - ETA: 0s - loss: 0.4793 - accuracy: 0.8325\n",
            "End of epoch 18, LoraLayer 0: 34800 Step\n",
            "End of epoch 18, LoraLayer 0: Decay factor: 0.40000003576278687\n",
            "End of epoch 18, LoraLayer 1: 34800 Step\n",
            "End of epoch 18, LoraLayer 1: Decay factor: 0.40000003576278687\n",
            "End of epoch 18, LoraLayer 2: 34800 Step\n",
            "End of epoch 18, LoraLayer 2: Decay factor: 0.40000003576278687\n",
            "\n",
            "Testing loss: 1.2435252666473389, acc: 0.5585833191871643\n",
            "\n",
            "1600/1600 [==============================] - 10s 7ms/step - loss: 0.4794 - accuracy: 0.8325 - val_loss: 1.2435 - val_accuracy: 0.5586\n",
            "Epoch 19/30\n",
            "1597/1600 [============================>.] - ETA: 0s - loss: 0.4751 - accuracy: 0.8337\n",
            "End of epoch 19, LoraLayer 0: 36400 Step\n",
            "End of epoch 19, LoraLayer 0: Decay factor: 0.36666667461395264\n",
            "End of epoch 19, LoraLayer 1: 36400 Step\n",
            "End of epoch 19, LoraLayer 1: Decay factor: 0.36666667461395264\n",
            "End of epoch 19, LoraLayer 2: 36400 Step\n",
            "End of epoch 19, LoraLayer 2: Decay factor: 0.36666667461395264\n",
            "\n",
            "Testing loss: 1.2150391340255737, acc: 0.5667499899864197\n",
            "\n",
            "1600/1600 [==============================] - 13s 8ms/step - loss: 0.4752 - accuracy: 0.8337 - val_loss: 1.2150 - val_accuracy: 0.5667\n",
            "Epoch 20/30\n",
            "1588/1600 [============================>.] - ETA: 0s - loss: 0.4717 - accuracy: 0.8346\n",
            "End of epoch 20, LoraLayer 0: 38000 Step\n",
            "End of epoch 20, LoraLayer 0: Decay factor: 0.3333333134651184\n",
            "End of epoch 20, LoraLayer 1: 38000 Step\n",
            "End of epoch 20, LoraLayer 1: Decay factor: 0.3333333134651184\n",
            "End of epoch 20, LoraLayer 2: 38000 Step\n",
            "End of epoch 20, LoraLayer 2: Decay factor: 0.3333333134651184\n",
            "\n",
            "Testing loss: 1.1405442953109741, acc: 0.5873333215713501\n",
            "\n",
            "1600/1600 [==============================] - 11s 7ms/step - loss: 0.4718 - accuracy: 0.8346 - val_loss: 1.1405 - val_accuracy: 0.5873\n",
            "Epoch 21/30\n",
            "1595/1600 [============================>.] - ETA: 0s - loss: 0.4697 - accuracy: 0.8364\n",
            "End of epoch 21, LoraLayer 0: 39600 Step\n",
            "End of epoch 21, LoraLayer 0: Decay factor: 0.30000001192092896\n",
            "End of epoch 21, LoraLayer 1: 39600 Step\n",
            "End of epoch 21, LoraLayer 1: Decay factor: 0.30000001192092896\n",
            "End of epoch 21, LoraLayer 2: 39600 Step\n",
            "End of epoch 21, LoraLayer 2: Decay factor: 0.30000001192092896\n",
            "\n",
            "Testing loss: 1.0843788385391235, acc: 0.6147500276565552\n",
            "\n",
            "1600/1600 [==============================] - 10s 6ms/step - loss: 0.4694 - accuracy: 0.8365 - val_loss: 1.0844 - val_accuracy: 0.6148\n",
            "Epoch 22/30\n",
            "1600/1600 [==============================] - ETA: 0s - loss: 0.4660 - accuracy: 0.8371\n",
            "End of epoch 22, LoraLayer 0: 41200 Step\n",
            "End of epoch 22, LoraLayer 0: Decay factor: 0.2666666507720947\n",
            "End of epoch 22, LoraLayer 1: 41200 Step\n",
            "End of epoch 22, LoraLayer 1: Decay factor: 0.2666666507720947\n",
            "End of epoch 22, LoraLayer 2: 41200 Step\n",
            "End of epoch 22, LoraLayer 2: Decay factor: 0.2666666507720947\n",
            "\n",
            "Testing loss: 0.9718385934829712, acc: 0.653333306312561\n",
            "\n",
            "1600/1600 [==============================] - 11s 7ms/step - loss: 0.4660 - accuracy: 0.8371 - val_loss: 0.9718 - val_accuracy: 0.6533\n",
            "Epoch 23/30\n",
            "1592/1600 [============================>.] - ETA: 0s - loss: 0.4645 - accuracy: 0.8378\n",
            "End of epoch 23, LoraLayer 0: 42800 Step\n",
            "End of epoch 23, LoraLayer 0: Decay factor: 0.23333334922790527\n",
            "End of epoch 23, LoraLayer 1: 42800 Step\n",
            "End of epoch 23, LoraLayer 1: Decay factor: 0.23333334922790527\n",
            "End of epoch 23, LoraLayer 2: 42800 Step\n",
            "End of epoch 23, LoraLayer 2: Decay factor: 0.23333334922790527\n",
            "\n",
            "Testing loss: 0.9251760244369507, acc: 0.6683333516120911\n",
            "\n",
            "1600/1600 [==============================] - 11s 7ms/step - loss: 0.4645 - accuracy: 0.8379 - val_loss: 0.9252 - val_accuracy: 0.6683\n",
            "Epoch 24/30\n",
            "1598/1600 [============================>.] - ETA: 0s - loss: 0.4630 - accuracy: 0.8391\n",
            "End of epoch 24, LoraLayer 0: 44400 Step\n",
            "End of epoch 24, LoraLayer 0: Decay factor: 0.19999998807907104\n",
            "End of epoch 24, LoraLayer 1: 44400 Step\n",
            "End of epoch 24, LoraLayer 1: Decay factor: 0.19999998807907104\n",
            "End of epoch 24, LoraLayer 2: 44400 Step\n",
            "End of epoch 24, LoraLayer 2: Decay factor: 0.19999998807907104\n",
            "\n",
            "Testing loss: 0.891757071018219, acc: 0.6815833449363708\n",
            "\n",
            "1600/1600 [==============================] - 10s 6ms/step - loss: 0.4628 - accuracy: 0.8391 - val_loss: 0.8918 - val_accuracy: 0.6816\n",
            "Epoch 25/30\n",
            "1591/1600 [============================>.] - ETA: 0s - loss: 0.4625 - accuracy: 0.8396\n",
            "End of epoch 25, LoraLayer 0: 46000 Step\n",
            "End of epoch 25, LoraLayer 0: Decay factor: 0.1666666865348816\n",
            "End of epoch 25, LoraLayer 1: 46000 Step\n",
            "End of epoch 25, LoraLayer 1: Decay factor: 0.1666666865348816\n",
            "End of epoch 25, LoraLayer 2: 46000 Step\n",
            "End of epoch 25, LoraLayer 2: Decay factor: 0.1666666865348816\n",
            "\n",
            "Testing loss: 0.8194338083267212, acc: 0.7108333110809326\n",
            "\n",
            "1600/1600 [==============================] - 11s 7ms/step - loss: 0.4619 - accuracy: 0.8399 - val_loss: 0.8194 - val_accuracy: 0.7108\n",
            "Epoch 26/30\n",
            "1599/1600 [============================>.] - ETA: 0s - loss: 0.4598 - accuracy: 0.8407\n",
            "End of epoch 26, LoraLayer 0: 47600 Step\n",
            "End of epoch 26, LoraLayer 0: Decay factor: 0.13333332538604736\n",
            "End of epoch 26, LoraLayer 1: 47600 Step\n",
            "End of epoch 26, LoraLayer 1: Decay factor: 0.13333332538604736\n",
            "End of epoch 26, LoraLayer 2: 47600 Step\n",
            "End of epoch 26, LoraLayer 2: Decay factor: 0.13333332538604736\n",
            "\n",
            "Testing loss: 0.7911205887794495, acc: 0.722083330154419\n",
            "\n",
            "1600/1600 [==============================] - 12s 8ms/step - loss: 0.4596 - accuracy: 0.8408 - val_loss: 0.7911 - val_accuracy: 0.7221\n",
            "Epoch 27/30\n",
            "1588/1600 [============================>.] - ETA: 0s - loss: 0.4591 - accuracy: 0.8407\n",
            "End of epoch 27, LoraLayer 0: 49200 Step\n",
            "End of epoch 27, LoraLayer 0: Decay factor: 0.10000002384185791\n",
            "End of epoch 27, LoraLayer 1: 49200 Step\n",
            "End of epoch 27, LoraLayer 1: Decay factor: 0.10000002384185791\n",
            "End of epoch 27, LoraLayer 2: 49200 Step\n",
            "End of epoch 27, LoraLayer 2: Decay factor: 0.10000002384185791\n",
            "\n",
            "Testing loss: 0.700191080570221, acc: 0.7519166469573975\n",
            "\n",
            "1600/1600 [==============================] - 9s 6ms/step - loss: 0.4595 - accuracy: 0.8406 - val_loss: 0.7002 - val_accuracy: 0.7519\n",
            "Epoch 28/30\n",
            "1598/1600 [============================>.] - ETA: 0s - loss: 0.4596 - accuracy: 0.8413\n",
            "End of epoch 28, LoraLayer 0: 50800 Step\n",
            "End of epoch 28, LoraLayer 0: Decay factor: 0.06666666269302368\n",
            "End of epoch 28, LoraLayer 1: 50800 Step\n",
            "End of epoch 28, LoraLayer 1: Decay factor: 0.06666666269302368\n",
            "End of epoch 28, LoraLayer 2: 50800 Step\n",
            "End of epoch 28, LoraLayer 2: Decay factor: 0.06666666269302368\n",
            "\n",
            "Testing loss: 0.6162658333778381, acc: 0.7806666493415833\n",
            "\n",
            "1600/1600 [==============================] - 12s 7ms/step - loss: 0.4596 - accuracy: 0.8414 - val_loss: 0.6163 - val_accuracy: 0.7807\n",
            "Epoch 29/30\n",
            "1597/1600 [============================>.] - ETA: 0s - loss: 0.4619 - accuracy: 0.8412\n",
            "End of epoch 29, LoraLayer 0: 52400 Step\n",
            "End of epoch 29, LoraLayer 0: Decay factor: 0.03333336114883423\n",
            "End of epoch 29, LoraLayer 1: 52400 Step\n",
            "End of epoch 29, LoraLayer 1: Decay factor: 0.03333336114883423\n",
            "End of epoch 29, LoraLayer 2: 52400 Step\n",
            "End of epoch 29, LoraLayer 2: Decay factor: 0.03333336114883423\n",
            "\n",
            "Testing loss: 0.5594247579574585, acc: 0.8003333210945129\n",
            "\n",
            "1600/1600 [==============================] - 14s 8ms/step - loss: 0.4620 - accuracy: 0.8412 - val_loss: 0.5594 - val_accuracy: 0.8003\n",
            "Epoch 30/30\n",
            "1598/1600 [============================>.] - ETA: 0s - loss: 0.4723 - accuracy: 0.8367\n",
            "End of epoch 30, LoraLayer 0: 54000 Step\n",
            "End of epoch 30, LoraLayer 0: Decay factor: 0.0\n",
            "End of epoch 30, LoraLayer 1: 54000 Step\n",
            "End of epoch 30, LoraLayer 1: Decay factor: 0.0\n",
            "End of epoch 30, LoraLayer 2: 54000 Step\n",
            "End of epoch 30, LoraLayer 2: Decay factor: 0.0\n",
            "\n",
            "Testing loss: 0.5013642907142639, acc: 0.8236666917800903\n",
            "\n",
            "1600/1600 [==============================] - 12s 7ms/step - loss: 0.4723 - accuracy: 0.8367 - val_loss: 0.5014 - val_accuracy: 0.8237\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x78b083fd78e0>"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = LoRA_modified_model.evaluate(test_images,  test_labels, verbose=2)\n",
        "\n",
        "print('\\nTest accuracy:', test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-KeyZ8Ju510N",
        "outputId": "52e0f503-5050-4ec2-b1a2-b32b5e5af091"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 - 3s - loss: 0.6926 - accuracy: 0.7622 - 3s/epoch - 9ms/step\n",
            "\n",
            "Test accuracy: 0.7621999979019165\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## lora ouput decay factor 로 나누지 않음.  "
      ],
      "metadata": {
        "id": "LlH5aSPh2akN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "현상도 유사함  Epoch 24/30"
      ],
      "metadata": {
        "id": "Mwze5D9g60YB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 학습을 시작합니다. 이제 검증 데이터로 val_images와 val_labels를 사용합니다.\n",
        "LoRA_modified_model.fit(train_images, train_labels,\n",
        "                        epochs=epochs,\n",
        "                        batch_size=batch_size,\n",
        "                        validation_data=(val_images, val_labels),\n",
        "                        callbacks=[print_step_callback, TestCallback((val_images,val_labels))])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6z2uJihusRoA",
        "outputId": "c3dd01da-7cfc-4349-c7de-131ee7359a3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "1590/1600 [============================>.] - ETA: 0s - loss: 0.1343 - accuracy: 0.9473\n",
            "End of epoch 1, LoraLayer 0: 7600 Step\n",
            "End of epoch 1, LoraLayer 0: Decay factor: 0.9666666984558105\n",
            "End of epoch 1, LoraLayer 1: 7600 Step\n",
            "End of epoch 1, LoraLayer 1: Decay factor: 0.9666666984558105\n",
            "End of epoch 1, LoraLayer 2: 7600 Step\n",
            "End of epoch 1, LoraLayer 2: Decay factor: 0.9666666984558105\n",
            "\n",
            "Testing loss: 2.3026435375213623, acc: 0.11733333021402359\n",
            "\n",
            "1600/1600 [==============================] - 12s 6ms/step - loss: 0.1342 - accuracy: 0.9475 - val_loss: 2.3026 - val_accuracy: 0.1173\n",
            "Epoch 2/30\n",
            "1596/1600 [============================>.] - ETA: 0s - loss: 0.1285 - accuracy: 0.9501\n",
            "End of epoch 2, LoraLayer 0: 9200 Step\n",
            "End of epoch 2, LoraLayer 0: Decay factor: 0.9333333373069763\n",
            "End of epoch 2, LoraLayer 1: 9200 Step\n",
            "End of epoch 2, LoraLayer 1: Decay factor: 0.9333333373069763\n",
            "End of epoch 2, LoraLayer 2: 9200 Step\n",
            "End of epoch 2, LoraLayer 2: Decay factor: 0.9333333373069763\n",
            "\n",
            "Testing loss: 2.3026819229125977, acc: 0.10849999636411667\n",
            "\n",
            "1600/1600 [==============================] - 12s 7ms/step - loss: 0.1285 - accuracy: 0.9501 - val_loss: 2.3027 - val_accuracy: 0.1085\n",
            "Epoch 3/30\n",
            "1597/1600 [============================>.] - ETA: 0s - loss: 0.1272 - accuracy: 0.9512\n",
            "End of epoch 3, LoraLayer 0: 10800 Step\n",
            "End of epoch 3, LoraLayer 0: Decay factor: 0.8999999761581421\n",
            "End of epoch 3, LoraLayer 1: 10800 Step\n",
            "End of epoch 3, LoraLayer 1: Decay factor: 0.8999999761581421\n",
            "End of epoch 3, LoraLayer 2: 10800 Step\n",
            "End of epoch 3, LoraLayer 2: Decay factor: 0.8999999761581421\n",
            "\n",
            "Testing loss: 2.3027398586273193, acc: 0.16699999570846558\n",
            "\n",
            "1600/1600 [==============================] - 11s 7ms/step - loss: 0.1271 - accuracy: 0.9513 - val_loss: 2.3027 - val_accuracy: 0.1670\n",
            "Epoch 4/30\n",
            "1589/1600 [============================>.] - ETA: 0s - loss: 0.1268 - accuracy: 0.9520\n",
            "End of epoch 4, LoraLayer 0: 12400 Step\n",
            "End of epoch 4, LoraLayer 0: Decay factor: 0.8666666746139526\n",
            "End of epoch 4, LoraLayer 1: 12400 Step\n",
            "End of epoch 4, LoraLayer 1: Decay factor: 0.8666666746139526\n",
            "End of epoch 4, LoraLayer 2: 12400 Step\n",
            "End of epoch 4, LoraLayer 2: Decay factor: 0.8666666746139526\n",
            "\n",
            "Testing loss: 2.3031368255615234, acc: 0.13099999725818634\n",
            "\n",
            "1600/1600 [==============================] - 9s 6ms/step - loss: 0.1265 - accuracy: 0.9521 - val_loss: 2.3031 - val_accuracy: 0.1310\n",
            "Epoch 5/30\n",
            "1596/1600 [============================>.] - ETA: 0s - loss: 0.1276 - accuracy: 0.9518\n",
            "End of epoch 5, LoraLayer 0: 14000 Step\n",
            "End of epoch 5, LoraLayer 0: Decay factor: 0.8333333134651184\n",
            "End of epoch 5, LoraLayer 1: 14000 Step\n",
            "End of epoch 5, LoraLayer 1: Decay factor: 0.8333333134651184\n",
            "End of epoch 5, LoraLayer 2: 14000 Step\n",
            "End of epoch 5, LoraLayer 2: Decay factor: 0.8333333134651184\n",
            "\n",
            "Testing loss: 2.3028392791748047, acc: 0.12983334064483643\n",
            "\n",
            "1600/1600 [==============================] - 12s 7ms/step - loss: 0.1277 - accuracy: 0.9518 - val_loss: 2.3028 - val_accuracy: 0.1298\n",
            "Epoch 6/30\n",
            "1593/1600 [============================>.] - ETA: 0s - loss: 0.1294 - accuracy: 0.9514\n",
            "End of epoch 6, LoraLayer 0: 15600 Step\n",
            "End of epoch 6, LoraLayer 0: Decay factor: 0.800000011920929\n",
            "End of epoch 6, LoraLayer 1: 15600 Step\n",
            "End of epoch 6, LoraLayer 1: Decay factor: 0.800000011920929\n",
            "End of epoch 6, LoraLayer 2: 15600 Step\n",
            "End of epoch 6, LoraLayer 2: Decay factor: 0.800000011920929\n",
            "\n",
            "Testing loss: 2.303687334060669, acc: 0.12349999696016312\n",
            "\n",
            "1600/1600 [==============================] - 11s 7ms/step - loss: 0.1293 - accuracy: 0.9515 - val_loss: 2.3037 - val_accuracy: 0.1235\n",
            "Epoch 7/30\n",
            "1593/1600 [============================>.] - ETA: 0s - loss: 0.1311 - accuracy: 0.9510\n",
            "End of epoch 7, LoraLayer 0: 17200 Step\n",
            "End of epoch 7, LoraLayer 0: Decay factor: 0.7666666507720947\n",
            "End of epoch 7, LoraLayer 1: 17200 Step\n",
            "End of epoch 7, LoraLayer 1: Decay factor: 0.7666666507720947\n",
            "End of epoch 7, LoraLayer 2: 17200 Step\n",
            "End of epoch 7, LoraLayer 2: Decay factor: 0.7666666507720947\n",
            "\n",
            "Testing loss: 2.3052356243133545, acc: 0.09616667032241821\n",
            "\n",
            "1600/1600 [==============================] - 9s 6ms/step - loss: 0.1310 - accuracy: 0.9511 - val_loss: 2.3052 - val_accuracy: 0.0962\n",
            "Epoch 8/30\n",
            "1594/1600 [============================>.] - ETA: 0s - loss: 0.1327 - accuracy: 0.9509\n",
            "End of epoch 8, LoraLayer 0: 18800 Step\n",
            "End of epoch 8, LoraLayer 0: Decay factor: 0.7333333492279053\n",
            "End of epoch 8, LoraLayer 1: 18800 Step\n",
            "End of epoch 8, LoraLayer 1: Decay factor: 0.7333333492279053\n",
            "End of epoch 8, LoraLayer 2: 18800 Step\n",
            "End of epoch 8, LoraLayer 2: Decay factor: 0.7333333492279053\n",
            "\n",
            "Testing loss: 2.308640241622925, acc: 0.09391666948795319\n",
            "\n",
            "1600/1600 [==============================] - 12s 8ms/step - loss: 0.1328 - accuracy: 0.9507 - val_loss: 2.3086 - val_accuracy: 0.0939\n",
            "Epoch 9/30\n",
            "1599/1600 [============================>.] - ETA: 0s - loss: 0.1345 - accuracy: 0.9508\n",
            "End of epoch 9, LoraLayer 0: 20400 Step\n",
            "End of epoch 9, LoraLayer 0: Decay factor: 0.7000000476837158\n",
            "End of epoch 9, LoraLayer 1: 20400 Step\n",
            "End of epoch 9, LoraLayer 1: Decay factor: 0.7000000476837158\n",
            "End of epoch 9, LoraLayer 2: 20400 Step\n",
            "End of epoch 9, LoraLayer 2: Decay factor: 0.7000000476837158\n",
            "\n",
            "Testing loss: 2.3159024715423584, acc: 0.10925000160932541\n",
            "\n",
            "1600/1600 [==============================] - 12s 8ms/step - loss: 0.1345 - accuracy: 0.9508 - val_loss: 2.3159 - val_accuracy: 0.1093\n",
            "Epoch 10/30\n",
            "1596/1600 [============================>.] - ETA: 0s - loss: 0.1368 - accuracy: 0.9504\n",
            "End of epoch 10, LoraLayer 0: 22000 Step\n",
            "End of epoch 10, LoraLayer 0: Decay factor: 0.6666666269302368\n",
            "End of epoch 10, LoraLayer 1: 22000 Step\n",
            "End of epoch 10, LoraLayer 1: Decay factor: 0.6666666269302368\n",
            "End of epoch 10, LoraLayer 2: 22000 Step\n",
            "End of epoch 10, LoraLayer 2: Decay factor: 0.6666666269302368\n",
            "\n",
            "Testing loss: 2.3224828243255615, acc: 0.07575000077486038\n",
            "\n",
            "1600/1600 [==============================] - 9s 6ms/step - loss: 0.1370 - accuracy: 0.9504 - val_loss: 2.3225 - val_accuracy: 0.0758\n",
            "Epoch 11/30\n",
            "1596/1600 [============================>.] - ETA: 0s - loss: 0.1394 - accuracy: 0.9497\n",
            "End of epoch 11, LoraLayer 0: 23600 Step\n",
            "End of epoch 11, LoraLayer 0: Decay factor: 0.6333333253860474\n",
            "End of epoch 11, LoraLayer 1: 23600 Step\n",
            "End of epoch 11, LoraLayer 1: Decay factor: 0.6333333253860474\n",
            "End of epoch 11, LoraLayer 2: 23600 Step\n",
            "End of epoch 11, LoraLayer 2: Decay factor: 0.6333333253860474\n",
            "\n",
            "Testing loss: 2.3279125690460205, acc: 0.0949999988079071\n",
            "\n",
            "1600/1600 [==============================] - 11s 7ms/step - loss: 0.1394 - accuracy: 0.9497 - val_loss: 2.3279 - val_accuracy: 0.0950\n",
            "Epoch 12/30\n",
            "1596/1600 [============================>.] - ETA: 0s - loss: 0.1417 - accuracy: 0.9479\n",
            "End of epoch 12, LoraLayer 0: 25200 Step\n",
            "End of epoch 12, LoraLayer 0: Decay factor: 0.6000000238418579\n",
            "End of epoch 12, LoraLayer 1: 25200 Step\n",
            "End of epoch 12, LoraLayer 1: Decay factor: 0.6000000238418579\n",
            "End of epoch 12, LoraLayer 2: 25200 Step\n",
            "End of epoch 12, LoraLayer 2: Decay factor: 0.6000000238418579\n",
            "\n",
            "Testing loss: 2.3298840522766113, acc: 0.1014999970793724\n",
            "\n",
            "1600/1600 [==============================] - 11s 7ms/step - loss: 0.1418 - accuracy: 0.9479 - val_loss: 2.3299 - val_accuracy: 0.1015\n",
            "Epoch 13/30\n",
            "1590/1600 [============================>.] - ETA: 0s - loss: 0.1442 - accuracy: 0.9476\n",
            "End of epoch 13, LoraLayer 0: 26800 Step\n",
            "End of epoch 13, LoraLayer 0: Decay factor: 0.5666666626930237\n",
            "End of epoch 13, LoraLayer 1: 26800 Step\n",
            "End of epoch 13, LoraLayer 1: Decay factor: 0.5666666626930237\n",
            "End of epoch 13, LoraLayer 2: 26800 Step\n",
            "End of epoch 13, LoraLayer 2: Decay factor: 0.5666666626930237\n",
            "\n",
            "Testing loss: 2.3433449268341064, acc: 0.04058333486318588\n",
            "\n",
            "1600/1600 [==============================] - 11s 7ms/step - loss: 0.1441 - accuracy: 0.9476 - val_loss: 2.3433 - val_accuracy: 0.0406\n",
            "Epoch 14/30\n",
            "1594/1600 [============================>.] - ETA: 0s - loss: 0.1466 - accuracy: 0.9479\n",
            "End of epoch 14, LoraLayer 0: 28400 Step\n",
            "End of epoch 14, LoraLayer 0: Decay factor: 0.5333333015441895\n",
            "End of epoch 14, LoraLayer 1: 28400 Step\n",
            "End of epoch 14, LoraLayer 1: Decay factor: 0.5333333015441895\n",
            "End of epoch 14, LoraLayer 2: 28400 Step\n",
            "End of epoch 14, LoraLayer 2: Decay factor: 0.5333333015441895\n",
            "\n",
            "Testing loss: 2.354343891143799, acc: 0.04100000113248825\n",
            "\n",
            "1600/1600 [==============================] - 12s 8ms/step - loss: 0.1466 - accuracy: 0.9479 - val_loss: 2.3543 - val_accuracy: 0.0410\n",
            "Epoch 15/30\n",
            "1593/1600 [============================>.] - ETA: 0s - loss: 0.1499 - accuracy: 0.9458\n",
            "End of epoch 15, LoraLayer 0: 30000 Step\n",
            "End of epoch 15, LoraLayer 0: Decay factor: 0.5\n",
            "End of epoch 15, LoraLayer 1: 30000 Step\n",
            "End of epoch 15, LoraLayer 1: Decay factor: 0.5\n",
            "End of epoch 15, LoraLayer 2: 30000 Step\n",
            "End of epoch 15, LoraLayer 2: Decay factor: 0.5\n",
            "\n",
            "Testing loss: 2.369128942489624, acc: 0.04874999821186066\n",
            "\n",
            "1600/1600 [==============================] - 12s 7ms/step - loss: 0.1497 - accuracy: 0.9459 - val_loss: 2.3691 - val_accuracy: 0.0487\n",
            "Epoch 16/30\n",
            "1588/1600 [============================>.] - ETA: 0s - loss: 0.1528 - accuracy: 0.9456\n",
            "End of epoch 16, LoraLayer 0: 31600 Step\n",
            "End of epoch 16, LoraLayer 0: Decay factor: 0.46666669845581055\n",
            "End of epoch 16, LoraLayer 1: 31600 Step\n",
            "End of epoch 16, LoraLayer 1: Decay factor: 0.46666669845581055\n",
            "End of epoch 16, LoraLayer 2: 31600 Step\n",
            "End of epoch 16, LoraLayer 2: Decay factor: 0.46666669845581055\n",
            "\n",
            "Testing loss: 2.388929843902588, acc: 0.021416665986180305\n",
            "\n",
            "1600/1600 [==============================] - 9s 5ms/step - loss: 0.1527 - accuracy: 0.9456 - val_loss: 2.3889 - val_accuracy: 0.0214\n",
            "Epoch 17/30\n",
            "1599/1600 [============================>.] - ETA: 0s - loss: 0.1553 - accuracy: 0.9449\n",
            "End of epoch 17, LoraLayer 0: 33200 Step\n",
            "End of epoch 17, LoraLayer 0: Decay factor: 0.4333333373069763\n",
            "End of epoch 17, LoraLayer 1: 33200 Step\n",
            "End of epoch 17, LoraLayer 1: Decay factor: 0.4333333373069763\n",
            "End of epoch 17, LoraLayer 2: 33200 Step\n",
            "End of epoch 17, LoraLayer 2: Decay factor: 0.4333333373069763\n",
            "\n",
            "Testing loss: 2.4085991382598877, acc: 0.05883333459496498\n",
            "\n",
            "1600/1600 [==============================] - 12s 7ms/step - loss: 0.1553 - accuracy: 0.9449 - val_loss: 2.4086 - val_accuracy: 0.0588\n",
            "Epoch 18/30\n",
            "1598/1600 [============================>.] - ETA: 0s - loss: 0.1587 - accuracy: 0.9431\n",
            "End of epoch 18, LoraLayer 0: 34800 Step\n",
            "End of epoch 18, LoraLayer 0: Decay factor: 0.40000003576278687\n",
            "End of epoch 18, LoraLayer 1: 34800 Step\n",
            "End of epoch 18, LoraLayer 1: Decay factor: 0.40000003576278687\n",
            "End of epoch 18, LoraLayer 2: 34800 Step\n",
            "End of epoch 18, LoraLayer 2: Decay factor: 0.40000003576278687\n",
            "\n",
            "Testing loss: 2.397627115249634, acc: 0.05249999836087227\n",
            "\n",
            "1600/1600 [==============================] - 12s 7ms/step - loss: 0.1587 - accuracy: 0.9431 - val_loss: 2.3976 - val_accuracy: 0.0525\n",
            "Epoch 19/30\n",
            "1599/1600 [============================>.] - ETA: 0s - loss: 0.1624 - accuracy: 0.9430\n",
            "End of epoch 19, LoraLayer 0: 36400 Step\n",
            "End of epoch 19, LoraLayer 0: Decay factor: 0.36666667461395264\n",
            "End of epoch 19, LoraLayer 1: 36400 Step\n",
            "End of epoch 19, LoraLayer 1: Decay factor: 0.36666667461395264\n",
            "End of epoch 19, LoraLayer 2: 36400 Step\n",
            "End of epoch 19, LoraLayer 2: Decay factor: 0.36666667461395264\n",
            "\n",
            "Testing loss: 2.4524641036987305, acc: 0.026916665956377983\n",
            "\n",
            "1600/1600 [==============================] - 9s 6ms/step - loss: 0.1623 - accuracy: 0.9430 - val_loss: 2.4525 - val_accuracy: 0.0269\n",
            "Epoch 20/30\n",
            "1595/1600 [============================>.] - ETA: 0s - loss: 0.1677 - accuracy: 0.9407\n",
            "End of epoch 20, LoraLayer 0: 38000 Step\n",
            "End of epoch 20, LoraLayer 0: Decay factor: 0.3333333134651184\n",
            "End of epoch 20, LoraLayer 1: 38000 Step\n",
            "End of epoch 20, LoraLayer 1: Decay factor: 0.3333333134651184\n",
            "End of epoch 20, LoraLayer 2: 38000 Step\n",
            "End of epoch 20, LoraLayer 2: Decay factor: 0.3333333134651184\n",
            "\n",
            "Testing loss: 2.4026358127593994, acc: 0.06391666829586029\n",
            "\n",
            "1600/1600 [==============================] - 12s 7ms/step - loss: 0.1676 - accuracy: 0.9407 - val_loss: 2.4026 - val_accuracy: 0.0639\n",
            "Epoch 21/30\n",
            "1598/1600 [============================>.] - ETA: 0s - loss: 0.1727 - accuracy: 0.9391\n",
            "End of epoch 21, LoraLayer 0: 39600 Step\n",
            "End of epoch 21, LoraLayer 0: Decay factor: 0.30000001192092896\n",
            "End of epoch 21, LoraLayer 1: 39600 Step\n",
            "End of epoch 21, LoraLayer 1: Decay factor: 0.30000001192092896\n",
            "End of epoch 21, LoraLayer 2: 39600 Step\n",
            "End of epoch 21, LoraLayer 2: Decay factor: 0.30000001192092896\n",
            "\n",
            "Testing loss: 2.4565088748931885, acc: 0.05533333495259285\n",
            "\n",
            "1600/1600 [==============================] - 11s 7ms/step - loss: 0.1728 - accuracy: 0.9390 - val_loss: 2.4565 - val_accuracy: 0.0553\n",
            "Epoch 22/30\n",
            "1600/1600 [==============================] - ETA: 0s - loss: 0.1792 - accuracy: 0.9381\n",
            "End of epoch 22, LoraLayer 0: 41200 Step\n",
            "End of epoch 22, LoraLayer 0: Decay factor: 0.2666666507720947\n",
            "End of epoch 22, LoraLayer 1: 41200 Step\n",
            "End of epoch 22, LoraLayer 1: Decay factor: 0.2666666507720947\n",
            "End of epoch 22, LoraLayer 2: 41200 Step\n",
            "End of epoch 22, LoraLayer 2: Decay factor: 0.2666666507720947\n",
            "\n",
            "Testing loss: 2.476243257522583, acc: 0.10241666436195374\n",
            "\n",
            "1600/1600 [==============================] - 9s 6ms/step - loss: 0.1792 - accuracy: 0.9381 - val_loss: 2.4762 - val_accuracy: 0.1024\n",
            "Epoch 23/30\n",
            "1597/1600 [============================>.] - ETA: 0s - loss: 0.1873 - accuracy: 0.9361\n",
            "End of epoch 23, LoraLayer 0: 42800 Step\n",
            "End of epoch 23, LoraLayer 0: Decay factor: 0.23333334922790527\n",
            "End of epoch 23, LoraLayer 1: 42800 Step\n",
            "End of epoch 23, LoraLayer 1: Decay factor: 0.23333334922790527\n",
            "End of epoch 23, LoraLayer 2: 42800 Step\n",
            "End of epoch 23, LoraLayer 2: Decay factor: 0.23333334922790527\n",
            "\n",
            "Testing loss: 2.3259406089782715, acc: 0.1380833387374878\n",
            "\n",
            "1600/1600 [==============================] - 12s 8ms/step - loss: 0.1873 - accuracy: 0.9361 - val_loss: 2.3259 - val_accuracy: 0.1381\n",
            "Epoch 24/30\n",
            "1598/1600 [============================>.] - ETA: 0s - loss: 0.1972 - accuracy: 0.9316\n",
            "End of epoch 24, LoraLayer 0: 44400 Step\n",
            "End of epoch 24, LoraLayer 0: Decay factor: 0.19999998807907104\n",
            "End of epoch 24, LoraLayer 1: 44400 Step\n",
            "End of epoch 24, LoraLayer 1: Decay factor: 0.19999998807907104\n",
            "End of epoch 24, LoraLayer 2: 44400 Step\n",
            "End of epoch 24, LoraLayer 2: Decay factor: 0.19999998807907104\n",
            "\n",
            "Testing loss: 2.3409061431884766, acc: 0.15600000321865082\n",
            "\n",
            "1600/1600 [==============================] - 12s 7ms/step - loss: 0.1972 - accuracy: 0.9316 - val_loss: 2.3409 - val_accuracy: 0.1560\n",
            "Epoch 25/30\n",
            "1595/1600 [============================>.] - ETA: 0s - loss: 0.2089 - accuracy: 0.9282\n",
            "End of epoch 25, LoraLayer 0: 46000 Step\n",
            "End of epoch 25, LoraLayer 0: Decay factor: 0.1666666865348816\n",
            "End of epoch 25, LoraLayer 1: 46000 Step\n",
            "End of epoch 25, LoraLayer 1: Decay factor: 0.1666666865348816\n",
            "End of epoch 25, LoraLayer 2: 46000 Step\n",
            "End of epoch 25, LoraLayer 2: Decay factor: 0.1666666865348816\n",
            "\n",
            "Testing loss: 2.1436660289764404, acc: 0.19741666316986084\n",
            "\n",
            "1600/1600 [==============================] - 15s 9ms/step - loss: 0.2088 - accuracy: 0.9282 - val_loss: 2.1437 - val_accuracy: 0.1974\n",
            "Epoch 26/30\n",
            "1590/1600 [============================>.] - ETA: 0s - loss: 0.2221 - accuracy: 0.9234\n",
            "End of epoch 26, LoraLayer 0: 47600 Step\n",
            "End of epoch 26, LoraLayer 0: Decay factor: 0.13333332538604736\n",
            "End of epoch 26, LoraLayer 1: 47600 Step\n",
            "End of epoch 26, LoraLayer 1: Decay factor: 0.13333332538604736\n",
            "End of epoch 26, LoraLayer 2: 47600 Step\n",
            "End of epoch 26, LoraLayer 2: Decay factor: 0.13333332538604736\n",
            "\n",
            "Testing loss: 1.9989913702011108, acc: 0.22366666793823242\n",
            "\n",
            "1600/1600 [==============================] - 9s 6ms/step - loss: 0.2223 - accuracy: 0.9232 - val_loss: 1.9990 - val_accuracy: 0.2237\n",
            "Epoch 27/30\n",
            "1595/1600 [============================>.] - ETA: 0s - loss: 0.2434 - accuracy: 0.9167\n",
            "End of epoch 27, LoraLayer 0: 49200 Step\n",
            "End of epoch 27, LoraLayer 0: Decay factor: 0.10000002384185791\n",
            "End of epoch 27, LoraLayer 1: 49200 Step\n",
            "End of epoch 27, LoraLayer 1: Decay factor: 0.10000002384185791\n",
            "End of epoch 27, LoraLayer 2: 49200 Step\n",
            "End of epoch 27, LoraLayer 2: Decay factor: 0.10000002384185791\n",
            "\n",
            "Testing loss: 1.6211949586868286, acc: 0.359333336353302\n",
            "\n",
            "1600/1600 [==============================] - 12s 8ms/step - loss: 0.2433 - accuracy: 0.9168 - val_loss: 1.6212 - val_accuracy: 0.3593\n",
            "Epoch 28/30\n",
            "1597/1600 [============================>.] - ETA: 0s - loss: 0.2716 - accuracy: 0.9067\n",
            "End of epoch 28, LoraLayer 0: 50800 Step\n",
            "End of epoch 28, LoraLayer 0: Decay factor: 0.06666666269302368\n",
            "End of epoch 28, LoraLayer 1: 50800 Step\n",
            "End of epoch 28, LoraLayer 1: Decay factor: 0.06666666269302368\n",
            "End of epoch 28, LoraLayer 2: 50800 Step\n",
            "End of epoch 28, LoraLayer 2: Decay factor: 0.06666666269302368\n",
            "\n",
            "Testing loss: 1.4439970254898071, acc: 0.4309166669845581\n",
            "\n",
            "1600/1600 [==============================] - 12s 7ms/step - loss: 0.2715 - accuracy: 0.9067 - val_loss: 1.4440 - val_accuracy: 0.4309\n",
            "Epoch 29/30\n",
            "1592/1600 [============================>.] - ETA: 0s - loss: 0.3304 - accuracy: 0.8890\n",
            "End of epoch 29, LoraLayer 0: 52400 Step\n",
            "End of epoch 29, LoraLayer 0: Decay factor: 0.03333336114883423\n",
            "End of epoch 29, LoraLayer 1: 52400 Step\n",
            "End of epoch 29, LoraLayer 1: Decay factor: 0.03333336114883423\n",
            "End of epoch 29, LoraLayer 2: 52400 Step\n",
            "End of epoch 29, LoraLayer 2: Decay factor: 0.03333336114883423\n",
            "\n",
            "Testing loss: 0.9343440532684326, acc: 0.6608333587646484\n",
            "\n",
            "1600/1600 [==============================] - 9s 6ms/step - loss: 0.3309 - accuracy: 0.8888 - val_loss: 0.9343 - val_accuracy: 0.6608\n",
            "Epoch 30/30\n",
            "1596/1600 [============================>.] - ETA: 0s - loss: 0.5027 - accuracy: 0.8245\n",
            "End of epoch 30, LoraLayer 0: 54000 Step\n",
            "End of epoch 30, LoraLayer 0: Decay factor: 0.0\n",
            "End of epoch 30, LoraLayer 1: 54000 Step\n",
            "End of epoch 30, LoraLayer 1: Decay factor: 0.0\n",
            "End of epoch 30, LoraLayer 2: 54000 Step\n",
            "End of epoch 30, LoraLayer 2: Decay factor: 0.0\n",
            "\n",
            "Testing loss: 0.6738871932029724, acc: 0.7689999938011169\n",
            "\n",
            "1600/1600 [==============================] - 12s 8ms/step - loss: 0.5031 - accuracy: 0.8244 - val_loss: 0.6739 - val_accuracy: 0.7690\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x78b08465da80>"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = LoRA_modified_model.evaluate(test_images,  test_labels, verbose=2)\n",
        "\n",
        "print('\\nTest accuracy:', test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BU64VNfVzOrA",
        "outputId": "eeaa728f-15f3-4b4b-e1ad-126f9a90ed63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 - 1s - loss: 0.7068 - accuracy: 0.7554 - 806ms/epoch - 3ms/step\n",
            "\n",
            "Test accuracy: 0.7554000020027161\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## lora ouput decay factor 로 나눔 -> 제대로 학습 되지 않음"
      ],
      "metadata": {
        "id": "RHNaeiOL7Rrb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 학습을 시작합니다. 이제 검증 데이터로 val_images와 val_labels를 사용합니다.\n",
        "LoRA_modified_model.fit(train_images, train_labels,\n",
        "                        epochs=epochs,\n",
        "                        batch_size=batch_size,\n",
        "                        validation_data=(val_images, val_labels),\n",
        "                        callbacks=[print_step_callback, TestCallback((val_images,val_labels))])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "buV92yKZKj1r",
        "outputId": "5e5dfc06-ee25-4536-c55c-edae6349ea7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "1277/1280 [============================>.] - ETA: 0s - loss: 0.1359 - accuracy: 0.9472\n",
            "End of epoch 1, LoraLayer 0: 7280 Step\n",
            "End of epoch 1, LoraLayer 0: Decay factor: 0.9733333587646484\n",
            "End of epoch 1, LoraLayer 1: 7280 Step\n",
            "End of epoch 1, LoraLayer 1: Decay factor: 0.9733333587646484\n",
            "End of epoch 1, LoraLayer 2: 7280 Step\n",
            "End of epoch 1, LoraLayer 2: Decay factor: 0.9733333587646484\n",
            "\n",
            "Testing loss: 17.393165588378906, acc: 0.08114583045244217\n",
            "\n",
            "1280/1280 [==============================] - 13s 9ms/step - loss: 0.1361 - accuracy: 0.9471 - val_loss: 17.3932 - val_accuracy: 0.0811\n",
            "Epoch 2/30\n",
            "1279/1280 [============================>.] - ETA: 0s - loss: 0.1291 - accuracy: 0.9499\n",
            "End of epoch 2, LoraLayer 0: 8560 Step\n",
            "End of epoch 2, LoraLayer 0: Decay factor: 0.9466666579246521\n",
            "End of epoch 2, LoraLayer 1: 8560 Step\n",
            "End of epoch 2, LoraLayer 1: Decay factor: 0.9466666579246521\n",
            "End of epoch 2, LoraLayer 2: 8560 Step\n",
            "End of epoch 2, LoraLayer 2: Decay factor: 0.9466666579246521\n",
            "\n",
            "Testing loss: 8.51168441772461, acc: 0.0326041653752327\n",
            "\n",
            "1280/1280 [==============================] - 8s 6ms/step - loss: 0.1291 - accuracy: 0.9499 - val_loss: 8.5117 - val_accuracy: 0.0326\n",
            "Epoch 3/30\n",
            "1277/1280 [============================>.] - ETA: 0s - loss: 0.1267 - accuracy: 0.9511\n",
            "End of epoch 3, LoraLayer 0: 9840 Step\n",
            "End of epoch 3, LoraLayer 0: Decay factor: 0.9200000166893005\n",
            "End of epoch 3, LoraLayer 1: 9840 Step\n",
            "End of epoch 3, LoraLayer 1: Decay factor: 0.9200000166893005\n",
            "End of epoch 3, LoraLayer 2: 9840 Step\n",
            "End of epoch 3, LoraLayer 2: Decay factor: 0.9200000166893005\n",
            "\n",
            "Testing loss: 5.485502243041992, acc: 0.04010416567325592\n",
            "\n",
            "1280/1280 [==============================] - 10s 8ms/step - loss: 0.1268 - accuracy: 0.9510 - val_loss: 5.4855 - val_accuracy: 0.0401\n",
            "Epoch 4/30\n",
            "1278/1280 [============================>.] - ETA: 0s - loss: 0.1258 - accuracy: 0.9520\n",
            "End of epoch 4, LoraLayer 0: 11120 Step\n",
            "End of epoch 4, LoraLayer 0: Decay factor: 0.8933333158493042\n",
            "End of epoch 4, LoraLayer 1: 11120 Step\n",
            "End of epoch 4, LoraLayer 1: Decay factor: 0.8933333158493042\n",
            "End of epoch 4, LoraLayer 2: 11120 Step\n",
            "End of epoch 4, LoraLayer 2: Decay factor: 0.8933333158493042\n",
            "\n",
            "Testing loss: 4.114836692810059, acc: 0.09687499701976776\n",
            "\n",
            "1280/1280 [==============================] - 10s 8ms/step - loss: 0.1259 - accuracy: 0.9520 - val_loss: 4.1148 - val_accuracy: 0.0969\n",
            "Epoch 5/30\n",
            "1280/1280 [==============================] - ETA: 0s - loss: 0.1259 - accuracy: 0.9518\n",
            "End of epoch 5, LoraLayer 0: 12400 Step\n",
            "End of epoch 5, LoraLayer 0: Decay factor: 0.8666666746139526\n",
            "End of epoch 5, LoraLayer 1: 12400 Step\n",
            "End of epoch 5, LoraLayer 1: Decay factor: 0.8666666746139526\n",
            "End of epoch 5, LoraLayer 2: 12400 Step\n",
            "End of epoch 5, LoraLayer 2: Decay factor: 0.8666666746139526\n",
            "\n",
            "Testing loss: 3.572572946548462, acc: 0.031041666865348816\n",
            "\n",
            "1280/1280 [==============================] - 8s 6ms/step - loss: 0.1259 - accuracy: 0.9518 - val_loss: 3.5726 - val_accuracy: 0.0310\n",
            "Epoch 6/30\n",
            "1279/1280 [============================>.] - ETA: 0s - loss: 0.1261 - accuracy: 0.9521\n",
            "End of epoch 6, LoraLayer 0: 13680 Step\n",
            "End of epoch 6, LoraLayer 0: Decay factor: 0.8400000333786011\n",
            "End of epoch 6, LoraLayer 1: 13680 Step\n",
            "End of epoch 6, LoraLayer 1: Decay factor: 0.8400000333786011\n",
            "End of epoch 6, LoraLayer 2: 13680 Step\n",
            "End of epoch 6, LoraLayer 2: Decay factor: 0.8400000333786011\n",
            "\n",
            "Testing loss: 3.684002161026001, acc: 0.03229166567325592\n",
            "\n",
            "1280/1280 [==============================] - 10s 8ms/step - loss: 0.1261 - accuracy: 0.9521 - val_loss: 3.6840 - val_accuracy: 0.0323\n",
            "Epoch 7/30\n",
            "1274/1280 [============================>.] - ETA: 0s - loss: 0.1269 - accuracy: 0.9527\n",
            "End of epoch 7, LoraLayer 0: 14960 Step\n",
            "End of epoch 7, LoraLayer 0: Decay factor: 0.8133333325386047\n",
            "End of epoch 7, LoraLayer 1: 14960 Step\n",
            "End of epoch 7, LoraLayer 1: Decay factor: 0.8133333325386047\n",
            "End of epoch 7, LoraLayer 2: 14960 Step\n",
            "End of epoch 7, LoraLayer 2: Decay factor: 0.8133333325386047\n",
            "\n",
            "Testing loss: 3.7337048053741455, acc: 0.04625000059604645\n",
            "\n",
            "1280/1280 [==============================] - 9s 7ms/step - loss: 0.1271 - accuracy: 0.9527 - val_loss: 3.7337 - val_accuracy: 0.0463\n",
            "Epoch 8/30\n",
            "1271/1280 [============================>.] - ETA: 0s - loss: 0.1277 - accuracy: 0.9529\n",
            "End of epoch 8, LoraLayer 0: 16240 Step\n",
            "End of epoch 8, LoraLayer 0: Decay factor: 0.7866666316986084\n",
            "End of epoch 8, LoraLayer 1: 16240 Step\n",
            "End of epoch 8, LoraLayer 1: Decay factor: 0.7866666316986084\n",
            "End of epoch 8, LoraLayer 2: 16240 Step\n",
            "End of epoch 8, LoraLayer 2: Decay factor: 0.7866666316986084\n",
            "\n",
            "Testing loss: 3.5756258964538574, acc: 0.045104168355464935\n",
            "\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.1278 - accuracy: 0.9528 - val_loss: 3.5756 - val_accuracy: 0.0451\n",
            "Epoch 9/30\n",
            "1270/1280 [============================>.] - ETA: 0s - loss: 0.1292 - accuracy: 0.9526\n",
            "End of epoch 9, LoraLayer 0: 17520 Step\n",
            "End of epoch 9, LoraLayer 0: Decay factor: 0.7599999904632568\n",
            "End of epoch 9, LoraLayer 1: 17520 Step\n",
            "End of epoch 9, LoraLayer 1: Decay factor: 0.7599999904632568\n",
            "End of epoch 9, LoraLayer 2: 17520 Step\n",
            "End of epoch 9, LoraLayer 2: Decay factor: 0.7599999904632568\n",
            "\n",
            "Testing loss: 3.9255473613739014, acc: 0.040312498807907104\n",
            "\n",
            "1280/1280 [==============================] - 7s 6ms/step - loss: 0.1292 - accuracy: 0.9526 - val_loss: 3.9255 - val_accuracy: 0.0403\n",
            "Epoch 10/30\n",
            "1269/1280 [============================>.] - ETA: 0s - loss: 0.1304 - accuracy: 0.9521\n",
            "End of epoch 10, LoraLayer 0: 18800 Step\n",
            "End of epoch 10, LoraLayer 0: Decay factor: 0.7333333492279053\n",
            "End of epoch 10, LoraLayer 1: 18800 Step\n",
            "End of epoch 10, LoraLayer 1: Decay factor: 0.7333333492279053\n",
            "End of epoch 10, LoraLayer 2: 18800 Step\n",
            "End of epoch 10, LoraLayer 2: Decay factor: 0.7333333492279053\n",
            "\n",
            "Testing loss: 4.040300369262695, acc: 0.058125000447034836\n",
            "\n",
            "1280/1280 [==============================] - 11s 8ms/step - loss: 0.1306 - accuracy: 0.9520 - val_loss: 4.0403 - val_accuracy: 0.0581\n",
            "Epoch 11/30\n",
            "1280/1280 [==============================] - ETA: 0s - loss: 0.1325 - accuracy: 0.9518\n",
            "End of epoch 11, LoraLayer 0: 20080 Step\n",
            "End of epoch 11, LoraLayer 0: Decay factor: 0.7066667079925537\n",
            "End of epoch 11, LoraLayer 1: 20080 Step\n",
            "End of epoch 11, LoraLayer 1: Decay factor: 0.7066667079925537\n",
            "End of epoch 11, LoraLayer 2: 20080 Step\n",
            "End of epoch 11, LoraLayer 2: Decay factor: 0.7066667079925537\n",
            "\n",
            "Testing loss: 4.331480979919434, acc: 0.06208333373069763\n",
            "\n",
            "1280/1280 [==============================] - 8s 6ms/step - loss: 0.1325 - accuracy: 0.9518 - val_loss: 4.3315 - val_accuracy: 0.0621\n",
            "Epoch 12/30\n",
            "1278/1280 [============================>.] - ETA: 0s - loss: 0.1338 - accuracy: 0.9514\n",
            "End of epoch 12, LoraLayer 0: 21360 Step\n",
            "End of epoch 12, LoraLayer 0: Decay factor: 0.6800000071525574\n",
            "End of epoch 12, LoraLayer 1: 21360 Step\n",
            "End of epoch 12, LoraLayer 1: Decay factor: 0.6800000071525574\n",
            "End of epoch 12, LoraLayer 2: 21360 Step\n",
            "End of epoch 12, LoraLayer 2: Decay factor: 0.6800000071525574\n",
            "\n",
            "Testing loss: 4.134157657623291, acc: 0.05062500014901161\n",
            "\n",
            "1280/1280 [==============================] - 9s 7ms/step - loss: 0.1337 - accuracy: 0.9515 - val_loss: 4.1342 - val_accuracy: 0.0506\n",
            "Epoch 13/30\n",
            "1279/1280 [============================>.] - ETA: 0s - loss: 0.1353 - accuracy: 0.9513\n",
            "End of epoch 13, LoraLayer 0: 22640 Step\n",
            "End of epoch 13, LoraLayer 0: Decay factor: 0.653333306312561\n",
            "End of epoch 13, LoraLayer 1: 22640 Step\n",
            "End of epoch 13, LoraLayer 1: Decay factor: 0.653333306312561\n",
            "End of epoch 13, LoraLayer 2: 22640 Step\n",
            "End of epoch 13, LoraLayer 2: Decay factor: 0.653333306312561\n",
            "\n",
            "Testing loss: 4.292471885681152, acc: 0.06427083164453506\n",
            "\n",
            "1280/1280 [==============================] - 8s 6ms/step - loss: 0.1353 - accuracy: 0.9513 - val_loss: 4.2925 - val_accuracy: 0.0643\n",
            "Epoch 14/30\n",
            "1278/1280 [============================>.] - ETA: 0s - loss: 0.1366 - accuracy: 0.9516\n",
            "End of epoch 14, LoraLayer 0: 23920 Step\n",
            "End of epoch 14, LoraLayer 0: Decay factor: 0.6266666650772095\n",
            "End of epoch 14, LoraLayer 1: 23920 Step\n",
            "End of epoch 14, LoraLayer 1: Decay factor: 0.6266666650772095\n",
            "End of epoch 14, LoraLayer 2: 23920 Step\n",
            "End of epoch 14, LoraLayer 2: Decay factor: 0.6266666650772095\n",
            "\n",
            "Testing loss: 4.304422378540039, acc: 0.05666666850447655\n",
            "\n",
            "1280/1280 [==============================] - 8s 6ms/step - loss: 0.1367 - accuracy: 0.9516 - val_loss: 4.3044 - val_accuracy: 0.0567\n",
            "Epoch 15/30\n",
            "1278/1280 [============================>.] - ETA: 0s - loss: 0.1387 - accuracy: 0.9505\n",
            "End of epoch 15, LoraLayer 0: 25200 Step\n",
            "End of epoch 15, LoraLayer 0: Decay factor: 0.6000000238418579\n",
            "End of epoch 15, LoraLayer 1: 25200 Step\n",
            "End of epoch 15, LoraLayer 1: Decay factor: 0.6000000238418579\n",
            "End of epoch 15, LoraLayer 2: 25200 Step\n",
            "End of epoch 15, LoraLayer 2: Decay factor: 0.6000000238418579\n",
            "\n",
            "Testing loss: 4.2603936195373535, acc: 0.06291666626930237\n",
            "\n",
            "1280/1280 [==============================] - 8s 7ms/step - loss: 0.1386 - accuracy: 0.9505 - val_loss: 4.2604 - val_accuracy: 0.0629\n",
            "Epoch 16/30\n",
            "1269/1280 [============================>.] - ETA: 0s - loss: 0.1415 - accuracy: 0.9502\n",
            "End of epoch 16, LoraLayer 0: 26480 Step\n",
            "End of epoch 16, LoraLayer 0: Decay factor: 0.5733333230018616\n",
            "End of epoch 16, LoraLayer 1: 26480 Step\n",
            "End of epoch 16, LoraLayer 1: Decay factor: 0.5733333230018616\n",
            "End of epoch 16, LoraLayer 2: 26480 Step\n",
            "End of epoch 16, LoraLayer 2: Decay factor: 0.5733333230018616\n",
            "\n",
            "Testing loss: 4.4137282371521, acc: 0.05947916582226753\n",
            "\n",
            "1280/1280 [==============================] - 9s 7ms/step - loss: 0.1413 - accuracy: 0.9503 - val_loss: 4.4137 - val_accuracy: 0.0595\n",
            "Epoch 17/30\n",
            "1276/1280 [============================>.] - ETA: 0s - loss: 0.1432 - accuracy: 0.9493\n",
            "End of epoch 17, LoraLayer 0: 27760 Step\n",
            "End of epoch 17, LoraLayer 0: Decay factor: 0.54666668176651\n",
            "End of epoch 17, LoraLayer 1: 27760 Step\n",
            "End of epoch 17, LoraLayer 1: Decay factor: 0.54666668176651\n",
            "End of epoch 17, LoraLayer 2: 27760 Step\n",
            "End of epoch 17, LoraLayer 2: Decay factor: 0.54666668176651\n",
            "\n",
            "Testing loss: 5.023769378662109, acc: 0.05927083268761635\n",
            "\n",
            "1280/1280 [==============================] - 10s 8ms/step - loss: 0.1431 - accuracy: 0.9493 - val_loss: 5.0238 - val_accuracy: 0.0593\n",
            "Epoch 18/30\n",
            "1273/1280 [============================>.] - ETA: 0s - loss: 0.1453 - accuracy: 0.9492\n",
            "End of epoch 18, LoraLayer 0: 29040 Step\n",
            "End of epoch 18, LoraLayer 0: Decay factor: 0.5199999809265137\n",
            "End of epoch 18, LoraLayer 1: 29040 Step\n",
            "End of epoch 18, LoraLayer 1: Decay factor: 0.5199999809265137\n",
            "End of epoch 18, LoraLayer 2: 29040 Step\n",
            "End of epoch 18, LoraLayer 2: Decay factor: 0.5199999809265137\n",
            "\n",
            "Testing loss: 4.292078018188477, acc: 0.06239583343267441\n",
            "\n",
            "1280/1280 [==============================] - 7s 6ms/step - loss: 0.1453 - accuracy: 0.9491 - val_loss: 4.2921 - val_accuracy: 0.0624\n",
            "Epoch 19/30\n",
            "1273/1280 [============================>.] - ETA: 0s - loss: 0.1473 - accuracy: 0.9477\n",
            "End of epoch 19, LoraLayer 0: 30320 Step\n",
            "End of epoch 19, LoraLayer 0: Decay factor: 0.4933333396911621\n",
            "End of epoch 19, LoraLayer 1: 30320 Step\n",
            "End of epoch 19, LoraLayer 1: Decay factor: 0.4933333396911621\n",
            "End of epoch 19, LoraLayer 2: 30320 Step\n",
            "End of epoch 19, LoraLayer 2: Decay factor: 0.4933333396911621\n",
            "\n",
            "Testing loss: 4.324694633483887, acc: 0.057083334773778915\n",
            "\n",
            "1280/1280 [==============================] - 10s 8ms/step - loss: 0.1472 - accuracy: 0.9477 - val_loss: 4.3247 - val_accuracy: 0.0571\n",
            "Epoch 20/30\n",
            "1277/1280 [============================>.] - ETA: 0s - loss: 0.1491 - accuracy: 0.9485\n",
            "End of epoch 20, LoraLayer 0: 31600 Step\n",
            "End of epoch 20, LoraLayer 0: Decay factor: 0.46666669845581055\n",
            "End of epoch 20, LoraLayer 1: 31600 Step\n",
            "End of epoch 20, LoraLayer 1: Decay factor: 0.46666669845581055\n",
            "End of epoch 20, LoraLayer 2: 31600 Step\n",
            "End of epoch 20, LoraLayer 2: Decay factor: 0.46666669845581055\n",
            "\n",
            "Testing loss: 4.375078201293945, acc: 0.054999999701976776\n",
            "\n",
            "1280/1280 [==============================] - 8s 6ms/step - loss: 0.1490 - accuracy: 0.9486 - val_loss: 4.3751 - val_accuracy: 0.0550\n",
            "Epoch 21/30\n",
            "1278/1280 [============================>.] - ETA: 0s - loss: 0.1521 - accuracy: 0.9477\n",
            "End of epoch 21, LoraLayer 0: 32880 Step\n",
            "End of epoch 21, LoraLayer 0: Decay factor: 0.4399999976158142\n",
            "End of epoch 21, LoraLayer 1: 32880 Step\n",
            "End of epoch 21, LoraLayer 1: Decay factor: 0.4399999976158142\n",
            "End of epoch 21, LoraLayer 2: 32880 Step\n",
            "End of epoch 21, LoraLayer 2: Decay factor: 0.4399999976158142\n",
            "\n",
            "Testing loss: 4.594671726226807, acc: 0.05885416641831398\n",
            "\n",
            "1280/1280 [==============================] - 11s 8ms/step - loss: 0.1521 - accuracy: 0.9477 - val_loss: 4.5947 - val_accuracy: 0.0589\n",
            "Epoch 22/30\n",
            "1280/1280 [==============================] - ETA: 0s - loss: 0.1546 - accuracy: 0.9459\n",
            "End of epoch 22, LoraLayer 0: 34160 Step\n",
            "End of epoch 22, LoraLayer 0: Decay factor: 0.41333335638046265\n",
            "End of epoch 22, LoraLayer 1: 34160 Step\n",
            "End of epoch 22, LoraLayer 1: Decay factor: 0.41333335638046265\n",
            "End of epoch 22, LoraLayer 2: 34160 Step\n",
            "End of epoch 22, LoraLayer 2: Decay factor: 0.41333335638046265\n",
            "\n",
            "Testing loss: 4.318772315979004, acc: 0.06031249836087227\n",
            "\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.1546 - accuracy: 0.9459 - val_loss: 4.3188 - val_accuracy: 0.0603\n",
            "Epoch 23/30\n",
            "1271/1280 [============================>.] - ETA: 0s - loss: 0.1578 - accuracy: 0.9457\n",
            "End of epoch 23, LoraLayer 0: 35440 Step\n",
            "End of epoch 23, LoraLayer 0: Decay factor: 0.3866666555404663\n",
            "End of epoch 23, LoraLayer 1: 35440 Step\n",
            "End of epoch 23, LoraLayer 1: Decay factor: 0.3866666555404663\n",
            "End of epoch 23, LoraLayer 2: 35440 Step\n",
            "End of epoch 23, LoraLayer 2: Decay factor: 0.3866666555404663\n",
            "\n",
            "Testing loss: 4.007905960083008, acc: 0.05458333343267441\n",
            "\n",
            "1280/1280 [==============================] - 8s 6ms/step - loss: 0.1579 - accuracy: 0.9456 - val_loss: 4.0079 - val_accuracy: 0.0546\n",
            "Epoch 24/30\n",
            "1274/1280 [============================>.] - ETA: 0s - loss: 0.1610 - accuracy: 0.9441\n",
            "End of epoch 24, LoraLayer 0: 36720 Step\n",
            "End of epoch 24, LoraLayer 0: Decay factor: 0.36000001430511475\n",
            "End of epoch 24, LoraLayer 1: 36720 Step\n",
            "End of epoch 24, LoraLayer 1: Decay factor: 0.36000001430511475\n",
            "End of epoch 24, LoraLayer 2: 36720 Step\n",
            "End of epoch 24, LoraLayer 2: Decay factor: 0.36000001430511475\n",
            "\n",
            "Testing loss: 4.007087230682373, acc: 0.06979166716337204\n",
            "\n",
            "1280/1280 [==============================] - 10s 8ms/step - loss: 0.1610 - accuracy: 0.9442 - val_loss: 4.0071 - val_accuracy: 0.0698\n",
            "Epoch 25/30\n",
            "1273/1280 [============================>.] - ETA: 0s - loss: 0.1652 - accuracy: 0.9425\n",
            "End of epoch 25, LoraLayer 0: 38000 Step\n",
            "End of epoch 25, LoraLayer 0: Decay factor: 0.3333333134651184\n",
            "End of epoch 25, LoraLayer 1: 38000 Step\n",
            "End of epoch 25, LoraLayer 1: Decay factor: 0.3333333134651184\n",
            "End of epoch 25, LoraLayer 2: 38000 Step\n",
            "End of epoch 25, LoraLayer 2: Decay factor: 0.3333333134651184\n",
            "\n",
            "Testing loss: 4.117982387542725, acc: 0.06427083164453506\n",
            "\n",
            "1280/1280 [==============================] - 8s 6ms/step - loss: 0.1648 - accuracy: 0.9426 - val_loss: 4.1180 - val_accuracy: 0.0643\n",
            "Epoch 26/30\n",
            "1277/1280 [============================>.] - ETA: 0s - loss: 0.1688 - accuracy: 0.9424\n",
            "End of epoch 26, LoraLayer 0: 39280 Step\n",
            "End of epoch 26, LoraLayer 0: Decay factor: 0.30666667222976685\n",
            "End of epoch 26, LoraLayer 1: 39280 Step\n",
            "End of epoch 26, LoraLayer 1: Decay factor: 0.30666667222976685\n",
            "End of epoch 26, LoraLayer 2: 39280 Step\n",
            "End of epoch 26, LoraLayer 2: Decay factor: 0.30666667222976685\n",
            "\n",
            "Testing loss: 3.6546459197998047, acc: 0.06572916358709335\n",
            "\n",
            "1280/1280 [==============================] - 11s 8ms/step - loss: 0.1689 - accuracy: 0.9424 - val_loss: 3.6546 - val_accuracy: 0.0657\n",
            "Epoch 27/30\n",
            "1277/1280 [============================>.] - ETA: 0s - loss: 0.1737 - accuracy: 0.9397\n",
            "End of epoch 27, LoraLayer 0: 40560 Step\n",
            "End of epoch 27, LoraLayer 0: Decay factor: 0.2800000309944153\n",
            "End of epoch 27, LoraLayer 1: 40560 Step\n",
            "End of epoch 27, LoraLayer 1: Decay factor: 0.2800000309944153\n",
            "End of epoch 27, LoraLayer 2: 40560 Step\n",
            "End of epoch 27, LoraLayer 2: Decay factor: 0.2800000309944153\n",
            "\n",
            "Testing loss: 3.3846218585968018, acc: 0.08510416746139526\n",
            "\n",
            "1280/1280 [==============================] - 8s 6ms/step - loss: 0.1740 - accuracy: 0.9395 - val_loss: 3.3846 - val_accuracy: 0.0851\n",
            "Epoch 28/30\n",
            "1280/1280 [==============================] - ETA: 0s - loss: 0.1804 - accuracy: 0.9375\n",
            "End of epoch 28, LoraLayer 0: 41840 Step\n",
            "End of epoch 28, LoraLayer 0: Decay factor: 0.25333333015441895\n",
            "End of epoch 28, LoraLayer 1: 41840 Step\n",
            "End of epoch 28, LoraLayer 1: Decay factor: 0.25333333015441895\n",
            "End of epoch 28, LoraLayer 2: 41840 Step\n",
            "End of epoch 28, LoraLayer 2: Decay factor: 0.25333333015441895\n",
            "\n",
            "Testing loss: 3.1647233963012695, acc: 0.1054166629910469\n",
            "\n",
            "1280/1280 [==============================] - 9s 7ms/step - loss: 0.1804 - accuracy: 0.9375 - val_loss: 3.1647 - val_accuracy: 0.1054\n",
            "Epoch 29/30\n",
            "1276/1280 [============================>.] - ETA: 0s - loss: 0.1861 - accuracy: 0.9364\n",
            "End of epoch 29, LoraLayer 0: 43120 Step\n",
            "End of epoch 29, LoraLayer 0: Decay factor: 0.22666668891906738\n",
            "End of epoch 29, LoraLayer 1: 43120 Step\n",
            "End of epoch 29, LoraLayer 1: Decay factor: 0.22666668891906738\n",
            "End of epoch 29, LoraLayer 2: 43120 Step\n",
            "End of epoch 29, LoraLayer 2: Decay factor: 0.22666668891906738\n",
            "\n",
            "Testing loss: 3.4391207695007324, acc: 0.11468750238418579\n",
            "\n",
            "1280/1280 [==============================] - 10s 8ms/step - loss: 0.1860 - accuracy: 0.9364 - val_loss: 3.4391 - val_accuracy: 0.1147\n",
            "Epoch 30/30\n",
            "1275/1280 [============================>.] - ETA: 0s - loss: 0.1961 - accuracy: 0.9322\n",
            "End of epoch 30, LoraLayer 0: 44400 Step\n",
            "End of epoch 30, LoraLayer 0: Decay factor: 0.19999998807907104\n",
            "End of epoch 30, LoraLayer 1: 44400 Step\n",
            "End of epoch 30, LoraLayer 1: Decay factor: 0.19999998807907104\n",
            "End of epoch 30, LoraLayer 2: 44400 Step\n",
            "End of epoch 30, LoraLayer 2: Decay factor: 0.19999998807907104\n",
            "\n",
            "Testing loss: 2.7804102897644043, acc: 0.17687499523162842\n",
            "\n",
            "1280/1280 [==============================] - 8s 6ms/step - loss: 0.1961 - accuracy: 0.9322 - val_loss: 2.7804 - val_accuracy: 0.1769\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x78afec300a30>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = LoRA_modified_model.evaluate(test_images,  test_labels, verbose=2)\n",
        "\n",
        "print('\\nTest accuracy:', test_acc)"
      ],
      "metadata": {
        "id": "G7PNz7VtF5nn"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}