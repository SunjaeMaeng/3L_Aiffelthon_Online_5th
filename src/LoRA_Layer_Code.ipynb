{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LhV0LHZfHpY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dense LoRA"
      ],
      "metadata": {
        "id": "0LfnlAyqDxI_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "class DenseLoraLayer(keras.layers.Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        original_layer,\n",
        "        total_iteration = 1000 ,  # Total number of iterations for the decay\n",
        "        start_percent=0.05,  # The percentage of total_iteration when decay starts\n",
        "        end_percent=0.85,  # The percentage of total_iteration when decay ends\n",
        "        min_decay_factor=0,  # The minimum value that decay factor can take\n",
        "        rank=64,\n",
        "        alpha=32,\n",
        "        trainable=True,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        original_layer_config = original_layer.get_config()\n",
        "        name = original_layer_config[\"name\"]\n",
        "        kwargs.pop(\"name\", None)\n",
        "\n",
        "        super().__init__(name=name, trainable=trainable, **kwargs)\n",
        "\n",
        "        self.rank = rank\n",
        "        self.alpha = alpha\n",
        "        self._scale = alpha / rank\n",
        "\n",
        "        self.original_layer = original_layer\n",
        "        self.original_layer.trainable = False\n",
        "\n",
        "\n",
        "        self.total_iteration = total_iteration\n",
        "        self.start_step = int(total_iteration * start_percent)\n",
        "        self.end_step = int(total_iteration * end_percent)\n",
        "        self.min_decay_factor = min_decay_factor\n",
        "\n",
        "        #trainable=False, 이 변수가 텐서플로우의 자동 미분 및 최적화 과정에 의해 업데이트되지 않는다는 뜻\n",
        "        #수동으로 업데이트될 수 있습니다. 예를 들어, 반복문 안에서 이 변수의 값을 업데이트하는 로직을 작성할 수 있음!\n",
        "        self.current_step = tf.Variable(0, dtype=tf.int32, trainable=False)\n",
        "        self.decay_factor = tf.Variable(1.0, dtype=tf.float32, trainable=False)\n",
        "\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # LoRA weights.\n",
        "        kernel_shape = self.original_layer.kernel.shape\n",
        "        self.A_weight = self.add_weight(\n",
        "            name=\"lora_A_weight\",\n",
        "            shape=(self.rank, kernel_shape[0]),\n",
        "            initializer=keras.initializers.VarianceScaling(\n",
        "                scale=math.sqrt(5), mode=\"fan_in\", distribution=\"uniform\"\n",
        "            ),\n",
        "            trainable=self.trainable,\n",
        "        )\n",
        "\n",
        "        self.B_weight = self.add_weight(\n",
        "            name=\"lora_B_weight\",\n",
        "            shape=(self.original_layer.units, self.rank),\n",
        "            initializer='zeros',\n",
        "            trainable=self.trainable,\n",
        "        )\n",
        "        self.C_weight = self.add_weight(\n",
        "            name=\"lora_C_weight\",\n",
        "            shape=(self.original_layer.units,),\n",
        "            initializer='zeros',\n",
        "            trainable=self.trainable,\n",
        "        )\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "            if training is None:\n",
        "                training = self.trainable\n",
        "\n",
        "            # Calculate the linear decay factor\n",
        "            if self.current_step < self.start_step:\n",
        "                self.decay_factor.assign(1.0)  # Decay has not started yet\n",
        "            elif self.current_step > self.end_step:\n",
        "                self.decay_factor.assign(tf.cast(self.min_decay_factor, dtype=tf.float32))  # Ensure float32 type for consistency\n",
        "            else:\n",
        "                # Linear decay between start_step and end_step\n",
        "                self.decay_factor.assign(1.0 - ((tf.cast(self.current_step, dtype=tf.float32) - self.start_step) /\n",
        "                                        (self.end_step - self.start_step) *\n",
        "                                        (1.0 - tf.cast(self.min_decay_factor, dtype=tf.float32))))\n",
        "\n",
        "            # Matrix multiplication for A and B weights with inputs\n",
        "            lora_A_output = tf.matmul(self.A_weight, tf.transpose(inputs))  # Ax\n",
        "            lora_output = tf.transpose(tf.matmul(self.B_weight, lora_A_output) * self._scale)  # BAx Transpose back to [batch_size, original_layer.units]\n",
        "\n",
        "            #lora_output *= (1 - self.decay_factor) # 멘토링 때 나온 의견\n",
        "\n",
        "            if training:\n",
        "                original_output = self.original_layer(inputs)\n",
        "                # 평균과 표준편차 계산\n",
        "                original_weight_matrix = self.original_layer.weights[0]\n",
        "                original_mean = tf.reduce_mean(original_weight_matrix, axis=0)\n",
        "                original_variance = tf.reduce_mean(tf.square(original_weight_matrix - original_mean), axis=0)\n",
        "                original_stddev = tf.sqrt(original_variance)\n",
        "\n",
        "                # decay_factor가 0.3보다 작으면 noise_mean과 noise_std를 0으로 설정\n",
        "                noise_mean = tf.where(self.decay_factor < 0.3, 0.0, original_mean * (1 - self.decay_factor))\n",
        "                noise_std = tf.where(self.decay_factor < 0.3, 0.0, original_stddev * tf.sqrt(1 - tf.square(self.decay_factor)))\n",
        "                noise = tf.random.normal(tf.shape(original_weight_matrix), mean=noise_mean, stddev=noise_std)\n",
        "\n",
        "                self.current_step.assign_add(1)\n",
        "\n",
        "                return original_output * self.decay_factor + (inputs @ noise) + lora_output + self.C_weight\n",
        "\n",
        "            else:\n",
        "                # 추론 모드에서는 LoRA 출력만 반환\n",
        "                return lora_output + self.C_weight\n"
      ],
      "metadata": {
        "id": "ygilGTnJDxJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Einsum LoRA"
      ],
      "metadata": {
        "id": "HFIejyZMEH2j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# EinsumLoraLayer_O\n",
        "import math\n",
        "from tensorflow import keras\n",
        "\n",
        "class EinsumLoraLayer_O(keras.layers.Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        original_layer,\n",
        "        total_iteration = 1000 ,  # Total number of iterations for the decay\n",
        "        start_percent=0.05,  # The percentage of total_iteration when decay starts\n",
        "        end_percent=0.85,  # The percentage of total_iteration when decay ends\n",
        "        min_decay_factor=0,  # The minimum value that decay factor can take\n",
        "        rank=64,\n",
        "        alpha=32,\n",
        "        trainable=True,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        original_layer_config = original_layer.get_config()\n",
        "        name = original_layer_config[\"name\"]\n",
        "        kwargs.pop(\"name\", None)\n",
        "\n",
        "        super().__init__(name=name, trainable=trainable, **kwargs)\n",
        "\n",
        "        self.rank = rank\n",
        "        self.alpha = alpha\n",
        "        self._scale = alpha / rank\n",
        "\n",
        "        self.original_layer = original_layer\n",
        "        self.original_layer.trainable = False\n",
        "\n",
        "\n",
        "        self.total_iteration = total_iteration\n",
        "        self.start_step = int(total_iteration * start_percent)\n",
        "        self.end_step = int(total_iteration * end_percent)\n",
        "        self.min_decay_factor = min_decay_factor\n",
        "\n",
        "        #trainable=False, 이 변수가 텐서플로우의 자동 미분 및 최적화 과정에 의해 업데이트되지 않는다는 뜻\n",
        "        #수동으로 업데이트될 수 있습니다. 예를 들어, 반복문 안에서 이 변수의 값을 업데이트하는 로직을 작성할 수 있음!\n",
        "        self.current_step = tf.Variable(0, dtype=tf.int32, trainable=False)\n",
        "        self.decay_factor = tf.Variable(1.0, dtype=tf.float32, trainable=False)\n",
        "\n",
        "\n",
        "    def build(self, inputs_shape):\n",
        "\n",
        "        kernel_shape = self.original_layer.kernel.shape\n",
        "        bias_shape = self.original_layer.bias.shape\n",
        "        self.A_weight = self.add_weight(\n",
        "            name=\"lora_A_weight\",\n",
        "            shape= kernel_shape[:-1] + (self.rank,),\n",
        "            initializer=keras.initializers.VarianceScaling(\n",
        "                scale=math.sqrt(5), mode=\"fan_in\", distribution=\"uniform\"\n",
        "            ),\n",
        "            trainable=self.trainable,\n",
        "        )\n",
        "\n",
        "        self.B_weight = self.add_weight(\n",
        "            name=\"lora_B_weight\",\n",
        "            shape=(self.rank, kernel_shape[-1]) ,\n",
        "            initializer=\"zeros\",\n",
        "            trainable=self.trainable,\n",
        "        )\n",
        "        self.C_weight = self.add_weight(\n",
        "            name=\"lora_C_weight\",\n",
        "            shape= bias_shape ,\n",
        "            initializer='zeros',\n",
        "            trainable=self.trainable,\n",
        "        )\n",
        "        super().build(inputs_shape)\n",
        "\n",
        "\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "            if training is None:\n",
        "                training = self.trainable\n",
        "\n",
        "            lora_A_output = tf.einsum(self.original_layer.equation, inputs , self.A_weight)\n",
        "            lora_output = tf.matmul(lora_A_output, self.B_weight) * self._scale\n",
        "\n",
        "            if training:\n",
        "                # Calculate the linear decay factor\n",
        "                if self.current_step < self.start_step:\n",
        "                    self.decay_factor.assign(1.0)  # Decay has not started yet\n",
        "                elif self.current_step > self.end_step:\n",
        "                    self.decay_factor.assign(tf.cast(self.min_decay_factor, dtype=tf.float32))  # Ensure float32 type for consistency\n",
        "                else:\n",
        "                # Linear decay between start_step and end_step\n",
        "                    self.decay_factor.assign(1.0 - ((tf.cast(self.current_step, dtype=tf.float32) - self.start_step) /\n",
        "                                        (self.end_step - self.start_step) *\n",
        "                                        (1.0 - tf.cast(self.min_decay_factor, dtype=tf.float32))))\n",
        "\n",
        "\n",
        "                # Matrix multiplication for A and B weights with inputs\n",
        "                original_output = self.original_layer(inputs) * self.decay_factor\n",
        "                # 평균과 표준편차 계산\n",
        "                original_weight_matrix = self.original_layer.weights[0]\n",
        "                original_mean = tf.reduce_mean(original_weight_matrix, axis=0)\n",
        "                original_variance = tf.reduce_mean(tf.square(original_weight_matrix - original_mean), axis=0)\n",
        "                original_stddev = tf.sqrt(original_variance)\n",
        "\n",
        "                # decay_factor가 0.3보다 작으면 noise_mean과 noise_std를 0으로 설정\n",
        "                noise_mean = tf.where(self.decay_factor < 0.3, 0.0, original_mean * (1 - self.decay_factor))\n",
        "                noise_std = tf.where(self.decay_factor < 0.3, 0.0, original_stddev * tf.sqrt(1 - tf.square(self.decay_factor)))\n",
        "                noise = tf.random.normal(tf.shape(original_weight_matrix), mean=noise_mean, stddev=noise_std)\n",
        "\n",
        "                # Increment the step counter\n",
        "                self.current_step.assign_add(1)\n",
        "\n",
        "                return original_output * self.decay_factor + tf.einsum(self.original_layer.equation, inputs , noise ) + lora_output + self.C_weight\n",
        "\n",
        "            else:\n",
        "                # 추론 모드에서는 LoRA 출력만 반환\n",
        "                return lora_output + self.C_weight"
      ],
      "metadata": {
        "id": "feyB-Zz1FABg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#EinsumLoraLayer_QKV\n",
        "import math\n",
        "from tensorflow import keras\n",
        "\n",
        "class EinsumLoraLayer_QKV(keras.layers.Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        original_layer,\n",
        "        total_iteration = 1000 ,  # Total number of iterations for the decay\n",
        "        start_percent=0.05,  # The percentage of total_iteration when decay starts\n",
        "        end_percent=0.85,  # The percentage of total_iteration when decay ends\n",
        "        min_decay_factor=0,  # The minimum value that decay factor can take\n",
        "        rank=64,\n",
        "        alpha=32,\n",
        "        trainable=True,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        original_layer_config = original_layer.get_config()\n",
        "        name = original_layer_config[\"name\"]\n",
        "        kwargs.pop(\"name\", None)\n",
        "\n",
        "        super().__init__(name=name, trainable=trainable, **kwargs)\n",
        "\n",
        "        self.rank = rank\n",
        "        self.alpha = alpha\n",
        "        self._scale = alpha / rank\n",
        "\n",
        "        self.original_layer = original_layer\n",
        "        self.original_layer.trainable = False\n",
        "\n",
        "\n",
        "        self.total_iteration = total_iteration\n",
        "        self.start_step = int(total_iteration * start_percent)\n",
        "        self.end_step = int(total_iteration * end_percent)\n",
        "        self.min_decay_factor = min_decay_factor\n",
        "\n",
        "        #trainable=False, 이 변수가 텐서플로우의 자동 미분 및 최적화 과정에 의해 업데이트되지 않는다는 뜻\n",
        "        #수동으로 업데이트될 수 있습니다. 예를 들어, 반복문 안에서 이 변수의 값을 업데이트하는 로직을 작성할 수 있음!\n",
        "        self.current_step = tf.Variable(0, dtype=tf.int32, trainable=False)\n",
        "        self.decay_factor = tf.Variable(1.0, dtype=tf.float32, trainable=False)\n",
        "\n",
        "\n",
        "    def build(self, inputs_shape):\n",
        "\n",
        "        kernel_shape = self.original_layer.kernel.shape\n",
        "        bias_shape = self.original_layer.bias.shape\n",
        "        self.A_weight = self.add_weight(\n",
        "            name=\"lora_A_weight\",\n",
        "            shape=(self.rank, kernel_shape[0]),\n",
        "            initializer=keras.initializers.VarianceScaling(\n",
        "                scale=math.sqrt(5), mode=\"fan_in\", distribution=\"uniform\"\n",
        "            ),\n",
        "            trainable=self.trainable,\n",
        "        )\n",
        "\n",
        "        self.B_weight = self.add_weight(\n",
        "            name=\"lora_B_weight\",\n",
        "            shape=(self.rank,) + kernel_shape[1:],\n",
        "            initializer=\"zeros\",\n",
        "            trainable=self.trainable,\n",
        "        )\n",
        "        self.C_weight = self.add_weight(\n",
        "            name=\"lora_C_weight\",\n",
        "            shape= bias_shape ,\n",
        "            initializer='zeros',\n",
        "            trainable=self.trainable,\n",
        "        )\n",
        "        super().build(inputs_shape)\n",
        "\n",
        "\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "            if training is None:\n",
        "                training = self.trainable\n",
        "\n",
        "            # Matrix multiplication for A and B weights with inputs\n",
        "            lora_A_output = tf.matmul(inputs, tf.transpose(self.A_weight)) #xA\n",
        "            lora_output = tf.einsum(self.original_layer.equation, lora_A_output , self.B_weight) * self._scale  # BAx Transpose back to [batch_size, original_layer.units]\n",
        "\n",
        "            if training:\n",
        "                # Calculate the linear decay factor\n",
        "                if self.current_step < self.start_step:\n",
        "                    self.decay_factor.assign(1.0)  # Decay has not started yet\n",
        "                elif self.current_step > self.end_step:\n",
        "                    self.decay_factor.assign(tf.cast(self.min_decay_factor, dtype=tf.float32))  # Ensure float32 type for consistency\n",
        "                else:\n",
        "                # Linear decay between start_step and end_step\n",
        "                    self.decay_factor.assign(1.0 - ((tf.cast(self.current_step, dtype=tf.float32) - self.start_step) /\n",
        "                                        (self.end_step - self.start_step) *\n",
        "                                        (1.0 - tf.cast(self.min_decay_factor, dtype=tf.float32))))\n",
        "\n",
        "\n",
        "                # Matrix multiplication for A and B weights with inputs\n",
        "                original_output = self.original_layer(inputs) * self.decay_factor\n",
        "                # 평균과 표준편차 계산\n",
        "                original_weight_matrix = self.original_layer.weights[0]\n",
        "                original_mean = tf.reduce_mean(original_weight_matrix, axis=0)\n",
        "                original_variance = tf.reduce_mean(tf.square(original_weight_matrix - original_mean), axis=0)\n",
        "                original_stddev = tf.sqrt(original_variance)\n",
        "\n",
        "                # decay_factor가 0.3보다 작으면 noise_mean과 noise_std를 0으로 설정\n",
        "                noise_mean = tf.where(self.decay_factor < 0.3, 0.0, original_mean * (1 - self.decay_factor))\n",
        "                noise_std = tf.where(self.decay_factor < 0.3, 0.0, original_stddev * tf.sqrt(1 - tf.square(self.decay_factor)))\n",
        "                noise = tf.random.normal(tf.shape(original_weight_matrix), mean=noise_mean, stddev=noise_std)\n",
        "\n",
        "                # Increment the step counter\n",
        "                self.current_step.assign_add(1)\n",
        "\n",
        "                return original_output * self.decay_factor +  tf.einsum(self.original_layer.equation, inputs , noise ) + lora_output + self.C_weight\n",
        "\n",
        "            else:\n",
        "                # 추론 모드에서는 LoRA 출력만 반환\n",
        "                return lora_output + self.C_weight"
      ],
      "metadata": {
        "id": "hoHCfP6_HllK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conv LoRA"
      ],
      "metadata": {
        "id": "h7X5cvYYCvit"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RuQb6mt9CrTd"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, initializers\n",
        "from tensorflow.keras.layers import Conv2D, Conv1D, Conv3D\n",
        "\n",
        "class ConvLoRALayer(layers.Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        original_conv_layer,\n",
        "        total_iteration = 1000 ,  # Total number of iterations for the decay\n",
        "        start_percent=0.1,  # The percentage of total_iteration when decay starts\n",
        "        end_percent=0.9,  # The percentage of total_iteration when decay ends\n",
        "        min_decay_factor=0,  # The minimum value that decay factor can take\n",
        "        rank=32,\n",
        "        alpha=32,\n",
        "        trainable=True,\n",
        "        **kwargs\n",
        "    ):\n",
        "        # Capture the original layer's configuration.\n",
        "        original_layer_config = original_conv_layer.get_config()\n",
        "        name = original_layer_config[\"name\"]\n",
        "        kwargs.pop(\"name\", None)\n",
        "\n",
        "        super().__init__(name=name, trainable=trainable, **kwargs)\n",
        "\n",
        "        self.rank = rank\n",
        "        self.alpha = alpha\n",
        "        self._scale = alpha / rank\n",
        "\n",
        "        # The original convolutional layer is set to non-trainable to freeze its weights.\n",
        "        self.original_conv_layer = original_conv_layer\n",
        "        self.original_conv_layer.trainable = False\n",
        "\n",
        "        self.kernel = None\n",
        "        self.filters = original_conv_layer.filters #\n",
        "        self.kernel_size = original_conv_layer.kernel_size[0] #\n",
        "        self.in_channels = None\n",
        "\n",
        "        self.total_iteration = total_iteration\n",
        "        self.start_step = int(total_iteration * start_percent)\n",
        "        self.end_step = int(total_iteration * end_percent)\n",
        "        self.min_decay_factor = min_decay_factor\n",
        "\n",
        "        #trainable=False, 이 변수가 텐서플로우의 자동 미분 및 최적화 과정에 의해 업데이트되지 않는다는 뜻\n",
        "        #수동으로 업데이트될 수 있습니다. 예를 들어, 반복문 안에서 이 변수의 값을 업데이트하는 로직을 작성할 수 있음!\n",
        "        self.current_step = tf.Variable(0, dtype=tf.int32, trainable=False)\n",
        "        self.decay_factor = tf.Variable(1.0, dtype=tf.float32, trainable=False)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Ensure the original convolutional layer is built.\n",
        "        #if not self.original_conv_layer.built:\n",
        "        #    self.original_conv_layer.build(input_shape)\n",
        "\n",
        "        # Calculate the shape for LoRA weights A and B.\n",
        "        #self.kernel = self.original_conv_layer.kernel\n",
        "        self.in_channels = input_shape[-1]\n",
        "\n",
        "        in_channels = self.in_channels\n",
        "        out_channels = self.filters\n",
        "        kernel_size = self.original_conv_layer.kernel_size[0]\n",
        "\n",
        "        # LoRA weights A and B.\n",
        "        self.A_weight = self.add_weight(\n",
        "            name=\"lora_A_weight\",\n",
        "            shape=(self.rank*kernel_size, in_channels*kernel_size),\n",
        "            initializer=initializers.VarianceScaling(scale=1.0, mode='fan_in', distribution='uniform'),\n",
        "            trainable=self.trainable\n",
        "        )\n",
        "\n",
        "        self.B_weight = self.add_weight(\n",
        "            name=\"lora_B_weight\",\n",
        "            shape=(out_channels*kernel_size, self.rank*kernel_size),\n",
        "            initializer=\"zeros\",\n",
        "            trainable=self.trainable\n",
        "        )\n",
        "\n",
        "        bias_shape = self.original_conv_layer.bias.shape\n",
        "        self.C_weight = self.add_weight(\n",
        "            name=\"lora_C_weight\",\n",
        "            shape=bias_shape,\n",
        "            initializer=\"zeros\",\n",
        "            trainable=self.trainable\n",
        "        )\n",
        "\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        if training is None:\n",
        "                training = self.trainable\n",
        "\n",
        "        # Calculate the linear decay factor\n",
        "        if self.current_step < self.start_step:\n",
        "            self.decay_factor.assign(1.0)  # Decay has not started yet\n",
        "        elif self.current_step > self.end_step:\n",
        "            self.decay_factor.assign(tf.cast(self.min_decay_factor, dtype=tf.float32))  # Ensure float32 type for consistency\n",
        "        else:\n",
        "            # Linear decay between start_step and end_step\n",
        "            self.decay_factor.assign(1.0 - ((tf.cast(self.current_step, dtype=tf.float32) - self.start_step) /\n",
        "                                    (self.end_step - self.start_step) *\n",
        "                                    (1.0 - tf.cast(self.min_decay_factor, dtype=tf.float32))))\n",
        "\n",
        "        lora_BA = (self.B_weight@self.A_weight)\n",
        "\n",
        "        kernel_size = self.original_conv_layer.kernel_size[0]\n",
        "        in_channels = self.in_channels\n",
        "        out_channels = self.filters\n",
        "\n",
        "           # lora_BA의 형태 변환\n",
        "           # lora_BA가 (out_channels*kernel_size*kernel_size, in_channels*kernel_size*kernel_size) 형태라고 가정\n",
        "           # 이를 (kernel_size, kernel_size, in_channels, out_channels)로 변환\n",
        "        lora_BA_reshaped = tf.reshape(lora_BA, (out_channels, kernel_size, kernel_size, in_channels))\n",
        "        lora_BA_reshaped = tf.transpose(lora_BA_reshaped, [1, 2, 3, 0])\n",
        "        lora_output = tf.nn.conv2d(inputs, lora_BA_reshaped, strides=[1, 1, 1, 1], padding='SAME') * self._scale\n",
        "\n",
        "        # original_output = self.original_conv_layer(inputs) * self.decay_factor\n",
        "\n",
        "        if training:\n",
        "            original_output = self.original_conv_layer(inputs)\n",
        "            # 평균과 표준편차 계산\n",
        "            original_weight_matrix = self.original_conv_layer.weights[0]\n",
        "            original_mean = tf.reduce_mean(original_weight_matrix)\n",
        "            original_variance = tf.reduce_mean(tf.square(original_weight_matrix - original_mean))\n",
        "            original_stddev = tf.sqrt(original_variance)\n",
        "\n",
        "            # decay_factor가 0.3보다 작으면 noise_mean과 noise_std를 0으로 설정\n",
        "            noise_mean = tf.where(self.decay_factor < 0.3, 0.0, original_mean * (1 - self.decay_factor))\n",
        "            noise_std = tf.where(self.decay_factor < 0.3, 0.0, original_stddev * tf.sqrt(1 - tf.square(self.decay_factor)))\n",
        "            noise = tf.random.normal(tf.shape(original_weight_matrix), mean=noise_mean, stddev=noise_std)\n",
        "            noise_output = tf.nn.conv2d(inputs, noise, strides=[1, 1, 1, 1], padding='SAME')\n",
        "\n",
        "            self.current_step.assign_add(1)\n",
        "\n",
        "            return original_output * self.decay_factor + noise_output + lora_output + self.C_weight\n",
        "\n",
        "        else:\n",
        "            # 추론 모드에서는 LoRA 출력만 반환\n",
        "            return lora_output + self.C_weight"
      ]
    }
  ]
}