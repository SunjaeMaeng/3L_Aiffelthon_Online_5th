{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "McBYcpzkTZ9_",
    "outputId": "a04948c3-a789-4ffb-cebc-551911fafe9f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-01 03:23:34.521883: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-01 03:23:34.521924: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-01 03:23:34.521955: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-01 03:23:34.531296: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.14.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!apt-get update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x3H9TbtJTO5k"
   },
   "source": [
    "# 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9GWr9Ep7TOZi",
    "outputId": "61f248ce-932e-4ab3-d215-c621d53787f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data\n",
      "(50000, 32, 32, 3)\n",
      "(50000, 1)\n",
      "test data\n",
      "(10000, 32, 32, 3)\n",
      "(10000, 1)\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "print('train data')\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print('test data')\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "5IyhwDy0TcOf"
   },
   "outputs": [],
   "source": [
    "# 데이터 전처리: 정규화\n",
    "x_train, x_test = x_train / 255.0,  x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s1xQRMYeTg3_",
    "outputId": "53e8f401-3c97-40b1-de46-fcc56e52e0ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data\n",
      "(50000, 32, 32, 3)\n",
      "(50000, 10)\n",
      "test data\n",
      "(10000, 32, 32, 3)\n",
      "(10000, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-01 03:23:44.195084: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18452 MB memory:  -> device: 0, name: NVIDIA RTX 4000 SFF Ada Generation, pci bus id: 0000:0c:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "# scalar 형태의 레이블(0-9)을 one-hot encoding 형태로 변환합니다\n",
    "\n",
    "y_train = tf.squeeze(tf.one_hot(y_train, 10),axis=1)\n",
    "y_test = tf.squeeze(tf.one_hot(y_test, 10), axis=1)\n",
    "\n",
    "print('train data')\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print('test data')\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nj-DhZZZTRxC"
   },
   "source": [
    "# 모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mfhhV71RMn2Y",
    "outputId": "ad1dfc62-9138-439c-9369-1dc02bf4b978"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "_LJi10btMdaM"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "best_vgg16 = load_model(\"./best_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "olYdP2YUM_Pi",
    "outputId": "d612fc3b-5117-4cde-cbf6-00c8d1327175"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 32, 32, 64)        1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 32, 32, 64)        36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 16, 16, 128)       73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 16, 16, 128)       147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 8, 8, 256)         295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 8, 8, 256)         590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 8, 8, 256)         590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 4, 4, 256)         0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 4, 4, 512)         1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 4, 4, 512)         2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 4, 4, 512)         2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 2, 2, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 2, 2, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 2, 2, 512)         2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 1, 1, 512)         0         \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 4096)              2101248   \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 512)               2097664   \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 18918730 (72.17 MB)\n",
      "Trainable params: 18918730 (72.17 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "best_vgg16.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zoj2Gx6bfMfH"
   },
   "source": [
    "# inference time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fJf4zX5-fGCf",
    "outputId": "8f9a98fb-01cf-42b0-9197-9890ae336575"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-01 03:23:46.270129: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8905\n",
      "2023-12-01 03:23:46.395304: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  9/313 [..............................] - ETA: 2s - loss: 0.5923 - accuracy: 0.8472  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-01 03:23:46.828156: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 3s 7ms/step - loss: 0.5326 - accuracy: 0.8536\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5325802564620972, 0.853600025177002]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 평가\n",
    "best_vgg16.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hv-4CTjZPIoJ"
   },
   "source": [
    "## 가중치 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s1m-0r85R0TO",
    "outputId": "baf4a76f-e104-43e1-8a8d-0a4b541c6971"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: input_3\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block1_conv1\n",
      "  Trainable parameters: 1792\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block1_conv2\n",
      "  Trainable parameters: 36928\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block1_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block2_conv1\n",
      "  Trainable parameters: 73856\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block2_conv2\n",
      "  Trainable parameters: 147584\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block2_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block3_conv1\n",
      "  Trainable parameters: 295168\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block3_conv2\n",
      "  Trainable parameters: 590080\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block3_conv3\n",
      "  Trainable parameters: 590080\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block3_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block4_conv1\n",
      "  Trainable parameters: 1180160\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block4_conv2\n",
      "  Trainable parameters: 2359808\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block4_conv3\n",
      "  Trainable parameters: 2359808\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block4_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block5_conv1\n",
      "  Trainable parameters: 2359808\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block5_conv2\n",
      "  Trainable parameters: 2359808\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block5_conv3\n",
      "  Trainable parameters: 2359808\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block5_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: flatten_2\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: dense_6\n",
      "  Trainable parameters: 2101248\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: dropout_4\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: dense_7\n",
      "  Trainable parameters: 2097664\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: dropout_5\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: dense_8\n",
      "  Trainable parameters: 5130\n",
      "  Non-trainable parameters: 0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "#모델의 각 레이어를 순회하며 파라미터 수를 계산\n",
    "for layer in best_vgg16.layers:\n",
    "    trainable_count = np.sum([tf.size(w).numpy() for w in layer.trainable_weights])\n",
    "    non_trainable_count = np.sum([tf.size(w).numpy() for w in layer.non_trainable_weights])\n",
    "\n",
    "    print(f\"Layer: {layer.name}\")\n",
    "    print(f\"  Trainable parameters: {trainable_count}\")\n",
    "    print(f\"  Non-trainable parameters: {non_trainable_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MX5RRcHq9CDt"
   },
   "source": [
    "# [Code] LoRA\n",
    "\n",
    "아래의 lora 코드에는 scheduling factor와 noise가 포함되어 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UylB2lpL9Gy5"
   },
   "source": [
    "## ConvLoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "npn-lPaP9Hev"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, initializers\n",
    "from tensorflow.keras.layers import Conv2D, Conv1D, Conv3D\n",
    "\n",
    "class ConvLoRALayer00_cdn2(layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        original_conv_layer,\n",
    "        total_iteration = 1000 ,  # Total number of iterations for the decay\n",
    "        start_percent=0.1,  # The percentage of total_iteration when decay starts\n",
    "        end_percent=0.9,  # The percentage of total_iteration when decay ends\n",
    "        min_decay_factor=0,  # The minimum value that decay factor can take\n",
    "        rank=32,\n",
    "        alpha=32,\n",
    "        trainable=True,\n",
    "        **kwargs\n",
    "    ):\n",
    "        # Capture the original layer's configuration.\n",
    "        original_layer_config = original_conv_layer.get_config()\n",
    "        name = original_layer_config[\"name\"]\n",
    "        kwargs.pop(\"name\", None)\n",
    "\n",
    "        super().__init__(name=name, trainable=trainable, **kwargs)\n",
    "\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self._scale = alpha / rank\n",
    "\n",
    "        # The original convolutional layer is set to non-trainable to freeze its weights.\n",
    "        self.original_conv_layer = original_conv_layer\n",
    "        self.original_conv_layer.trainable = False\n",
    "\n",
    "        self.kernel = None\n",
    "        self.filters = original_conv_layer.filters #\n",
    "        self.kernel_size = original_conv_layer.kernel_size[0] #\n",
    "        self.in_channels = None\n",
    "\n",
    "        self.total_iteration = total_iteration\n",
    "        self.start_step = int(total_iteration * start_percent)\n",
    "        self.end_step = int(total_iteration * end_percent)\n",
    "        self.min_decay_factor = min_decay_factor\n",
    "\n",
    "        #trainable=False, 이 변수가 텐서플로우의 자동 미분 및 최적화 과정에 의해 업데이트되지 않는다는 뜻\n",
    "        #수동으로 업데이트될 수 있습니다. 예를 들어, 반복문 안에서 이 변수의 값을 업데이트하는 로직을 작성할 수 있음!\n",
    "        self.current_step = tf.Variable(0, dtype=tf.int32, trainable=False)\n",
    "        self.decay_factor = tf.Variable(1.0, dtype=tf.float32, trainable=False)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Ensure the original convolutional layer is built.\n",
    "        #if not self.original_conv_layer.built:\n",
    "        #    self.original_conv_layer.build(input_shape)\n",
    "\n",
    "        # Calculate the shape for LoRA weights A and B.\n",
    "        #self.kernel = self.original_conv_layer.kernel\n",
    "        self.in_channels = input_shape[-1]\n",
    "\n",
    "        in_channels = self.in_channels\n",
    "        out_channels = self.filters\n",
    "        kernel_size = self.original_conv_layer.kernel_size[0]\n",
    "\n",
    "        # LoRA weights A and B.\n",
    "        self.A_weight = self.add_weight(\n",
    "            name=\"lora_A_weight\",\n",
    "            shape=(self.rank*kernel_size, in_channels*kernel_size),\n",
    "            initializer=initializers.VarianceScaling(scale=1.0, mode='fan_in', distribution='uniform'),\n",
    "            trainable=self.trainable\n",
    "        )\n",
    "\n",
    "        self.B_weight = self.add_weight(\n",
    "            name=\"lora_B_weight\",\n",
    "            shape=(out_channels*kernel_size, self.rank*kernel_size),\n",
    "            initializer=\"zeros\",\n",
    "            trainable=self.trainable\n",
    "        )\n",
    "\n",
    "        super().build(input_shape)\n",
    "\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        if training is None:\n",
    "                training = self.trainable\n",
    "\n",
    "        # Calculate the linear decay factor\n",
    "        if self.current_step < self.start_step:\n",
    "            self.decay_factor.assign(1.0)  # Decay has not started yet\n",
    "        elif self.current_step > self.end_step:\n",
    "            self.decay_factor.assign(tf.cast(self.min_decay_factor, dtype=tf.float32))  # Ensure float32 type for consistency\n",
    "        else:\n",
    "            # Linear decay between start_step and end_step\n",
    "            self.decay_factor.assign(1.0 - ((tf.cast(self.current_step, dtype=tf.float32) - self.start_step) /\n",
    "                                    (self.end_step - self.start_step) *\n",
    "                                    (1.0 - tf.cast(self.min_decay_factor, dtype=tf.float32))))\n",
    "\n",
    "        lora_BA = (self.B_weight@self.A_weight)\n",
    "\n",
    "        kernel_size = self.original_conv_layer.kernel_size[0]\n",
    "        in_channels = self.in_channels\n",
    "        out_channels = self.filters\n",
    "\n",
    "           # lora_BA의 형태 변환\n",
    "           # lora_BA가 (out_channels*kernel_size*kernel_size, in_channels*kernel_size*kernel_size) 형태라고 가정\n",
    "           # 이를 (kernel_size, kernel_size, in_channels, out_channels)로 변환\n",
    "        lora_BA_reshaped = tf.reshape(lora_BA, (out_channels, kernel_size, kernel_size, in_channels))\n",
    "        lora_BA_reshaped = tf.transpose(lora_BA_reshaped, [1, 2, 3, 0])\n",
    "        lora_output = tf.nn.conv2d(inputs, lora_BA_reshaped, strides=[1, 1, 1, 1], padding='SAME') * self._scale\n",
    "\n",
    "        # original_output = self.original_conv_layer(inputs) * self.decay_factor\n",
    "\n",
    "        if training:\n",
    "            original_output = self.original_conv_layer(inputs)\n",
    "            # 평균과 표준편차 계산\n",
    "            original_weight_matrix = self.original_conv_layer.weights[0]\n",
    "            original_mean = tf.reduce_mean(original_weight_matrix)\n",
    "            original_variance = tf.reduce_mean(tf.square(original_weight_matrix - original_mean))\n",
    "            original_stddev = tf.sqrt(original_variance)\n",
    "\n",
    "            # decay_factor가 0.3보다 작으면 noise_mean과 noise_std를 0으로 설정\n",
    "            noise_mean = tf.where(self.decay_factor < 0.3, 0.0, original_mean * (1 - self.decay_factor))\n",
    "            noise_std = tf.where(self.decay_factor < 0.3, 0.0, original_stddev * tf.sqrt(1 - tf.square(self.decay_factor)))\n",
    "            noise = tf.random.normal(tf.shape(original_weight_matrix), mean=noise_mean, stddev=noise_std)\n",
    "            noise_output = tf.nn.conv2d(inputs, noise, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "            self.current_step.assign_add(1)\n",
    "\n",
    "            return original_output * self.decay_factor + lora_output\n",
    "\n",
    "        else:\n",
    "            # 추론 모드에서는 LoRA 출력만 반환\n",
    "            return lora_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CLLXZoFi9UgM"
   },
   "source": [
    "## DenseLoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "b3zto3tY9Wu8"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from tensorflow import keras\n",
    "\n",
    "class LoraLayer(keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        original_layer,\n",
    "        total_iteration = 1000 ,  # Total number of iterations for the decay\n",
    "        start_percent=0.1,  # The percentage of total_iteration when decay starts\n",
    "        end_percent=0.9,  # The percentage of total_iteration when decay ends\n",
    "        min_decay_factor=0,  # The minimum value that decay factor can take\n",
    "        rank=32,\n",
    "        alpha=32,\n",
    "        trainable=True,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        original_layer_config = original_layer.get_config()\n",
    "        name = original_layer_config[\"name\"]\n",
    "        kwargs.pop(\"name\", None)\n",
    "\n",
    "        super().__init__(name=name, trainable=trainable, **kwargs)\n",
    "\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self._scale = alpha / rank\n",
    "\n",
    "        self.original_layer = original_layer\n",
    "        self.original_layer.trainable = False\n",
    "\n",
    "\n",
    "        self.total_iteration = total_iteration\n",
    "        self.start_step = int(total_iteration * start_percent)\n",
    "        self.end_step = int(total_iteration * end_percent)\n",
    "        self.min_decay_factor = min_decay_factor\n",
    "\n",
    "        #trainable=False, 이 변수가 텐서플로우의 자동 미분 및 최적화 과정에 의해 업데이트되지 않는다는 뜻\n",
    "        #수동으로 업데이트될 수 있습니다. 예를 들어, 반복문 안에서 이 변수의 값을 업데이트하는 로직을 작성할 수 있음!\n",
    "        self.current_step = tf.Variable(0, dtype=tf.int32, trainable=False)\n",
    "        self.decay_factor = tf.Variable(1.0, dtype=tf.float32, trainable=False)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # LoRA weights.\n",
    "        kernel_shape = self.original_layer.kernel.shape\n",
    "        self.A_weight = self.add_weight(\n",
    "            name=\"lora_A_weight\",\n",
    "            shape=(self.rank, kernel_shape[0]),\n",
    "            initializer=keras.initializers.VarianceScaling(\n",
    "                scale=math.sqrt(5), mode=\"fan_in\", distribution=\"uniform\"\n",
    "            ),\n",
    "            trainable=self.trainable,\n",
    "        )\n",
    "\n",
    "        self.B_weight = self.add_weight(\n",
    "            name=\"lora_B_weight\",\n",
    "            shape=(self.original_layer.units, self.rank),\n",
    "            initializer='zeros',\n",
    "            trainable=self.trainable,\n",
    "        )\n",
    "\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "            if training is None:\n",
    "                training = self.trainable\n",
    "\n",
    "            # Calculate the linear decay factor\n",
    "            if self.current_step < self.start_step:\n",
    "                self.decay_factor.assign(1.0)  # Decay has not started yet\n",
    "            elif self.current_step > self.end_step:\n",
    "                self.decay_factor.assign(tf.cast(self.min_decay_factor, dtype=tf.float32))  # Ensure float32 type for consistency\n",
    "            else:\n",
    "                # Linear decay between start_step and end_step\n",
    "                self.decay_factor.assign(1.0 - ((tf.cast(self.current_step, dtype=tf.float32) - self.start_step) /\n",
    "                                        (self.end_step - self.start_step) *\n",
    "                                        (1.0 - tf.cast(self.min_decay_factor, dtype=tf.float32))))\n",
    "\n",
    "            # Matrix multiplication for A and B weights with inputs\n",
    "            lora_A_output = tf.matmul(self.A_weight, tf.transpose(inputs))  # Ax\n",
    "            lora_output = tf.transpose(tf.matmul(self.B_weight, lora_A_output) * self._scale)  # BAx Transpose back to [batch_size, original_layer.units]\n",
    "\n",
    "            #lora_output *= (1 - self.decay_factor) # 멘토링 때 나온 의견\n",
    "\n",
    "            if training:\n",
    "                original_output = self.original_layer(inputs)\n",
    "                # 평균과 표준편차 계산\n",
    "                original_weight_matrix = self.original_layer.weights[0]\n",
    "                original_mean = tf.reduce_mean(original_weight_matrix, axis=0)\n",
    "                original_variance = tf.reduce_mean(tf.square(original_weight_matrix - original_mean), axis=0)\n",
    "                original_stddev = tf.sqrt(original_variance)\n",
    "\n",
    "                # decay_factor가 0.3보다 작으면 noise_mean과 noise_std를 0으로 설정\n",
    "                noise_mean = tf.where(self.decay_factor < 0.3, 0.0, original_mean * (1 - self.decay_factor))\n",
    "                noise_std = tf.where(self.decay_factor < 0.3, 0.0, original_stddev * tf.sqrt(1 - tf.square(self.decay_factor)))\n",
    "                noise = tf.random.normal(tf.shape(original_weight_matrix), mean=noise_mean, stddev=noise_std)\n",
    "\n",
    "                self.current_step.assign_add(1)\n",
    "\n",
    "                return original_output * self.decay_factor + lora_output\n",
    "\n",
    "            else:\n",
    "                # 추론 모드에서는 LoRA 출력만 반환\n",
    "                return lora_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KBM9sOz3_Yt1"
   },
   "source": [
    "# LoRA 적용. Exp1: Convlora + Denselayer는 ❄"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pww7eFLm30pR"
   },
   "source": [
    "## 1-1. (16, -)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "zuTOhngBFiYl"
   },
   "outputs": [],
   "source": [
    "dataset_size = x_train.shape[0]\n",
    "batch_size = 30\n",
    "epochs = 20\n",
    "\n",
    "total_iteration= int(epochs * (dataset_size / batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "BPKqW1Hk_X6m"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout, InputLayer\n",
    "import tensorflow as tf\n",
    "\n",
    "# 새로운 레이어를 추가하기 위한 레이어 리스트 초기화\n",
    "new_layers = []\n",
    "\n",
    "# VGG 모델의 각 레이어 순회\n",
    "for layer in best_vgg16.layers:\n",
    "    if isinstance(layer, InputLayer):\n",
    "        continue\n",
    "\n",
    "    if isinstance(layer, Conv2D):\n",
    "        # Conv2D 레이어를 ConvLoRALayer로 대체\n",
    "        conv_lora_layer = ConvLoRALayer00_cdn2(original_conv_layer=layer, rank=16, alpha=32, total_iteration = total_iteration, trainable=True)\n",
    "        new_layers.append(conv_lora_layer)\n",
    "\n",
    "    else:\n",
    "        # 다른 유형의 레이어는 그대로 유지\n",
    "        new_layers.append(layer)\n",
    "\n",
    "# 새로운 입력 텐서 생성\n",
    "input_tensor = best_vgg16.input\n",
    "\n",
    "# 새로운 모델 구성\n",
    "x = input_tensor\n",
    "for layer in new_layers:\n",
    "    x = layer(x)\n",
    "\n",
    "# 새로운 모델 생성\n",
    "lora_vgg161 = Model(inputs=input_tensor, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "uFd65OKPpW9T"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D\n",
    "\n",
    "# 새로 생성된 모델에서 Dense 레이어 중 LoRA가 적용되지 않은 레이어의 trainable을 False로 설정\n",
    "for layer in lora_vgg161.layers:\n",
    "    if isinstance(layer, Dense):\n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fP-6DUC7FopE",
    "outputId": "ffcabf22-e139-49d0-d76a-1fba020e7e1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " block1_conv1 (ConvLoRALaye  (None, 32, 32, 64)        11442     \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block1_conv2 (ConvLoRALaye  (None, 32, 32, 64)        55362     \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " block2_conv1 (ConvLoRALaye  (None, 16, 16, 128)       101506    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block2_conv2 (ConvLoRALaye  (None, 16, 16, 128)       184450    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " block3_conv1 (ConvLoRALaye  (None, 8, 8, 256)         350466    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block3_conv2 (ConvLoRALaye  (None, 8, 8, 256)         663810    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block3_conv3 (ConvLoRALaye  (None, 8, 8, 256)         663810    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 4, 4, 256)         0         \n",
      "                                                                 \n",
      " block4_conv1 (ConvLoRALaye  (None, 4, 4, 512)         1290754   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block4_conv2 (ConvLoRALaye  (None, 4, 4, 512)         2507266   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block4_conv3 (ConvLoRALaye  (None, 4, 4, 512)         2507266   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      " block5_conv1 (ConvLoRALaye  (None, 2, 2, 512)         2507266   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block5_conv2 (ConvLoRALaye  (None, 2, 2, 512)         2507266   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block5_conv3 (ConvLoRALaye  (None, 2, 2, 512)         2507266   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 1, 1, 512)         0         \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 4096)              2101248   \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 512)               2097664   \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20061972 (76.53 MB)\n",
      "Trainable params: 1143216 (4.36 MB)\n",
      "Non-trainable params: 18918756 (72.17 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lora_vgg161.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4gdRpVfKPM3t"
   },
   "source": [
    "## 가중치 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VEO3IP_PRqSl",
    "outputId": "a19ebea2-61a9-4f67-9d53-619bc2692331"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: input_3\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block1_conv1\n",
      "  Trainable parameters: 9648\n",
      "  Non-trainable parameters: 1794\n",
      "Layer: block1_conv2\n",
      "  Trainable parameters: 18432\n",
      "  Non-trainable parameters: 36930\n",
      "Layer: block1_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block2_conv1\n",
      "  Trainable parameters: 27648\n",
      "  Non-trainable parameters: 73858\n",
      "Layer: block2_conv2\n",
      "  Trainable parameters: 36864\n",
      "  Non-trainable parameters: 147586\n",
      "Layer: block2_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block3_conv1\n",
      "  Trainable parameters: 55296\n",
      "  Non-trainable parameters: 295170\n",
      "Layer: block3_conv2\n",
      "  Trainable parameters: 73728\n",
      "  Non-trainable parameters: 590082\n",
      "Layer: block3_conv3\n",
      "  Trainable parameters: 73728\n",
      "  Non-trainable parameters: 590082\n",
      "Layer: block3_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block4_conv1\n",
      "  Trainable parameters: 110592\n",
      "  Non-trainable parameters: 1180162\n",
      "Layer: block4_conv2\n",
      "  Trainable parameters: 147456\n",
      "  Non-trainable parameters: 2359810\n",
      "Layer: block4_conv3\n",
      "  Trainable parameters: 147456\n",
      "  Non-trainable parameters: 2359810\n",
      "Layer: block4_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block5_conv1\n",
      "  Trainable parameters: 147456\n",
      "  Non-trainable parameters: 2359810\n",
      "Layer: block5_conv2\n",
      "  Trainable parameters: 147456\n",
      "  Non-trainable parameters: 2359810\n",
      "Layer: block5_conv3\n",
      "  Trainable parameters: 147456\n",
      "  Non-trainable parameters: 2359810\n",
      "Layer: block5_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: flatten_2\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: dense_6\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 2101248\n",
      "Layer: dropout_4\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: dense_7\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 2097664\n",
      "Layer: dropout_5\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: dense_8\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 5130\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "#모델의 각 레이어를 순회하며 파라미터 수를 계산\n",
    "for layer in lora_vgg161.layers:\n",
    "    trainable_count = np.sum([tf.size(w).numpy() for w in layer.trainable_weights])\n",
    "    non_trainable_count = np.sum([tf.size(w).numpy() for w in layer.non_trainable_weights])\n",
    "\n",
    "    print(f\"Layer: {layer.name}\")\n",
    "    print(f\"  Trainable parameters: {trainable_count}\")\n",
    "    print(f\"  Non-trainable parameters: {non_trainable_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mMG9bhGIU2ch"
   },
   "source": [
    "## 스케줄링 및 노이즈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Knnl1gljU8aT"
   },
   "outputs": [],
   "source": [
    "lora_layers = []  # lora 레이어들을 저장할 리스트\n",
    "\n",
    "for layer in lora_vgg161.layers:\n",
    "    if isinstance(layer, ConvLoRALayer00_cdn2) or isinstance(layer, LoraLayer):\n",
    "        lora_layers.append(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "ESXAgLxgZVzR"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "class PrintCurrentStepCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, lora_layers):\n",
    "        super().__init__()\n",
    "        self.lora_layers = lora_layers\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print()\n",
    "        for i, lora_layer in enumerate(self.lora_layers):\n",
    "            current_step = lora_layer.current_step.value()\n",
    "            decay_factor = lora_layer.decay_factor.value()\n",
    "            print(f\"End of epoch {epoch + 1}, LoraLayer {i}: {current_step} Step\")\n",
    "            print(f\"End of epoch {epoch + 1}, LoraLayer {i}: Decay factor: {decay_factor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "f70WB6oBaKLk"
   },
   "outputs": [],
   "source": [
    "# 콜백 생성 시 lora_layers 딕셔너리의 값만 사용\n",
    "print_step_callback = PrintCurrentStepCallback(lora_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "PhpCpsu_aMtY"
   },
   "outputs": [],
   "source": [
    "class TestCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, test_data):\n",
    "        self.test_data = test_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        x, y = self.test_data\n",
    "        loss, acc = self.model.evaluate(x, y, verbose=0)\n",
    "        print('\\nTesting loss: {}, acc: {}\\n'.format(loss, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bKWXrQnopKB6"
   },
   "source": [
    "##학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "pnAakdZ8aRCW"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# 모델 컴파일\n",
    "lora_vgg161.compile(optimizer=Adam(learning_rate=0.0001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XQ1mcREqapIR",
    "outputId": "1413af14-7277-4ffd-c91d-8dfbd2443aea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " block1_conv1 (ConvLoRALaye  (None, 32, 32, 64)        11442     \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block1_conv2 (ConvLoRALaye  (None, 32, 32, 64)        55362     \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " block2_conv1 (ConvLoRALaye  (None, 16, 16, 128)       101506    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block2_conv2 (ConvLoRALaye  (None, 16, 16, 128)       184450    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " block3_conv1 (ConvLoRALaye  (None, 8, 8, 256)         350466    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block3_conv2 (ConvLoRALaye  (None, 8, 8, 256)         663810    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block3_conv3 (ConvLoRALaye  (None, 8, 8, 256)         663810    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 4, 4, 256)         0         \n",
      "                                                                 \n",
      " block4_conv1 (ConvLoRALaye  (None, 4, 4, 512)         1290754   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block4_conv2 (ConvLoRALaye  (None, 4, 4, 512)         2507266   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block4_conv3 (ConvLoRALaye  (None, 4, 4, 512)         2507266   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      " block5_conv1 (ConvLoRALaye  (None, 2, 2, 512)         2507266   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block5_conv2 (ConvLoRALaye  (None, 2, 2, 512)         2507266   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block5_conv3 (ConvLoRALaye  (None, 2, 2, 512)         2507266   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 1, 1, 512)         0         \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 4096)              2101248   \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 512)               2097664   \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20061972 (76.53 MB)\n",
      "Trainable params: 1143216 (4.36 MB)\n",
      "Non-trainable params: 18918756 (72.17 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lora_vgg161.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PQWxiR4Qas7b",
    "outputId": "d75c95be-0610-4c6e-f0f1-a8c64ade3cbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-01 01:58:08.760619: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f4229333bc0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-12-01 01:58:08.760659: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA RTX 4000 SFF Ada Generation, Compute Capability 8.9\n",
      "2023-12-01 01:58:08.767940: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-12-01 01:58:08.945955: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1667/1667 [==============================] - ETA: 0s - loss: 0.2204 - accuracy: 0.9309\n",
      "End of epoch 1, LoraLayer 0: 1667 Step\n",
      "End of epoch 1, LoraLayer 0: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 1: 1667 Step\n",
      "End of epoch 1, LoraLayer 1: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 2: 1667 Step\n",
      "End of epoch 1, LoraLayer 2: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 3: 1667 Step\n",
      "End of epoch 1, LoraLayer 3: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 4: 1667 Step\n",
      "End of epoch 1, LoraLayer 4: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 5: 1667 Step\n",
      "End of epoch 1, LoraLayer 5: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 6: 1667 Step\n",
      "End of epoch 1, LoraLayer 6: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 7: 1667 Step\n",
      "End of epoch 1, LoraLayer 7: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 8: 1667 Step\n",
      "End of epoch 1, LoraLayer 8: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 9: 1667 Step\n",
      "End of epoch 1, LoraLayer 9: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 10: 1667 Step\n",
      "End of epoch 1, LoraLayer 10: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 11: 1667 Step\n",
      "End of epoch 1, LoraLayer 11: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 12: 1667 Step\n",
      "End of epoch 1, LoraLayer 12: Decay factor: 1.0\n",
      "\n",
      "Testing loss: 2.3033711910247803, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 57s 28ms/step - loss: 0.2204 - accuracy: 0.9309 - val_loss: 2.3034 - val_accuracy: 0.1000\n",
      "Epoch 2/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.1938 - accuracy: 0.9377\n",
      "End of epoch 2, LoraLayer 0: 3334 Step\n",
      "End of epoch 2, LoraLayer 0: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 1: 3334 Step\n",
      "End of epoch 2, LoraLayer 1: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 2: 3334 Step\n",
      "End of epoch 2, LoraLayer 2: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 3: 3334 Step\n",
      "End of epoch 2, LoraLayer 3: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 4: 3334 Step\n",
      "End of epoch 2, LoraLayer 4: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 5: 3334 Step\n",
      "End of epoch 2, LoraLayer 5: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 6: 3334 Step\n",
      "End of epoch 2, LoraLayer 6: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 7: 3334 Step\n",
      "End of epoch 2, LoraLayer 7: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 8: 3334 Step\n",
      "End of epoch 2, LoraLayer 8: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 9: 3334 Step\n",
      "End of epoch 2, LoraLayer 9: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 10: 3334 Step\n",
      "End of epoch 2, LoraLayer 10: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 11: 3334 Step\n",
      "End of epoch 2, LoraLayer 11: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 12: 3334 Step\n",
      "End of epoch 2, LoraLayer 12: Decay factor: 0.9999625086784363\n",
      "\n",
      "Testing loss: 2.303370714187622, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 43s 26ms/step - loss: 0.1938 - accuracy: 0.9377 - val_loss: 2.3034 - val_accuracy: 0.1000\n",
      "Epoch 3/20\n",
      "1665/1667 [============================>.] - ETA: 0s - loss: 0.1725 - accuracy: 0.9437\n",
      "End of epoch 3, LoraLayer 0: 5001 Step\n",
      "End of epoch 3, LoraLayer 0: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 1: 5001 Step\n",
      "End of epoch 3, LoraLayer 1: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 2: 5001 Step\n",
      "End of epoch 3, LoraLayer 2: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 3: 5001 Step\n",
      "End of epoch 3, LoraLayer 3: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 4: 5001 Step\n",
      "End of epoch 3, LoraLayer 4: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 5: 5001 Step\n",
      "End of epoch 3, LoraLayer 5: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 6: 5001 Step\n",
      "End of epoch 3, LoraLayer 6: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 7: 5001 Step\n",
      "End of epoch 3, LoraLayer 7: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 8: 5001 Step\n",
      "End of epoch 3, LoraLayer 8: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 9: 5001 Step\n",
      "End of epoch 3, LoraLayer 9: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 10: 5001 Step\n",
      "End of epoch 3, LoraLayer 10: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 11: 5001 Step\n",
      "End of epoch 3, LoraLayer 11: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 12: 5001 Step\n",
      "End of epoch 3, LoraLayer 12: Decay factor: 0.9374484419822693\n",
      "\n",
      "Testing loss: 2.3033688068389893, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 44s 26ms/step - loss: 0.1725 - accuracy: 0.9437 - val_loss: 2.3034 - val_accuracy: 0.1000\n",
      "Epoch 4/20\n",
      "1665/1667 [============================>.] - ETA: 0s - loss: 0.1684 - accuracy: 0.9445\n",
      "End of epoch 4, LoraLayer 0: 6668 Step\n",
      "End of epoch 4, LoraLayer 0: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 1: 6668 Step\n",
      "End of epoch 4, LoraLayer 1: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 2: 6668 Step\n",
      "End of epoch 4, LoraLayer 2: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 3: 6668 Step\n",
      "End of epoch 4, LoraLayer 3: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 4: 6668 Step\n",
      "End of epoch 4, LoraLayer 4: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 5: 6668 Step\n",
      "End of epoch 4, LoraLayer 5: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 6: 6668 Step\n",
      "End of epoch 4, LoraLayer 6: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 7: 6668 Step\n",
      "End of epoch 4, LoraLayer 7: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 8: 6668 Step\n",
      "End of epoch 4, LoraLayer 8: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 9: 6668 Step\n",
      "End of epoch 4, LoraLayer 9: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 10: 6668 Step\n",
      "End of epoch 4, LoraLayer 10: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 11: 6668 Step\n",
      "End of epoch 4, LoraLayer 11: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 12: 6668 Step\n",
      "End of epoch 4, LoraLayer 12: Decay factor: 0.8749343752861023\n",
      "\n",
      "Testing loss: 2.3033668994903564, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 44s 26ms/step - loss: 0.1684 - accuracy: 0.9445 - val_loss: 2.3034 - val_accuracy: 0.1000\n",
      "Epoch 5/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.1650 - accuracy: 0.9440\n",
      "End of epoch 5, LoraLayer 0: 8335 Step\n",
      "End of epoch 5, LoraLayer 0: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 1: 8335 Step\n",
      "End of epoch 5, LoraLayer 1: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 2: 8335 Step\n",
      "End of epoch 5, LoraLayer 2: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 3: 8335 Step\n",
      "End of epoch 5, LoraLayer 3: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 4: 8335 Step\n",
      "End of epoch 5, LoraLayer 4: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 5: 8335 Step\n",
      "End of epoch 5, LoraLayer 5: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 6: 8335 Step\n",
      "End of epoch 5, LoraLayer 6: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 7: 8335 Step\n",
      "End of epoch 5, LoraLayer 7: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 8: 8335 Step\n",
      "End of epoch 5, LoraLayer 8: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 9: 8335 Step\n",
      "End of epoch 5, LoraLayer 9: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 10: 8335 Step\n",
      "End of epoch 5, LoraLayer 10: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 11: 8335 Step\n",
      "End of epoch 5, LoraLayer 11: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 12: 8335 Step\n",
      "End of epoch 5, LoraLayer 12: Decay factor: 0.8124203085899353\n",
      "\n",
      "Testing loss: 2.303368330001831, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 43s 26ms/step - loss: 0.1650 - accuracy: 0.9440 - val_loss: 2.3034 - val_accuracy: 0.1000\n",
      "Epoch 6/20\n",
      "1665/1667 [============================>.] - ETA: 0s - loss: 0.1753 - accuracy: 0.9417\n",
      "End of epoch 6, LoraLayer 0: 10002 Step\n",
      "End of epoch 6, LoraLayer 0: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 1: 10002 Step\n",
      "End of epoch 6, LoraLayer 1: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 2: 10002 Step\n",
      "End of epoch 6, LoraLayer 2: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 3: 10002 Step\n",
      "End of epoch 6, LoraLayer 3: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 4: 10002 Step\n",
      "End of epoch 6, LoraLayer 4: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 5: 10002 Step\n",
      "End of epoch 6, LoraLayer 5: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 6: 10002 Step\n",
      "End of epoch 6, LoraLayer 6: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 7: 10002 Step\n",
      "End of epoch 6, LoraLayer 7: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 8: 10002 Step\n",
      "End of epoch 6, LoraLayer 8: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 9: 10002 Step\n",
      "End of epoch 6, LoraLayer 9: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 10: 10002 Step\n",
      "End of epoch 6, LoraLayer 10: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 11: 10002 Step\n",
      "End of epoch 6, LoraLayer 11: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 12: 10002 Step\n",
      "End of epoch 6, LoraLayer 12: Decay factor: 0.7499062418937683\n",
      "\n",
      "Testing loss: 2.3033640384674072, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 43s 26ms/step - loss: 0.1752 - accuracy: 0.9417 - val_loss: 2.3034 - val_accuracy: 0.1000\n",
      "Epoch 7/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.1902 - accuracy: 0.9355\n",
      "End of epoch 7, LoraLayer 0: 11669 Step\n",
      "End of epoch 7, LoraLayer 0: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 1: 11669 Step\n",
      "End of epoch 7, LoraLayer 1: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 2: 11669 Step\n",
      "End of epoch 7, LoraLayer 2: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 3: 11669 Step\n",
      "End of epoch 7, LoraLayer 3: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 4: 11669 Step\n",
      "End of epoch 7, LoraLayer 4: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 5: 11669 Step\n",
      "End of epoch 7, LoraLayer 5: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 6: 11669 Step\n",
      "End of epoch 7, LoraLayer 6: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 7: 11669 Step\n",
      "End of epoch 7, LoraLayer 7: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 8: 11669 Step\n",
      "End of epoch 7, LoraLayer 8: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 9: 11669 Step\n",
      "End of epoch 7, LoraLayer 9: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 10: 11669 Step\n",
      "End of epoch 7, LoraLayer 10: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 11: 11669 Step\n",
      "End of epoch 7, LoraLayer 11: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 12: 11669 Step\n",
      "End of epoch 7, LoraLayer 12: Decay factor: 0.6873922348022461\n",
      "\n",
      "Testing loss: 2.3033666610717773, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 43s 26ms/step - loss: 0.1901 - accuracy: 0.9355 - val_loss: 2.3034 - val_accuracy: 0.1000\n",
      "Epoch 8/20\n",
      "1665/1667 [============================>.] - ETA: 0s - loss: 0.2087 - accuracy: 0.9296\n",
      "End of epoch 8, LoraLayer 0: 13336 Step\n",
      "End of epoch 8, LoraLayer 0: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 1: 13336 Step\n",
      "End of epoch 8, LoraLayer 1: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 2: 13336 Step\n",
      "End of epoch 8, LoraLayer 2: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 3: 13336 Step\n",
      "End of epoch 8, LoraLayer 3: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 4: 13336 Step\n",
      "End of epoch 8, LoraLayer 4: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 5: 13336 Step\n",
      "End of epoch 8, LoraLayer 5: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 6: 13336 Step\n",
      "End of epoch 8, LoraLayer 6: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 7: 13336 Step\n",
      "End of epoch 8, LoraLayer 7: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 8: 13336 Step\n",
      "End of epoch 8, LoraLayer 8: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 9: 13336 Step\n",
      "End of epoch 8, LoraLayer 9: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 10: 13336 Step\n",
      "End of epoch 8, LoraLayer 10: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 11: 13336 Step\n",
      "End of epoch 8, LoraLayer 11: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 12: 13336 Step\n",
      "End of epoch 8, LoraLayer 12: Decay factor: 0.6248781681060791\n",
      "\n",
      "Testing loss: 2.303351640701294, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 43s 26ms/step - loss: 0.2086 - accuracy: 0.9297 - val_loss: 2.3034 - val_accuracy: 0.1000\n",
      "Epoch 9/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.2372 - accuracy: 0.9191\n",
      "End of epoch 9, LoraLayer 0: 15003 Step\n",
      "End of epoch 9, LoraLayer 0: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 1: 15003 Step\n",
      "End of epoch 9, LoraLayer 1: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 2: 15003 Step\n",
      "End of epoch 9, LoraLayer 2: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 3: 15003 Step\n",
      "End of epoch 9, LoraLayer 3: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 4: 15003 Step\n",
      "End of epoch 9, LoraLayer 4: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 5: 15003 Step\n",
      "End of epoch 9, LoraLayer 5: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 6: 15003 Step\n",
      "End of epoch 9, LoraLayer 6: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 7: 15003 Step\n",
      "End of epoch 9, LoraLayer 7: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 8: 15003 Step\n",
      "End of epoch 9, LoraLayer 8: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 9: 15003 Step\n",
      "End of epoch 9, LoraLayer 9: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 10: 15003 Step\n",
      "End of epoch 9, LoraLayer 10: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 11: 15003 Step\n",
      "End of epoch 9, LoraLayer 11: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 12: 15003 Step\n",
      "End of epoch 9, LoraLayer 12: Decay factor: 0.5623641014099121\n",
      "\n",
      "Testing loss: 2.3033182621002197, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 43s 26ms/step - loss: 0.2372 - accuracy: 0.9191 - val_loss: 2.3033 - val_accuracy: 0.1000\n",
      "Epoch 10/20\n",
      "1665/1667 [============================>.] - ETA: 0s - loss: 0.2746 - accuracy: 0.9044\n",
      "End of epoch 10, LoraLayer 0: 16670 Step\n",
      "End of epoch 10, LoraLayer 0: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 1: 16670 Step\n",
      "End of epoch 10, LoraLayer 1: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 2: 16670 Step\n",
      "End of epoch 10, LoraLayer 2: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 3: 16670 Step\n",
      "End of epoch 10, LoraLayer 3: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 4: 16670 Step\n",
      "End of epoch 10, LoraLayer 4: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 5: 16670 Step\n",
      "End of epoch 10, LoraLayer 5: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 6: 16670 Step\n",
      "End of epoch 10, LoraLayer 6: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 7: 16670 Step\n",
      "End of epoch 10, LoraLayer 7: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 8: 16670 Step\n",
      "End of epoch 10, LoraLayer 8: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 9: 16670 Step\n",
      "End of epoch 10, LoraLayer 9: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 10: 16670 Step\n",
      "End of epoch 10, LoraLayer 10: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 11: 16670 Step\n",
      "End of epoch 10, LoraLayer 11: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 12: 16670 Step\n",
      "End of epoch 10, LoraLayer 12: Decay factor: 0.4998500347137451\n",
      "\n",
      "Testing loss: 2.303349018096924, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 43s 26ms/step - loss: 0.2746 - accuracy: 0.9043 - val_loss: 2.3033 - val_accuracy: 0.1000\n",
      "Epoch 11/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.3079 - accuracy: 0.8957\n",
      "End of epoch 11, LoraLayer 0: 18337 Step\n",
      "End of epoch 11, LoraLayer 0: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 1: 18337 Step\n",
      "End of epoch 11, LoraLayer 1: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 2: 18337 Step\n",
      "End of epoch 11, LoraLayer 2: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 3: 18337 Step\n",
      "End of epoch 11, LoraLayer 3: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 4: 18337 Step\n",
      "End of epoch 11, LoraLayer 4: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 5: 18337 Step\n",
      "End of epoch 11, LoraLayer 5: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 6: 18337 Step\n",
      "End of epoch 11, LoraLayer 6: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 7: 18337 Step\n",
      "End of epoch 11, LoraLayer 7: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 8: 18337 Step\n",
      "End of epoch 11, LoraLayer 8: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 9: 18337 Step\n",
      "End of epoch 11, LoraLayer 9: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 10: 18337 Step\n",
      "End of epoch 11, LoraLayer 10: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 11: 18337 Step\n",
      "End of epoch 11, LoraLayer 11: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 12: 18337 Step\n",
      "End of epoch 11, LoraLayer 12: Decay factor: 0.4373359680175781\n",
      "\n",
      "Testing loss: 2.303243637084961, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 42s 25ms/step - loss: 0.3078 - accuracy: 0.8957 - val_loss: 2.3032 - val_accuracy: 0.1000\n",
      "Epoch 12/20\n",
      "1665/1667 [============================>.] - ETA: 0s - loss: 0.3588 - accuracy: 0.8771\n",
      "End of epoch 12, LoraLayer 0: 20004 Step\n",
      "End of epoch 12, LoraLayer 0: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 1: 20004 Step\n",
      "End of epoch 12, LoraLayer 1: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 2: 20004 Step\n",
      "End of epoch 12, LoraLayer 2: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 3: 20004 Step\n",
      "End of epoch 12, LoraLayer 3: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 4: 20004 Step\n",
      "End of epoch 12, LoraLayer 4: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 5: 20004 Step\n",
      "End of epoch 12, LoraLayer 5: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 6: 20004 Step\n",
      "End of epoch 12, LoraLayer 6: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 7: 20004 Step\n",
      "End of epoch 12, LoraLayer 7: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 8: 20004 Step\n",
      "End of epoch 12, LoraLayer 8: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 9: 20004 Step\n",
      "End of epoch 12, LoraLayer 9: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 10: 20004 Step\n",
      "End of epoch 12, LoraLayer 10: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 11: 20004 Step\n",
      "End of epoch 12, LoraLayer 11: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 12: 20004 Step\n",
      "End of epoch 12, LoraLayer 12: Decay factor: 0.37482190132141113\n",
      "\n",
      "Testing loss: 2.3028647899627686, acc: 0.0997999981045723\n",
      "\n",
      "1667/1667 [==============================] - 43s 26ms/step - loss: 0.3589 - accuracy: 0.8771 - val_loss: 2.3029 - val_accuracy: 0.0998\n",
      "Epoch 13/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.4087 - accuracy: 0.8608\n",
      "End of epoch 13, LoraLayer 0: 21671 Step\n",
      "End of epoch 13, LoraLayer 0: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 1: 21671 Step\n",
      "End of epoch 13, LoraLayer 1: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 2: 21671 Step\n",
      "End of epoch 13, LoraLayer 2: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 3: 21671 Step\n",
      "End of epoch 13, LoraLayer 3: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 4: 21671 Step\n",
      "End of epoch 13, LoraLayer 4: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 5: 21671 Step\n",
      "End of epoch 13, LoraLayer 5: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 6: 21671 Step\n",
      "End of epoch 13, LoraLayer 6: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 7: 21671 Step\n",
      "End of epoch 13, LoraLayer 7: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 8: 21671 Step\n",
      "End of epoch 13, LoraLayer 8: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 9: 21671 Step\n",
      "End of epoch 13, LoraLayer 9: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 10: 21671 Step\n",
      "End of epoch 13, LoraLayer 10: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 11: 21671 Step\n",
      "End of epoch 13, LoraLayer 11: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 12: 21671 Step\n",
      "End of epoch 13, LoraLayer 12: Decay factor: 0.31230783462524414\n",
      "\n",
      "Testing loss: 2.3025612831115723, acc: 0.1005999967455864\n",
      "\n",
      "1667/1667 [==============================] - 43s 26ms/step - loss: 0.4087 - accuracy: 0.8608 - val_loss: 2.3026 - val_accuracy: 0.1003\n",
      "Epoch 14/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.4654 - accuracy: 0.8432\n",
      "End of epoch 14, LoraLayer 0: 23338 Step\n",
      "End of epoch 14, LoraLayer 0: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 1: 23338 Step\n",
      "End of epoch 14, LoraLayer 1: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 2: 23338 Step\n",
      "End of epoch 14, LoraLayer 2: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 3: 23338 Step\n",
      "End of epoch 14, LoraLayer 3: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 4: 23338 Step\n",
      "End of epoch 14, LoraLayer 4: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 5: 23338 Step\n",
      "End of epoch 14, LoraLayer 5: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 6: 23338 Step\n",
      "End of epoch 14, LoraLayer 6: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 7: 23338 Step\n",
      "End of epoch 14, LoraLayer 7: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 8: 23338 Step\n",
      "End of epoch 14, LoraLayer 8: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 9: 23338 Step\n",
      "End of epoch 14, LoraLayer 9: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 10: 23338 Step\n",
      "End of epoch 14, LoraLayer 10: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 11: 23338 Step\n",
      "End of epoch 14, LoraLayer 11: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 12: 23338 Step\n",
      "End of epoch 14, LoraLayer 12: Decay factor: 0.24979376792907715\n",
      "\n",
      "Testing loss: 2.301924705505371, acc: 0.10890000313520432\n",
      "\n",
      "1667/1667 [==============================] - 43s 26ms/step - loss: 0.4654 - accuracy: 0.8432 - val_loss: 2.3019 - val_accuracy: 0.1090\n",
      "Epoch 15/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.5276 - accuracy: 0.8197\n",
      "End of epoch 15, LoraLayer 0: 25005 Step\n",
      "End of epoch 15, LoraLayer 0: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 1: 25005 Step\n",
      "End of epoch 15, LoraLayer 1: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 2: 25005 Step\n",
      "End of epoch 15, LoraLayer 2: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 3: 25005 Step\n",
      "End of epoch 15, LoraLayer 3: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 4: 25005 Step\n",
      "End of epoch 15, LoraLayer 4: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 5: 25005 Step\n",
      "End of epoch 15, LoraLayer 5: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 6: 25005 Step\n",
      "End of epoch 15, LoraLayer 6: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 7: 25005 Step\n",
      "End of epoch 15, LoraLayer 7: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 8: 25005 Step\n",
      "End of epoch 15, LoraLayer 8: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 9: 25005 Step\n",
      "End of epoch 15, LoraLayer 9: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 10: 25005 Step\n",
      "End of epoch 15, LoraLayer 10: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 11: 25005 Step\n",
      "End of epoch 15, LoraLayer 11: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 12: 25005 Step\n",
      "End of epoch 15, LoraLayer 12: Decay factor: 0.18727970123291016\n",
      "\n",
      "Testing loss: 2.2982289791107178, acc: 0.12430000305175781\n",
      "\n",
      "1667/1667 [==============================] - 43s 26ms/step - loss: 0.5276 - accuracy: 0.8197 - val_loss: 2.2982 - val_accuracy: 0.1243\n",
      "Epoch 16/20\n",
      "1665/1667 [============================>.] - ETA: 0s - loss: 0.5852 - accuracy: 0.8014\n",
      "End of epoch 16, LoraLayer 0: 26672 Step\n",
      "End of epoch 16, LoraLayer 0: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 1: 26672 Step\n",
      "End of epoch 16, LoraLayer 1: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 2: 26672 Step\n",
      "End of epoch 16, LoraLayer 2: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 3: 26672 Step\n",
      "End of epoch 16, LoraLayer 3: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 4: 26672 Step\n",
      "End of epoch 16, LoraLayer 4: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 5: 26672 Step\n",
      "End of epoch 16, LoraLayer 5: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 6: 26672 Step\n",
      "End of epoch 16, LoraLayer 6: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 7: 26672 Step\n",
      "End of epoch 16, LoraLayer 7: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 8: 26672 Step\n",
      "End of epoch 16, LoraLayer 8: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 9: 26672 Step\n",
      "End of epoch 16, LoraLayer 9: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 10: 26672 Step\n",
      "End of epoch 16, LoraLayer 10: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 11: 26672 Step\n",
      "End of epoch 16, LoraLayer 11: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 12: 26672 Step\n",
      "End of epoch 16, LoraLayer 12: Decay factor: 0.12476563453674316\n",
      "\n",
      "Testing loss: 2.2303626537323, acc: 0.21690000593662262\n",
      "\n",
      "1667/1667 [==============================] - 43s 26ms/step - loss: 0.5852 - accuracy: 0.8014 - val_loss: 2.2304 - val_accuracy: 0.2166\n",
      "Epoch 17/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.6644 - accuracy: 0.7750\n",
      "End of epoch 17, LoraLayer 0: 28339 Step\n",
      "End of epoch 17, LoraLayer 0: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 1: 28339 Step\n",
      "End of epoch 17, LoraLayer 1: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 2: 28339 Step\n",
      "End of epoch 17, LoraLayer 2: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 3: 28339 Step\n",
      "End of epoch 17, LoraLayer 3: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 4: 28339 Step\n",
      "End of epoch 17, LoraLayer 4: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 5: 28339 Step\n",
      "End of epoch 17, LoraLayer 5: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 6: 28339 Step\n",
      "End of epoch 17, LoraLayer 6: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 7: 28339 Step\n",
      "End of epoch 17, LoraLayer 7: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 8: 28339 Step\n",
      "End of epoch 17, LoraLayer 8: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 9: 28339 Step\n",
      "End of epoch 17, LoraLayer 9: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 10: 28339 Step\n",
      "End of epoch 17, LoraLayer 10: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 11: 28339 Step\n",
      "End of epoch 17, LoraLayer 11: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 12: 28339 Step\n",
      "End of epoch 17, LoraLayer 12: Decay factor: 0.06225156784057617\n",
      "\n",
      "Testing loss: 1.7617297172546387, acc: 0.4049000144004822\n",
      "\n",
      "1667/1667 [==============================] - 44s 26ms/step - loss: 0.6644 - accuracy: 0.7750 - val_loss: 1.7617 - val_accuracy: 0.4048\n",
      "Epoch 18/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.7507 - accuracy: 0.7493\n",
      "End of epoch 18, LoraLayer 0: 30006 Step\n",
      "End of epoch 18, LoraLayer 0: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 1: 30006 Step\n",
      "End of epoch 18, LoraLayer 1: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 2: 30006 Step\n",
      "End of epoch 18, LoraLayer 2: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 3: 30006 Step\n",
      "End of epoch 18, LoraLayer 3: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 4: 30006 Step\n",
      "End of epoch 18, LoraLayer 4: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 5: 30006 Step\n",
      "End of epoch 18, LoraLayer 5: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 6: 30006 Step\n",
      "End of epoch 18, LoraLayer 6: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 7: 30006 Step\n",
      "End of epoch 18, LoraLayer 7: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 8: 30006 Step\n",
      "End of epoch 18, LoraLayer 8: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 9: 30006 Step\n",
      "End of epoch 18, LoraLayer 9: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 10: 30006 Step\n",
      "End of epoch 18, LoraLayer 10: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 11: 30006 Step\n",
      "End of epoch 18, LoraLayer 11: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 12: 30006 Step\n",
      "End of epoch 18, LoraLayer 12: Decay factor: 0.0\n",
      "\n",
      "Testing loss: 0.8670042753219604, acc: 0.7145000100135803\n",
      "\n",
      "1667/1667 [==============================] - 44s 26ms/step - loss: 0.7507 - accuracy: 0.7493 - val_loss: 0.8670 - val_accuracy: 0.7146\n",
      "Epoch 19/20\n",
      "1665/1667 [============================>.] - ETA: 0s - loss: 0.7538 - accuracy: 0.7504\n",
      "End of epoch 19, LoraLayer 0: 31673 Step\n",
      "End of epoch 19, LoraLayer 0: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 1: 31673 Step\n",
      "End of epoch 19, LoraLayer 1: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 2: 31673 Step\n",
      "End of epoch 19, LoraLayer 2: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 3: 31673 Step\n",
      "End of epoch 19, LoraLayer 3: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 4: 31673 Step\n",
      "End of epoch 19, LoraLayer 4: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 5: 31673 Step\n",
      "End of epoch 19, LoraLayer 5: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 6: 31673 Step\n",
      "End of epoch 19, LoraLayer 6: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 7: 31673 Step\n",
      "End of epoch 19, LoraLayer 7: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 8: 31673 Step\n",
      "End of epoch 19, LoraLayer 8: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 9: 31673 Step\n",
      "End of epoch 19, LoraLayer 9: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 10: 31673 Step\n",
      "End of epoch 19, LoraLayer 10: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 11: 31673 Step\n",
      "End of epoch 19, LoraLayer 11: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 12: 31673 Step\n",
      "End of epoch 19, LoraLayer 12: Decay factor: 0.0\n",
      "\n",
      "Testing loss: 0.8099938035011292, acc: 0.7329999804496765\n",
      "\n",
      "1667/1667 [==============================] - 44s 27ms/step - loss: 0.7541 - accuracy: 0.7503 - val_loss: 0.8099 - val_accuracy: 0.7329\n",
      "Epoch 20/20\n",
      "1665/1667 [============================>.] - ETA: 0s - loss: 0.6814 - accuracy: 0.7748\n",
      "End of epoch 20, LoraLayer 0: 33340 Step\n",
      "End of epoch 20, LoraLayer 0: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 1: 33340 Step\n",
      "End of epoch 20, LoraLayer 1: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 2: 33340 Step\n",
      "End of epoch 20, LoraLayer 2: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 3: 33340 Step\n",
      "End of epoch 20, LoraLayer 3: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 4: 33340 Step\n",
      "End of epoch 20, LoraLayer 4: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 5: 33340 Step\n",
      "End of epoch 20, LoraLayer 5: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 6: 33340 Step\n",
      "End of epoch 20, LoraLayer 6: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 7: 33340 Step\n",
      "End of epoch 20, LoraLayer 7: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 8: 33340 Step\n",
      "End of epoch 20, LoraLayer 8: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 9: 33340 Step\n",
      "End of epoch 20, LoraLayer 9: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 10: 33340 Step\n",
      "End of epoch 20, LoraLayer 10: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 11: 33340 Step\n",
      "End of epoch 20, LoraLayer 11: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 12: 33340 Step\n",
      "End of epoch 20, LoraLayer 12: Decay factor: 0.0\n",
      "\n",
      "Testing loss: 0.8239779472351074, acc: 0.7272999882698059\n",
      "\n",
      "1667/1667 [==============================] - 43s 26ms/step - loss: 0.6816 - accuracy: 0.7748 - val_loss: 0.8239 - val_accuracy: 0.7272\n"
     ]
    }
   ],
   "source": [
    "history32 = lora_vgg161.fit(x_train, y_train, batch_size=30, epochs=20, validation_data=(x_test, y_test), callbacks=[print_step_callback, TestCallback((x_test, y_test))] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bxbUXaLxfZIF"
   },
   "source": [
    "## inference time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fj8riYOifa42",
    "outputId": "f5b18b66-2306-4971-d08f-fa4cf52d5fbc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 3s 8ms/step - loss: 0.8240 - accuracy: 0.7273\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8239779472351074, 0.7272999882698059]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 평가\n",
    "lora_vgg161.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "id": "xLQEo46Gbt-o",
    "outputId": "0a3818f4-3558-434b-fcf9-ab6cc8536ccc"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 학습 및 검증 정확도 시각화\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# 정확도 그래프\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history32.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history32.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# 손실 그래프\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history32.history['loss'], label='Training Loss')\n",
    "plt.plot(history32.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GvAH25Td4TPl"
   },
   "source": [
    "## 1-2. (32, -)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "oX1SFBOl4f0i"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout, InputLayer\n",
    "import tensorflow as tf\n",
    "\n",
    "# 새로운 레이어를 추가하기 위한 레이어 리스트 초기화\n",
    "new_layers = []\n",
    "\n",
    "# VGG 모델의 각 레이어 순회\n",
    "for layer in best_vgg16.layers:\n",
    "    if isinstance(layer, InputLayer):\n",
    "        continue\n",
    "\n",
    "    if isinstance(layer, Conv2D):\n",
    "        # Conv2D 레이어를 ConvLoRALayer로 대체\n",
    "        conv_lora_layer = ConvLoRALayer00_cdn2(original_conv_layer=layer, rank=32, alpha=32, total_iteration = total_iteration, trainable=True)\n",
    "        new_layers.append(conv_lora_layer)\n",
    "\n",
    "    else:\n",
    "        # 다른 유형의 레이어는 그대로 유지\n",
    "        new_layers.append(layer)\n",
    "\n",
    "# 새로운 입력 텐서 생성\n",
    "input_tensor = best_vgg16.input\n",
    "\n",
    "# 새로운 모델 구성\n",
    "x = input_tensor\n",
    "for layer in new_layers:\n",
    "    x = layer(x)\n",
    "\n",
    "# 새로운 모델 생성\n",
    "lora_vgg162 = Model(inputs=input_tensor, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "wk9goxZs4f0i"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D\n",
    "\n",
    "# 새로 생성된 모델에서 Dense 레이어 중 LoRA가 적용되지 않은 레이어의 trainable을 False로 설정\n",
    "for layer in lora_vgg162.layers:\n",
    "    if isinstance(layer, Dense):\n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OaPm45ig4f0i",
    "outputId": "ffcabf22-e139-49d0-d76a-1fba020e7e1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " block1_conv1 (ConvLoRALaye  (None, 32, 32, 64)        21090     \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block1_conv2 (ConvLoRALaye  (None, 32, 32, 64)        73794     \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " block2_conv1 (ConvLoRALaye  (None, 16, 16, 128)       129154    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block2_conv2 (ConvLoRALaye  (None, 16, 16, 128)       221314    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " block3_conv1 (ConvLoRALaye  (None, 8, 8, 256)         405762    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block3_conv2 (ConvLoRALaye  (None, 8, 8, 256)         737538    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block3_conv3 (ConvLoRALaye  (None, 8, 8, 256)         737538    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 4, 4, 256)         0         \n",
      "                                                                 \n",
      " block4_conv1 (ConvLoRALaye  (None, 4, 4, 512)         1401346   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block4_conv2 (ConvLoRALaye  (None, 4, 4, 512)         2654722   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block4_conv3 (ConvLoRALaye  (None, 4, 4, 512)         2654722   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      " block5_conv1 (ConvLoRALaye  (None, 2, 2, 512)         2654722   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block5_conv2 (ConvLoRALaye  (None, 2, 2, 512)         2654722   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block5_conv3 (ConvLoRALaye  (None, 2, 2, 512)         2654722   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 1, 1, 512)         0         \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 4096)              2101248   \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 512)               2097664   \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21205188 (80.89 MB)\n",
      "Trainable params: 2286432 (8.72 MB)\n",
      "Non-trainable params: 18918756 (72.17 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lora_vgg162.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F77s-EYN4vcq",
    "outputId": "a19ebea2-61a9-4f67-9d53-619bc2692331"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: input_3\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block1_conv1\n",
      "  Trainable parameters: 19296\n",
      "  Non-trainable parameters: 1794\n",
      "Layer: block1_conv2\n",
      "  Trainable parameters: 36864\n",
      "  Non-trainable parameters: 36930\n",
      "Layer: block1_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block2_conv1\n",
      "  Trainable parameters: 55296\n",
      "  Non-trainable parameters: 73858\n",
      "Layer: block2_conv2\n",
      "  Trainable parameters: 73728\n",
      "  Non-trainable parameters: 147586\n",
      "Layer: block2_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block3_conv1\n",
      "  Trainable parameters: 110592\n",
      "  Non-trainable parameters: 295170\n",
      "Layer: block3_conv2\n",
      "  Trainable parameters: 147456\n",
      "  Non-trainable parameters: 590082\n",
      "Layer: block3_conv3\n",
      "  Trainable parameters: 147456\n",
      "  Non-trainable parameters: 590082\n",
      "Layer: block3_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block4_conv1\n",
      "  Trainable parameters: 221184\n",
      "  Non-trainable parameters: 1180162\n",
      "Layer: block4_conv2\n",
      "  Trainable parameters: 294912\n",
      "  Non-trainable parameters: 2359810\n",
      "Layer: block4_conv3\n",
      "  Trainable parameters: 294912\n",
      "  Non-trainable parameters: 2359810\n",
      "Layer: block4_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block5_conv1\n",
      "  Trainable parameters: 294912\n",
      "  Non-trainable parameters: 2359810\n",
      "Layer: block5_conv2\n",
      "  Trainable parameters: 294912\n",
      "  Non-trainable parameters: 2359810\n",
      "Layer: block5_conv3\n",
      "  Trainable parameters: 294912\n",
      "  Non-trainable parameters: 2359810\n",
      "Layer: block5_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: flatten_2\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: dense_6\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 2101248\n",
      "Layer: dropout_4\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: dense_7\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 2097664\n",
      "Layer: dropout_5\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: dense_8\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 5130\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "#모델의 각 레이어를 순회하며 파라미터 수를 계산\n",
    "for layer in lora_vgg162.layers:\n",
    "    trainable_count = np.sum([tf.size(w).numpy() for w in layer.trainable_weights])\n",
    "    non_trainable_count = np.sum([tf.size(w).numpy() for w in layer.non_trainable_weights])\n",
    "\n",
    "    print(f\"Layer: {layer.name}\")\n",
    "    print(f\"  Trainable parameters: {trainable_count}\")\n",
    "    print(f\"  Non-trainable parameters: {non_trainable_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "OK4liexw4zdy"
   },
   "outputs": [],
   "source": [
    "lora_layers = []  # lora 레이어들을 저장할 리스트\n",
    "\n",
    "for layer in lora_vgg162.layers:\n",
    "    if isinstance(layer, ConvLoRALayer00_cdn2) or isinstance(layer, LoraLayer):\n",
    "        lora_layers.append(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "1iKvTdFN4zdy"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "class PrintCurrentStepCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, lora_layers):\n",
    "        super().__init__()\n",
    "        self.lora_layers = lora_layers\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print()\n",
    "        for i, lora_layer in enumerate(self.lora_layers):\n",
    "            current_step = lora_layer.current_step.value()\n",
    "            decay_factor = lora_layer.decay_factor.value()\n",
    "            print(f\"End of epoch {epoch + 1}, LoraLayer {i}: {current_step} Step\")\n",
    "            print(f\"End of epoch {epoch + 1}, LoraLayer {i}: Decay factor: {decay_factor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "lL8z4Ky84zdz"
   },
   "outputs": [],
   "source": [
    "# 콜백 생성 시 lora_layers 딕셔너리의 값만 사용\n",
    "print_step_callback = PrintCurrentStepCallback(lora_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "vmjY3HT-4zdz"
   },
   "outputs": [],
   "source": [
    "class TestCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, test_data):\n",
    "        self.test_data = test_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        x, y = self.test_data\n",
    "        loss, acc = self.model.evaluate(x, y, verbose=0)\n",
    "        print('\\nTesting loss: {}, acc: {}\\n'.format(loss, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TtqOiU8844Ta"
   },
   "source": [
    "##학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "pCKYMJdu44Tb"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# 모델 컴파일\n",
    "lora_vgg162.compile(optimizer=Adam(learning_rate=0.0001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YSTgDSLX44Tb",
    "outputId": "1413af14-7277-4ffd-c91d-8dfbd2443aea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " block1_conv1 (ConvLoRALaye  (None, 32, 32, 64)        21090     \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block1_conv2 (ConvLoRALaye  (None, 32, 32, 64)        73794     \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " block2_conv1 (ConvLoRALaye  (None, 16, 16, 128)       129154    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block2_conv2 (ConvLoRALaye  (None, 16, 16, 128)       221314    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " block3_conv1 (ConvLoRALaye  (None, 8, 8, 256)         405762    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block3_conv2 (ConvLoRALaye  (None, 8, 8, 256)         737538    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block3_conv3 (ConvLoRALaye  (None, 8, 8, 256)         737538    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 4, 4, 256)         0         \n",
      "                                                                 \n",
      " block4_conv1 (ConvLoRALaye  (None, 4, 4, 512)         1401346   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block4_conv2 (ConvLoRALaye  (None, 4, 4, 512)         2654722   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block4_conv3 (ConvLoRALaye  (None, 4, 4, 512)         2654722   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      " block5_conv1 (ConvLoRALaye  (None, 2, 2, 512)         2654722   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block5_conv2 (ConvLoRALaye  (None, 2, 2, 512)         2654722   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block5_conv3 (ConvLoRALaye  (None, 2, 2, 512)         2654722   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 1, 1, 512)         0         \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 4096)              2101248   \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 512)               2097664   \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21205188 (80.89 MB)\n",
      "Trainable params: 2286432 (8.72 MB)\n",
      "Non-trainable params: 18918756 (72.17 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lora_vgg162.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HW3wKPlB44Tb",
    "outputId": "d75c95be-0610-4c6e-f0f1-a8c64ade3cbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.1455 - accuracy: 0.9529\n",
      "End of epoch 1, LoraLayer 0: 1667 Step\n",
      "End of epoch 1, LoraLayer 0: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 1: 1667 Step\n",
      "End of epoch 1, LoraLayer 1: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 2: 1667 Step\n",
      "End of epoch 1, LoraLayer 2: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 3: 1667 Step\n",
      "End of epoch 1, LoraLayer 3: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 4: 1667 Step\n",
      "End of epoch 1, LoraLayer 4: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 5: 1667 Step\n",
      "End of epoch 1, LoraLayer 5: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 6: 1667 Step\n",
      "End of epoch 1, LoraLayer 6: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 7: 1667 Step\n",
      "End of epoch 1, LoraLayer 7: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 8: 1667 Step\n",
      "End of epoch 1, LoraLayer 8: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 9: 1667 Step\n",
      "End of epoch 1, LoraLayer 9: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 10: 1667 Step\n",
      "End of epoch 1, LoraLayer 10: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 11: 1667 Step\n",
      "End of epoch 1, LoraLayer 11: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 12: 1667 Step\n",
      "End of epoch 1, LoraLayer 12: Decay factor: 1.0\n",
      "\n",
      "Testing loss: 2.3033711910247803, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 51s 26ms/step - loss: 0.1455 - accuracy: 0.9529 - val_loss: 2.3034 - val_accuracy: 0.1000\n",
      "Epoch 2/20\n",
      "1665/1667 [============================>.] - ETA: 0s - loss: 0.1131 - accuracy: 0.9640\n",
      "End of epoch 2, LoraLayer 0: 3334 Step\n",
      "End of epoch 2, LoraLayer 0: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 1: 3334 Step\n",
      "End of epoch 2, LoraLayer 1: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 2: 3334 Step\n",
      "End of epoch 2, LoraLayer 2: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 3: 3334 Step\n",
      "End of epoch 2, LoraLayer 3: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 4: 3334 Step\n",
      "End of epoch 2, LoraLayer 4: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 5: 3334 Step\n",
      "End of epoch 2, LoraLayer 5: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 6: 3334 Step\n",
      "End of epoch 2, LoraLayer 6: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 7: 3334 Step\n",
      "End of epoch 2, LoraLayer 7: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 8: 3334 Step\n",
      "End of epoch 2, LoraLayer 8: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 9: 3334 Step\n",
      "End of epoch 2, LoraLayer 9: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 10: 3334 Step\n",
      "End of epoch 2, LoraLayer 10: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 11: 3334 Step\n",
      "End of epoch 2, LoraLayer 11: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 12: 3334 Step\n",
      "End of epoch 2, LoraLayer 12: Decay factor: 0.9999625086784363\n",
      "\n",
      "Testing loss: 2.3033711910247803, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 41s 25ms/step - loss: 0.1131 - accuracy: 0.9640 - val_loss: 2.3034 - val_accuracy: 0.1000\n",
      "Epoch 3/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.0975 - accuracy: 0.9677\n",
      "End of epoch 3, LoraLayer 0: 5001 Step\n",
      "End of epoch 3, LoraLayer 0: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 1: 5001 Step\n",
      "End of epoch 3, LoraLayer 1: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 2: 5001 Step\n",
      "End of epoch 3, LoraLayer 2: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 3: 5001 Step\n",
      "End of epoch 3, LoraLayer 3: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 4: 5001 Step\n",
      "End of epoch 3, LoraLayer 4: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 5: 5001 Step\n",
      "End of epoch 3, LoraLayer 5: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 6: 5001 Step\n",
      "End of epoch 3, LoraLayer 6: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 7: 5001 Step\n",
      "End of epoch 3, LoraLayer 7: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 8: 5001 Step\n",
      "End of epoch 3, LoraLayer 8: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 9: 5001 Step\n",
      "End of epoch 3, LoraLayer 9: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 10: 5001 Step\n",
      "End of epoch 3, LoraLayer 10: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 11: 5001 Step\n",
      "End of epoch 3, LoraLayer 11: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 12: 5001 Step\n",
      "End of epoch 3, LoraLayer 12: Decay factor: 0.9374484419822693\n",
      "\n",
      "Testing loss: 2.3033711910247803, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 42s 25ms/step - loss: 0.0975 - accuracy: 0.9677 - val_loss: 2.3034 - val_accuracy: 0.1000\n",
      "Epoch 4/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.0829 - accuracy: 0.9725\n",
      "End of epoch 4, LoraLayer 0: 6668 Step\n",
      "End of epoch 4, LoraLayer 0: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 1: 6668 Step\n",
      "End of epoch 4, LoraLayer 1: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 2: 6668 Step\n",
      "End of epoch 4, LoraLayer 2: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 3: 6668 Step\n",
      "End of epoch 4, LoraLayer 3: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 4: 6668 Step\n",
      "End of epoch 4, LoraLayer 4: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 5: 6668 Step\n",
      "End of epoch 4, LoraLayer 5: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 6: 6668 Step\n",
      "End of epoch 4, LoraLayer 6: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 7: 6668 Step\n",
      "End of epoch 4, LoraLayer 7: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 8: 6668 Step\n",
      "End of epoch 4, LoraLayer 8: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 9: 6668 Step\n",
      "End of epoch 4, LoraLayer 9: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 10: 6668 Step\n",
      "End of epoch 4, LoraLayer 10: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 11: 6668 Step\n",
      "End of epoch 4, LoraLayer 11: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 12: 6668 Step\n",
      "End of epoch 4, LoraLayer 12: Decay factor: 0.8749343752861023\n",
      "\n",
      "Testing loss: 2.3033711910247803, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 42s 25ms/step - loss: 0.0829 - accuracy: 0.9725 - val_loss: 2.3034 - val_accuracy: 0.1000\n",
      "Epoch 5/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.0884 - accuracy: 0.9711\n",
      "End of epoch 5, LoraLayer 0: 8335 Step\n",
      "End of epoch 5, LoraLayer 0: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 1: 8335 Step\n",
      "End of epoch 5, LoraLayer 1: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 2: 8335 Step\n",
      "End of epoch 5, LoraLayer 2: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 3: 8335 Step\n",
      "End of epoch 5, LoraLayer 3: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 4: 8335 Step\n",
      "End of epoch 5, LoraLayer 4: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 5: 8335 Step\n",
      "End of epoch 5, LoraLayer 5: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 6: 8335 Step\n",
      "End of epoch 5, LoraLayer 6: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 7: 8335 Step\n",
      "End of epoch 5, LoraLayer 7: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 8: 8335 Step\n",
      "End of epoch 5, LoraLayer 8: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 9: 8335 Step\n",
      "End of epoch 5, LoraLayer 9: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 10: 8335 Step\n",
      "End of epoch 5, LoraLayer 10: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 11: 8335 Step\n",
      "End of epoch 5, LoraLayer 11: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 12: 8335 Step\n",
      "End of epoch 5, LoraLayer 12: Decay factor: 0.8124203085899353\n",
      "\n",
      "Testing loss: 2.3033711910247803, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 43s 26ms/step - loss: 0.0884 - accuracy: 0.9711 - val_loss: 2.3034 - val_accuracy: 0.1000\n",
      "Epoch 6/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.0938 - accuracy: 0.9685\n",
      "End of epoch 6, LoraLayer 0: 10002 Step\n",
      "End of epoch 6, LoraLayer 0: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 1: 10002 Step\n",
      "End of epoch 6, LoraLayer 1: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 2: 10002 Step\n",
      "End of epoch 6, LoraLayer 2: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 3: 10002 Step\n",
      "End of epoch 6, LoraLayer 3: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 4: 10002 Step\n",
      "End of epoch 6, LoraLayer 4: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 5: 10002 Step\n",
      "End of epoch 6, LoraLayer 5: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 6: 10002 Step\n",
      "End of epoch 6, LoraLayer 6: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 7: 10002 Step\n",
      "End of epoch 6, LoraLayer 7: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 8: 10002 Step\n",
      "End of epoch 6, LoraLayer 8: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 9: 10002 Step\n",
      "End of epoch 6, LoraLayer 9: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 10: 10002 Step\n",
      "End of epoch 6, LoraLayer 10: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 11: 10002 Step\n",
      "End of epoch 6, LoraLayer 11: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 12: 10002 Step\n",
      "End of epoch 6, LoraLayer 12: Decay factor: 0.7499062418937683\n",
      "\n",
      "Testing loss: 2.3033711910247803, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 42s 25ms/step - loss: 0.0939 - accuracy: 0.9685 - val_loss: 2.3034 - val_accuracy: 0.1000\n",
      "Epoch 7/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.0973 - accuracy: 0.9677\n",
      "End of epoch 7, LoraLayer 0: 11669 Step\n",
      "End of epoch 7, LoraLayer 0: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 1: 11669 Step\n",
      "End of epoch 7, LoraLayer 1: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 2: 11669 Step\n",
      "End of epoch 7, LoraLayer 2: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 3: 11669 Step\n",
      "End of epoch 7, LoraLayer 3: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 4: 11669 Step\n",
      "End of epoch 7, LoraLayer 4: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 5: 11669 Step\n",
      "End of epoch 7, LoraLayer 5: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 6: 11669 Step\n",
      "End of epoch 7, LoraLayer 6: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 7: 11669 Step\n",
      "End of epoch 7, LoraLayer 7: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 8: 11669 Step\n",
      "End of epoch 7, LoraLayer 8: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 9: 11669 Step\n",
      "End of epoch 7, LoraLayer 9: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 10: 11669 Step\n",
      "End of epoch 7, LoraLayer 10: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 11: 11669 Step\n",
      "End of epoch 7, LoraLayer 11: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 12: 11669 Step\n",
      "End of epoch 7, LoraLayer 12: Decay factor: 0.6873922348022461\n",
      "\n",
      "Testing loss: 2.3033711910247803, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 42s 25ms/step - loss: 0.0973 - accuracy: 0.9677 - val_loss: 2.3034 - val_accuracy: 0.1000\n",
      "Epoch 8/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.1110 - accuracy: 0.9622\n",
      "End of epoch 8, LoraLayer 0: 13336 Step\n",
      "End of epoch 8, LoraLayer 0: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 1: 13336 Step\n",
      "End of epoch 8, LoraLayer 1: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 2: 13336 Step\n",
      "End of epoch 8, LoraLayer 2: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 3: 13336 Step\n",
      "End of epoch 8, LoraLayer 3: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 4: 13336 Step\n",
      "End of epoch 8, LoraLayer 4: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 5: 13336 Step\n",
      "End of epoch 8, LoraLayer 5: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 6: 13336 Step\n",
      "End of epoch 8, LoraLayer 6: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 7: 13336 Step\n",
      "End of epoch 8, LoraLayer 7: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 8: 13336 Step\n",
      "End of epoch 8, LoraLayer 8: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 9: 13336 Step\n",
      "End of epoch 8, LoraLayer 9: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 10: 13336 Step\n",
      "End of epoch 8, LoraLayer 10: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 11: 13336 Step\n",
      "End of epoch 8, LoraLayer 11: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 12: 13336 Step\n",
      "End of epoch 8, LoraLayer 12: Decay factor: 0.6248781681060791\n",
      "\n",
      "Testing loss: 2.3033716678619385, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 43s 26ms/step - loss: 0.1110 - accuracy: 0.9622 - val_loss: 2.3034 - val_accuracy: 0.1000\n",
      "Epoch 9/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.1255 - accuracy: 0.9572\n",
      "End of epoch 9, LoraLayer 0: 15003 Step\n",
      "End of epoch 9, LoraLayer 0: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 1: 15003 Step\n",
      "End of epoch 9, LoraLayer 1: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 2: 15003 Step\n",
      "End of epoch 9, LoraLayer 2: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 3: 15003 Step\n",
      "End of epoch 9, LoraLayer 3: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 4: 15003 Step\n",
      "End of epoch 9, LoraLayer 4: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 5: 15003 Step\n",
      "End of epoch 9, LoraLayer 5: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 6: 15003 Step\n",
      "End of epoch 9, LoraLayer 6: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 7: 15003 Step\n",
      "End of epoch 9, LoraLayer 7: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 8: 15003 Step\n",
      "End of epoch 9, LoraLayer 8: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 9: 15003 Step\n",
      "End of epoch 9, LoraLayer 9: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 10: 15003 Step\n",
      "End of epoch 9, LoraLayer 10: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 11: 15003 Step\n",
      "End of epoch 9, LoraLayer 11: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 12: 15003 Step\n",
      "End of epoch 9, LoraLayer 12: Decay factor: 0.5623641014099121\n",
      "\n",
      "Testing loss: 2.3033714294433594, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 44s 26ms/step - loss: 0.1255 - accuracy: 0.9572 - val_loss: 2.3034 - val_accuracy: 0.1000\n",
      "Epoch 10/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.1396 - accuracy: 0.9524\n",
      "End of epoch 10, LoraLayer 0: 16670 Step\n",
      "End of epoch 10, LoraLayer 0: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 1: 16670 Step\n",
      "End of epoch 10, LoraLayer 1: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 2: 16670 Step\n",
      "End of epoch 10, LoraLayer 2: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 3: 16670 Step\n",
      "End of epoch 10, LoraLayer 3: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 4: 16670 Step\n",
      "End of epoch 10, LoraLayer 4: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 5: 16670 Step\n",
      "End of epoch 10, LoraLayer 5: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 6: 16670 Step\n",
      "End of epoch 10, LoraLayer 6: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 7: 16670 Step\n",
      "End of epoch 10, LoraLayer 7: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 8: 16670 Step\n",
      "End of epoch 10, LoraLayer 8: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 9: 16670 Step\n",
      "End of epoch 10, LoraLayer 9: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 10: 16670 Step\n",
      "End of epoch 10, LoraLayer 10: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 11: 16670 Step\n",
      "End of epoch 10, LoraLayer 11: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 12: 16670 Step\n",
      "End of epoch 10, LoraLayer 12: Decay factor: 0.4998500347137451\n",
      "\n",
      "Testing loss: 2.303372859954834, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 43s 26ms/step - loss: 0.1396 - accuracy: 0.9524 - val_loss: 2.3034 - val_accuracy: 0.1000\n",
      "Epoch 11/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.1659 - accuracy: 0.9446\n",
      "End of epoch 11, LoraLayer 0: 18337 Step\n",
      "End of epoch 11, LoraLayer 0: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 1: 18337 Step\n",
      "End of epoch 11, LoraLayer 1: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 2: 18337 Step\n",
      "End of epoch 11, LoraLayer 2: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 3: 18337 Step\n",
      "End of epoch 11, LoraLayer 3: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 4: 18337 Step\n",
      "End of epoch 11, LoraLayer 4: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 5: 18337 Step\n",
      "End of epoch 11, LoraLayer 5: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 6: 18337 Step\n",
      "End of epoch 11, LoraLayer 6: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 7: 18337 Step\n",
      "End of epoch 11, LoraLayer 7: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 8: 18337 Step\n",
      "End of epoch 11, LoraLayer 8: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 9: 18337 Step\n",
      "End of epoch 11, LoraLayer 9: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 10: 18337 Step\n",
      "End of epoch 11, LoraLayer 10: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 11: 18337 Step\n",
      "End of epoch 11, LoraLayer 11: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 12: 18337 Step\n",
      "End of epoch 11, LoraLayer 12: Decay factor: 0.4373359680175781\n",
      "\n",
      "Testing loss: 2.3033766746520996, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 43s 26ms/step - loss: 0.1658 - accuracy: 0.9446 - val_loss: 2.3034 - val_accuracy: 0.1000\n",
      "Epoch 12/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.1926 - accuracy: 0.9340\n",
      "End of epoch 12, LoraLayer 0: 20004 Step\n",
      "End of epoch 12, LoraLayer 0: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 1: 20004 Step\n",
      "End of epoch 12, LoraLayer 1: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 2: 20004 Step\n",
      "End of epoch 12, LoraLayer 2: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 3: 20004 Step\n",
      "End of epoch 12, LoraLayer 3: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 4: 20004 Step\n",
      "End of epoch 12, LoraLayer 4: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 5: 20004 Step\n",
      "End of epoch 12, LoraLayer 5: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 6: 20004 Step\n",
      "End of epoch 12, LoraLayer 6: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 7: 20004 Step\n",
      "End of epoch 12, LoraLayer 7: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 8: 20004 Step\n",
      "End of epoch 12, LoraLayer 8: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 9: 20004 Step\n",
      "End of epoch 12, LoraLayer 9: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 10: 20004 Step\n",
      "End of epoch 12, LoraLayer 10: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 11: 20004 Step\n",
      "End of epoch 12, LoraLayer 11: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 12: 20004 Step\n",
      "End of epoch 12, LoraLayer 12: Decay factor: 0.37482190132141113\n",
      "\n",
      "Testing loss: 2.3033478260040283, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 42s 25ms/step - loss: 0.1926 - accuracy: 0.9340 - val_loss: 2.3033 - val_accuracy: 0.1000\n",
      "Epoch 13/20\n",
      "1665/1667 [============================>.] - ETA: 0s - loss: 0.2244 - accuracy: 0.9234\n",
      "End of epoch 13, LoraLayer 0: 21671 Step\n",
      "End of epoch 13, LoraLayer 0: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 1: 21671 Step\n",
      "End of epoch 13, LoraLayer 1: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 2: 21671 Step\n",
      "End of epoch 13, LoraLayer 2: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 3: 21671 Step\n",
      "End of epoch 13, LoraLayer 3: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 4: 21671 Step\n",
      "End of epoch 13, LoraLayer 4: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 5: 21671 Step\n",
      "End of epoch 13, LoraLayer 5: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 6: 21671 Step\n",
      "End of epoch 13, LoraLayer 6: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 7: 21671 Step\n",
      "End of epoch 13, LoraLayer 7: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 8: 21671 Step\n",
      "End of epoch 13, LoraLayer 8: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 9: 21671 Step\n",
      "End of epoch 13, LoraLayer 9: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 10: 21671 Step\n",
      "End of epoch 13, LoraLayer 10: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 11: 21671 Step\n",
      "End of epoch 13, LoraLayer 11: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 12: 21671 Step\n",
      "End of epoch 13, LoraLayer 12: Decay factor: 0.31230783462524414\n",
      "\n",
      "Testing loss: 2.3037235736846924, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 42s 25ms/step - loss: 0.2243 - accuracy: 0.9235 - val_loss: 2.3037 - val_accuracy: 0.1000\n",
      "Epoch 14/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.2748 - accuracy: 0.9065\n",
      "End of epoch 14, LoraLayer 0: 23338 Step\n",
      "End of epoch 14, LoraLayer 0: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 1: 23338 Step\n",
      "End of epoch 14, LoraLayer 1: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 2: 23338 Step\n",
      "End of epoch 14, LoraLayer 2: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 3: 23338 Step\n",
      "End of epoch 14, LoraLayer 3: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 4: 23338 Step\n",
      "End of epoch 14, LoraLayer 4: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 5: 23338 Step\n",
      "End of epoch 14, LoraLayer 5: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 6: 23338 Step\n",
      "End of epoch 14, LoraLayer 6: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 7: 23338 Step\n",
      "End of epoch 14, LoraLayer 7: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 8: 23338 Step\n",
      "End of epoch 14, LoraLayer 8: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 9: 23338 Step\n",
      "End of epoch 14, LoraLayer 9: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 10: 23338 Step\n",
      "End of epoch 14, LoraLayer 10: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 11: 23338 Step\n",
      "End of epoch 14, LoraLayer 11: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 12: 23338 Step\n",
      "End of epoch 14, LoraLayer 12: Decay factor: 0.24979376792907715\n",
      "\n",
      "Testing loss: 2.3032169342041016, acc: 0.10119999945163727\n",
      "\n",
      "1667/1667 [==============================] - 43s 26ms/step - loss: 0.2748 - accuracy: 0.9064 - val_loss: 2.3032 - val_accuracy: 0.1012\n",
      "Epoch 15/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.3307 - accuracy: 0.8864\n",
      "End of epoch 15, LoraLayer 0: 25005 Step\n",
      "End of epoch 15, LoraLayer 0: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 1: 25005 Step\n",
      "End of epoch 15, LoraLayer 1: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 2: 25005 Step\n",
      "End of epoch 15, LoraLayer 2: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 3: 25005 Step\n",
      "End of epoch 15, LoraLayer 3: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 4: 25005 Step\n",
      "End of epoch 15, LoraLayer 4: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 5: 25005 Step\n",
      "End of epoch 15, LoraLayer 5: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 6: 25005 Step\n",
      "End of epoch 15, LoraLayer 6: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 7: 25005 Step\n",
      "End of epoch 15, LoraLayer 7: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 8: 25005 Step\n",
      "End of epoch 15, LoraLayer 8: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 9: 25005 Step\n",
      "End of epoch 15, LoraLayer 9: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 10: 25005 Step\n",
      "End of epoch 15, LoraLayer 10: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 11: 25005 Step\n",
      "End of epoch 15, LoraLayer 11: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 12: 25005 Step\n",
      "End of epoch 15, LoraLayer 12: Decay factor: 0.18727970123291016\n",
      "\n",
      "Testing loss: 2.2976152896881104, acc: 0.11190000176429749\n",
      "\n",
      "1667/1667 [==============================] - 43s 26ms/step - loss: 0.3307 - accuracy: 0.8864 - val_loss: 2.2976 - val_accuracy: 0.1117\n",
      "Epoch 16/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.4051 - accuracy: 0.8612\n",
      "End of epoch 16, LoraLayer 0: 26672 Step\n",
      "End of epoch 16, LoraLayer 0: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 1: 26672 Step\n",
      "End of epoch 16, LoraLayer 1: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 2: 26672 Step\n",
      "End of epoch 16, LoraLayer 2: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 3: 26672 Step\n",
      "End of epoch 16, LoraLayer 3: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 4: 26672 Step\n",
      "End of epoch 16, LoraLayer 4: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 5: 26672 Step\n",
      "End of epoch 16, LoraLayer 5: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 6: 26672 Step\n",
      "End of epoch 16, LoraLayer 6: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 7: 26672 Step\n",
      "End of epoch 16, LoraLayer 7: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 8: 26672 Step\n",
      "End of epoch 16, LoraLayer 8: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 9: 26672 Step\n",
      "End of epoch 16, LoraLayer 9: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 10: 26672 Step\n",
      "End of epoch 16, LoraLayer 10: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 11: 26672 Step\n",
      "End of epoch 16, LoraLayer 11: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 12: 26672 Step\n",
      "End of epoch 16, LoraLayer 12: Decay factor: 0.12476563453674316\n",
      "\n",
      "Testing loss: 2.2358813285827637, acc: 0.23759999871253967\n",
      "\n",
      "1667/1667 [==============================] - 42s 25ms/step - loss: 0.4051 - accuracy: 0.8611 - val_loss: 2.2359 - val_accuracy: 0.2374\n",
      "Epoch 17/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.4942 - accuracy: 0.8329\n",
      "End of epoch 17, LoraLayer 0: 28339 Step\n",
      "End of epoch 17, LoraLayer 0: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 1: 28339 Step\n",
      "End of epoch 17, LoraLayer 1: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 2: 28339 Step\n",
      "End of epoch 17, LoraLayer 2: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 3: 28339 Step\n",
      "End of epoch 17, LoraLayer 3: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 4: 28339 Step\n",
      "End of epoch 17, LoraLayer 4: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 5: 28339 Step\n",
      "End of epoch 17, LoraLayer 5: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 6: 28339 Step\n",
      "End of epoch 17, LoraLayer 6: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 7: 28339 Step\n",
      "End of epoch 17, LoraLayer 7: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 8: 28339 Step\n",
      "End of epoch 17, LoraLayer 8: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 9: 28339 Step\n",
      "End of epoch 17, LoraLayer 9: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 10: 28339 Step\n",
      "End of epoch 17, LoraLayer 10: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 11: 28339 Step\n",
      "End of epoch 17, LoraLayer 11: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 12: 28339 Step\n",
      "End of epoch 17, LoraLayer 12: Decay factor: 0.06225156784057617\n",
      "\n",
      "Testing loss: 1.8797552585601807, acc: 0.400299996137619\n",
      "\n",
      "1667/1667 [==============================] - 43s 26ms/step - loss: 0.4942 - accuracy: 0.8329 - val_loss: 1.8797 - val_accuracy: 0.4003\n",
      "Epoch 18/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.6314 - accuracy: 0.7878\n",
      "End of epoch 18, LoraLayer 0: 30006 Step\n",
      "End of epoch 18, LoraLayer 0: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 1: 30006 Step\n",
      "End of epoch 18, LoraLayer 1: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 2: 30006 Step\n",
      "End of epoch 18, LoraLayer 2: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 3: 30006 Step\n",
      "End of epoch 18, LoraLayer 3: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 4: 30006 Step\n",
      "End of epoch 18, LoraLayer 4: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 5: 30006 Step\n",
      "End of epoch 18, LoraLayer 5: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 6: 30006 Step\n",
      "End of epoch 18, LoraLayer 6: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 7: 30006 Step\n",
      "End of epoch 18, LoraLayer 7: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 8: 30006 Step\n",
      "End of epoch 18, LoraLayer 8: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 9: 30006 Step\n",
      "End of epoch 18, LoraLayer 9: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 10: 30006 Step\n",
      "End of epoch 18, LoraLayer 10: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 11: 30006 Step\n",
      "End of epoch 18, LoraLayer 11: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 12: 30006 Step\n",
      "End of epoch 18, LoraLayer 12: Decay factor: 0.0\n",
      "\n",
      "Testing loss: 0.7999058961868286, acc: 0.7368000149726868\n",
      "\n",
      "1667/1667 [==============================] - 42s 25ms/step - loss: 0.6314 - accuracy: 0.7878 - val_loss: 0.7999 - val_accuracy: 0.7369\n",
      "Epoch 19/20\n",
      "1665/1667 [============================>.] - ETA: 0s - loss: 0.6412 - accuracy: 0.7894\n",
      "End of epoch 19, LoraLayer 0: 31673 Step\n",
      "End of epoch 19, LoraLayer 0: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 1: 31673 Step\n",
      "End of epoch 19, LoraLayer 1: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 2: 31673 Step\n",
      "End of epoch 19, LoraLayer 2: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 3: 31673 Step\n",
      "End of epoch 19, LoraLayer 3: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 4: 31673 Step\n",
      "End of epoch 19, LoraLayer 4: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 5: 31673 Step\n",
      "End of epoch 19, LoraLayer 5: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 6: 31673 Step\n",
      "End of epoch 19, LoraLayer 6: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 7: 31673 Step\n",
      "End of epoch 19, LoraLayer 7: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 8: 31673 Step\n",
      "End of epoch 19, LoraLayer 8: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 9: 31673 Step\n",
      "End of epoch 19, LoraLayer 9: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 10: 31673 Step\n",
      "End of epoch 19, LoraLayer 10: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 11: 31673 Step\n",
      "End of epoch 19, LoraLayer 11: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 12: 31673 Step\n",
      "End of epoch 19, LoraLayer 12: Decay factor: 0.0\n",
      "\n",
      "Testing loss: 0.7179270386695862, acc: 0.7627000212669373\n",
      "\n",
      "1667/1667 [==============================] - 43s 26ms/step - loss: 0.6411 - accuracy: 0.7894 - val_loss: 0.7179 - val_accuracy: 0.7628\n",
      "Epoch 20/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.5394 - accuracy: 0.8215\n",
      "End of epoch 20, LoraLayer 0: 33340 Step\n",
      "End of epoch 20, LoraLayer 0: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 1: 33340 Step\n",
      "End of epoch 20, LoraLayer 1: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 2: 33340 Step\n",
      "End of epoch 20, LoraLayer 2: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 3: 33340 Step\n",
      "End of epoch 20, LoraLayer 3: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 4: 33340 Step\n",
      "End of epoch 20, LoraLayer 4: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 5: 33340 Step\n",
      "End of epoch 20, LoraLayer 5: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 6: 33340 Step\n",
      "End of epoch 20, LoraLayer 6: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 7: 33340 Step\n",
      "End of epoch 20, LoraLayer 7: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 8: 33340 Step\n",
      "End of epoch 20, LoraLayer 8: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 9: 33340 Step\n",
      "End of epoch 20, LoraLayer 9: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 10: 33340 Step\n",
      "End of epoch 20, LoraLayer 10: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 11: 33340 Step\n",
      "End of epoch 20, LoraLayer 11: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 12: 33340 Step\n",
      "End of epoch 20, LoraLayer 12: Decay factor: 0.0\n",
      "\n",
      "Testing loss: 0.7557053565979004, acc: 0.7656000256538391\n",
      "\n",
      "1667/1667 [==============================] - 44s 27ms/step - loss: 0.5393 - accuracy: 0.8215 - val_loss: 0.7557 - val_accuracy: 0.7658\n"
     ]
    }
   ],
   "source": [
    "history33 = lora_vgg162.fit(x_train, y_train, batch_size=30, epochs=20, validation_data=(x_test, y_test), callbacks=[print_step_callback, TestCallback((x_test, y_test))] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4iVURdjC48v9",
    "outputId": "f5b18b66-2306-4971-d08f-fa4cf52d5fbc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 3s 10ms/step - loss: 0.7557 - accuracy: 0.7656\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7557053565979004, 0.7656000256538391]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 평가\n",
    "lora_vgg162.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "oMHh5D1X48v-",
    "outputId": "0a3818f4-3558-434b-fcf9-ab6cc8536ccc"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 학습 및 검증 정확도 시각화\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# 정확도 그래프\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history33.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history33.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# 손실 그래프\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history33.history['loss'], label='Training Loss')\n",
    "plt.plot(history33.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1iiF1k-9D0hd"
   },
   "source": [
    "---\n",
    "#Experiments 2 : 모든 Convlora + dense6,7에 denselora + dense8 ❄"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DmYS1wIAD5x8"
   },
   "source": [
    "## 2-1. (32, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "GFXXUUlKFeK7"
   },
   "outputs": [],
   "source": [
    "dataset_size = x_train.shape[0]\n",
    "batch_size = 30\n",
    "epochs = 20\n",
    "\n",
    "total_iteration= int(epochs * (dataset_size / batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Z2OFtKnqD4bc"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout, InputLayer\n",
    "import tensorflow as tf\n",
    "\n",
    "# 새로운 레이어를 추가하기 위한 레이어 리스트 초기화\n",
    "new_layers = []\n",
    "\n",
    "# Dense 레이어 카운터\n",
    "dense_layer_count = 0\n",
    "\n",
    "# VGG 모델의 각 레이어 순회\n",
    "for layer in best_vgg16.layers:\n",
    "    if isinstance(layer, InputLayer):\n",
    "        continue\n",
    "    if isinstance(layer, Conv2D):\n",
    "        # Conv2D 레이어를 ConvLoRALayer로 대체\n",
    "        conv_lora_layer = ConvLoRALayer00_cdn2(original_conv_layer=layer, rank=32, alpha=32, total_iteration = total_iteration, trainable=True)\n",
    "        new_layers.append(conv_lora_layer)\n",
    "    elif isinstance(layer, Dense):\n",
    "        dense_layer_count += 1\n",
    "        # 첫 번째와 두 번째 Dense 레이어에만 LoraLayer 적용\n",
    "        if dense_layer_count in [1, 2]:\n",
    "            dense_lora_layer = LoraLayer(original_layer=layer, rank=32, alpha=32, total_iteration=total_iteration, trainable=True)\n",
    "            new_layers.append(dense_lora_layer)\n",
    "        else:\n",
    "            new_layers.append(layer)  # 그대로 유지함\n",
    "    else:\n",
    "        # 다른 유형의 레이어는 그대로 유지\n",
    "        new_layers.append(layer)\n",
    "\n",
    "# 새로운 입력 텐서 생성\n",
    "input_tensor = best_vgg16.input\n",
    "\n",
    "# 새로운 모델 구성\n",
    "x = input_tensor\n",
    "for layer in new_layers:\n",
    "    x = layer(x)\n",
    "\n",
    "# 새로운 모델 생성\n",
    "exp21_lora_vgg16 = Model(inputs=input_tensor, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "GbLnmfaLELbE"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D\n",
    "\n",
    "# 새로 생성된 모델에서 Dense 레이어 중 LoRA가 적용되지 않은 레이어의 trainable을 False로 설정\n",
    "for layer in exp21_lora_vgg16.layers:\n",
    "    if isinstance(layer, Dense) and not isinstance(layer, LoraLayer):\n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aP_ss6DrEcBt",
    "outputId": "63671162-e007-44da-f92a-49b173cff6c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " block1_conv1 (ConvLoRALaye  (None, 32, 32, 64)        21090     \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block1_conv2 (ConvLoRALaye  (None, 32, 32, 64)        73794     \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " block2_conv1 (ConvLoRALaye  (None, 16, 16, 128)       129154    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block2_conv2 (ConvLoRALaye  (None, 16, 16, 128)       221314    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " block3_conv1 (ConvLoRALaye  (None, 8, 8, 256)         405762    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block3_conv2 (ConvLoRALaye  (None, 8, 8, 256)         737538    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block3_conv3 (ConvLoRALaye  (None, 8, 8, 256)         737538    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 4, 4, 256)         0         \n",
      "                                                                 \n",
      " block4_conv1 (ConvLoRALaye  (None, 4, 4, 512)         1401346   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block4_conv2 (ConvLoRALaye  (None, 4, 4, 512)         2654722   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block4_conv3 (ConvLoRALaye  (None, 4, 4, 512)         2654722   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      " block5_conv1 (ConvLoRALaye  (None, 2, 2, 512)         2654722   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block5_conv2 (ConvLoRALaye  (None, 2, 2, 512)         2654722   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block5_conv3 (ConvLoRALaye  (None, 2, 2, 512)         2654722   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 1, 1, 512)         0         \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_6 (LoraLayer)         (None, 4096)              2248706   \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_7 (LoraLayer)         (None, 512)               2245122   \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21500104 (82.02 MB)\n",
      "Trainable params: 2581344 (9.85 MB)\n",
      "Non-trainable params: 18918760 (72.17 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "exp21_lora_vgg16.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "155io3wREiB2",
    "outputId": "1a1abe8e-8ae7-4f7b-a46a-add76eb92b22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: input_3\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block1_conv1\n",
      "  Trainable parameters: 19296\n",
      "  Non-trainable parameters: 1794\n",
      "Layer: block1_conv2\n",
      "  Trainable parameters: 36864\n",
      "  Non-trainable parameters: 36930\n",
      "Layer: block1_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block2_conv1\n",
      "  Trainable parameters: 55296\n",
      "  Non-trainable parameters: 73858\n",
      "Layer: block2_conv2\n",
      "  Trainable parameters: 73728\n",
      "  Non-trainable parameters: 147586\n",
      "Layer: block2_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block3_conv1\n",
      "  Trainable parameters: 110592\n",
      "  Non-trainable parameters: 295170\n",
      "Layer: block3_conv2\n",
      "  Trainable parameters: 147456\n",
      "  Non-trainable parameters: 590082\n",
      "Layer: block3_conv3\n",
      "  Trainable parameters: 147456\n",
      "  Non-trainable parameters: 590082\n",
      "Layer: block3_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block4_conv1\n",
      "  Trainable parameters: 221184\n",
      "  Non-trainable parameters: 1180162\n",
      "Layer: block4_conv2\n",
      "  Trainable parameters: 294912\n",
      "  Non-trainable parameters: 2359810\n",
      "Layer: block4_conv3\n",
      "  Trainable parameters: 294912\n",
      "  Non-trainable parameters: 2359810\n",
      "Layer: block4_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block5_conv1\n",
      "  Trainable parameters: 294912\n",
      "  Non-trainable parameters: 2359810\n",
      "Layer: block5_conv2\n",
      "  Trainable parameters: 294912\n",
      "  Non-trainable parameters: 2359810\n",
      "Layer: block5_conv3\n",
      "  Trainable parameters: 294912\n",
      "  Non-trainable parameters: 2359810\n",
      "Layer: block5_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: flatten_2\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: dense_6\n",
      "  Trainable parameters: 147456\n",
      "  Non-trainable parameters: 2101250\n",
      "Layer: dropout_4\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: dense_7\n",
      "  Trainable parameters: 147456\n",
      "  Non-trainable parameters: 2097666\n",
      "Layer: dropout_5\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: dense_8\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 5130\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#모델의 각 레이어를 순회하며 파라미터 수를 계산\n",
    "for layer in exp21_lora_vgg16.layers:\n",
    "    trainable_count = np.sum([tf.size(w).numpy() for w in layer.trainable_weights])\n",
    "    non_trainable_count = np.sum([tf.size(w).numpy() for w in layer.non_trainable_weights])\n",
    "\n",
    "    print(f\"Layer: {layer.name}\")\n",
    "    print(f\"  Trainable parameters: {trainable_count}\")\n",
    "    print(f\"  Non-trainable parameters: {non_trainable_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "6eHa70iQEo_N"
   },
   "outputs": [],
   "source": [
    "lora_layers = []  # lora 레이어들을 저장할 리스트\n",
    "\n",
    "for layer in exp21_lora_vgg16.layers:\n",
    "    if isinstance(layer, LoraLayer) or isinstance(layer, ConvLoRALayer00_cdn2):\n",
    "        lora_layers.append(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "4vQVbHIuEsVk"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "class PrintCurrentStepCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, lora_layers):\n",
    "        super().__init__()\n",
    "        self.lora_layers = lora_layers\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print()\n",
    "        for i, lora_layer in enumerate(self.lora_layers):\n",
    "            current_step = lora_layer.current_step.value()\n",
    "            decay_factor = lora_layer.decay_factor.value()\n",
    "            print(f\"End of epoch {epoch + 1}, LoraLayer {i}: {current_step} Step\")\n",
    "            print(f\"End of epoch {epoch + 1}, LoraLayer {i}: Decay factor: {decay_factor}\")\n",
    "\n",
    "class TestCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, test_data):\n",
    "        self.test_data = test_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        x, y = self.test_data\n",
    "        loss, acc = self.model.evaluate(x, y, verbose=0)\n",
    "        print('\\nTesting loss: {}, acc: {}\\n'.format(loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "sWlqdMvwFBdM"
   },
   "outputs": [],
   "source": [
    "# 콜백 생성 시 lora_layers 딕셔너리의 값만 사용\n",
    "print_step_callback = PrintCurrentStepCallback(lora_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Nv9o4F_-FEAb"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# 모델 컴파일\n",
    "exp21_lora_vgg16.compile(optimizer=Adam(learning_rate=0.00005),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2OU9-UYR-4U1"
   },
   "source": [
    "### 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VH7DGSABFKfM",
    "outputId": "4c8bc6cb-79de-4456-d641-301e2d97ded9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-01 03:24:15.048034: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fbe0ec11560 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-12-01 03:24:15.048074: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA RTX 4000 SFF Ada Generation, Compute Capability 8.9\n",
      "2023-12-01 03:24:15.054272: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-12-01 03:24:15.202780: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1667/1667 [==============================] - ETA: 0s - loss: 0.0826 - accuracy: 0.9752\n",
      "End of epoch 1, LoraLayer 0: 1667 Step\n",
      "End of epoch 1, LoraLayer 0: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 1: 1667 Step\n",
      "End of epoch 1, LoraLayer 1: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 2: 1667 Step\n",
      "End of epoch 1, LoraLayer 2: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 3: 1667 Step\n",
      "End of epoch 1, LoraLayer 3: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 4: 1667 Step\n",
      "End of epoch 1, LoraLayer 4: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 5: 1667 Step\n",
      "End of epoch 1, LoraLayer 5: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 6: 1667 Step\n",
      "End of epoch 1, LoraLayer 6: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 7: 1667 Step\n",
      "End of epoch 1, LoraLayer 7: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 8: 1667 Step\n",
      "End of epoch 1, LoraLayer 8: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 9: 1667 Step\n",
      "End of epoch 1, LoraLayer 9: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 10: 1667 Step\n",
      "End of epoch 1, LoraLayer 10: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 11: 1667 Step\n",
      "End of epoch 1, LoraLayer 11: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 12: 1667 Step\n",
      "End of epoch 1, LoraLayer 12: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 13: 1667 Step\n",
      "End of epoch 1, LoraLayer 13: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 14: 1667 Step\n",
      "End of epoch 1, LoraLayer 14: Decay factor: 1.0\n",
      "\n",
      "Testing loss: 2.302607297897339, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 61s 30ms/step - loss: 0.0826 - accuracy: 0.9752 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 2/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.0561 - accuracy: 0.9827\n",
      "End of epoch 2, LoraLayer 0: 3334 Step\n",
      "End of epoch 2, LoraLayer 0: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 1: 3334 Step\n",
      "End of epoch 2, LoraLayer 1: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 2: 3334 Step\n",
      "End of epoch 2, LoraLayer 2: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 3: 3334 Step\n",
      "End of epoch 2, LoraLayer 3: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 4: 3334 Step\n",
      "End of epoch 2, LoraLayer 4: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 5: 3334 Step\n",
      "End of epoch 2, LoraLayer 5: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 6: 3334 Step\n",
      "End of epoch 2, LoraLayer 6: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 7: 3334 Step\n",
      "End of epoch 2, LoraLayer 7: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 8: 3334 Step\n",
      "End of epoch 2, LoraLayer 8: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 9: 3334 Step\n",
      "End of epoch 2, LoraLayer 9: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 10: 3334 Step\n",
      "End of epoch 2, LoraLayer 10: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 11: 3334 Step\n",
      "End of epoch 2, LoraLayer 11: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 12: 3334 Step\n",
      "End of epoch 2, LoraLayer 12: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 13: 3334 Step\n",
      "End of epoch 2, LoraLayer 13: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 14: 3334 Step\n",
      "End of epoch 2, LoraLayer 14: Decay factor: 0.9999625086784363\n",
      "\n",
      "Testing loss: 2.302607297897339, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 48s 29ms/step - loss: 0.0560 - accuracy: 0.9827 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 3/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.0484 - accuracy: 0.9849\n",
      "End of epoch 3, LoraLayer 0: 5001 Step\n",
      "End of epoch 3, LoraLayer 0: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 1: 5001 Step\n",
      "End of epoch 3, LoraLayer 1: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 2: 5001 Step\n",
      "End of epoch 3, LoraLayer 2: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 3: 5001 Step\n",
      "End of epoch 3, LoraLayer 3: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 4: 5001 Step\n",
      "End of epoch 3, LoraLayer 4: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 5: 5001 Step\n",
      "End of epoch 3, LoraLayer 5: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 6: 5001 Step\n",
      "End of epoch 3, LoraLayer 6: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 7: 5001 Step\n",
      "End of epoch 3, LoraLayer 7: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 8: 5001 Step\n",
      "End of epoch 3, LoraLayer 8: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 9: 5001 Step\n",
      "End of epoch 3, LoraLayer 9: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 10: 5001 Step\n",
      "End of epoch 3, LoraLayer 10: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 11: 5001 Step\n",
      "End of epoch 3, LoraLayer 11: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 12: 5001 Step\n",
      "End of epoch 3, LoraLayer 12: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 13: 5001 Step\n",
      "End of epoch 3, LoraLayer 13: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 14: 5001 Step\n",
      "End of epoch 3, LoraLayer 14: Decay factor: 0.9374484419822693\n",
      "\n",
      "Testing loss: 2.302607297897339, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 49s 29ms/step - loss: 0.0484 - accuracy: 0.9849 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 4/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.0479 - accuracy: 0.9845\n",
      "End of epoch 4, LoraLayer 0: 6668 Step\n",
      "End of epoch 4, LoraLayer 0: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 1: 6668 Step\n",
      "End of epoch 4, LoraLayer 1: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 2: 6668 Step\n",
      "End of epoch 4, LoraLayer 2: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 3: 6668 Step\n",
      "End of epoch 4, LoraLayer 3: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 4: 6668 Step\n",
      "End of epoch 4, LoraLayer 4: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 5: 6668 Step\n",
      "End of epoch 4, LoraLayer 5: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 6: 6668 Step\n",
      "End of epoch 4, LoraLayer 6: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 7: 6668 Step\n",
      "End of epoch 4, LoraLayer 7: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 8: 6668 Step\n",
      "End of epoch 4, LoraLayer 8: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 9: 6668 Step\n",
      "End of epoch 4, LoraLayer 9: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 10: 6668 Step\n",
      "End of epoch 4, LoraLayer 10: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 11: 6668 Step\n",
      "End of epoch 4, LoraLayer 11: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 12: 6668 Step\n",
      "End of epoch 4, LoraLayer 12: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 13: 6668 Step\n",
      "End of epoch 4, LoraLayer 13: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 14: 6668 Step\n",
      "End of epoch 4, LoraLayer 14: Decay factor: 0.8749343752861023\n",
      "\n",
      "Testing loss: 2.302607297897339, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 47s 28ms/step - loss: 0.0479 - accuracy: 0.9845 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 5/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.0529 - accuracy: 0.9827\n",
      "End of epoch 5, LoraLayer 0: 8335 Step\n",
      "End of epoch 5, LoraLayer 0: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 1: 8335 Step\n",
      "End of epoch 5, LoraLayer 1: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 2: 8335 Step\n",
      "End of epoch 5, LoraLayer 2: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 3: 8335 Step\n",
      "End of epoch 5, LoraLayer 3: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 4: 8335 Step\n",
      "End of epoch 5, LoraLayer 4: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 5: 8335 Step\n",
      "End of epoch 5, LoraLayer 5: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 6: 8335 Step\n",
      "End of epoch 5, LoraLayer 6: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 7: 8335 Step\n",
      "End of epoch 5, LoraLayer 7: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 8: 8335 Step\n",
      "End of epoch 5, LoraLayer 8: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 9: 8335 Step\n",
      "End of epoch 5, LoraLayer 9: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 10: 8335 Step\n",
      "End of epoch 5, LoraLayer 10: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 11: 8335 Step\n",
      "End of epoch 5, LoraLayer 11: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 12: 8335 Step\n",
      "End of epoch 5, LoraLayer 12: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 13: 8335 Step\n",
      "End of epoch 5, LoraLayer 13: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 14: 8335 Step\n",
      "End of epoch 5, LoraLayer 14: Decay factor: 0.8124203085899353\n",
      "\n",
      "Testing loss: 2.302607297897339, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 47s 28ms/step - loss: 0.0529 - accuracy: 0.9827 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 6/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.0547 - accuracy: 0.9824\n",
      "End of epoch 6, LoraLayer 0: 10002 Step\n",
      "End of epoch 6, LoraLayer 0: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 1: 10002 Step\n",
      "End of epoch 6, LoraLayer 1: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 2: 10002 Step\n",
      "End of epoch 6, LoraLayer 2: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 3: 10002 Step\n",
      "End of epoch 6, LoraLayer 3: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 4: 10002 Step\n",
      "End of epoch 6, LoraLayer 4: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 5: 10002 Step\n",
      "End of epoch 6, LoraLayer 5: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 6: 10002 Step\n",
      "End of epoch 6, LoraLayer 6: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 7: 10002 Step\n",
      "End of epoch 6, LoraLayer 7: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 8: 10002 Step\n",
      "End of epoch 6, LoraLayer 8: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 9: 10002 Step\n",
      "End of epoch 6, LoraLayer 9: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 10: 10002 Step\n",
      "End of epoch 6, LoraLayer 10: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 11: 10002 Step\n",
      "End of epoch 6, LoraLayer 11: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 12: 10002 Step\n",
      "End of epoch 6, LoraLayer 12: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 13: 10002 Step\n",
      "End of epoch 6, LoraLayer 13: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 14: 10002 Step\n",
      "End of epoch 6, LoraLayer 14: Decay factor: 0.7499062418937683\n",
      "\n",
      "Testing loss: 2.302607297897339, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 49s 29ms/step - loss: 0.0547 - accuracy: 0.9824 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 7/20\n",
      "1665/1667 [============================>.] - ETA: 0s - loss: 0.0653 - accuracy: 0.9785\n",
      "End of epoch 7, LoraLayer 0: 11669 Step\n",
      "End of epoch 7, LoraLayer 0: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 1: 11669 Step\n",
      "End of epoch 7, LoraLayer 1: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 2: 11669 Step\n",
      "End of epoch 7, LoraLayer 2: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 3: 11669 Step\n",
      "End of epoch 7, LoraLayer 3: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 4: 11669 Step\n",
      "End of epoch 7, LoraLayer 4: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 5: 11669 Step\n",
      "End of epoch 7, LoraLayer 5: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 6: 11669 Step\n",
      "End of epoch 7, LoraLayer 6: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 7: 11669 Step\n",
      "End of epoch 7, LoraLayer 7: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 8: 11669 Step\n",
      "End of epoch 7, LoraLayer 8: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 9: 11669 Step\n",
      "End of epoch 7, LoraLayer 9: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 10: 11669 Step\n",
      "End of epoch 7, LoraLayer 10: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 11: 11669 Step\n",
      "End of epoch 7, LoraLayer 11: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 12: 11669 Step\n",
      "End of epoch 7, LoraLayer 12: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 13: 11669 Step\n",
      "End of epoch 7, LoraLayer 13: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 14: 11669 Step\n",
      "End of epoch 7, LoraLayer 14: Decay factor: 0.6873922348022461\n",
      "\n",
      "Testing loss: 2.302607297897339, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 48s 29ms/step - loss: 0.0652 - accuracy: 0.9785 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 8/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.0737 - accuracy: 0.9749\n",
      "End of epoch 8, LoraLayer 0: 13336 Step\n",
      "End of epoch 8, LoraLayer 0: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 1: 13336 Step\n",
      "End of epoch 8, LoraLayer 1: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 2: 13336 Step\n",
      "End of epoch 8, LoraLayer 2: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 3: 13336 Step\n",
      "End of epoch 8, LoraLayer 3: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 4: 13336 Step\n",
      "End of epoch 8, LoraLayer 4: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 5: 13336 Step\n",
      "End of epoch 8, LoraLayer 5: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 6: 13336 Step\n",
      "End of epoch 8, LoraLayer 6: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 7: 13336 Step\n",
      "End of epoch 8, LoraLayer 7: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 8: 13336 Step\n",
      "End of epoch 8, LoraLayer 8: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 9: 13336 Step\n",
      "End of epoch 8, LoraLayer 9: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 10: 13336 Step\n",
      "End of epoch 8, LoraLayer 10: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 11: 13336 Step\n",
      "End of epoch 8, LoraLayer 11: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 12: 13336 Step\n",
      "End of epoch 8, LoraLayer 12: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 13: 13336 Step\n",
      "End of epoch 8, LoraLayer 13: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 14: 13336 Step\n",
      "End of epoch 8, LoraLayer 14: Decay factor: 0.6248781681060791\n",
      "\n",
      "Testing loss: 2.302607297897339, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 48s 29ms/step - loss: 0.0737 - accuracy: 0.9749 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 9/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.0892 - accuracy: 0.9699\n",
      "End of epoch 9, LoraLayer 0: 15003 Step\n",
      "End of epoch 9, LoraLayer 0: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 1: 15003 Step\n",
      "End of epoch 9, LoraLayer 1: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 2: 15003 Step\n",
      "End of epoch 9, LoraLayer 2: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 3: 15003 Step\n",
      "End of epoch 9, LoraLayer 3: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 4: 15003 Step\n",
      "End of epoch 9, LoraLayer 4: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 5: 15003 Step\n",
      "End of epoch 9, LoraLayer 5: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 6: 15003 Step\n",
      "End of epoch 9, LoraLayer 6: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 7: 15003 Step\n",
      "End of epoch 9, LoraLayer 7: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 8: 15003 Step\n",
      "End of epoch 9, LoraLayer 8: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 9: 15003 Step\n",
      "End of epoch 9, LoraLayer 9: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 10: 15003 Step\n",
      "End of epoch 9, LoraLayer 10: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 11: 15003 Step\n",
      "End of epoch 9, LoraLayer 11: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 12: 15003 Step\n",
      "End of epoch 9, LoraLayer 12: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 13: 15003 Step\n",
      "End of epoch 9, LoraLayer 13: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 14: 15003 Step\n",
      "End of epoch 9, LoraLayer 14: Decay factor: 0.5623641014099121\n",
      "\n",
      "Testing loss: 2.302607536315918, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 46s 28ms/step - loss: 0.0892 - accuracy: 0.9699 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 10/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.1102 - accuracy: 0.9624\n",
      "End of epoch 10, LoraLayer 0: 16670 Step\n",
      "End of epoch 10, LoraLayer 0: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 1: 16670 Step\n",
      "End of epoch 10, LoraLayer 1: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 2: 16670 Step\n",
      "End of epoch 10, LoraLayer 2: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 3: 16670 Step\n",
      "End of epoch 10, LoraLayer 3: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 4: 16670 Step\n",
      "End of epoch 10, LoraLayer 4: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 5: 16670 Step\n",
      "End of epoch 10, LoraLayer 5: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 6: 16670 Step\n",
      "End of epoch 10, LoraLayer 6: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 7: 16670 Step\n",
      "End of epoch 10, LoraLayer 7: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 8: 16670 Step\n",
      "End of epoch 10, LoraLayer 8: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 9: 16670 Step\n",
      "End of epoch 10, LoraLayer 9: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 10: 16670 Step\n",
      "End of epoch 10, LoraLayer 10: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 11: 16670 Step\n",
      "End of epoch 10, LoraLayer 11: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 12: 16670 Step\n",
      "End of epoch 10, LoraLayer 12: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 13: 16670 Step\n",
      "End of epoch 10, LoraLayer 13: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 14: 16670 Step\n",
      "End of epoch 10, LoraLayer 14: Decay factor: 0.4998500347137451\n",
      "\n",
      "Testing loss: 2.302607297897339, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 47s 28ms/step - loss: 0.1102 - accuracy: 0.9624 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 11/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.1369 - accuracy: 0.9533\n",
      "End of epoch 11, LoraLayer 0: 18337 Step\n",
      "End of epoch 11, LoraLayer 0: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 1: 18337 Step\n",
      "End of epoch 11, LoraLayer 1: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 2: 18337 Step\n",
      "End of epoch 11, LoraLayer 2: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 3: 18337 Step\n",
      "End of epoch 11, LoraLayer 3: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 4: 18337 Step\n",
      "End of epoch 11, LoraLayer 4: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 5: 18337 Step\n",
      "End of epoch 11, LoraLayer 5: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 6: 18337 Step\n",
      "End of epoch 11, LoraLayer 6: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 7: 18337 Step\n",
      "End of epoch 11, LoraLayer 7: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 8: 18337 Step\n",
      "End of epoch 11, LoraLayer 8: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 9: 18337 Step\n",
      "End of epoch 11, LoraLayer 9: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 10: 18337 Step\n",
      "End of epoch 11, LoraLayer 10: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 11: 18337 Step\n",
      "End of epoch 11, LoraLayer 11: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 12: 18337 Step\n",
      "End of epoch 11, LoraLayer 12: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 13: 18337 Step\n",
      "End of epoch 11, LoraLayer 13: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 14: 18337 Step\n",
      "End of epoch 11, LoraLayer 14: Decay factor: 0.4373359680175781\n",
      "\n",
      "Testing loss: 2.30260968208313, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 49s 29ms/step - loss: 0.1369 - accuracy: 0.9533 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 12/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.1677 - accuracy: 0.9410\n",
      "End of epoch 12, LoraLayer 0: 20004 Step\n",
      "End of epoch 12, LoraLayer 0: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 1: 20004 Step\n",
      "End of epoch 12, LoraLayer 1: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 2: 20004 Step\n",
      "End of epoch 12, LoraLayer 2: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 3: 20004 Step\n",
      "End of epoch 12, LoraLayer 3: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 4: 20004 Step\n",
      "End of epoch 12, LoraLayer 4: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 5: 20004 Step\n",
      "End of epoch 12, LoraLayer 5: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 6: 20004 Step\n",
      "End of epoch 12, LoraLayer 6: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 7: 20004 Step\n",
      "End of epoch 12, LoraLayer 7: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 8: 20004 Step\n",
      "End of epoch 12, LoraLayer 8: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 9: 20004 Step\n",
      "End of epoch 12, LoraLayer 9: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 10: 20004 Step\n",
      "End of epoch 12, LoraLayer 10: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 11: 20004 Step\n",
      "End of epoch 12, LoraLayer 11: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 12: 20004 Step\n",
      "End of epoch 12, LoraLayer 12: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 13: 20004 Step\n",
      "End of epoch 12, LoraLayer 13: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 14: 20004 Step\n",
      "End of epoch 12, LoraLayer 14: Decay factor: 0.37482190132141113\n",
      "\n",
      "Testing loss: 2.3026084899902344, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 47s 28ms/step - loss: 0.1678 - accuracy: 0.9409 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 13/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.2042 - accuracy: 0.9285\n",
      "End of epoch 13, LoraLayer 0: 21671 Step\n",
      "End of epoch 13, LoraLayer 0: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 1: 21671 Step\n",
      "End of epoch 13, LoraLayer 1: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 2: 21671 Step\n",
      "End of epoch 13, LoraLayer 2: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 3: 21671 Step\n",
      "End of epoch 13, LoraLayer 3: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 4: 21671 Step\n",
      "End of epoch 13, LoraLayer 4: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 5: 21671 Step\n",
      "End of epoch 13, LoraLayer 5: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 6: 21671 Step\n",
      "End of epoch 13, LoraLayer 6: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 7: 21671 Step\n",
      "End of epoch 13, LoraLayer 7: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 8: 21671 Step\n",
      "End of epoch 13, LoraLayer 8: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 9: 21671 Step\n",
      "End of epoch 13, LoraLayer 9: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 10: 21671 Step\n",
      "End of epoch 13, LoraLayer 10: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 11: 21671 Step\n",
      "End of epoch 13, LoraLayer 11: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 12: 21671 Step\n",
      "End of epoch 13, LoraLayer 12: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 13: 21671 Step\n",
      "End of epoch 13, LoraLayer 13: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 14: 21671 Step\n",
      "End of epoch 13, LoraLayer 14: Decay factor: 0.31230783462524414\n",
      "\n",
      "Testing loss: 2.302654981613159, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 47s 28ms/step - loss: 0.2042 - accuracy: 0.9285 - val_loss: 2.3027 - val_accuracy: 0.1000\n",
      "Epoch 14/20\n",
      "1665/1667 [============================>.] - ETA: 0s - loss: 0.2416 - accuracy: 0.9153\n",
      "End of epoch 14, LoraLayer 0: 23338 Step\n",
      "End of epoch 14, LoraLayer 0: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 1: 23338 Step\n",
      "End of epoch 14, LoraLayer 1: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 2: 23338 Step\n",
      "End of epoch 14, LoraLayer 2: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 3: 23338 Step\n",
      "End of epoch 14, LoraLayer 3: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 4: 23338 Step\n",
      "End of epoch 14, LoraLayer 4: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 5: 23338 Step\n",
      "End of epoch 14, LoraLayer 5: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 6: 23338 Step\n",
      "End of epoch 14, LoraLayer 6: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 7: 23338 Step\n",
      "End of epoch 14, LoraLayer 7: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 8: 23338 Step\n",
      "End of epoch 14, LoraLayer 8: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 9: 23338 Step\n",
      "End of epoch 14, LoraLayer 9: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 10: 23338 Step\n",
      "End of epoch 14, LoraLayer 10: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 11: 23338 Step\n",
      "End of epoch 14, LoraLayer 11: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 12: 23338 Step\n",
      "End of epoch 14, LoraLayer 12: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 13: 23338 Step\n",
      "End of epoch 14, LoraLayer 13: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 14: 23338 Step\n",
      "End of epoch 14, LoraLayer 14: Decay factor: 0.24979376792907715\n",
      "\n",
      "Testing loss: 2.3027310371398926, acc: 0.094200000166893\n",
      "\n",
      "1667/1667 [==============================] - 47s 28ms/step - loss: 0.2418 - accuracy: 0.9152 - val_loss: 2.3027 - val_accuracy: 0.0941\n",
      "Epoch 15/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.2971 - accuracy: 0.8960\n",
      "End of epoch 15, LoraLayer 0: 25005 Step\n",
      "End of epoch 15, LoraLayer 0: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 1: 25005 Step\n",
      "End of epoch 15, LoraLayer 1: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 2: 25005 Step\n",
      "End of epoch 15, LoraLayer 2: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 3: 25005 Step\n",
      "End of epoch 15, LoraLayer 3: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 4: 25005 Step\n",
      "End of epoch 15, LoraLayer 4: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 5: 25005 Step\n",
      "End of epoch 15, LoraLayer 5: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 6: 25005 Step\n",
      "End of epoch 15, LoraLayer 6: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 7: 25005 Step\n",
      "End of epoch 15, LoraLayer 7: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 8: 25005 Step\n",
      "End of epoch 15, LoraLayer 8: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 9: 25005 Step\n",
      "End of epoch 15, LoraLayer 9: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 10: 25005 Step\n",
      "End of epoch 15, LoraLayer 10: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 11: 25005 Step\n",
      "End of epoch 15, LoraLayer 11: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 12: 25005 Step\n",
      "End of epoch 15, LoraLayer 12: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 13: 25005 Step\n",
      "End of epoch 15, LoraLayer 13: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 14: 25005 Step\n",
      "End of epoch 15, LoraLayer 14: Decay factor: 0.18727970123291016\n",
      "\n",
      "Testing loss: 2.3028969764709473, acc: 0.08699999749660492\n",
      "\n",
      "1667/1667 [==============================] - 48s 29ms/step - loss: 0.2971 - accuracy: 0.8960 - val_loss: 2.3029 - val_accuracy: 0.0870\n",
      "Epoch 16/20\n",
      "1665/1667 [============================>.] - ETA: 0s - loss: 0.3779 - accuracy: 0.8681\n",
      "End of epoch 16, LoraLayer 0: 26672 Step\n",
      "End of epoch 16, LoraLayer 0: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 1: 26672 Step\n",
      "End of epoch 16, LoraLayer 1: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 2: 26672 Step\n",
      "End of epoch 16, LoraLayer 2: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 3: 26672 Step\n",
      "End of epoch 16, LoraLayer 3: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 4: 26672 Step\n",
      "End of epoch 16, LoraLayer 4: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 5: 26672 Step\n",
      "End of epoch 16, LoraLayer 5: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 6: 26672 Step\n",
      "End of epoch 16, LoraLayer 6: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 7: 26672 Step\n",
      "End of epoch 16, LoraLayer 7: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 8: 26672 Step\n",
      "End of epoch 16, LoraLayer 8: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 9: 26672 Step\n",
      "End of epoch 16, LoraLayer 9: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 10: 26672 Step\n",
      "End of epoch 16, LoraLayer 10: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 11: 26672 Step\n",
      "End of epoch 16, LoraLayer 11: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 12: 26672 Step\n",
      "End of epoch 16, LoraLayer 12: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 13: 26672 Step\n",
      "End of epoch 16, LoraLayer 13: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 14: 26672 Step\n",
      "End of epoch 16, LoraLayer 14: Decay factor: 0.12476563453674316\n",
      "\n",
      "Testing loss: 2.281834602355957, acc: 0.17679999768733978\n",
      "\n",
      "1667/1667 [==============================] - 46s 27ms/step - loss: 0.3778 - accuracy: 0.8682 - val_loss: 2.2818 - val_accuracy: 0.1765\n",
      "Epoch 17/20\n",
      "1665/1667 [============================>.] - ETA: 0s - loss: 0.4624 - accuracy: 0.8410\n",
      "End of epoch 17, LoraLayer 0: 28339 Step\n",
      "End of epoch 17, LoraLayer 0: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 1: 28339 Step\n",
      "End of epoch 17, LoraLayer 1: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 2: 28339 Step\n",
      "End of epoch 17, LoraLayer 2: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 3: 28339 Step\n",
      "End of epoch 17, LoraLayer 3: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 4: 28339 Step\n",
      "End of epoch 17, LoraLayer 4: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 5: 28339 Step\n",
      "End of epoch 17, LoraLayer 5: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 6: 28339 Step\n",
      "End of epoch 17, LoraLayer 6: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 7: 28339 Step\n",
      "End of epoch 17, LoraLayer 7: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 8: 28339 Step\n",
      "End of epoch 17, LoraLayer 8: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 9: 28339 Step\n",
      "End of epoch 17, LoraLayer 9: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 10: 28339 Step\n",
      "End of epoch 17, LoraLayer 10: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 11: 28339 Step\n",
      "End of epoch 17, LoraLayer 11: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 12: 28339 Step\n",
      "End of epoch 17, LoraLayer 12: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 13: 28339 Step\n",
      "End of epoch 17, LoraLayer 13: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 14: 28339 Step\n",
      "End of epoch 17, LoraLayer 14: Decay factor: 0.06225156784057617\n",
      "\n",
      "Testing loss: 1.9301066398620605, acc: 0.4593000113964081\n",
      "\n",
      "1667/1667 [==============================] - 47s 28ms/step - loss: 0.4625 - accuracy: 0.8410 - val_loss: 1.9301 - val_accuracy: 0.4593\n",
      "Epoch 18/20\n",
      "1665/1667 [============================>.] - ETA: 0s - loss: 0.5860 - accuracy: 0.8014\n",
      "End of epoch 18, LoraLayer 0: 30006 Step\n",
      "End of epoch 18, LoraLayer 0: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 1: 30006 Step\n",
      "End of epoch 18, LoraLayer 1: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 2: 30006 Step\n",
      "End of epoch 18, LoraLayer 2: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 3: 30006 Step\n",
      "End of epoch 18, LoraLayer 3: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 4: 30006 Step\n",
      "End of epoch 18, LoraLayer 4: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 5: 30006 Step\n",
      "End of epoch 18, LoraLayer 5: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 6: 30006 Step\n",
      "End of epoch 18, LoraLayer 6: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 7: 30006 Step\n",
      "End of epoch 18, LoraLayer 7: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 8: 30006 Step\n",
      "End of epoch 18, LoraLayer 8: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 9: 30006 Step\n",
      "End of epoch 18, LoraLayer 9: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 10: 30006 Step\n",
      "End of epoch 18, LoraLayer 10: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 11: 30006 Step\n",
      "End of epoch 18, LoraLayer 11: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 12: 30006 Step\n",
      "End of epoch 18, LoraLayer 12: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 13: 30006 Step\n",
      "End of epoch 18, LoraLayer 13: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 14: 30006 Step\n",
      "End of epoch 18, LoraLayer 14: Decay factor: 0.0\n",
      "\n",
      "Testing loss: 0.8395006060600281, acc: 0.7258999943733215\n",
      "\n",
      "1667/1667 [==============================] - 47s 28ms/step - loss: 0.5861 - accuracy: 0.8014 - val_loss: 0.8395 - val_accuracy: 0.7258\n",
      "Epoch 19/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.6010 - accuracy: 0.7993\n",
      "End of epoch 19, LoraLayer 0: 31673 Step\n",
      "End of epoch 19, LoraLayer 0: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 1: 31673 Step\n",
      "End of epoch 19, LoraLayer 1: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 2: 31673 Step\n",
      "End of epoch 19, LoraLayer 2: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 3: 31673 Step\n",
      "End of epoch 19, LoraLayer 3: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 4: 31673 Step\n",
      "End of epoch 19, LoraLayer 4: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 5: 31673 Step\n",
      "End of epoch 19, LoraLayer 5: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 6: 31673 Step\n",
      "End of epoch 19, LoraLayer 6: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 7: 31673 Step\n",
      "End of epoch 19, LoraLayer 7: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 8: 31673 Step\n",
      "End of epoch 19, LoraLayer 8: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 9: 31673 Step\n",
      "End of epoch 19, LoraLayer 9: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 10: 31673 Step\n",
      "End of epoch 19, LoraLayer 10: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 11: 31673 Step\n",
      "End of epoch 19, LoraLayer 11: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 12: 31673 Step\n",
      "End of epoch 19, LoraLayer 12: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 13: 31673 Step\n",
      "End of epoch 19, LoraLayer 13: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 14: 31673 Step\n",
      "End of epoch 19, LoraLayer 14: Decay factor: 0.0\n",
      "\n",
      "Testing loss: 0.7359296679496765, acc: 0.7594000101089478\n",
      "\n",
      "1667/1667 [==============================] - 47s 28ms/step - loss: 0.6011 - accuracy: 0.7993 - val_loss: 0.7359 - val_accuracy: 0.7593\n",
      "Epoch 20/20\n",
      "1665/1667 [============================>.] - ETA: 0s - loss: 0.4874 - accuracy: 0.8359\n",
      "End of epoch 20, LoraLayer 0: 33340 Step\n",
      "End of epoch 20, LoraLayer 0: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 1: 33340 Step\n",
      "End of epoch 20, LoraLayer 1: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 2: 33340 Step\n",
      "End of epoch 20, LoraLayer 2: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 3: 33340 Step\n",
      "End of epoch 20, LoraLayer 3: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 4: 33340 Step\n",
      "End of epoch 20, LoraLayer 4: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 5: 33340 Step\n",
      "End of epoch 20, LoraLayer 5: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 6: 33340 Step\n",
      "End of epoch 20, LoraLayer 6: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 7: 33340 Step\n",
      "End of epoch 20, LoraLayer 7: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 8: 33340 Step\n",
      "End of epoch 20, LoraLayer 8: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 9: 33340 Step\n",
      "End of epoch 20, LoraLayer 9: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 10: 33340 Step\n",
      "End of epoch 20, LoraLayer 10: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 11: 33340 Step\n",
      "End of epoch 20, LoraLayer 11: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 12: 33340 Step\n",
      "End of epoch 20, LoraLayer 12: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 13: 33340 Step\n",
      "End of epoch 20, LoraLayer 13: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 14: 33340 Step\n",
      "End of epoch 20, LoraLayer 14: Decay factor: 0.0\n",
      "\n",
      "Testing loss: 0.6917142868041992, acc: 0.7767999768257141\n",
      "\n",
      "1667/1667 [==============================] - 47s 28ms/step - loss: 0.4872 - accuracy: 0.8359 - val_loss: 0.6917 - val_accuracy: 0.7767\n"
     ]
    }
   ],
   "source": [
    "history_exp21 = exp21_lora_vgg16.fit(x_train, y_train, batch_size=30, epochs=20, validation_data=(x_test, y_test), callbacks=[print_step_callback, TestCallback((x_test, y_test))] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HW-llVuqfwUi",
    "outputId": "7913fd46-3d5e-43c3-a21b-37e64438053b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 3s 10ms/step - loss: 0.6917 - accuracy: 0.7768\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6917142868041992, 0.7767999768257141]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 평가\n",
    "exp21_lora_vgg16.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "id": "cQNbKdwVPyWo",
    "outputId": "42b5f2b2-e3fe-4e09-81b9-84460b7ed130"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA+kAAAHWCAYAAAALjsguAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAACig0lEQVR4nOzdZ3QUZRuH8WvTeyFAEiAQeu8dpKgoAqKABRBpAjZQEX1VLDQLdlFQsVBUpCgCoigIKChFATH0LhBaQk3vm3k/LFmISSCBJJPy/50zZ2efnXLvsmTm3qdZDMMwEBERERERERHTOZgdgIiIiIiIiIjYKEkXERERERERKSKUpIuIiIiIiIgUEUrSRURERERERIoIJekiIiIiIiIiRYSSdBEREREREZEiQkm6iIiIiIiISBGhJF1ERERERESkiFCSLiIiIiIiIlJEKEmXImXIkCGEhoZe074TJkzAYrHkb0BFzJEjR7BYLMyePbvQz22xWJgwYYL9+ezZs7FYLBw5cuSq+4aGhjJkyJB8jed6visiIlIy6L7hynTfcInuG6Q4UZIuuWKxWHK1rFmzxuxQS73HH38ci8XCwYMHc9zmhRdewGKxsH379kKMLO9OnjzJhAkTCAsLMzuUbO3ZsweLxYKbmxtRUVFmhyMiUmTovqH40H1Dwcr4oeTtt982OxQpRpzMDkCKh6+++irT8y+//JKVK1dmKa9bt+51neezzz4jPT39mvZ98cUXee65567r/CXBgAEDmDp1KnPnzmXcuHHZbjNv3jwaNmxIo0aNrvk8AwcOpF+/fri6ul7zMa7m5MmTTJw4kdDQUJo0aZLptev5ruSXOXPmEBQUxIULF1i4cCHDhw83NR4RkaJC9w3Fh+4bRIoeJemSK/fff3+m53/++ScrV67MUv5fCQkJeHh45Po8zs7O1xQfgJOTE05O+kq3bt2aGjVqMG/evGwvths3buTw4cO8/vrr13UeR0dHHB0dr+sY1+N6viv5wTAM5s6dy3333cfhw4f5+uuvi2ySHh8fj6enp9lhiEgpovuG4kP3DSJFj5q7S77p3LkzDRo04O+//6Zjx454eHjw/PPPA/D999/To0cPKlSogKurK9WrV+fll1/GarVmOsZ/+wtd3kTo008/pXr16ri6utKyZUs2b96cad/s+pZZLBZGjRrFkiVLaNCgAa6urtSvX5/ly5dniX/NmjW0aNECNzc3qlevzieffJLr/mp//PEH99xzD5UrV8bV1ZWQkBCefPJJEhMTs7w/Ly8vTpw4Qa9evfDy8qJcuXI8/fTTWT6LqKgohgwZgq+vL35+fgwePDjXTaoHDBjA3r172bp1a5bX5s6di8VioX///qSkpDBu3DiaN2+Or68vnp6edOjQgd9+++2q58iub5lhGLzyyitUqlQJDw8PbrzxRnbt2pVl3/Pnz/P000/TsGFDvLy88PHxoVu3bmzbts2+zZo1a2jZsiUAQ4cOtTeNzOhXl13fsvj4eJ566ilCQkJwdXWldu3avP322xiGkWm7vHwvcrJ+/XqOHDlCv3796NevH7///jvHjx/Psl16ejrvv/8+DRs2xM3NjXLlynHbbbexZcuWTNvNmTOHVq1a4eHhgb+/Px07duSXX37JFPPlffsy/LffXsa/y9q1a3n00UcpX748lSpVAuDo0aM8+uij1K5dG3d3dwICArjnnnuy7R8YFRXFk08+SWhoKK6urlSqVIlBgwZx9uxZ4uLi8PT05Iknnsiy3/Hjx3F0dGTy5Mm5/CRFpLTSfYPuG0rTfcPVnD59mmHDhhEYGIibmxuNGzfmiy++yLLd/Pnzad68Od7e3vj4+NCwYUPef/99++upqalMnDiRmjVr4ubmRkBAADfccAMrV67Mt1il4OnnQ8lX586do1u3bvTr14/777+fwMBAwPaH2cvLizFjxuDl5cWvv/7KuHHjiImJ4a233rrqcefOnUtsbCwPPfQQFouFN998kz59+vDvv/9e9ZfRdevWsWjRIh599FG8vb354IMPuOuuuwgPDycgIACAf/75h9tuu43g4GAmTpyI1Wpl0qRJlCtXLlfv+9tvvyUhIYFHHnmEgIAANm3axNSpUzl+/Djffvttpm2tVitdu3aldevWvP3226xatYp33nmH6tWr88gjjwC2i9add97JunXrePjhh6lbty6LFy9m8ODBuYpnwIABTJw4kblz59KsWbNM5/7mm2/o0KEDlStX5uzZs3z++ef079+fESNGEBsby4wZM+jatSubNm3K0lTsasaNG8crr7xC9+7d6d69O1u3buXWW28lJSUl03b//vsvS5Ys4Z577qFq1apERkbyySef0KlTJ3bv3k2FChWoW7cukyZNYty4cTz44IN06NABgHbt2mV7bsMwuOOOO/jtt98YNmwYTZo0YcWKFfzvf//jxIkTvPfee5m2z8334kq+/vprqlevTsuWLWnQoAEeHh7MmzeP//3vf5m2GzZsGLNnz6Zbt24MHz6ctLQ0/vjjD/78809atGgBwMSJE5kwYQLt2rVj0qRJuLi48Ndff/Hrr79y66235vrzv9yjjz5KuXLlGDduHPHx8QBs3ryZDRs20K9fPypVqsSRI0f4+OOP6dy5M7t377bXXsXFxdGhQwf27NnDAw88QLNmzTh79ixLly7l+PHjNGnShN69e7NgwQLefffdTDUj8+bNwzAMBgwYcE1xi0jpovsG3TeUlvuGK0lMTKRz584cPHiQUaNGUbVqVb799luGDBlCVFSU/UfxlStX0r9/f26++WbeeOMNwDY+zvr16+3bTJgwgcmTJzN8+HBatWpFTEwMW7ZsYevWrdxyyy3XFacUIkPkGowcOdL479enU6dOBmBMnz49y/YJCQlZyh566CHDw8PDSEpKspcNHjzYqFKliv354cOHDcAICAgwzp8/by///vvvDcD44Ycf7GXjx4/PEhNguLi4GAcPHrSXbdu2zQCMqVOn2st69uxpeHh4GCdOnLCXHThwwHBycspyzOxk9/4mT55sWCwW4+jRo5neH2BMmjQp07ZNmzY1mjdvbn++ZMkSAzDefPNNe1laWprRoUMHAzBmzZp11ZhatmxpVKpUybBarfay5cuXG4DxySef2I+ZnJycab8LFy4YgYGBxgMPPJCpHDDGjx9vfz5r1iwDMA4fPmwYhmGcPn3acHFxMXr06GGkp6fbt3v++ecNwBg8eLC9LCkpKVNchmH7t3Z1dc302WzevDnH9/vf70rGZ/bKK69k2u7uu+82LBZLpu9Abr8XOUlJSTECAgKMF154wV523333GY0bN8603a+//moAxuOPP57lGBmf0YEDBwwHBwejd+/eWT6Tyz/H/37+GapUqZLps834d7nhhhuMtLS0TNtm9z3duHGjARhffvmlvWzcuHEGYCxatCjHuFesWGEAxs8//5zp9UaNGhmdOnXKsp+IlG66b7j6+9N9g01Ju2/I+E6+9dZbOW4zZcoUAzDmzJljL0tJSTHatm1reHl5GTExMYZhGMYTTzxh+Pj4ZLm+X65x48ZGjx49rhiTFH1q7i75ytXVlaFDh2Ypd3d3t6/HxsZy9uxZOnToQEJCAnv37r3qcfv27Yu/v7/9ecavo//+++9V9+3SpQvVq1e3P2/UqBE+Pj72fa1WK6tWraJXr15UqFDBvl2NGjXo1q3bVY8Pmd9ffHw8Z8+epV27dhiGwT///JNl+4cffjjT8w4dOmR6Lz/99BNOTk72X8jB1pfrsccey1U8YOsPePz4cX7//Xd72dy5c3FxceGee+6xH9PFxQWwNcs+f/48aWlptGjRItsmb1eyatUqUlJSeOyxxzI19Rs9enSWbV1dXXFwsP35sVqtnDt3Di8vL2rXrp3n82b46aefcHR05PHHH89U/tRTT2EYBj///HOm8qt9L67k559/5ty5c/Tv399e1r9/f7Zt25apmd53332HxWJh/PjxWY6R8RktWbKE9PR0xo0bZ/9M/rvNtRgxYkSWvn+Xf09TU1M5d+4cNWrUwM/PL9Pn/t1339G4cWN69+6dY9xdunShQoUKfP311/bXdu7cyfbt26/a51REJIPuG3TfUBruG3ITS1BQUKb7CmdnZx5//HHi4uJYu3YtAH5+fsTHx1+x6bqfnx+7du3iwIED1x2XmEdJuuSrihUr2v94X27Xrl307t0bX19ffHx8KFeunP1GPjo6+qrHrVy5cqbnGRfeCxcu5HnfjP0z9j19+jSJiYnUqFEjy3bZlWUnPDycIUOGUKZMGXt/sU6dOgFZ319Gv+Sc4gFb3+Hg4GC8vLwybVe7du1cxQPQr18/HB0dmTt3LgBJSUksXryYbt26Zbpx+eKLL2jUqJG931K5cuVYtmxZrv5dLnf06FEAatasmam8XLlymc4Htgv7e++9R82aNXF1daVs2bKUK1eO7du35/m8l5+/QoUKeHt7ZyrPGDk4I74MV/teXMmcOXOoWrUqrq6uHDx4kIMHD1K9enU8PDwyJa2HDh2iQoUKlClTJsdjHTp0CAcHB+rVq3fV8+ZF1apVs5QlJiYybtw4e9+7jM89Kioq0+d+6NAhGjRocMXjOzg4MGDAAJYsWUJCQgJg6wLg5uZmv5kTEbka3TfovqE03DfkJpaaNWtm+bH+v7E8+uij1KpVi27dulGpUiUeeOCBLP3iJ02aRFRUFLVq1aJhw4b873//K/JT50lWStIlX13+y3CGqKgoOnXqxLZt25g0aRI//PADK1eutPelyc10GDmNBmr8Z2CP/N43N6xWK7fccgvLli3j2WefZcmSJaxcudI+UMl/319hjWxavnx5brnlFr777jtSU1P54YcfiI2NzdRXeM6cOQwZMoTq1aszY8YMli9fzsqVK7npppsKdJqS1157jTFjxtCxY0fmzJnDihUrWLlyJfXr1y+06VGu9XsRExPDDz/8wOHDh6lZs6Z9qVevHgkJCcydOzffvlu58d+BgzJk93/xscce49VXX+Xee+/lm2++4ZdffmHlypUEBARc0+c+aNAg4uLiWLJkiX20+9tvvx1fX988H0tESifdN+i+ITeK831DfipfvjxhYWEsXbrU3p++W7dumcYe6NixI4cOHWLmzJk0aNCAzz//nGbNmvH5558XWpxy/TRwnBS4NWvWcO7cORYtWkTHjh3t5YcPHzYxqkvKly+Pm5sbBw8ezPJadmX/tWPHDvbv388XX3zBoEGD7OXXM4pmlSpVWL16NXFxcZl+Fd+3b1+ejjNgwACWL1/Ozz//zNy5c/Hx8aFnz5721xcuXEi1atVYtGhRpqZm2TXPzk3MAAcOHKBatWr28jNnzmT5lXnhwoXceOONzJgxI1N5VFQUZcuWtT/PS3PvKlWqsGrVKmJjYzP9Kp7RLDIjvuu1aNEikpKS+PjjjzPFCrZ/nxdffJH169dzww03UL16dVasWMH58+dzrE2vXr066enp7N69+4oD7vj7+2cZpTclJYVTp07lOvaFCxcyePBg3nnnHXtZUlJSluNWr16dnTt3XvV4DRo0oGnTpnz99ddUqlSJ8PBwpk6dmut4RESyo/uGvNN9g01RvG/IbSzbt28nPT09U216drG4uLjQs2dPevbsSXp6Oo8++iiffPIJL730kr0lR5kyZRg6dChDhw4lLi6Ojh07MmHChCI7VaxkpZp0KXAZvzxe/ktjSkoKH330kVkhZeLo6EiXLl1YsmQJJ0+etJcfPHgwS3+knPaHzO/PMIxM02HkVffu3UlLS+Pjjz+2l1mt1jwnQL169cLDw4OPPvqIn3/+mT59+uDm5nbF2P/66y82btyY55i7dOmCs7MzU6dOzXS8KVOmZNnW0dExyy/P3377LSdOnMhUljG3d26mkOnevTtWq5Vp06ZlKn/vvfewWCy57id4NXPmzKFatWo8/PDD3H333ZmWp59+Gi8vL3uT97vuugvDMJg4cWKW42S8/169euHg4MCkSZOy1AZc/hlVr149Uz9BgE8//TTHmvTsZPe5T506Ncsx7rrrLrZt28bixYtzjDvDwIED+eWXX5gyZQoBAQH59jmLSOml+4a8032DTVG8b8iN7t27ExERwYIFC+xlaWlpTJ06FS8vL3tXiHPnzmXaz8HBgUaNGgGQnJyc7TZeXl7UqFHD/roUD6pJlwLXrl07/P39GTx4MI8//jgWi4WvvvqqUJsHXc2ECRP45ZdfaN++PY888oj9j3aDBg0ICwu74r516tShevXqPP3005w4cQIfHx++++676+qj1LNnT9q3b89zzz3HkSNHqFevHosWLcpzvysvLy969epl71/232mxbr/9dhYtWkTv3r3p0aMHhw8fZvr06dSrV4+4uLg8nStj3tbJkydz++230717d/755x9+/vnnLDXOt99+O5MmTWLo0KG0a9eOHTt28PXXX2f6JR1siamfnx/Tp0/H29sbT09PWrdunW1/6549e3LjjTfywgsvcOTIERo3bswvv/zC999/z+jRozMN9nKtTp48yW+//ZZlkJkMrq6udO3alW+//ZYPPviAG2+8kYEDB/LBBx9w4MABbrvtNtLT0/njjz+48cYbGTVqFDVq1OCFF17g5ZdfpkOHDvTp0wdXV1c2b95MhQoV7PONDx8+nIcffpi77rqLW265hW3btrFixYosn+2V3H777Xz11Vf4+vpSr149Nm7cyKpVq7JMHfO///2PhQsXcs899/DAAw/QvHlzzp8/z9KlS5k+fTqNGze2b3vffffxzDPPsHjxYh555JGrTm0kInI1um/IO9032BS1+4bLrV69mqSkpCzlvXr14sEHH+STTz5hyJAh/P3334SGhrJw4ULWr1/PlClT7DX9w4cP5/z589x0001UqlSJo0ePMnXqVJo0aWLvv16vXj06d+5M8+bNKVOmDFu2bGHhwoWMGjUqX9+PFLBCGEFeSqCcplKpX79+ttuvX7/eaNOmjeHu7m5UqFDBeOaZZ+xTOP3222/27XKaSiW7aSv4z9QeOU2lMnLkyCz7/nfaKsMwjNWrVxtNmzY1XFxcjOrVqxuff/658dRTTxlubm45fAqX7N692+jSpYvh5eVllC1b1hgxYoR9ao7LpwEZPHiw4enpmWX/7GI/d+6cMXDgQMPHx8fw9fU1Bg4caPzzzz+5nkolw7JlywzACA4OznaKr9dee82oUqWK4erqajRt2tT48ccfs/w7GMbVp1IxDMOwWq3GxIkTjeDgYMPd3d3o3LmzsXPnziyfd1JSkvHUU0/Zt2vfvr2xceNGo1OnTlmm7/r++++NevXq2ae1yXjv2cUYGxtrPPnkk0aFChUMZ2dno2bNmsZbb72VaWqXjPeS2+/F5d555x0DMFavXp3jNrNnzzYA4/vvvzcMwzZdzVtvvWXUqVPHcHFxMcqVK2d069bN+PvvvzPtN3PmTKNp06aGq6ur4e/vb3Tq1MlYuXKl/XWr1Wo8++yzRtmyZQ0PDw+ja9euxsGDB3Ocgm3z5s1ZYrtw4YIxdOhQo2zZsoaXl5fRtWtXY+/evdm+73PnzhmjRo0yKlasaLi4uBiVKlUyBg8ebJw9ezbLcbt3724AxoYNG3L8XESkdNN9Q2a6b7Ap6fcNhnHpO5nT8tVXXxmGYRiRkZH2a7SLi4vRsGHDLP9uCxcuNG699VajfPnyhouLi1G5cmXjoYceMk6dOmXf5pVXXjFatWpl+Pn5Ge7u7kadOnWMV1991UhJSblinFK0WAyjCP0sKVLE9OrVS9NYiFxF79692bFjR676YoqIlGS6bxCR/KA+6SIXJSYmZnp+4MABfvrpJzp37mxOQCLFwKlTp1i2bBkDBw40OxQRkUKl+wYRKSiqSRe5KDg4mCFDhlCtWjWOHj3Kxx9/THJyMv/880+WOTxFSrvDhw+zfv16Pv/8czZv3syhQ4cICgoyOywRkUKj+wYRKSgaOE7kottuu4158+YRERGBq6srbdu25bXXXtOFViQba9euZejQoVSuXJkvvvhCCbqIlDq6bxCRgqKadBEREREREZEiQn3SRURERERERIoIJekiIiIiIiIiRUSp65Oenp7OyZMn8fb2xmKxmB2OiIgIhmEQGxtLhQoVcHDQ7+f5Qdd7EREpSvJyrS91SfrJkycJCQkxOwwREZEsjh07RqVKlcwOo0TQ9V5ERIqi3FzrS12S7u3tDdg+HB8fH5OjERERgZiYGEJCQuzXKLl+ut6LiEhRkpdrfalL0jOavPn4+OiiLSIiRYqaZecfXe9FRKQoys21Xh3fRERERERERIoIJekiIiIiIiIiRYSSdBEREREREZEiQkm6iIiIiIiISBFhapL++++/07NnTypUqIDFYmHJkiVX3WfNmjU0a9YMV1dXatSowezZsws8ThEREREREZHCYGqSHh8fT+PGjfnwww9ztf3hw4fp0aMHN954I2FhYYwePZrhw4ezYsWKAo5UREREREREpOCZOgVbt27d6NatW663nz59OlWrVuWdd94BoG7duqxbt4733nuPrl27FlSYIiIiIiIiIoWiWPVJ37hxI126dMlU1rVrVzZu3JjjPsnJycTExGRaRERERERERIqiYpWkR0REEBgYmKksMDCQmJgYEhMTs91n8uTJ+Pr62peQkJDCCFVEREREREQkz4pVkn4txo4dS3R0tH05duyY2SGJiIiIiIiIZMvUPul5FRQURGRkZKayyMhIfHx8cHd3z3YfV1dXXF1dCyM8ERERERERketSrGrS27Zty+rVqzOVrVy5krZt25oUkYiIiIiIiEj+MTVJj4uLIywsjLCwMMA2xVpYWBjh4eGAran6oEGD7Ns//PDD/PvvvzzzzDPs3buXjz76iG+++YYnn3zSjPBFRERERERE8pWpzd23bNnCjTfeaH8+ZswYAAYPHszs2bM5deqUPWEHqFq1KsuWLePJJ5/k/fffp1KlSnz++eclbvq19HSDpDQrCSlWElOsJKba1hNS0khMuVSekJJGYmo6iSlpttdTL5UnpFhJurifNd3AwWLBwQEcLBYsFguOFtu67TmZXrctl7Z1sICjw6VtnRwsuDk74ubsiKuTA67Ojrg5O+Dm5Hix3OHSo5Mjrhe3y/yaI25ODjg5FqvGHCIiUtoc3wLLx5odhUgpZeRys1xud6Xj5XiM3O5j5KI8h+Nevr1HGShXG8rVgbK1bOtegWCx5ByHlDgWw8j1t7pEiImJwdfXl+joaHx8fEyJwTAMzsencORcPIfPJnDkbDyHz8Vz9Fw8R88lEJuUZkpcZnBysODq5IDFYsECYAEL4OBge55Rbrn4ouXi67bHSz8wkFF28bm3mxP+Hi4XF2f8Lj76e7rYy/08nCnj6YKHiyMW/eETERMVhWtTSZNvn+nBVTDnrvwLTEQkr9x8oWzti8l77UvrviHgoAqv4iIv16ViNXBccROVkMLhs/GZknHbenyuE3E3Zwc8XJxwd3bE3cURDxdH3J1tjx4uTrhf9tz++sXtM8qcHCykG5BuGBiGQXq6bT2jLGPdyFhPz1ye6fV0g7R0g6RUK0mp6bbHNNt6ctrF56lWklPTL5Zftl2qlaS0dFLS0u3vLy3dIC3FWlD/BLni4uiAn4ezLXn3dL6YwNuS+jKel9Z93Z3x83DG190FX3dnXJz0R1FEpMQLagz95podhUgplsuKlFxXuFxhuxyPkZd9LNmuZjlGpv0y1g2IjYAz++DsfjizFy4cgaRoOL7JtlzO2QMCathq3cvVulj7XhvKVAVH55xjliJPSfp1ik5MzZR822rFEzh6Lp6ohNQr7lvB143Qsp6ElvWkaoAnVQI8CC3rib+Hiz0Zd3AoeTW86ekGKdZ0ewKfnGbFMGyNfgzDuPRoLwODi88vW+fia+mX73Px+DFJqVyIT+VCQgpRCamcT0ghKiHFXmZbUklJSyfFms7p2GROxybn6X14uDji5+6Mr4cLvu5O+Lnbaud9MxL6i8/93J3xuZjg+3m44KmaexGR4sOrHNTpYXYUIlJapSbBuYNwdh+cuZi4n91vK0tNgIjttuVyDs4QUN3WXL5Ke2j9kJrLFzNK0q/DC4t38PVf4VfcJtDHldAAT6peTMYz1qsEeODm7FhIkRYtDg4W3BwcTX//hmGQmGrlQkIqF+IvJe4Z61EJtoT+fHwK0YmpRCemEpWQSkxSKobBxXECrJyMTsrTeZ0cLPh7ulDOy5XyPq6U93alvLebfb2ct5utzMcVV6fS+R0REREREcDZDYIa2JbLWdNstexn99lq3s/su5TIp8bbkvkze2HPUihbE2rcbEr4cm2UpF+H8t5uAJTzdqVqgCehZT0uqxW3Pfdw0UdcVFksFjxcnPBwcaKin3uu97OmG8QmXUraoxJTiUpIISbT89SLiX2KvSw6IZUUazpp6QZnYpM5E5vM7lNXPpevu7M9YS9/MXkv5+1KeZ+LifzFdS9Xfc9ERERESg1HJyhbw7Zc3tonPR1iTtgS9r8+hQMrYNs8JenFjO7sr8OQ9qEM61BVCVIp4+hgwe9iv/UqAbnfzzAMklLT7bXzp2OTOB2TfLGp/aX1jAQ+xZpur8E/cDruisf2cnWyd5ew/WDkSdWyHoQGeFLG00XN60VERERKAwcH8AuxLW5+tiR9z4+QFANuGpi0uFB2eR183TUgg+SexWKxDfTn4k4FP3fAN8dtDcMgKiGVM3HJF5P3JFsyf9n6mdhkTsckEZ9iJS45jV0nY9h1MibLsXzcnLJ0t8hI5n099B0WERERKZEqNoeAmnDuAOxeAs0GmR2R5JKSdJEiyGKx9Vv393ShVqD3FbeNT07jVHRipun8jlwcxPBkdBIxSWlsOx7NtuPRWfb193C+WOt+eQ287VEtRERERESKMYsFmvSH1ZNg23wl6cWI7sJFijlPVydqlPemRvmsyXxSqpWj5xIuTQV45lISfzo22TZQXngU/4RHZdm3gq8btYO8qR3kQ50gb2oHeVO9nJemnhMREREpLhr1g9Uvw9H1toHm/EPNjkhyQUm6SAnm5ux4MdHOmsDHJ6dx5Fw8R84mZJpC8Mi5eM7GpXAyOomT0Un8tu+MfR8nBwvVynnaE/dagd7UCfKmop97iZwuUERERKRY860I1TrBv2tstemdnzM7IskFJekipZSnqxP1K/hSv0LWvvHRiansj4xlb0Qs+yJi2BdhW49NSmN/ZBz7I+P4Ydtlx3JxpFaQLWGvHXip9t3f06UQ35GIiIiIZNH4votJ+jzo9KzmTC8GlKSLSBa+7s60DC1Dy9Ay9jLDMDgVncS+yFj2RcTaE/dDp+OIT7HyTzbN5st7u9pq8gO9aRziR8vQMgT5uhXyuxEREREpxereDsu8bM3dwzdClXZmRyRXoSRdRHLFYrFQwc82Mv2Ntcvby1Ot6Rw5G3+x1v1i7XtkDMfOJ16cXi6ZPw6ctW8fUsadlqFlaBVahpZVy1CtrKemiBMREREpKC6eUO9OCPsawuYqSS8GLIZhGGYHUZhiYmLw9fUlOjoaHx/NFShSUOKT09h/sdZ9z6kY/g6/wO6TMaT/5y9OgKcLLUL9bYl71TLUC/bByVGD00npomtT/tNnKiJymcN/wBe3g4s3/O8AOLubHVGpk5frkmrSRaRAeLo60bSyP00r+9vLYpNS2RoexebD59l85Dxhx6I4F5/Cil2RrNgVCYCHiyPNKtuS9pZV/Wka4o+7i6NZb0NERESk+KvSHnwrQ3Q47F0GDe82OyK5AiXpIlJovN2c6VSrHJ1qlQMgOc3KzhPRbDp8gc1HzrPlyHliktJYd/As6w7amsg7OVhoUNGXVlVtfeRbVPHXgHQiIiIieeHgAI37we9v2pq8K0kv0tTcXUSKjPR0g/2nY9l8+Dybjlxg8+HzRMQkZdmuZnkvWlYtQ+uLiXsFPzXZkuJN16b8p89UROQ/zh2Cqc3A4gBP7gafYLMjKlXU3F1EiiUHBwt1gnyoE+TDwLahGIbB8QuJbD5iax6/6fB5Dp2J58DpOA6cjmPuX+EAVPJ3p9XFPu0ajE5EREQkGwHVIaQNHPsTti+AG0abHZHkQEm6iBRZFouFkDIehJTxoE+zSgCci0tm85EL9sR954lojl9I5PiFEyz65wQAZb1c7APRtQwtQ91gHxwdlLSLiIhIKde4ny1J3zYP2j+hOdOLKDV3F5FiLS45ja1HL7Dp8Hk2XRyMLiUtPdM23q5ONA/1p1VV29RvDSv54uqkweik6NC1Kf/pMxURyUZiFLxdC6zJ8OAaqNDU7IhKDTV3F5FSw8vViY61ytHxssHoth+PtiXth8/z99ELxCansWbfGdbsOwOAq5MDTUL8bH3aq5ahWWV/PF3151BERERKOHc/qNMDdi2CsHlK0oso1aSLSImWZk1nb0SsPWnffOQ85+JTMm3j6GChQQUf2lYvS9vqAbQM9cfDRUm7FB5dm/KfPlMRkRwcWAlf3w3uZeCpfeCkWXMKg2rSRUQucnJ0oEFFXxpU9OWBG6piGAaHzsTbB6LbdPg8J6IS2XY8mm3Ho5m+9hDOjhYaV/KjXfUA2lYvS9PKfrg5q3m8iIiIlADVbgSvQIiLhAO/QN3bzY5I/kNJuoiUKhaLhRrlvahR3ov+rSoDcCIqkb/+PceGQ+fYeOgcJ6IS2XL0AluOXuCDXw/i4uRA88r+F5P2ABqH+OHs6GDyOxERERG5Bo5O0Ohe2DDVNoCckvQiR83dRUQuYxgGx84nsuHQWTb+a0vaT8cmZ9rGw8WRFqFlaFc9gHbVA6hfwVejx8t10bUp/+kzFRG5gshd8HE7cHC2NXn3DDA7ohJPzd1FRK6RxWKhcoAHlQMq069VZXvzeFvCfpaNh85xISGV3/ef4ff9toHovN2caF3VVsvernoAtQO9cVDSLiIiIkVVYH0IagQR22Hnd9D6QbMjkssoSRcRuYLLm8cPbFOF9HSDfZGx9qbxfx0+R2xSGqv2RLJqTyQA/h7OtKtelh6Ngrm5bnlN9yYiIiJFT5P7YPl22DZXSXoRo+buIiLXwZpusOtktD1p33zkPAkpVvvrvu7O9GwcTJ9mlWga4ofFohp2yUrXpvynz1RE5Criz8I7tSE9DR79C8rXMTuiEk3N3UVEComjg4VGlfxoVMmPhztVJ9WazvbjUazac5rFW08QEZPEnD/DmfNnONXKeXJXs0r0blqRCn7uZocuIiIipZlnWah5K+z7yVabfssksyOSi1STLiJSQKzpBhsPneO7rcdZvjOCxFRbDbvFAm2rBXBXs0rc1iAIT1f9Xlra6dqU//SZiojkwu6l8M1A8A6GJ3eBg7roFZS8XJeUpIuIFIK45DR+2nGKRVuP8+e/5+3lHi6OdGsQzF3NKtKmWoAGnCuldG3Kf/pMRURyIS3Z1uQ98QLc/x3U6GJ2RCWWmruLiBQxXq5O3NsihHtbhHDsfAKL/znBoq3HOXIuge+2Hue7rcep6OdO76YV6dOsItXKeZkdsoiIiJR0Tq7Q4C7Y/Dlsm68kvYhQTbqIiEkMw2Br+AUW/n2CH7efJDYpzf5a08p+3NWsEj0bVcDXw9nEKKUw6NqU//SZiojk0vG/4fObwMkdnt4PbvqbWRDU3P0KdNEWkaIoKdXKqj2RfPf3cX4/cBZruu1Ps4ujA13qleeuZpXoVKscTo4OJkcqBUHXpvynz1REJJcMAz5sBWf3wx1TodkgsyMqkfJyXdLdnohIEeDm7MjtjSowa2grNo69iRe616VOkDcp1nR+2hHBsC+2cOuU3/l5xylK2W+rIiIiUpAsFmjc37YeNs/cWARQki4iUuSU93ZjRMdqLB/dkWWP38AD7avi5+HMv2fieeTrrfT6aAMbDp01O0wREREpKRr1BSwQvgHOHzY7mlJPSbqISBFWv4Iv43rW449nbuTxm2rg7uzItmNR3PfZXwyeuYldJ6PNDlFERESKO9+KUK2zbX3bfFNDESXpIiLFgrebM2Nurc3aZzozsE0VnBwsrN1/hh4frOOJ+f8Qfi7B7BBFRESkOGtyn+1x2zxbP3UxjZJ0EZFipLy3Gy/3asCqMZ3o2bgCAN+HneTmd9cw/vudnI1LNjlCERERKZbq9AAXL4g6CuEbzY6mVFOSLiJSDIWW9WRq/6b8+NgNdKhZllSrwRcbj9Lxzd94b+V+4pLTrn4QERERkQwunlCvl209bK6poZR2StJFRIqxBhV9+WpYa+YOb02jSr4kpFh5f/UBOr35G7PWHyY5zWp2iCIiIlJcNLk4yvuuJZCirnRmUZIuIlICtKtRlu9HtufD+5pRtawn5+JTmPjDbm5+Zy1L/jlBerr6lomIiMhVVG4HfpUhJRb2LjM7mlJLSbqISAlhsVjo0SiYX57syKu9G1De25XjFxIZvSCMHlPX8du+05pjXURERHLm4HBpzvRtavJuFiXpIiIljLOjAwNaV2Ht/27kf11r4+3mxJ5TMQydtZl+n/7J1vALZocopcjkyZNp2bIl3t7elC9fnl69erFv376r7vftt99Sp04d3NzcaNiwIT/99FMhRCsiIjTuZ3v8dw3EnDQ1lNJKSbqISAnl7uLIyBtr8Pv/bmREh6q4ODnw1+Hz9PloAw99tYWDp2PNDlFKgbVr1zJy5Ej+/PNPVq5cSWpqKrfeeivx8fE57rNhwwb69+/PsGHD+Oeff+jVqxe9evVi586dhRi5iEgpVaYahLQBIx22f2N2NKWSxShlbR9jYmLw9fUlOjoaHx8fs8MRESk0J6ISmbJyP99tPU66AQ4W6NOsEqO71KSSv4fZ4ZVqpenadObMGcqXL8/atWvp2LFjttv07duX+Ph4fvzxR3tZmzZtaNKkCdOnT8/VeUrTZyoiku/+ng0/PAHl6sCjf4LFYnZExV5erkuqSRcRKSUq+rnz1j2NWT66I7fUCyTdgIV/H+fGt9cwYekuzsRqjnUpeNHR0QCUKVMmx202btxIly5dMpV17dqVjRtznrc3OTmZmJiYTIuIiFyj+r3ByQ3O7IWT/5gdTamjJF1EpJSpFejNZ4NasPjRdrSvEUCq1WD2hiN0fPM33lqxl+iEVLNDlBIqPT2d0aNH0759exo0aJDjdhEREQQGBmYqCwwMJCIiIsd9Jk+ejK+vr30JCQnJt7hFREodN1+o08O2vm2eubGUQkrSRURKqaaV/fl6eBu+Ht6aJiF+JKZa+fC3Q3R481c+/O0gCSlpZocoJczIkSPZuXMn8+fPz/djjx07lujoaPty7NixfD+HiEip0vg+2+OOhZCWYm4spYySdBGRUq59jbIsfrQdnw1qQe1Ab2KS0nhrxT46vrmG2esPk5xmNTtEKQFGjRrFjz/+yG+//UalSpWuuG1QUBCRkZGZyiIjIwkKCspxH1dXV3x8fDItIiJyHarfCF5BkHgeDvxidjSlipJ0ERHBYrFwS71AfnqiA1P6NqFyGQ/OxiUz4Yfd3PT2Wr7Zcow0a7rZYUoxZBgGo0aNYvHixfz6669UrVr1qvu0bduW1atXZypbuXIlbdu2LagwRUTkvxwcodG9tnU1eS9UStJFRMTO0cFCr6YVWf1UJ17t3YBAH1dORCXyzMLtdJ3yOz/tOEV6eqmaFESu08iRI5kzZw5z587F29ubiIgIIiIiSExMtG8zaNAgxo4da3/+xBNPsHz5ct555x327t3LhAkT2LJlC6NGjTLjLYiIlF6N+9se96+A+HPmxlKKKEkXEZEsnB0dGNC6Cmv/dyPPd6+Dn4czh87E8+jXW7njw3Ws2XeaUjaDp1yjjz/+mOjoaDp37kxwcLB9WbBggX2b8PBwTp06ZX/erl075s6dy6effkrjxo1ZuHAhS5YsueJgcyIiUgAC60FwY0hPhZ0LzY6m1NA86SIiclWxSal8/sdhPv/jX+JTbH3UW4WW4X+31aZlaM5TaUnu6NqU//SZiojkkz+nw/JnIbgJPLTW7GiKLc2TLiIi+crbzZknb6nFH8/exIgOVXFxcmDTkfPcM30jQ2dtYueJaLNDFBERkYLQ8G5wcIJTYXB6j9nRlApK0kVEJNfKeLrwQo96rP1fZ+5rXRlHBwu/7TvD7VPX8fi8f4hK0BQtIiIiJYpnWajZ1bYeNtfcWEoJJekiIpJnwb7uvNa7IavHdOLOJhWwWGDptpP0+GAd249HmR1eoUizpmvEexERKR2aXBxAbvs3kK6pWQuaknQREblmoWU9eb9fU74f2Z4qAR6ciErk7o838tXGIyV6YLlj5xPo9+mffPDrQbNDERERKXg1u4K7P8RFwL9rzI6mxFOSLiIi161RJT+WjrqBW+sFkmJN56Xvd/H4/DDik9PMDi1fGYbBd38fp9v7f7Dl6AW+2HCE6MRUs8MSEREpWE4ul5q8n9xqbiylgJJ0ERHJF77uznwysDkv9qiLk4OFH7ad5I5p69gfGWt2aPkiOiGVUfP+4alvtxGXnEbLUH9+fOwGfN2dzQ5NRESk4PmH2h6jjpkaRmmgJF1ERPKNxWJheIdqzH+wDUE+bhw6E8+d09azaOtxs0O7LhsOnuW2939n2fZTODlY+F/X2sx/sC0hZTzMDk1ERKRw+IXYHqOVpBc0JekiIpLvWoSWYdnjN9ChZlkSU62M+WYbYxftICm1eA02k5xm5bWf9jBgxl+cik6iallPvnukHSNvrIGjg8Xs8ERERAqPbyXbY3Tx/uG9OFCSLiIiBSLAy5XZQ1sxuktNLBaYtymcuz7ewNFz8WaHlisHImPp/eEGPv39XwwD+reqzLLHb6BxiJ/ZoYmIiBQ+34s16VHHoAQPDlsUKEkXEZEC4+hgYXSXWnz5QCvKeLqw62QMt09dx4pdEWaHliPDMPhiwxFun7qO3adiKOPpwqcDmzO5T0M8XJzMDk9ERMQcPhVtj2mJkHDe3FhKOCXpIiJS4DrULMeyx2+geRV/YpPSeOirv3l12W5Si9g846djkxg6ezPjl+4iOS2dTrXKsXx0B26tH2R2aCIiIuZydgOvQNt6dLi5sZRwStJFRKRQBPu6M//BNozoUBWAz/44TP9P/yQiOsnkyGxW7o7ktil/sGbfGVydHJh4R31mD21JeW83s0MTEREpGi5v8i4FRkm6iIgUGmdHB17oUY/p9zfH29WJLUcv0P2DP/jjwBnTYkpISWPsoh2M+HIL5+NTqBvsww+P3cDgdqFYLBocTkRExE6DxxUKJekiIlLobmsQxI+P30D9Cj6cj09h0MxNTFm1H2t64Q5Es/14FLd/sI55m2zN9h7sWI0lI9tRK9C7UOMQEREpFjQNW6EwPUn/8MMPCQ0Nxc3NjdatW7Np06Yrbj9lyhRq166Nu7s7ISEhPPnkkyQlFY2mkiIikntVAmzTmfVvVRnDgCmrDjBk1ibOxSUX+Lmt6QYf/naQPh9t4N+z8QT5uDF3eGue714XVyfHAj+/iIhIseSrJL0wmJqkL1iwgDFjxjB+/Hi2bt1K48aN6dq1K6dPn852+7lz5/Lcc88xfvx49uzZw4wZM1iwYAHPP/98IUcuIiL5wc3Zkcl9GvLuvY1xd3bkjwNn6fHBOrYcKbhRY4+dT6Dfpxt5a8U+0tINejQMZvnoDrSrUbbAzikiIlIiqE96oTB1Lpl3332XESNGMHToUACmT5/OsmXLmDlzJs8991yW7Tds2ED79u257777AAgNDaV///789ddfhRq3iIjkrz7NKtGgoi8Pz/mbf8/E0+/TP3nyllo0reyHu7MjHi5OeLg44ubsiIeLI+7Ojjg45L2/+JJ/TvDSkp3EJqfh5erExDvq06dZRfU9FxERyQ17c3f1SS9IpiXpKSkp/P3334wdO9Ze5uDgQJcuXdi4cWO2+7Rr1445c+awadMmWrVqxb///stPP/3EwIEDczxPcnIyycmXmk7GxMTk35sQEZF8UyvQm6WjbmDsoh38sO0kb63Yd8XtXZ0c7Am7u4stkb+07phl/cDpOH7eaZufvXkVf6b0bUJIGY/CeGsiIiIlQ8bAcQlnISUBXHQdLQimJelnz57FarUSGBiYqTwwMJC9e/dmu899993H2bNnueGGGzAMg7S0NB5++OErNnefPHkyEydOzNfYRUSkYHi5OvFBvya0rRbAN1uOEZecRmKKlcRUKwkpaSSlXppXPTktneS0dC6QmuvjOzpYeOLmmjzauTpOjqYPyyIiIlK8uPmBizekxNpq08vVMjuiEsnU5u55tWbNGl577TU++ugjWrduzcGDB3niiSd4+eWXeemll7LdZ+zYsYwZM8b+PCYmhpCQkMIKWURE8shisXBf68rc17pyltfS0w2S0qwkplhJuJi8X1pPIzEl/WIybytLSLHa1w0M7m4eQpMQv8J/UyIiIiWBxWKrTT+zxzZ4nJL0AmFakl62bFkcHR2JjIzMVB4ZGUlQUFC2+7z00ksMHDiQ4cOHA9CwYUPi4+N58MEHeeGFF3BwyFor4urqiqura/6/ARERKXQODpaL/dOdCDA7GBERkdLIL+RSki4FwrS2fi4uLjRv3pzVq1fby9LT01m9ejVt27bNdp+EhIQsibijo22qHMMo3Ll1RURERERESh1fDR5X0Ext7j5mzBgGDx5MixYtaNWqFVOmTCE+Pt4+2vugQYOoWLEikydPBqBnz568++67NG3a1N7c/aWXXqJnz572ZF1EREREREQKSMbgcZqGrcCYmqT37duXM2fOMG7cOCIiImjSpAnLly+3DyYXHh6eqeb8xRdfxGKx8OKLL3LixAnKlStHz549efXVV816CyIiIiIiIqWH38UxY9TcvcBYjFLWTjwmJgZfX1+io6Px8fExOxwRERFdmwqAPlMRkQIS/ifM7GpL1kfvMDuaYiMv1yXNPyMiIiIiIiK5k9EnPeYkpFvNjaWEUpIuIiKSF2kpsH8FLH4EVo4zOxoREZHC5R0EDk6QngaxEWZHUyIVq3nSRURETJGWAofXwq7FsPdHSIq2lXuUhZvGgaMupyIiUko4OIJPBYgKt/VL961odkQlju4qREREsmNNhX/Xwu7FsOdHSIq69JpXINS7E+r3BosapYmISCnjW9mWpEcdg8ptzI6mxFGSLiIiksGaCod/v1Rjnnjh0mue5S8l5pXb2GoSRERESqOMadg0wnuBUJIuIiKlmzUNjvwOu5bAnh8g8fyl1zzL2RLzer2gSjsl5iIiIgB+FwePU5JeIJSki4hI6WNNg6PrbDXme36AhHOXXvMoC/XusNWYV2mvxFxEROS/7DXpx82No4RSki4iIqVDuhWOrrcl5ruXQsLZS695BEDdO6B+L6hygwaCExERuZKMadiiVJNeEHQXIiIiJVt6OvzzJfz6KsSfvlTuXgbq9rTVmId2UGIuIiKSW36VbY/Rx8AwwGIxN54SRnckIiJScp07BD88AUf+sD1397cl5vV6QdWO4OhsangiIiLFks/FaddS4myzn7j7mxpOSaMkXURESh5rGmycBmsmQ1oSOHvATS9CqweVmIuIiFwvFw/bGC4JZ21N3pWk5ysl6SIiUrKc2g5LR8Gpbbbn1TpDz/fBP9TMqEREREoW30q2JD36OAQ3MjuaEkVJuoiIlAypSbD2DVj/PhhWcPOFrq9BkwHqKyciIpLf/ELgVJimYSsAStJFRKT4O7oBlj4G5w7ante7E7q9Bd6B5sYlIiJSUvleNnic5Csl6SIiUnwlxcCqCbBlhu25VxD0eNs2OJyIiIgUnIy50jUNW75Tki4iIsXTvuWwbAzEnLA9bzYIbnkZ3P1MDUtERKRU8Ls4V7pq0vOdknQRESle4s/Cz8/CzoW25/6h0PMDqNbJ1LBERERKlYya9Ojj5sZRAilJFxGR4sEwYPs3sPw5SDwPFgdoOxI6P2+bCkZEREQKT0af9LhI2+Ctzm7mxlOCKEkXEZGiL+oY/PgkHFxpex7YAO6YChWbmRuXiIhIaeVRBpw9IDXB1vUsoLrZEZUYStJFRKToSk+HzZ/D6omQEgeOLtDpGWg/GhydzY5ORESk9LJYbE3ez+639UtXkp5vlKSLiEjRdGafbVq1Y3/Znoe0sdWel6tlblwiIiJi4xtiS9I1wnu+UpIuIiJFz54fYeFQsKaAixd0mQAthoGDg9mRiYiISAYNHlcglKSLiEjRs36KLUGvdqOt9jxjmhcREREpOjQNW4FQlYSIiBQt1jSI2Glb7/62EnQREZGiKmOEdyXp+UpJuoiIFC3nDkBaoq2Ze5lqZkcjIiIiOclo7q4+6flKSbqIiBQtp7bZHoMaqg+6iIhIUZbR2i3mhG1GFskXuvsREZGiJSNJD25sbhwiIiJyZd7BYHGwjSMTf9rsaEoMJekiIlK0KEkXEREpHhydwbuCbV1N3vONknQRESk60tPh1HbbupJ0ERGRok8jvOc7JekiIlJ0XDgMKbHg5AZla5sdjYiIiFyNfa50Jen5RUm6iIgUHRlN3cvXA0cnc2MRERGRq/PNqEk/bm4cJYiSdBERKTrUH11ERKR40TRs+U5JuoiIFB1K0kVERIoXv8q2RzV3zzdK0kVEpGgwDCXpIiIixY36pOc7JekiIlI0RB+HxPPg4GTrky4iIiJFX0af9KRoSIoxN5YSQkm6iIgUDREXp14rVxec3cyNRURERHLH1Qvc/W3rGjwuXyhJFxGRokFN3UVERIonNXnPV0rSRUSkaLAn6Y3MjUNERETyxvfi4HFR4ebGUUIoSRcRkaJBNekiIiLFk70mXc3d84OSdBERMV9sJMSeAiwQ2MDsaERERCQv/C4OHqfm7vlCSbqIiJgvY9C4sjVtA9CIiIhI8ZExwrtq0vOFknQRETHfqTDbo5q6i4iIFD8ZSXqUatLzg5J0EREx36mLNelK0kuc33//nZ49e1KhQgUsFgtLliy54vZr1qzBYrFkWSIiIgonYBERybuM5u6xp8Caam4sJYCSdBERMV/GoHFBGtm9pImPj6dx48Z8+OGHedpv3759nDp1yr6UL1++gCIUEZHr5lEWHF0BA2JOmB1NsedkdgAiIlLKJV6AqKO2dU2/VuJ069aNbt265Xm/8uXL4+fnl/8BiYhI/nNwsI3wfv6Qrcm7f6jZERVrqkkXERFzZTR196sC7v7mxiJFRpMmTQgODuaWW25h/fr1V90+OTmZmJiYTIuIiBQiPw0el1+UpIuIiLk0P7pcJjg4mOnTp/Pdd9/x3XffERISQufOndm6desV95s8eTK+vr72JSQkpJAiFhER4LK50jV43PVSc3cRETGXknS5TO3ataldu7b9ebt27Th06BDvvfceX331VY77jR07ljFjxtifx8TEKFEXESlMvpVtj0rSr5uSdBERMVfGHOnBTUwNQ4quVq1asW7duitu4+rqiqurayFFJCIiWWTUpGsatuum5u4iImKe5Dg4e8C2rkHjJAdhYWEEBwebHYaIiFyJvU+6kvTrpZp0ERExT+ROwADvYPDSFFslUVxcHAcPHrQ/P3z4MGFhYZQpU4bKlSszduxYTpw4wZdffgnAlClTqFq1KvXr1ycpKYnPP/+cX3/9lV9++cWstyAiIrnhe9nAcYYBFou58RRjStJFRMQ86o9e4m3ZsoUbb7zR/jyj3/jgwYOZPXs2p06dIjw83P56SkoKTz31FCdOnMDDw4NGjRqxatWqTMcQEZEiyKciYIG0JIg/C17lzI6o2FKSLiIi5lGSXuJ17twZwzByfH327NmZnj/zzDM888wzBRyViIjkOycX8A6C2FO2Ju9K0q+Z+qSLiIh5lKSLiIiUHJqGLV8oSRcREXOkJsGZvbZ1JekiIiLFX0a/dI3wfl2UpIuIiDlO74b0NPAIuNiPTURERIo1v8sGj5NrpiRdRETMkdHUPaiRRoAVEREpCXw1DVt+UJIuIiLmUH90ERGRkkVJer5Qki4iIuZQki4iIlKyZAwcpz7p10VJuoiIFD5rKkTusq0rSRcRESkZMvqkJ56HlHhzYynGlKSLiEjhO7MPrMng6gP+Vc2ORkRERPKDm6/t2g4aPO46KEkXEZHCF7Hd9hjUCBx0KRIRESkxNA3bddOdkYiIFD71RxcRESmZ/DR43PVSki4iIoXPnqQ3MjcOERERyV8Zg8cpSb9mStJFRKRwpafDqYvN3VWTLiIiUrLYp2FTn/RrpSRdREQK1/lDkBoPTu4QUNPsaERERCQ/aRq266YkXURECldGU/egBuDoZG4sIiIikr/8Ktse1dz9mpmepH/44YeEhobi5uZG69at2bRp0xW3j4qKYuTIkQQHB+Pq6kqtWrX46aefCilaERG5bho0TkREpOTKaO4ecxKsaebGUkzlOUkPDQ1l0qRJhIeHX/fJFyxYwJgxYxg/fjxbt26lcePGdO3aldOnT2e7fUpKCrfccgtHjhxh4cKF7Nu3j88++4yKFStedywiIlJIlKSLiIiUXF6B4OAMhhViT5kdTbGU5yR99OjRLFq0iGrVqnHLLbcwf/58kpOTr+nk7777LiNGjGDo0KHUq1eP6dOn4+HhwcyZM7PdfubMmZw/f54lS5bQvn17QkND6dSpE40b60ZPRKRYMIzLmrtrZHcREZESx8EBfC9WomrwuGtyTUl6WFgYmzZtom7dujz22GMEBwczatQotm7dmuvjpKSk8Pfff9OlS5dLwTg40KVLFzZu3JjtPkuXLqVt27aMHDmSwMBAGjRowGuvvYbVas3xPMnJycTExGRaRETEJFHhkBRl+4W9fF2zoxEREZGC4Ku50q/HNfdJb9asGR988AEnT55k/PjxfP7557Rs2ZImTZowc+ZMDMO44v5nz57FarUSGBiYqTwwMJCIiIhs9/n3339ZuHAhVquVn376iZdeeol33nmHV155JcfzTJ48GV9fX/sSEhKS9zcrIiL5I6MWvXxdcHI1NxYREREpGBlJetT1d5Euja55WN3U1FQWL17MrFmzWLlyJW3atGHYsGEcP36c559/nlWrVjF37tz8jJX09HTKly/Pp59+iqOjI82bN+fEiRO89dZbjB8/Ptt9xo4dy5gxY+zPY2JicpWoW61WUlNT8y12kaLC2dkZR0dHs8OQ0kr90aUI0bVeCoKusyKAn+ZKvx55TtK3bt3KrFmzmDdvHg4ODgwaNIj33nuPOnXq2Lfp3bs3LVu2vOJxypYti6OjI5GRkZnKIyMjCQoKynaf4ODgLH/46tatS0REBCkpKbi4uGTZx9XVFVfX3NfWGIZBREQEUVFRud5HpLjx8/MjKCgIi8VidihS2ihJlyJA13opaLrOSqmXMVe6mrtfkzwn6S1btuSWW27h448/plevXjg7O2fZpmrVqvTr1++Kx3FxcaF58+asXr2aXr16Abaa8tWrVzNq1Khs92nfvj1z584lPT0dBwdbS/39+/cTHBycbYJ+LTIu2uXLl8fDw0N/XKVEMQyDhIQE+wwKwcHBJkckpU7EdttjcBNTw5DSTdd6KSi6zopc5Kua9OuR5yT933//pUqVKlfcxtPTk1mzZl31WGPGjGHw4MG0aNGCVq1aMWXKFOLj4xk6dCgAgwYNomLFikyePBmARx55hGnTpvHEE0/w2GOPceDAAV577TUef/zxvL6NbFmtVvtFOyAgIF+OKVLUuLu7A3D69GnKly+vJnlSeGIjIC4SLA4QWN/saKSU0rVeCpqusyJc1if9mG1mF/0Ymid5TtJPnz5NREQErVu3zlT+119/4ejoSIsWLXJ9rL59+3LmzBnGjRtHREQETZo0Yfny5fbB5MLDw+015gAhISGsWLGCJ598kkaNGlGxYkWeeOIJnn322by+jWxl9Evz8PDIl+OJFFUZ3/HU1FTdPEjhyWjqXrYWuOjvrJhD13opDLrOSqmX0dw9NR4SL4BHGXPjKWbynKSPHDmSZ555JkuSfuLECd544w3++uuvPB1v1KhROTZvX7NmTZaytm3b8ueff+bpHHmlZm9S0uk7LqZQf3QpQvR3UAqSvl9S6jm7gWd5iD9t65euJD1P8jwF2+7du2nWrFmW8qZNm7J79+58CUpEREogJekiIiKlR0ZtepQGj8urPCfprq6uWUZkBzh16hROTtc8o5sUQaGhoUyZMiXX269ZswaLxaLRckUke0rSRYocXetFpMBoGrZrluck/dZbb2Xs2LFER0fby6Kionj++ee55ZZb8jU4yR2LxXLFZcKECdd03M2bN/Pggw/mevt27dpx6tQpfH19r+l816JOnTq4uroSERFRaOcUkWuQcP7SNCxBDc2NRaQYKm3Xev0YIFIC2Ed4V016XuW56vvtt9+mY8eOVKlShaZNmwIQFhZGYGAgX331Vb4HKFd36tQp+/qCBQsYN24c+/bts5d5eXnZ1w3DwGq15qrVQ7ly5fIUh4uLS45z3BeEdevWkZiYyN13380XX3yRbwMIXqvU1NRspyQUES7VopepBm6F90OeSElRWq/1IlKMKUm/ZnmuSa9YsSLbt2/nzTffpF69ejRv3pz333+fHTt2EBISUhAxylUEBQXZF19fXywWi/353r178fb25ueff6Z58+a4urqybt06Dh06xJ133klgYCBeXl60bNmSVatWZTruf5vAWSwWPv/8c3r37o2Hhwc1a9Zk6dKl9tf/+6v37Nmz8fPzY8WKFdStWxcvLy9uu+22TDcaaWlpPP744/j5+REQEMCzzz7L4MGD6dWr11Xf94wZM7jvvvsYOHAgM2fOzPL68ePH6d+/P2XKlMHT05MWLVpkGtjwhx9+oGXLlri5uVG2bFl69+6d6b0uWbIk0/H8/PyYPXs2AEeOHMFisbBgwQI6deqEm5sbX3/9NefOnaN///5UrFgRDw8PGjZsyLx58zIdJz09nTfffJMaNWrg6upK5cqVefXVVwG46aabsgykeObMGVxcXFi9evVVPxORIisjSQ9qZG4cIsVUab3W5+TChQsMGjQIf39/PDw86NatGwcOHLC/fvToUXr27Im/vz+enp7Ur1+fn376yb7vgAEDKFeuHO7u7tSsWTNXUweLSB75XTYNm+RJnpN0sM2D/uCDD/Lhhx/y9ttvM2jQoBJbg2gYBgkpaaYshmHk2/t47rnneP3119mzZw+NGjUiLi6O7t27s3r1av755x9uu+02evbsSXh4+BWPM3HiRO699162b99O9+7dGTBgAOfPn89x+4SEBN5++22++uorfv/9d8LDw3n66aftr7/xxht8/fXXzJo1i/Xr1xMTE5MlOc5ObGws3377Lffffz+33HIL0dHR/PHHH/bX4+Li6NSpEydOnGDp0qVs27aNZ555hvT0dACWLVtG79696d69O//88w+rV6+mVatWVz3vfz333HM88cQT7Nmzh65du5KUlETz5s1ZtmwZO3fu5MEHH2TgwIFs2rTJvs/YsWN5/fXXeemll9i9ezdz5861Tzs4fPhw5s6dS3Jysn37OXPmULFiRW666aY8xydSZKg/uhRhutZnVlSu9VcyZMgQtmzZwtKlS9m4cSOGYdC9e3f7FHsjR44kOTmZ33//nR07dvDGG2/YWxtkXH9//vln9uzZw8cff0zZsmWvKx4RyUbGwHGqSc+zax7pbffu3YSHh5OSkpKp/I477rjuoIqSxFQr9catMOXcuyd1xcMlfwbjmzRpUqYxA8qUKUPjxpdull9++WUWL17M0qVLc5wSD2wXxf79+wPw2muv8cEHH7Bp0yZuu+22bLdPTU1l+vTpVK9eHbBNuTdp0iT761OnTmXs2LH2Wuxp06bZf+m+kvnz51OzZk3q168PQL9+/ZgxYwYdOnQAYO7cuZw5c4bNmzdTpoxtyocaNWrY93/11Vfp168fEydOtJdd/nnk1ujRo+nTp0+msstvTB577DFWrFjBN998Q6tWrYiNjeX9999n2rRpDB48GIDq1atzww03ANCnTx9GjRrF999/z7333gvYaimGDBmi6VykeFOSLkWYrvWZFZVrfU4OHDjA0qVLWb9+Pe3atQPg66+/JiQkhCVLlnDPPfcQHh7OXXfdRcOGtjEwqlWrZt8/PDycpk2b0qJFC8DWmkBECkBGc/f4M5CaCM7u5sZTjOT5qvDvv//Su3dvduzYgcVisf8CnJFAWK3W/I1Q8kXGhShDXFwcEyZMYNmyZZw6dYq0tDQSExOv+ut6o0aXmqp6enri4+PD6dOnc9zew8PDftEGCA4Otm8fHR1NZGRkphpsR0dHmjdvbq/xzsnMmTO5//777c/vv/9+OnXqxNSpU/H29iYsLIymTZvaE/T/CgsLY8SIEVc8R27893O1Wq289tprfPPNN5w4cYKUlBSSk5Px8PAAYM+ePSQnJ3PzzTdnezw3Nzd78/17772XrVu3snPnzkxNDUWKnaQYOH/Itq4kvVg5duwYFouFSpVstSGbNm1i7ty51KtXL0+DjUnhKGnX+pzs2bMHJycnWrdubS8LCAigdu3a7NmzB4DHH3+cRx55hF9++YUuXbpw11132d/XI488wl133cXWrVu59dZb6dWrlz3ZF5F85O4Pzp6QGg/RJ6BsjavvI8A1JOlPPPEEVatWZfXq1VStWpVNmzZx7tw5nnrqKd5+++2CiNFU7s6O7J7U1bRz5xdPT89Mz59++mlWrlzJ22+/TY0aNXB3d+fuu+/O0jLiv/7brcFisVzxIpvd9tfbtG/37t38+eefbNq0KdNgcVarlfnz5zNixAjc3a/8S93VXs8uzowmdJf77+f61ltv8f777zNlyhQaNmyIp6cno0ePtn+uVzsv2Jq8N2nShOPHjzNr1ixuuukmqlSpctX9RIqsyJ22R59K4KkmpcXJfffdZ++2ExERwS233EL9+vX5+uuviYiIYNy4cWaHmC90rc+sKFzrr9fw4cPp2rUry5Yt45dffmHy5Mm88847PPbYY3Tr1o2jR4/y008/sXLlSm6++WZGjhxZIu9jRUxlsdj6pZ/Za2vyriQ91/LcJ33jxo1MmjSJsmXL4uDggIODAzfccAOTJ0/m8ccfL4gYTWWxWPBwcTJlKcjmzevXr2fIkCH07t2bhg0bEhQUxJEjRwrsfNnx9fUlMDCQzZs328usVitbt2694n4zZsygY8eObNu2jbCwMPsyZswYZsyYAdhqAcLCwnLsQ9eoUaMrDsRWrly5TIPeHDhwgISEhKu+p/Xr13PnnXdy//3307hxY6pVq8b+/fvtr9esWRN3d/crnrthw4a0aNGCzz77jLlz5/LAAw9c9bwiRZqauhdbO3futNeAfvPNNzRo0IANGzbw9ddf2wfSLAl0rS8413qtv5K6deuSlpaWaTDYc+fOsW/fPurVq2cvCwkJ4eGHH2bRokU89dRTfPbZZ/bXypUrx+DBg5kzZw5Tpkzh008/veZ4ROQKNML7NclzTbrVasXb2xuAsmXLcvLkSWrXrk2VKlUyTQUiRVvNmjVZtGgRPXv2xGKx8NJLL11zs7Pr8dhjjzF58mRq1KhBnTp1mDp1KhcuXMjxpiU1NZWvvvqKSZMm0aBBg0yvDR8+nHfffZddu3bRv39/XnvtNXr16sXkyZMJDg7mn3/+oUKFCrRt25bx48dz8803U716dfr160daWho//fSTvWb+pptuYtq0abRt2xar1cqzzz6bq8ERa9asycKFC9mwYQP+/v68++67REZG2m8a3NzcePbZZ3nmmWdwcXGhffv2nDlzhl27djFs2LBM72XUqFF4enpmGnVepFhSkl5spaam4urqCsCqVavs487UqVMn0w+ZUjQV12v95Xbs2GG/7wTbDyqNGzfmzjvvZMSIEXzyySd4e3vz3HPPUbFiRe68807ANmZMt27dqFWrFhcuXOC3336jbt26AIwbN47mzZtTv359kpOT+fHHH+2viUg+yxg8TiO850mea9IbNGjAtm22G67WrVvz5ptvsn79eiZNmpRpUA4p2t599138/f1p164dPXv2pGvXrjRr1qzQ43j22Wfp378/gwYNom3btnh5edG1a1fc3Nyy3X7p0qWcO3cu28S1bt261K1blxkzZuDi4sIvv/xC+fLl6d69Ow0bNuT111/H0dHWrLBz5858++23LF26lCZNmnDTTTdlGoH9nXfeISQkhA4dOnDffffx9NNP2/uVX8mLL75Is2bN6Nq1K507dyYoKCjLFDMvvfQSTz31FOPGjaNu3br07ds3S1+//v374+TkRP/+/XP8LESKDXuSrunXipv69eszffp0/vjjD1auXGkfOOzkyZMEBASYHJ1cTXG91l+uY8eONG3a1L40b94cgFmzZtG8eXNuv/122rZti2EY/PTTT/Yf1K1WKyNHjqRu3brcdttt1KpVi48++giwzfU+duxYGjVqRMeOHXF0dGT+/PkF9wGIlGYZ07BFHzc3jmLGYuSx09CKFSuIj4+nT58+HDx4kNtvv539+/cTEBDAggULivw0UTExMfj6+hIdHY2Pj0+m15KSkjh8+DBVq1ZVYmSS9PR06taty7333svLL79sdjimOXLkCNWrV2fz5s0FckOl77oUmpQEmFwRjHQYswd8KpgdUZF0pWuTmdasWUPv3r2JiYlh8ODBzJw5E4Dnn3+evXv3smjRIpMjzFlOn6n+/pmvNFzr9T0TuWj7N7BoBIR2gCE/mh2NqfJyrc9zc/euXS8NrFKjRg327t3L+fPn8ff31xRRkmdHjx7ll19+oVOnTiQnJzNt2jQOHz7MfffdZ3ZopkhNTeXcuXO8+OKLtGnTxpQaD5F8dXq3LUH3LAfewWZHI3nUuXNnzp49S0xMDP7+/vbyBx98MFeti0RA13qRUk190q9Jnpq7p6am4uTkxM6dOzOVlylTRgm6XBMHBwdmz55Ny5Ytad++PTt27GDVqlWltm/Y+vXrCQ4OZvPmzUyfPt3scESu36kw22NwY9sor1KsJCYmkpycbE/Qjx49ypQpU9i3bx/ly5c3OTopLnStFynFMvqkR58AE8bEKK7yVJPu7OxM5cqVNRe65JuQkBDWr19vdhhFRufOnU2ftkYkX53abnvUoHHF0p133kmfPn14+OGHiYqKonXr1jg7O3P27FneffddHnnkEbNDlGJA13qRUsw7GCyOkJ4KcZHgo1Z1uZHngeNeeOEFnn/++RynthIREbHTyO7F2tatW+nQoQMACxcuJDAwkKNHj/Lll1/ywQcfmBydiIgUeY5O4FPRtq4m77mW5z7p06ZN4+DBg1SoUIEqVarg6emZ6fXrmfdSRERKkLQUW590UJJeTCUkJNinv/rll1/o06cPDg4OtGnThqNHj5ocnYiIFAu+lSA6HKLCIaSV2dEUC3lO0v87nZSIiEi2zuwFawq4+YJfFbOjkWtQo0YNlixZQu/evVmxYgVPPvkkAKdPny5So9CLiEgR5hcC4WgatjzIc5I+fvz4gohDRERKmoym7kGNNGhcMTVu3Djuu+8+nnzySW666Sbatm0L2GrVmzZtanJ0IiJSLNgHj1Nz99zKc5IuIiKSK+qPXuzdfffd3HDDDZw6dYrGjS/9O95888307t3bxMhERKTYsE/Dppr03Mpzku7g4HDF6dY08ruIiAAQkTGyexNTw5DrExQURFBQEMeP226uKlWqRKtW6lMoIiK55HcxSY9STXpu5Xl098WLF7No0SL7smDBAp577jmCg4P59NNPCyJGKSSdO3dm9OjR9uehoaFMmTLlivtYLBaWLFly3efOr+OISBGRboWIHbZ11aQXW+np6UyaNAlfX1+qVKlClSpV8PPz4+WXXyZd890WS7rWi0ihs9ekK0nPrTzXpN95551Zyu6++27q16/PggULGDZsWL4EJrnXs2dPUlNTWb58eZbX/vjjDzp27Mi2bdto1KhRno67efPmLKP3X68JEyawZMkSwsLCMpWfOnUKf3//fD1XThITE6lYsSIODg6cOHECV1fXQjmvSKly7iCkJoCzJwRUNzsauUYvvPACM2bM4PXXX6d9+/YArFu3jgkTJpCUlMSrr75qcoSlh671uTN79mxGjx5NVFRUgZ5HRPIgo096cgwkRdsGlJUrynNNek7atGnD6tWr8+twkgfDhg1j5cqV9qaIl5s1axYtWrTI80UboFy5cnh4eORHiFcVFBRUaMnyd999R/369alTp47pv+gbhkFaWpqpMYgUCPugcQ3AwdHcWOSaffHFF3z++ec88sgjNGrUiEaNGvHoo4/y2WefMXv2bLPDK1V0rReRYsvFE9zL2NbV5D1X8iVJT0xM5IMPPqBixYr5cTjJo9tvv51y5cpluWGKi4vj22+/ZdiwYZw7d47+/ftTsWJFPDw8aNiwIfPmzbvicf/bBO7AgQN07NgRNzc36tWrx8qVK7Ps8+yzz1KrVi08PDyoVq0aL730EqmpqYDt1+2JEyeybds2LBYLFovFHvN/m8Dt2LGDm266CXd3dwICAnjwwQeJi4uzvz5kyBB69erF22+/TXBwMAEBAYwcOdJ+riuZMWMG999/P/fffz8zZszI8vquXbu4/fbb8fHxwdvbmw4dOnDo0CH76zNnzqR+/fq4uroSHBzMqFGjADhy5AgWiyVTzUFUVBQWi4U1a9YAsGbNGiwWCz///DPNmzfH1dWVdevWcejQIe68804CAwPx8vKiZcuWrFq1KlNcycnJPPvss4SEhODq6kqNGjWYMWMGhmFQo0YN3n777Uzbh4WFYbFYOHjw4FU/E5F8p0HjSoTz589Tp06dLOV16tTh/PnzJkRUeulan7drfU7Cw8O588478fLywsfHh3vvvZfIyEj769u2bePGG2/E29sbHx8fmjdvzpYtWwA4evQoPXv2xN/fH09PT+rXr89PP/10zbGIlCp+GjwuL/Lc3N3f3z/TwHGGYRAbG4uHhwdz5szJ1+CKBMOwNdk0g7NHrqYtcnJyYtCgQcyePZsXXnjB/u/z7bffYrVa6d+/P3FxcTRv3pxnn30WHx8fli1bxsCBA6levXquBgBKT0+nT58+BAYG8tdffxEdHZ2pT1sGb29vZs+eTYUKFdixYwcjRozA29ubZ555hr59+7Jz506WL19uT0B9fbM2d4mPj6dr1660bduWzZs3c/r0aYYPH86oUaMy3Zz89ttvBAcH89tvv3Hw4EH69u1LkyZNGDFiRI7v49ChQ2zcuJFFixZhGAZPPvkkR48epUoV2xzOJ06coGPHjnTu3Jlff/0VHx8f1q9fb6/t/vjjjxkzZgyvv/463bp1Izo6mvXr11/18/uv5557jrfffptq1arh7+/PsWPH6N69O6+++iqurq58+eWX9OzZk3379lG5cmUABg0axMaNG/nggw9o3Lgxhw8f5uzZs1gsFh544AFmzZrF008/bT/HrFmz6NixIzVq1MhzfCLXTUl6idC4cWOmTZvGBx98kKl82rRp11RrW2TpWg+UnGv9ld5fRoK+du1a0tLSGDlyJH379rX/mD5gwACaNm3Kxx9/jKOjI2FhYTg7OwMwcuRIUlJS+P333/H09GT37t14eXnlOQ6RUsk3xHZvoH7puZLnJP29997LlKQ7ODhQrlw5WrduXWh9igtVagK8VsGccz9/0tY8JBceeOAB3nrrLdauXUvnzp0BW5J211134evri6+vb6YE7rHHHmPFihV88803ubpwr1q1ir1797JixQoqVLB9Hq+99hrdunXLtN2LL75oXw8NDeXpp59m/vz5PPPMM7i7u+Pl5YWTkxNBQUE5nmvu3LkkJSXx5Zdf2vvJTZs2jZ49e/LGG28QGBgI2H4wmjZtGo6OjtSpU4cePXqwevXqK164Z86cSbdu3ezf1a5duzJr1iwmTJgAwIcffoivry/z58+3X5Rr1apl3/+VV17hqaee4oknnrCXtWzZ8qqf339NmjSJW265xf68TJkymaY3evnll1m8eDFLly5l1KhR7N+/n2+++YaVK1fSpUsXAKpVq2bffsiQIYwbN45NmzbRqlUrUlNTmTt3bpbadZFCYRhwKmNkdyXpxdmbb75Jjx49WLVqlX2O9I0bN3Ls2LGSVYOoaz1Qcq71OVm9ejU7duzg8OHDhITYavW+/PJL6tevz+bNm2nZsiXh4eH873//s7cgqVmzpn3/8PBw7rrrLho2bAhkvg6LyFVkDB4XFW5uHMVEnpu7DxkyhMGDB9uXgQMHctttt5XMBL0YqVOnDu3atWPmzJkAHDx4kD/++MM+kJ/VauXll1+mYcOGlClTBi8vL1asWEF4eO7+o+zZs4eQkBD7RRuw37BdbsGCBbRv356goCC8vLx48cUXc32Oy8/VuHHjTAPZtG/fnvT0dPbt22cvq1+/Po6Ol/q6BgcHc/r06RyPa7Va+eKLL7j//vvtZffffz+zZ8+2j1IcFhZGhw4d7An65U6fPs3Jkye5+eab8/R+stOiRYtMz+Pi4nj66aepW7cufn5+eHl5sWfPHvtnFxYWhqOjI506dcr2eBUqVKBHjx72f/8ffviB5ORk7rnnnuuOVSTPLhyB5GhwdIFyWZtKS/HRqVMn9u/fT+/evYmKiiIqKoo+ffqwa9cuvvrqK7PDK3V0rb/6tf5q5wwJCbEn6AD16tXDz8+PPXv2ADBmzBiGDx9Oly5deP311zN1d3v88cd55ZVXaN++PePHj2f79u3XFIdIqaTm7nmS55r0WbNm4eXlleXm/9tvvyUhIYHBgwfnW3BFgrOH7Vdus86dB8OGDeOxxx7jww8/ZNasWVSvXt2e1L311lu8//77TJkyhYYNG+Lp6cno0aNJSUnJt3A3btzIgAEDmDhxIl27drXXSL/zzjv5do7L/TeRtlgsV5wSaMWKFZw4cYK+fftmKrdaraxevZpbbrkFd3f3HPe/0mtga1UCti4gGXLqN/ffkXSffvppVq5cydtvv02NGjVwd3fn7rvvtv/7XO3cAMOHD2fgwIG89957zJo1i759+xbaYEAimWQ0dQ+sD45Zf/CS4qVChQpZRnHftm0bM2bMKDlTr+pan2tF/Vp/vSZMmMB9993HsmXL+Pnnnxk/fjzz58+nd+/eDB8+nK5du7Js2TJ++eUXJk+ezDvvvMNjjz1WYPGIlBgZI7yruXuu5LkmffLkyZQtWzZLefny5XnttdfyJagixWKxNUMzY8lFH7XL3XvvvTg4ODB37ly+/PJLHnjgAXvXhPXr13PnnXdy//3307hxY6pVq8b+/ftzfey6dety7NgxTp06ZS/7888/M22zYcMGqlSpwgsvvECLFi2oWbMmR48ezbSNi4sLVqv1qufatm0b8fHx9rL169fj4OBA7dq1cx3zf82YMYN+/foRFhaWaenXr599ALlGjRrxxx9/ZJtce3t7ExoamuMsBuXKlQPI9Bn9d/qZnKxfv54hQ4bQu3dvGjZsSFBQEEeOHLG/3rBhQ9LT01m7dm2Ox+jevTuenp58/PHHLF++nAceeCBX5xbJd+qPLsWNrvVAybjWX+2cx44d49ixS0nC7t27iYqKol69evayWrVq8eSTT/LLL7/Qp08fZs2aZX8tJCSEhx9+mEWLFvHUU0/x2WefFUisIiWOr2rS8yLPSXp4eDhVq1bNUl6lSpU8N3WS/OXl5UXfvn0ZO3Ysp06dYsiQIfbXatasycqVK9mwYQN79uzhoYceyjSa6dV06dKFWrVqMXjwYLZt28Yff/zBCy+8kGmbmjVrEh4ezvz58zl06BAffPABixcvzrRNaGgohw8fJiwsjLNnz5KcnJzlXAMGDMDNzY3Bgwezc+dOfvvtNx577DEGDhxo76OWV2fOnOGHH35g8ODBNGjQINMyaNAglixZwvnz5xk1ahQxMTH069ePLVu2cODAAb766it707sJEybwzjvv8MEHH3DgwAG2bt3K1KlTAVttd5s2bXj99dfZs2cPa9euzdRv70pq1qzJokWLCAsLY9u2bdx3332ZagpCQ0MZPHgwDzzwAEuWLOHw4cOsWbOGb775xr6No6MjQ4YMYezYsdSsWTPbJooihcI+/VoJGlhMpIjQtf7qrFZrlh/k9+zZQ5cuXWjYsCEDBgxg69atbNq0iUGDBtGpUydatGhBYmIio0aNYs2aNRw9epT169ezefNm6tatC8Do0aNZsWIFhw8fZuvWrfz222/210TkKvxsAyETGwFp+de6p6TKc5Jevnz5bPvgbNu2jYCAgHwJSq7dsGHDuHDhAl27ds3Up+zFF1+kWbNmdO3alc6dOxMUFESvXr1yfVwHBwcWL15MYmIirVq1Yvjw4VmaP95xxx08+eSTjBo1iiZNmrBhwwZeeumlTNvcdddd3Hbbbdx4442UK1cu26lhPDw8WLFiBefPn6dly5bcfffd3HzzzUybNi1vH8ZlMgamya4/+c0334y7uztz5swhICCAX3/9lbi4ODp16kTz5s357LPP7M3tBg8ezJQpU/joo4+oX78+t99+OwcOHLAfa+bMmaSlpdG8eXNGjx7NK6+8kqv43n33Xfz9/WnXrh09e/aka9euNGvWLNM2H3/8MXfffTePPvooderUYcSIEZlqIMD275+SksLQoUPz+hGJ5A/DuKwmvYmpoYiUVLrWX1lcXBxNmzbNtPTs2ROLxcL333+Pv78/HTt2pEuXLlSrVo0FCxYAth+7z507x6BBg6hVqxb33nsv3bp1Y+LEiYAt+R85ciR169bltttuo1atWnz00UfXHa9IqeARAE7ugAExJ8yOpsizGJd3oM2FZ599lgULFtindwJYu3YtDzzwAHfffXeRH006JiYGX19foqOj8fHxyfRaUlIShw8fpmrVqri5uZkUoci1++OPP7j55ps5duzYFWsi9F2XAhN9At6rBxZHeP4EOF99PAW58rXJDH369Lni61FRUaxdu/aqTZrNlNNnqr9/Uhj0PRPJxtQWcO4ADP4BqnY0O5pCl5drfZ4Hjnv55Zc5cuQIN998M05Ott3T09MZNGhQyeyTLlIMJCcnc+bMGSZMmMA999xz3U0FRa5ZxMWWVuXqKEEvxrKb1/q/rw8aNKiQohERkRLBt5ItSY/S4HFXk+ck3cXFhQULFvDKK68QFhaGu7s7DRs2pEqVKgURn4jkwrx58xg2bBhNmjThyy+/NDscKc00aFyJcPlAWSIiIvlC07DlWp6T9Aw1a9akZs2a+RmLiFyjIUOGZBo8SMQ0StJFREQkO74XB4+L1mDjV5PngePuuusu3njjjSzlb775Zpa500VEpJSxJ+ka2V1EREQuY58rXTXpV5PnJP3333+ne/fuWcq7devG77//ni9BmS2PY+mJFDv6jkuBiDtzacTWoIbmxiJyFfo7KAVJ3y+RbGQ0d1ef9KvKc5IeFxeHi4tLlnJnZ2diYmLyJSizZEyzlZCQYHIkIgUr4zue8Z0XyRcRF2vRA2qAq7e5sYjkQNd6KQy6zopk4/Ka9PR0c2Mp4vLcJ71hw4YsWLCAcePGZSqfP38+9erVy7fAzODo6Iifnx+nT58GbHN4WiwWk6MSyT+GYZCQkMDp06fx8/PD0dHR7JCkJDl1cWR39UeXIkzXeilIus6KXIFPRcAC1mRIOAte5c2OqMjKc5L+0ksv0adPHw4dOsRNN90EwOrVq5k7dy4LFy7M9wALW1BQEID94i1SEvn5+dm/6yL5RoPGSTGha70UNF1nRbLh6AzewRB70tbkXUl6jvKcpPfs2ZMlS5bw2muvsXDhQtzd3WncuDG//vorZcqUKYgYC5XFYiE4OJjy5cuTmppqdjgi+c7Z2Vm/7EvBUJIu2fj999956623+Pvvvzl16hSLFy+mV69eV9xnzZo1jBkzhl27dhESEsKLL76YrzNY6FovBUnXWZEr8AuxJenRx6BSc7OjKbKuaQq2Hj160KNHDwBiYmKYN28eTz/9NH///TdWqzVfAzSLo6Oj/sCKiORWYhRcOGxbD9LI7nJJfHw8jRs35oEHHqBPnz5X3f7w4cP06NGDhx9+mK+//prVq1czfPhwgoOD6dq1a77Gpmu9iEgh8w2BY3/ZknTJ0TXPk/77778zY8YMvvvuOypUqECfPn348MMP8zM2EREpLiJ22B59K4NH8W9VJfmnW7dudOvWLdfbT58+napVq/LOO+8AULduXdatW8d77713xSQ9OTmZ5ORk+/PiPpitiEiJlDF4nEZ4v6I8je4eERHB66+/Ts2aNbnnnnvw8fEhOTmZJUuW8Prrr9OyZcuCilNERIoyzY8u+WTjxo106dIlU1nXrl3ZuHHjFfebPHkyvr6+9iUkJKQgwxQRkWuRMQ2b5kq/olwn6T179qR27dps376dKVOmcPLkSaZOnVqQsYmISHERkTGyexNTw5DiLyIigsDAwExlgYGBxMTEkJiYmON+Y8eOJTo62r4cO6ZaGhGRIsc3I0kPNzeOIi7Xzd1//vlnHn/8cR555BFq1qxZkDGJiEhxo0HjxGSurq64urqaHYaIiFyJr2rScyPXNenr1q0jNjaW5s2b07p1a6ZNm8bZs2cLMjYRESkOUhPh7H7belBDc2ORYi8oKIjIyMhMZZGRkfj4+ODu7m5SVCIiki8y+qQnXoDkOHNjKcJynaS3adOGzz77jFOnTvHQQw8xf/58KlSoQHp6OitXriQ2NrYg4xQRkaLq9G4w0sGjLHhrXmC5Pm3btmX16tWZylauXEnbtm1NikhERPKNmw+4+drWVZueozwNHAfg6enJAw88wLp169ixYwdPPfUUr7/+OuXLl+eOO+4oiBhFRKQoi9hpewxqABaLubFIkRMXF0dYWBhhYWGAbYq1sLAwwsNt/RHHjh3LoEGD7Ns//PDD/PvvvzzzzDPs3buXjz76iG+++YYnn3zSjPBFRCS/+Va2PWoathzlOUm/XO3atXnzzTc5fvw48+bNy6+YRESkOIm8mKQHNjA3DimStmzZQtOmTWnatCkAY8aMoWnTpowbNw6AU6dO2RN2gKpVq7Js2TJWrlxJ48aNeeedd/j888/zfY50ERExiX0aNg0el5Nrnif9co6OjvTq1YtevXrlx+FERKQ4sdekqz+6ZNW5c2cMw8jx9dmzZ2e7zz///FOAUYmIiGk0DdtVXVdNuoiIlHKGAZG7bOuqSRcREZGrsY/wrubuOVGSLiIi1y4qHJKjwcEZytYyOxoREREp6jKau6smPUdK0kVE5Npl9EcvVwecXMyNRURERIo+v4sDx0WpJj0nStJFROTaXT6yu4iIiMjVZNSkx54Ea6q5sRRRStJFROTaRe6wPao/uoiIiOSGZ3lwdAEjHWJPmR1NkaQkXURErl1GTXpgfXPjEBERkeLBweGyadjU5D07StJFROTaJMfChcO2dU2/JiIiIrmlweOuSEm6iIhcm8jdtkevIPAsa24sIiIiUnz4Xhw8Ljrc3DiKKCXpIiJybTL6o2vQOBEREckLNXe/IiXpIiJybez90ZWki4iISB74hdge1dw9W0rSRUTk2kTusj2qP7qIiIjkhW9Gkq6a9OwoSRcRkbxLT7+UpKsmXURERPLi8oHjDMPcWIogJekiIpJ3Fw5Dajw4ukJADbOjERERkeIkI0lPTYCE8+bGUgQpSRcRkbyLvNgfvXxdcHQyNxYREREpXpxcwSvQtq4m71kUiST9ww8/JDQ0FDc3N1q3bs2mTZtytd/8+fOxWCz06tWrYAMUEZHMMgaN08juIiIici3ULz1HpifpCxYsYMyYMYwfP56tW7fSuHFjunbtyunTp6+435EjR3j66afp0KFDIUUqIiJ2GTXpgRo0TkRERK6BpmHLkelJ+rvvvsuIESMYOnQo9erVY/r06Xh4eDBz5swc97FarQwYMICJEydSrVq1QoxWREQA1aSLiIjI9cmYhu38v+bGUQSZmqSnpKTw999/06VLF3uZg4MDXbp0YePGjTnuN2nSJMqXL8+wYcOueo7k5GRiYmIyLSIich0SoyA63LYeWN/UUERERKSYypgdZvPn8Me7GuX9MqYm6WfPnsVqtRIYGJipPDAwkIiIiGz3WbduHTNmzOCzzz7L1TkmT56Mr6+vfQkJCbnuuEVESrWMqdd8Q8Dd39xYREREpHhqcBc0GwQYsHoifDsYkmPNjqpIML25e17ExsYycOBAPvvsM8qWLZurfcaOHUt0dLR9OXZMfR5ERK6LvT+6mrqLiIjINXJ0hjumwu1TwMEZdn8Pn3eBc4fMjsx0ps6bU7ZsWRwdHYmMjMxUHhkZSVBQUJbtDx06xJEjR+jZs6e9LD09HQAnJyf27dtH9erVM+3j6uqKq6trAUQvIlJKReywPao/uoiIiFyvFkNt3ecWDIQze+HTG6HPp1D7NrMjM42pNekuLi40b96c1atX28vS09NZvXo1bdu2zbJ9nTp12LFjB2FhYfbljjvu4MYbbyQsLExN2UVECoNq0kVERCQ/hbSCh9ZCSBtIjoZ5fWHN63CxQra0MbUmHWDMmDEMHjyYFi1a0KpVK6ZMmUJ8fDxDhw4FYNCgQVSsWJHJkyfj5uZGgwaZbwr9/PwAspSLiEgBsKbB6T229SBNvyYiIiL5xDsIBv8AK56HzZ/BmslwMgz6fAJuvmZHV6hMT9L79u3LmTNnGDduHBERETRp0oTly5fbB5MLDw/HwaFYdZ0XESm5zh+CtCRw9gT/qmZHIyIiIiWJkwv0eBsqNIUfn4T9P8NnN0Hfr6F8HbOjKzQWwyhdY93HxMTg6+tLdHQ0Pj4+ZocjIlK87FgI3w2DSi1h+CqzoykxdG3Kf/pMRUSKuRNbbf3UY46Dixf0+hjq3WF2VNcsL9clVVGLiEjuqT+6iIiIFIaKzeDBNRDaAVLi4JuBsGoipFvNjqzAKUkXEZHci7iYpGtkdxERESloXuVg4BJoM9L2fN278PU9kHDe1LAKmpJ0ERHJPXtNugaNExERkULg6AS3vQZ9Pgcndzi0Gj678VLFQQmkJF1ERHIn/hzEnrKtB9YzNxYREREpXRrdA8N+Ab/KcOEIzLjFNlZOCaQkXUREcidyh+3Rvyq4epsbi4iIiJQ+wY3gwbVQ/SZITbANZrviBdsUsSWIknQREckd9UcXERERs3mUgQEL4YYnbc83ToM5vSH+rLlx5SMl6SIikjvqjy4iIiJFgYMjdJkA93wBzp5w+Hf4tDOc/MfsyPKFknQREckd1aSLiIhIUVK/F4xYDWWqQ/QxmNEV/ngXok+YHdl1UZIuIiJXl5YCZ/ba1jVHuoiIiBQV5evCiF+hZlewJsPqifBePVvC/tcnEBthdoR5piRdRESu7ux+SE8FV1/bqKoiIiIiRYW7H/SfDz3fh8ptbWXH/oSfn4F36sDs22HzDIg7Y2qYueVkdgAiIlIM2Puj1weLxdxYRERERP7LwQGaD7EtMSdh1xLYtQiOb4Yjf9iWn/4HVTtA/T5Qt6dtELoiSEm6iIhcXcTF6dfUH11ERESKOp8K0PZR2xIVDrsW25aT/8C/a2zLsjFQ7Uao3xvq9LDVxhcRStJFROTq7DXpStJFRESkGPGrDO2fsC3n/7Ul6zsXQ+QOOLjStvzoAtVvhgZ9oNZt4OZjashK0kVE5MoMQyO7i4iISPFXphp0eMq2nD1wMWFfBGf2wP6fbYujK9S85VLC7uJZ6GEqSRcRkSuLi4SEs2BxgPL1zI5GRERE5PqVrQmdnrEtp/fYkvVdi+DcQdj7o21x9oBaXeGmlyCgeqGFptHdRUTkyjJq0QNqgLO7ubGIiIiI5LfydeGmF2DUFnh4HdwwBvxDITXBNgCds0ehhqOadBERubLIi4PGqT+6iIiIlGQWCwQ1tC03j7MNNHfib/AJLtQwlKSLiMiVqT+6iIiIlDYWC1RsZlsKmZq7i4jIldlHdm9obhwiIiIipYCSdBERyVlqkm30U4DA+ubGIiIiIlIKKEkXEZGcndkDhhXc/cGngtnRiIiIiJR4StJFRCRnGf3RAxvY+maJiIiISIFSki4iIjmL3GV7DFJ/dBEREZHCoCRdRERyFnlZTbqIiIiIFDgl6SIikj3DgIiLc6Rr+jURERGRQqEkXUREshdzApKiwMEJytUxOxoRERGRUkFJuoiIZC9j0LiytcDJ1dxYREREREoJJekiIpK9yItN3dUfXURERKTQOJkdgIiIFFEZNenqjy4iIiJFiGEYRCWkciIqkZNRiZyKTuJkVCKnY5NpX6MsdzWriKUYTx2rJF1ERLKnkd1FRETEBIkpVk5GX0zAo5I4EZXIqehETkYl2cuTUtOz3XfxPyfYcTyKcT3r4+hQPBN1JekiIpJVSjycO2Rb1xzpIiIiks+s6Qa/7T3NwTNxnIpK5ERU0sVEPJELCam5OkZZL1cq+rkR7OtOBT930g2D2RuO8MXGo5yMTuKDfk1xd3Es4HeS/5Ski4hIVqf3AAZ4lgev8mZHIyIiIiVIcpqVJ+aFsXxXRI7beLk6UeGyBLyCr5vt0c+dCn5uBPm64eqUNQFvVbUMoxeEsXJ3JP0++5MZg1tQ1qt4DYCrJF1ERLLS/OgiIiJSABJTrDw0529+338GFycHujcIsiffFf3cCfazJeM+bs7XdPzuDYMp7+3KiC+3sO1YFL0/Ws/soa2oXs4rn99JwVGSLiIiWak/uoiIiOSz2KRUhs3ewqYj53F3duTzwS1oX6Nsvp+nRWgZvnukHUNmbSb8fAJ3fbyBzwa1oGVomXw/V0HQFGwiIpKVfWR39UcXERGR63chPoUBn//FpiPn8XZzYs7wVgWSoGeoVs6LRY+2o0mIH1EJqQz4/C9+3H6ywM6Xn5Ski4hIZunpELnLtq6adBEREblOp2OS6PvpRrYfj6aMpwvzRrSheZWCr9Uu6+XKvBFtuLVeIClp6Yya+w+frD2EYRgFfu7roSRdREQyizoKKbHg6AJla5odjYiIiBRjxy8kcO8nG9kfGUegjysLHmxDg4q+hXZ+dxdHPr6/OUPahQIw+ee9jPt+F2nW7KdwKwqUpIuISGYZ/dHL1QHHaxu0RUREROTfM3HcO30jR84lUMnfnW8fakfNQO9Cj8PRwcKEO+rz0u31sFjgqz+P8tBXf5OQklboseSGknQREclM/dFFRETkOu05FcO9n/zJyegkqpfzZOHD7agc4GFqTMNuqMpH9zXD1cmB1XtP0+/TPzkdm2RqTNlRki4iIplpZHcRERG5DmHHouj36Z+cjUumXrAPCx5qS5Cvm9lhAdCtYTBzR7TB38OZ7cej6fPRBg6ejjM7rEyUpIuISGaaI11ERESu0Z//nmPAZ38SnZhKs8p+zHuwDWW9XM0OK5PmVfxZ9Gh7QgM8OH4hkbs+3sBf/54zOyw7JekiInJJUoxt4DhQTbrkqw8//JDQ0FDc3Nxo3bo1mzZtynHb2bNnY7FYMi1ubkWjBkZERHL2277TDJ65ifgUK+2qB/DVsNb4uhfN8W2qlvXku0fa0bSyH9GJqQycsYml24rGFG1K0kVE5JKMqdd8KoJHwU+NIqXDggULGDNmDOPHj2fr1q00btyYrl27cvr06Rz38fHx4dSpU/bl6NGjhRixiIjk1c87TvHgl1tITkunS93yzBzSEk9XJ7PDuqKAi1O03VY/iBRrOo/P+4eP15g/RZuSdBERuUT90aUAvPvuu4wYMYKhQ4dSr149pk+fjoeHBzNnzsxxH4vFQlBQkH0JDAwsxIhFRCQvFv59nJFzt5JqNbi9UTAf398cN2dHs8PKFTdnRz4c0IwH2lcF4I3le3lxyU5Tp2hTki4iIpeoP7rks5SUFP7++2+6dOliL3NwcKBLly5s3Lgxx/3i4uKoUqUKISEh3HnnnezateuK50lOTiYmJibTIiIiBe+rjUd4+tttpBvQt0UI7/drirNj8UozHR0sjOtZj3EXp2j7+q9wHvzqb+KTzZmirXh9eiIiUrBUky757OzZs1it1iw14YGBgURERGS7T+3atZk5cybff/89c+bMIT09nXbt2nH8+PEczzN58mR8fX3tS0hISL6+DxERyerjNYd46Xvbj6hD24cyuU9DHB0sJkd17R64oSofD2iOq5MDv+49Td9PN5oyRZuSdBERsUm3QuRu27rmSBcTtW3blkGDBtGkSRM6derEokWLKFeuHJ988kmO+4wdO5bo6Gj7cuzYsUKMWESkdDEMg7dW7OWN5XsBeOymGoy7vR4OxThBz3BbgyDmPdiGMp4u7DwRQ+8PN3DwdGyhxlC0e/KLiEjhOf8vpCWCkzuUqWZ2NFJClC1bFkdHRyIjIzOVR0ZGEhQUlKtjODs707RpUw4ePJjjNq6urri6Fq0pfkRESqL0dINJP+5m9oYjADzXrQ4Pd6publD5rFllfxY90o6hszcTm5SKq1Ph9q9XTbqIiNhk9EcPrAcOxWOwFyn6XFxcaN68OatXr7aXpaens3r1atq2bZurY1itVnbs2EFwcHBBhSkiIrlgTTd49rvt9gT95V4NSlyCniH04hRtXw1rTUgZj0I9t2rSRUTERv3RpYCMGTOGwYMH06JFC1q1asWUKVOIj49n6NChAAwaNIiKFSsyefJkACZNmkSbNm2oUaMGUVFRvPXWWxw9epThw4eb+TZEREq1lLR0nlwQxrIdp3CwwNv3NKZPs0pmh1Wgyni6UMbTpdDPqyRdRERsIi4m6eqPLvmsb9++nDlzhnHjxhEREUGTJk1Yvny5fTC58PBwHBwuNe67cOECI0aMICIiAn9/f5o3b86GDRuoV6+eWW9BRKRU2xcRyzPfbWfbsSicHS1M7d+U2xqodVNBsRhmz9ReyGJiYvD19SU6OhofHx+zwxERKTrerQcxJ2DocqiSu2bIkj90bcp/+kxFRK5fSlo6H605yIe/HSTVauDt6sTU+5rSuXZ5s0MrdvJyXVJNuoiIQMJ5W4IOEFjf3FhERETEdNuORfHMwu3si7SNbN6lbnle6dWQIF83kyMr+ZSki4jIpf7oflXATbWOIiIipVViipX3Vu3n8z/+Jd2AAE8XJtxRn9sbBWOxFP8p1ooDJekiIqL+6CIiIsLGQ+cYu2g7R84lANCrSQXG9axvyuBppZmSdBER0cjuIiIipVhMUiqv/7yXuX+FAxDs68arvRtwU51AkyMrnZSki4jIpTnSg5Ski4iIlCa/7o3k+UU7iYhJAmBA68o8160O3m7OJkdWeilJFxEp7aypcGavbV016SIiIqXC+fgUJv6wi+/DTgIQGuDB63c1ok21AJMjEyXpIiKl3dkDYE0BFy/bwHEiIiJSYhmGwQ/bTzFh6S7Ox6fgYIHhHarxZJdauLs4mh2eoCRdRETs/dHrg4ODubGIiIhIgYmITuLFJTtZtScSgDpB3rxxVyMah/iZG5hkoiRdRKS006BxIiIiJZphGMzffIzXlu0hNjkNZ0cLo26sySOdq+PipB/oixol6SIipZ19+jUl6SIiIiXN0XPxPPfdDjb+ew6AJiF+vHl3I2oFepscmeRESbqISGlnr0nXHOkiIiIlhTXdYNb6w7z9yz6SUtNxc3bg6VtrM7R9VRwdLGaHJ1egJF1EpDSLOwNxkYAFAuuZHY2IiIjkg50nonlh8Q62HY8GoF31AF7v04jKAR4mRya5oSRdRKQ0i7w4P3qZauDiaW4sIiIicl3iktN495f9zN5wmHQDvF2deKFHXfq2DMFiUe15caEkXUSkNFN/dBERkWLPMAxW7IpgwtLdRMQkAdCzcQVe6lGX8j5uJkcneaUkXUSkNFN/dBERkWLt2PkExi/dxa97TwNQJcCDSXc2oFOtciZHJteqSIy3/+GHHxIaGoqbmxutW7dm06ZNOW772Wef0aFDB/z9/fH396dLly5X3F5ERK5ANekiIiLFUqo1nY/XHOKW99by697TODtaeOymGqwY3VEJejFnepK+YMECxowZw/jx49m6dSuNGzema9eunD59Otvt16xZQ//+/fntt9/YuHEjISEh3HrrrZw4caKQIxcRKebSkuHsPtu65kgXEREpNjYfOU+PD/7gjeV7SUpNp021Mvz8REeeurU2bs6OZocn18liGIZhZgCtW7emZcuWTJs2DYD09HRCQkJ47LHHeO655666v9Vqxd/fn2nTpjFo0KCrbh8TE4Ovry/R0dH4+Phcd/wiIsXWqe3wSQdw84Vnj4IGlDGNrk35T5+piJREF+JTeP3nvSzYcgyAMp4uvNC9Ln2aVdTAcEVcXq5LpvZJT0lJ4e+//2bs2LH2MgcHB7p06cLGjRtzdYyEhARSU1MpU6ZMtq8nJyeTnJxsfx4TE3N9QYuIlBSX90fXhV1ERKTIMgyD77ae4LWf9nA+PgWAfi1DePa2Ovh7upgcneQ3U5P0s2fPYrVaCQwMzFQeGBjI3r17c3WMZ599lgoVKtClS5dsX588eTITJ0687lhFREoc9UcXEREp8g6ejuWFxTv56/B5AGoHevNq7wa0CM2+klKKv2I9uvvrr7/O/PnzWbNmDW5u2U8tMHbsWMaMGWN/HhMTQ0hISGGFKCJSdGXMka7+6CIiIkVOUqqVab8e5JPfD5FqNXBzdmB0l1oMu6Eqzo6mDy0mBcjUJL1s2bI4OjoSGRmZqTwyMpKgoKAr7vv222/z+uuvs2rVKho1apTjdq6urri6uuZLvCIiJYZhqCZdRESkiFq7/wwvLdlJ+PkEAG6qU56Jd9QnpIyHyZFJYTD1JxgXFxeaN2/O6tWr7WXp6emsXr2atm3b5rjfm2++ycsvv8zy5ctp0aJFYYQqIlKyxJ6CxPNgcYRydc2ORkRERIDTMUmMmruVwTM3EX4+gSAfN6bf34wZg1soQS9FTG/uPmbMGAYPHkyLFi1o1aoVU6ZMIT4+nqFDhwIwaNAgKlasyOTJkwF44403GDduHHPnziU0NJSIiAgAvLy88PLyMu19iIgUKxm16GVrgnP23YVERESkcMQnpzF/8zGmrNxPbHIaDhYY0q4qY26thZer6SmbFDLT/8X79u3LmTNnGDduHBERETRp0oTly5fbB5MLDw/HweFShf/HH39MSkoKd999d6bjjB8/ngkTJhRm6CIixZf6o4uIiJju8Nl4vtx4hIVbjhObnAZA40q+vNq7IQ0q+pocnZjF9CQdYNSoUYwaNSrb19asWZPp+ZEjRwo+IBGRkk790UVEREyRnm6w9sAZvthwhDX7ztjLq5b1ZESHavRtGYKjg6ZGLc2KRJIuIiKF7PI50kVERKTAxSSl8u2W43y18QhHztkGhLNY4Mba5RnUtgoda5bDQcm5oCRdRKT0SU2Ecwdt66pJFxERKVD7I2P5YsMRFv9zgoQUKwDebk7c2yKEgW2qEFrW0+QIpahRki4iUtr8MweMdPCuAF6BZkcjIiJS4qRZ01m15zRfbjzChkPn7OW1Ar0Y3C6U3k0r4uGiVEyyp2+GiEhpknAefn3Ftt5hjK2dnYiIiOSLC/EpzN98jDl/HuVEVCIADha4tV4Qg9pVoW21ACy69spVKEkXESlN1kyGpCgoXx+aDzU7GhERkRJh54lovtx4hO/DTpKclg6Av4cz/VpV5v42/2/v7qOiOO89gH9ngX1hgQXk/UXANzSKJDFKMElNlCsaT6OtrS/Xk2Bra2PVJsfm1rSNwdyeXtOmTXuaeE1yri/pSauJPdXkxkSvUjWJYjRiBBND1CCKyJvALm/LLrvP/WPZlRUWBGFndvl+zpkzszPPjL9nn93z88fMzqQgMVwnc4TkS1ikExENF9VfAqe2OpbnvggEMAUQERENlNVmx/5zVXjz+GV8Vt7gWj8pMQx52an4dmYCtEEBMkZIvor/QyMiGg6EAPavB4QNmPAYkPYtuSMiIiLyOU1mK45drMOR0locOl+DuuZ2AECgSsKjGfHIm56Ce0dG8JJ2uiMs0omIhoOv3gfKPgICNMDs38gdDRERkU8QQuBiTTMOl9bg8Fe1OHW5Hh124doeHarBv08biWVZIxETppUxUvInLNKJiPyd1Qwc+LVjefpaICJV1nCIiIiUrNXSgeMXb+BwaQ2OlNa6bgDnNCpKj4fTY/BwejTuHzUC6kCVTJGSv2KRTkTk7wpfBRrLHY9ce2id3NEQEREpTlldCw5/VYPDpTX49Jt6WGx21zZNoAr3jxqBR9Kj8XB6DJ9rTkOORToRkT8zVQIfv+xY/rcXADX/Y0FERGS22nDimxs4UlqLI6U1uHyj1W17UoQOM8fH4JH0GNw/agR0at4AjryHRToRkT879AJgbQGSs4CM78sdDRERkWyu1rfiSGkNDpfW4vilOpitN8+WBwVImJYWiUfSY/BwegxGR+t58zeSDYt0IiJ/dfUUULwLgATMeRHgfzaIiGgYabPYcKLsBo6W1uKjC7X4prbFbXu8QYuHOy9hf2BMFEI0LI1IGfhJJCLyR3Y78OEvHMv3LAMS75U3HiIioiEmhMDX1c346OtaHP26Ficv18PScfNseYBKwpSUCDySHoNHxkcjPTaUZ8tJkVikExH5o7M7gcoiQB0KzHxe7miIiIiGhLHVik8u1uHo1zX46Os6VJnMbtsTw3X41rhozBgXhezRUTDogmSKlOj2sUgnIvI3ZhNwaKNjecZ/AKGxsoZDREQ0WGx2gbMVja6z5WevNqLLY8tdd2J3FObR/G05+SQW6URE/ubjPwAtNUDkaCBrldzREBER3ZEqo9lRlF+oxScX6mBss7ptHxsTghnjovGtcdGYlhYJbRDvxE6+jUU6EZE/uXEJKPxvx3LufwGBannjISIi6ieT2YrT5Q0ovOS46VtpdZPb9jBtIB4cG4UZ46Lx0NhoJITrZIqUaGiwSCci8if/9xxgtwJjcoBxuXJHQ0RE1KdqkxmnLtfjVFk9Tl5uwFdVJogul7BLEpCZFO76bXlmUjgCA1TyBUw0xFikExH5i4sFQOkHgCoQyN3ER64REZHiCCHwTV0LTpXV49TlBpy6XI8r9a3d2qWOCMbU1Eh8a1w0HhwThQg9rwyj4YNFOhGRP7BZgf2/dCxP+wkQPU7eeIiIiAB02Oz48roJJ8vqcepyPT673IAbLRa3NioJmBAfhqmpkZiWFon7UiIQE6aVKWIi+bFIJyLyB6f+B6grBYJHADN+IXc0REQ0TLVZbDhzpcF1lrzoSgNaLTa3NppAFe5ODsfU1EhMTYvEvSPDEarlo9GInFikExH5upY64PAmx/LMDYAuXNZwiIhoeBBC4PKNVhRXNKK4wojT5Q04d82Ijq7PRIPjRm/OgnxqagQmJRqgCeQd2Ik8YZFOROTrDv8WaDcCcRnAvU/IHQ0REfkhIQQqjWYUX21E8TUjiisaUVJhhMnc0a1tvEHrKsqnpUZibEwIVCreJ4XodrFIJyLyZVUlwOkdjuW5vwdUPDNBRER3rrap3XWGvLiiESXXjKhrtnRrpwlU4a6EMGQmhSMz2YCpqZFIigiWIWIi/8EinYjIVwkBfPgsIOzAxO8CKdPljoiIiHxQY6sFJdeMroK8uMKI60Zzt3aBKgnpcaGYnBSOyUkGTE4yYFxsKIL4ODSiQcUinYjIV325Fyj/BAjUAf/2n3JHQ0RECme3C1Q0tOFibRMuVDfjXKUJxRWNKL/R/RFokgSMiQ5xK8gnxIdBG8QrtoiGGot0IiJfZG0D/m+DY/nBp4HwZFnDISIi5bB02FF+owUXa5pxoaYZFzunb+qaYbbae9wnZUSwoyBPdBTkExMNCNGwVCCSA795RES+6NhfAONVICwJmP4zuaMhIiIZtFo6cKmmBRdrm1yF+IWaZly50drtDutO6gAVRkXrMTomBHfFh2FykgEZiQaEB6u9HD0RecIinYjI1xgrgE/+5Fie/Z+AmjfoISLyVza7QG1TO642tOJSl0L8Yk0zrjW2edxPrw7AmJgQjIkJ7Zw7puQIHQL5G3IiRWORTkTkaw7mAx1twMjpjhvGEdGQMVtt+LSsHhmJBkTqeaaRBl+rpQOVjWZca2xDZWMbrjV0zjunKqPZ41lxABihV2O0swiPDsHYWMdyXJgWksTHnhH5Ihbpd+LI74CiN+WOgogGQ4AaCEsEDImd8yTH5FynDXfcRUdu5YXAuX8AkIC5LyojJiI/du6aEXnbTgIAkiJ0nZcGhyOz8ze7Bl2QzBGSkgkhUNdscRXdlY1tqOgswiuNjoK8odXa53ECVRLiDFqkRekx9pYz4/zjEZH/YZF+J9pNgOma3FEQ0WBpKPO8LUjfpYBPdPwW3JDUZTkRUOuHNj67DfjwF47lKXlAfObQ/ntEhKb2DqRF6VFW14KKBkeB9UFJlWt7WpQeGZ032spI5M22/JXdLtBs6YCx1QqT2QpjmxWmNitMbR0wtnW+7lzv3FbfYkGl0QxLR883ausqVBOIxAgdEsJ1SAjXIjE8GAnhWiR1rosJ1SJAxT/KEg0XkhDC8/UzfshkMsFgMMBoNCIsLOzODma8BrTUDk5gRCQvaytgqnT83ttY4fgDnHPeeuP2jqEN73L2PQmISAUi04CINMeyJuTOYjz9JvC/PwM0BuBnRYA+6s6OR4oxqLmJAAz+e2pss+KLa0YUXzOipMKI4muNuFrf/ffAkgSMjg7B5EQDMjofW3VXvAE6NR9bpQRCCJjMHWhoseBGiwUNLRbUdy43tllgchXZ7sW3qc2KXq4475UkAbGhWrciPCncuaxDYoQOYVpekUHk7/qTl/in3jth6DyjRkT+zdp2s4B3Fu+u5WuOebsJMDc6pupzPR9HH3OzaL91ro/q/dJ1sxEo6HwW+sPrWaATeZlBF4TpY6IwfczN715Di6WzaG9EcYURJdeMuG40u+6y/c8zjqvtAlQSxsaEOM62dz7iKi1ajxB1IFQ8O3pHrDY7GlodhXZ9swX1zuUuxXdDl9cNrRZYbQM/P6UOVMGgC3JNYdrAm8td5mHaIEQEByEhXIc4gxZBvFEbEfUDz6QTEQ0Gs+lm0W686ijiG8qA+jLHvK2h9/3VIZ1Fe2r3Ij4sCTiUDxS+CkSNA1YdBwJ41sWfMDcNPrne05omM85dMzqK9gojzlYYUdfc3mNbSQJC1IEI1QYiVBvUOe+67JiH9bDOuRyiCfTaZdBWmx1tVhvMFhvMVsdym9WGNosN5i7LbVbH65vrHG07bHbYhIDN3sPUZX2HXcDunAuBDlvnvMt6m12gyWyFydwxoL7o1QGI0KsxQq9GhF6NSL0aEcHqLgV3l+Jbe7P41gbxiggiGhieSSci8jZtmGOKmdDz9rZG96K9vgxouOyYm64BlmagusQx3UoVCIjO3zTmbmKBTqRgMaFazByvxczxsQAcl1dXm9pR3Hm23XnmvaHVCiEcv3lvau8AjOYB/5shmkDo1AGQ4Cj8JUidc7ju7i1JPW+TAOCW186LepyFuLmz8O7tDuNykiQgIthRaEfq1YgMViMypHOuV2NEiNp9u17NYpuIFI1FOhGRN+jCAd09QMI93bdZzUDjlR6K+M5C3mZxtEufB4zN8WbURHSHJMlxV+44QxxmT4wD4Cjc2zvsaDJ3oMls7ZzfXDb1sK6p3X2dydzhuiFZc3sHmtsHdkZ5YH0CdEEB0AUFQBsUAJ064OZrdQB0QSrHa3Xn9s52gQESAlUSVJJjHqCSEKBSIUAF97nk3NbZXtVlvwDHPEAlIUQTiEi94+w3b6pGRP6ERToRkdyCtED0OMd0K7sdaKoETNeBuEnej42IBp0kSdB2Fq7RoZoBH6e9w+Yq3NssNggIOH/EKAQgIGAXjj8KiM516GzjfN11m4Bjg/N8ubPA7lqEa4JU0ASq+PxtIqIhxCKdiEjJVKqbz2wnIupCExgATUgAokIGXugTEZHy8FaTRERERERERArBIp2IiIiG3ObNm5GamgqtVousrCycPHmy1/a7d+/G+PHjodVqkZGRgQ8++MBLkRIREcmLRToRERENqbfffhvr1q1Dfn4+ioqKkJmZidzcXNTU1PTY/vjx41i6dClWrFiBM2fOYMGCBViwYAHOnTvn5ciJiIi8j89JJyIikpm/56asrCxMnToVr776KgDAbrcjOTkZa9euxbPPPtut/eLFi9HS0oL333/fte7+++/H3Xffjddee+22/k1/f0+JiMi39Ccv8Uw6ERERDRmLxYLTp08jJ+fm4wNVKhVycnJQWFjY4z6FhYVu7QEgNzfXY3sAaG9vh8lkcpuIiIh8EYt0IiIiGjJ1dXWw2WyIjY11Wx8bG4uqqqoe96mqqupXewDYtGkTDAaDa0pOTr7z4ImIiGTAIp2IiIh83i9/+UsYjUbXdPXqVblDIiIiGhA+J52IiIiGTFRUFAICAlBdXe22vrq6GnFxcT3uExcX16/2AKDRaKDR8HnhRETk+3gmnYiIiIaMWq3GlClTUFBQ4Fpnt9tRUFCA7OzsHvfJzs52aw8ABw8e9NieiIjIn/BMOhEREQ2pdevWIS8vD/fddx+mTZuGP//5z2hpacEPfvADAMATTzyBxMREbNq0CQDw1FNPYcaMGfjjH/+IefPmYdeuXfjss8/wxhtvyNkNIiIir2CRTkRERENq8eLFqK2txfPPP4+qqircfffd2L9/v+vmcFeuXIFKdfPivunTp+Pvf/87nnvuOfzqV7/C2LFjsXfvXkyaNEmuLhAREXkNn5NOREQkM+amwcf3lIiIlITPSSciIiIiIiLyQSzSiYiIiIiIiBRi2P0m3Xl1v8lkkjkSIiIiB2dOGma/QBtSzPdERKQk/cn1w65Ib2pqAgAkJyfLHAkREZG7pqYmGAwGucPwC8z3RESkRLeT64fdjePsdjsqKysRGhoKSZLu6FgmkwnJycm4evWqz9+Uhn1RHn/pB+A/ffGXfgD+0xd/6YcQAk1NTUhISHC7yzkNHPN9d/7SD8B/+uIv/QDYFyXyl34A/tGX/uT6YXcmXaVSISkpaVCPGRYW5rMflluxL8rjL/0A/Kcv/tIPwH/64g/94Bn0wcV875m/9APwn774Sz8A9kWJ/KUfgO/35XZzPf9cT0RERERERKQQLNKJiIiIiIiIFIJF+h3QaDTIz8+HRqORO5Q7xr4oj7/0A/CfvvhLPwD/6Yu/9IOUzV8+Z/7SD8B/+uIv/QDYFyXyl34A/tWX2zHsbhxHREREREREpFQ8k05ERERERESkECzSiYiIiIiIiBSCRToRERERERGRQrBIJyIiIiIiIlIIFul92Lx5M1JTU6HVapGVlYWTJ0/22n737t0YP348tFotMjIy8MEHH3gpUs82bdqEqVOnIjQ0FDExMViwYAFKS0t73WfHjh2QJMlt0mq1XorYs40bN3aLa/z48b3uo8QxSU1N7dYPSZKwevXqHtsraTw++ugjfPvb30ZCQgIkScLevXvdtgsh8PzzzyM+Ph46nQ45OTm4cOFCn8ft73dtMPTWF6vVivXr1yMjIwN6vR4JCQl44oknUFlZ2esxB/IZHcp+AMDy5cu7xTRnzpw+j6u0MQHQ4/dGkiS89NJLHo8px5iQ7/H1fM9cr6zxcPLVfM9cz1w/lJjr+8YivRdvv/021q1bh/z8fBQVFSEzMxO5ubmoqanpsf3x48exdOlSrFixAmfOnMGCBQuwYMECnDt3zsuRuzt69ChWr16NEydO4ODBg7BarZg9ezZaWlp63S8sLAzXr193TeXl5V6KuHcTJ050i+uTTz7x2FapY3Lq1Cm3Phw8eBAA8P3vf9/jPkoZj5aWFmRmZmLz5s09bv/973+Pv/zlL3jttdfw6aefQq/XIzc3F2az2eMx+/tdGyy99aW1tRVFRUXYsGEDioqK8M9//hOlpaV47LHH+jxufz6jg6GvMQGAOXPmuMW0c+fOXo+pxDEB4NaH69evY9u2bZAkCQsXLuz1uN4eE/It/pDvmeuVNR5OvprvmeuZ64cSc/1tEOTRtGnTxOrVq12vbTabSEhIEJs2beqx/aJFi8S8efPc1mVlZYmf/OQnQxpnf9XU1AgA4ujRox7bbN++XRgMBu8FdZvy8/NFZmbmbbf3lTF56qmnxOjRo4Xdbu9xu1LHA4DYs2eP67XdbhdxcXHipZdecq1rbGwUGo1G7Ny50+Nx+vtdGwq39qUnJ0+eFABEeXm5xzb9/YwOtp76kZeXJ+bPn9+v4/jKmMyfP1/MnDmz1zZyjwkpnz/me+Z6ZY2Hky/me+b67uTOK8z13ck9JoONZ9I9sFgsOH36NHJyclzrVCoVcnJyUFhY2OM+hYWFbu0BIDc312N7uRiNRgBAZGRkr+2am5uRkpKC5ORkzJ8/H1988YU3wuvThQsXkJCQgFGjRmHZsmW4cuWKx7a+MCYWiwVvvfUWfvjDH0KSJI/tlDoeXZWVlaGqqsrtPTcYDMjKyvL4ng/kuyYXo9EISZIQHh7ea7v+fEa95ciRI4iJiUF6ejpWrVqFGzdueGzrK2NSXV2Nffv2YcWKFX22VeKYkDL4a75nrlfWeAD+k++Z6x2UmFeY65U3JgPFIt2Duro62Gw2xMbGuq2PjY1FVVVVj/tUVVX1q70c7HY7nn76aTzwwAOYNGmSx3bp6enYtm0b3n33Xbz11luw2+2YPn06KioqvBhtd1lZWdixYwf279+PLVu2oKysDA899BCampp6bO8LY7J37140NjZi+fLlHtsodTxu5Xxf+/OeD+S7Jgez2Yz169dj6dKlCAsL89iuv59Rb5gzZw7++te/oqCgAL/73e9w9OhRzJ07Fzabrcf2vjImb775JkJDQ/Hd736313ZKHBNSDn/M98z1yhoPJ3/J98z1yswrzPXKG5M7ESh3AORdq1evxrlz5/r8jUZ2djays7Ndr6dPn44JEybg9ddfx29+85uhDtOjuXPnupYnT56MrKwspKSk4J133rmtv7Ap0datWzF37lwkJCR4bKPU8RgurFYrFi1aBCEEtmzZ0mtbJX5GlyxZ4lrOyMjA5MmTMXr0aBw5cgSzZs2SJabBsG3bNixbtqzPmyopcUyIhhJzvTIx3ysbc70yDddczzPpHkRFRSEgIADV1dVu66urqxEXF9fjPnFxcf1q721r1qzB+++/j8OHDyMpKalf+wYFBeGee+7BxYsXhyi6gQkPD8e4ceM8xqX0MSkvL8ehQ4fwox/9qF/7KXU8nO9rf97zgXzXvMmZtMvLy3Hw4MFe/7Lek74+o3IYNWoUoqKiPMak9DEBgI8//hilpaX9/u4AyhwTko+/5XvmegeljIeTP+V75vrulJhXmOuVNyb9wSLdA7VajSlTpqCgoMC1zm63o6CgwO0vnF1lZ2e7tQeAgwcPemzvLUIIrFmzBnv27MG//vUvpKWl9fsYNpsNJSUliI+PH4IIB665uRmXLl3yGJdSx8Rp+/btiImJwbx58/q1n1LHIy0tDXFxcW7vuclkwqeffurxPR/Id81bnEn7woULOHToEEaMGNHvY/T1GZVDRUUFbty44TEmJY+J09atWzFlyhRkZmb2e18ljgnJx1/yPXO9ssbjVv6U75nru1NiXmGuV96Y9Iu8961Ttl27dgmNRiN27NghvvzyS7Fy5UoRHh4uqqqqhBBCPP744+LZZ591tT927JgIDAwUf/jDH8T58+dFfn6+CAoKEiUlJXJ1QQghxKpVq4TBYBBHjhwR169fd02tra2uNrf25YUXXhAHDhwQly5dEqdPnxZLliwRWq1WfPHFF3J0weXnP/+5OHLkiCgrKxPHjh0TOTk5IioqStTU1AghfGdMhHDcQXPkyJFi/fr13bYpeTyamprEmTNnxJkzZwQA8fLLL4szZ8647oL64osvivDwcPHuu++K4uJiMX/+fJGWliba2tpcx5g5c6Z45ZVXXK/7+q7J0ReLxSIee+wxkZSUJD7//HO37057e7vHvvT1GfV2P5qamsQzzzwjCgsLRVlZmTh06JC49957xdixY4XZbPbYDyWOiZPRaBTBwcFiy5YtPR5DCWNCvsUf8j1zvbLGoytfzPfM9cz1Q4m5vm8s0vvwyiuviJEjRwq1Wi2mTZsmTpw44do2Y8YMkZeX59b+nXfeEePGjRNqtVpMnDhR7Nu3z8sRdwegx2n79u2uNrf25emnn3b1OzY2Vjz66KOiqKjI+8HfYvHixSI+Pl6o1WqRmJgoFi9eLC5evOja7itjIoQQBw4cEABEaWlpt21KHo/Dhw/3+Hlyxmu328WGDRtEbGys0Gg0YtasWd36mJKSIvLz893W9fZdk6MvZWVlHr87hw8f9tiXvj6j3u5Ha2urmD17toiOjhZBQUEiJSVF/PjHP+6WgH1hTJxef/11odPpRGNjY4/HUMKYkO/x9XzPXK+s8ejKF/M9cz1zvVx9cRruuV4SQoiBnoUnIiIiIiIiosHD36QTERERERERKQSLdCIiIiIiIiKFYJFOREREREREpBAs0omIiIiIiIgUgkU6ERERERERkUKwSCciIiIiIiJSCBbpRERERERERArBIp2IiIiIiIhIIVikE5HXSZKEvXv3yh0GERERDRHmeqKBY5FONMwsX74ckiR1m+bMmSN3aERERDQImOuJfFug3AEQkffNmTMH27dvd1un0WhkioaIiIgGG3M9ke/imXSiYUij0SAuLs5tioiIAOC4PG3Lli2YO3cudDodRo0ahX/84x9u+5eUlGDmzJnQ6XQYMWIEVq5ciebmZrc227Ztw8SJE6HRaBAfH481a9a4ba+rq8N3vvMdBAcHY+zYsXjvvfeGttNERETDCHM9ke9ikU5E3WzYsAELFy7E2bNnsWzZMixZsgTnz58HALS0tCA3NxcRERE4deoUdu/ejUOHDrkl5i1btmD16tVYuXIlSkpK8N5772HMmDFu/8YLL7yARYsWobi4GI8++iiWLVuG+vp6r/aTiIhouGKuJ1IwQUTDSl5enggICBB6vd5t+u1vfyuEEAKAePLJJ932ycrKEqtWrRJCCPHGG2+IiIgI0dzc7Nq+b98+oVKpRFVVlRBCiISEBPHrX//aYwwAxHPPPed63dzcLACIDz/8cND6SURENFwx1xP5Nv4mnWgYeuSRR7Blyxa3dZGRka7l7Oxst23Z2dn4/PPPAQDnz59HZmYm9Hq9a/sDDzwAu92O0tJSSJKEyspKzJo1q9cYJk+e7FrW6/UICwtDTU3NQLtEREREXTDXE/kuFulEw5Ber+92Sdpg0el0t9UuKCjI7bUkSbDb7UMREhER0bDDXE/ku/ibdCLq5sSJE91eT5gwAQAwYcIEnD17Fi0tLa7tx44dg0qlQnp6OkJDQ5GamoqCggKvxkxERES3j7meSLl4Jp1oGGpvb0dVVZXbusDAQERFRQEAdu/ejfvuuw8PPvgg/va3v+HkyZPYunUrAGDZsmXIz89HXl4eNm7ciNraWqxduxaPP/44YmNjAQAbN27Ek08+iZiYGMydOxdNTU04duwY1q5d692OEhERDVPM9US+i0U60TC0f/9+xMfHu61LT0/HV199BcBxN9Zdu3bhpz/9KeLj47Fz507cddddAIDg4GAcOHAATz31FKZOnYrg4GAsXLgQL7/8sutYeXl5MJvN+NOf/oRnnnkGUVFR+N73vue9DhIREQ1zzPVEvksSQgi5gyAi5ZAkCXv27MGCBQvkDoWIiIiGAHM9kbLxN+lERERERERECsEinYiIiIiIiEgheLk7ERERERERkULwTDoRERERERGRQrBIJyIiIiIiIlIIFulERERERERECsEinYiIiIiIiEghWKQTERERERERKQSLdCIiIiIiIiKFYJFOREREREREpBAs0omIiIiIiIgU4v8BvaODVxWMlw8AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 학습 및 검증 정확도 시각화\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# 정확도 그래프\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_exp21.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history_exp21.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# 손실 그래프\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_exp21.history['loss'], label='Training Loss')\n",
    "plt.plot(history_exp21.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NZajLvMZefqO"
   },
   "source": [
    "## 2-2. (32, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "EiCGNbU7eQCm"
   },
   "outputs": [],
   "source": [
    "dataset_size = x_train.shape[0]\n",
    "batch_size = 30\n",
    "epochs = 20\n",
    "\n",
    "total_iteration= int(epochs * (dataset_size / batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "z_Ywjw2xfCHR"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout, InputLayer\n",
    "import tensorflow as tf\n",
    "\n",
    "# 새로운 레이어를 추가하기 위한 레이어 리스트 초기화\n",
    "new_layers = []\n",
    "\n",
    "# Dense 레이어 카운터\n",
    "dense_layer_count = 0\n",
    "\n",
    "# VGG 모델의 각 레이어 순회\n",
    "for layer in best_vgg16.layers:\n",
    "    if isinstance(layer, InputLayer):\n",
    "        continue\n",
    "    if isinstance(layer, Conv2D):\n",
    "        # Conv2D 레이어를 ConvLoRALayer로 대체\n",
    "        conv_lora_layer = ConvLoRALayer00_cdn2(original_conv_layer=layer, rank=32, alpha=32, total_iteration = total_iteration, trainable=True)\n",
    "        new_layers.append(conv_lora_layer)\n",
    "    elif isinstance(layer, Dense):\n",
    "        dense_layer_count += 1\n",
    "        # 첫 번째와 두 번째 Dense 레이어에만 LoraLayer 적용\n",
    "        if dense_layer_count in [1, 2]:\n",
    "            dense_lora_layer = LoraLayer(original_layer=layer, rank=16, alpha=32, total_iteration=total_iteration, trainable=True)\n",
    "            new_layers.append(dense_lora_layer)\n",
    "        else:\n",
    "            new_layers.append(layer)  # 그대로 유지함\n",
    "    else:\n",
    "        # 다른 유형의 레이어는 그대로 유지\n",
    "        new_layers.append(layer)\n",
    "\n",
    "# 새로운 입력 텐서 생성\n",
    "input_tensor = best_vgg16.input\n",
    "\n",
    "# 새로운 모델 구성\n",
    "x = input_tensor\n",
    "for layer in new_layers:\n",
    "    x = layer(x)\n",
    "\n",
    "# 새로운 모델 생성\n",
    "exp22_lora_vgg16 = Model(inputs=input_tensor, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "7FBFKA9pfDgm"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D\n",
    "\n",
    "# 새로 생성된 모델에서 Dense 레이어 중 LoRA가 적용되지 않은 레이어의 trainable을 False로 설정\n",
    "for layer in exp22_lora_vgg16.layers:\n",
    "    if isinstance(layer, Dense) and not isinstance(layer, LoraLayer):\n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "05V-KQUSfEoE",
    "outputId": "a1c2c52a-a08c-48b3-b0f0-3f497d2e8cb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " block1_conv1 (ConvLoRALaye  (None, 32, 32, 64)        21090     \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block1_conv2 (ConvLoRALaye  (None, 32, 32, 64)        73794     \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " block2_conv1 (ConvLoRALaye  (None, 16, 16, 128)       129154    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block2_conv2 (ConvLoRALaye  (None, 16, 16, 128)       221314    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " block3_conv1 (ConvLoRALaye  (None, 8, 8, 256)         405762    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block3_conv2 (ConvLoRALaye  (None, 8, 8, 256)         737538    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block3_conv3 (ConvLoRALaye  (None, 8, 8, 256)         737538    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 4, 4, 256)         0         \n",
      "                                                                 \n",
      " block4_conv1 (ConvLoRALaye  (None, 4, 4, 512)         1401346   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block4_conv2 (ConvLoRALaye  (None, 4, 4, 512)         2654722   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block4_conv3 (ConvLoRALaye  (None, 4, 4, 512)         2654722   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      " block5_conv1 (ConvLoRALaye  (None, 2, 2, 512)         2654722   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block5_conv2 (ConvLoRALaye  (None, 2, 2, 512)         2654722   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block5_conv3 (ConvLoRALaye  (None, 2, 2, 512)         2654722   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 1, 1, 512)         0         \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_6 (LoraLayer)         (None, 4096)              2174978   \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_7 (LoraLayer)         (None, 512)               2171394   \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21352648 (81.45 MB)\n",
      "Trainable params: 2433888 (9.28 MB)\n",
      "Non-trainable params: 18918760 (72.17 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "exp22_lora_vgg16.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mtt2vlBufF-G",
    "outputId": "a8e87d2b-fe6e-43ac-bc90-6e1d0281c3c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: input_3\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block1_conv1\n",
      "  Trainable parameters: 19296\n",
      "  Non-trainable parameters: 1794\n",
      "Layer: block1_conv2\n",
      "  Trainable parameters: 36864\n",
      "  Non-trainable parameters: 36930\n",
      "Layer: block1_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block2_conv1\n",
      "  Trainable parameters: 55296\n",
      "  Non-trainable parameters: 73858\n",
      "Layer: block2_conv2\n",
      "  Trainable parameters: 73728\n",
      "  Non-trainable parameters: 147586\n",
      "Layer: block2_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block3_conv1\n",
      "  Trainable parameters: 110592\n",
      "  Non-trainable parameters: 295170\n",
      "Layer: block3_conv2\n",
      "  Trainable parameters: 147456\n",
      "  Non-trainable parameters: 590082\n",
      "Layer: block3_conv3\n",
      "  Trainable parameters: 147456\n",
      "  Non-trainable parameters: 590082\n",
      "Layer: block3_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block4_conv1\n",
      "  Trainable parameters: 221184\n",
      "  Non-trainable parameters: 1180162\n",
      "Layer: block4_conv2\n",
      "  Trainable parameters: 294912\n",
      "  Non-trainable parameters: 2359810\n",
      "Layer: block4_conv3\n",
      "  Trainable parameters: 294912\n",
      "  Non-trainable parameters: 2359810\n",
      "Layer: block4_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block5_conv1\n",
      "  Trainable parameters: 294912\n",
      "  Non-trainable parameters: 2359810\n",
      "Layer: block5_conv2\n",
      "  Trainable parameters: 294912\n",
      "  Non-trainable parameters: 2359810\n",
      "Layer: block5_conv3\n",
      "  Trainable parameters: 294912\n",
      "  Non-trainable parameters: 2359810\n",
      "Layer: block5_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: flatten_2\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: dense_6\n",
      "  Trainable parameters: 73728\n",
      "  Non-trainable parameters: 2101250\n",
      "Layer: dropout_4\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: dense_7\n",
      "  Trainable parameters: 73728\n",
      "  Non-trainable parameters: 2097666\n",
      "Layer: dropout_5\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: dense_8\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 5130\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#모델의 각 레이어를 순회하며 파라미터 수를 계산\n",
    "for layer in exp22_lora_vgg16.layers:\n",
    "    trainable_count = np.sum([tf.size(w).numpy() for w in layer.trainable_weights])\n",
    "    non_trainable_count = np.sum([tf.size(w).numpy() for w in layer.non_trainable_weights])\n",
    "\n",
    "    print(f\"Layer: {layer.name}\")\n",
    "    print(f\"  Trainable parameters: {trainable_count}\")\n",
    "    print(f\"  Non-trainable parameters: {non_trainable_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "p7JVgT5zfG8x"
   },
   "outputs": [],
   "source": [
    "lora_layers = []  # lora 레이어들을 저장할 리스트\n",
    "\n",
    "for layer in exp22_lora_vgg16.layers:\n",
    "    if isinstance(layer, LoraLayer) or isinstance(layer, ConvLoRALayer00_cdn2):\n",
    "        lora_layers.append(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "ywJfZm9PfIlc"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "class PrintCurrentStepCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, lora_layers):\n",
    "        super().__init__()\n",
    "        self.lora_layers = lora_layers\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print()\n",
    "        for i, lora_layer in enumerate(self.lora_layers):\n",
    "            current_step = lora_layer.current_step.value()\n",
    "            decay_factor = lora_layer.decay_factor.value()\n",
    "            print(f\"End of epoch {epoch + 1}, LoraLayer {i}: {current_step} Step\")\n",
    "            print(f\"End of epoch {epoch + 1}, LoraLayer {i}: Decay factor: {decay_factor}\")\n",
    "\n",
    "class TestCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, test_data):\n",
    "        self.test_data = test_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        x, y = self.test_data\n",
    "        loss, acc = self.model.evaluate(x, y, verbose=0)\n",
    "        print('\\nTesting loss: {}, acc: {}\\n'.format(loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "yc4NMzPJfJf6"
   },
   "outputs": [],
   "source": [
    "# 콜백 생성 시 lora_layers 딕셔너리의 값만 사용\n",
    "print_step_callback = PrintCurrentStepCallback(lora_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "3uc9kMrGfKXi"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# 모델 컴파일\n",
    "exp22_lora_vgg16.compile(optimizer=Adam(learning_rate=0.00005),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vo9O94YY--zH"
   },
   "source": [
    "### 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kSIOk0uqfMog",
    "outputId": "cd37895e-6326-49ba-f63c-b00cbb229672"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.0803 - accuracy: 0.9740\n",
      "End of epoch 1, LoraLayer 0: 1667 Step\n",
      "End of epoch 1, LoraLayer 0: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 1: 1667 Step\n",
      "End of epoch 1, LoraLayer 1: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 2: 1667 Step\n",
      "End of epoch 1, LoraLayer 2: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 3: 1667 Step\n",
      "End of epoch 1, LoraLayer 3: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 4: 1667 Step\n",
      "End of epoch 1, LoraLayer 4: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 5: 1667 Step\n",
      "End of epoch 1, LoraLayer 5: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 6: 1667 Step\n",
      "End of epoch 1, LoraLayer 6: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 7: 1667 Step\n",
      "End of epoch 1, LoraLayer 7: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 8: 1667 Step\n",
      "End of epoch 1, LoraLayer 8: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 9: 1667 Step\n",
      "End of epoch 1, LoraLayer 9: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 10: 1667 Step\n",
      "End of epoch 1, LoraLayer 10: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 11: 1667 Step\n",
      "End of epoch 1, LoraLayer 11: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 12: 1667 Step\n",
      "End of epoch 1, LoraLayer 12: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 13: 1667 Step\n",
      "End of epoch 1, LoraLayer 13: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 14: 1667 Step\n",
      "End of epoch 1, LoraLayer 14: Decay factor: 1.0\n",
      "\n",
      "Testing loss: 2.302607297897339, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 58s 30ms/step - loss: 0.0804 - accuracy: 0.9740 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 2/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.0608 - accuracy: 0.9807\n",
      "End of epoch 2, LoraLayer 0: 3334 Step\n",
      "End of epoch 2, LoraLayer 0: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 1: 3334 Step\n",
      "End of epoch 2, LoraLayer 1: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 2: 3334 Step\n",
      "End of epoch 2, LoraLayer 2: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 3: 3334 Step\n",
      "End of epoch 2, LoraLayer 3: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 4: 3334 Step\n",
      "End of epoch 2, LoraLayer 4: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 5: 3334 Step\n",
      "End of epoch 2, LoraLayer 5: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 6: 3334 Step\n",
      "End of epoch 2, LoraLayer 6: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 7: 3334 Step\n",
      "End of epoch 2, LoraLayer 7: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 8: 3334 Step\n",
      "End of epoch 2, LoraLayer 8: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 9: 3334 Step\n",
      "End of epoch 2, LoraLayer 9: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 10: 3334 Step\n",
      "End of epoch 2, LoraLayer 10: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 11: 3334 Step\n",
      "End of epoch 2, LoraLayer 11: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 12: 3334 Step\n",
      "End of epoch 2, LoraLayer 12: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 13: 3334 Step\n",
      "End of epoch 2, LoraLayer 13: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 14: 3334 Step\n",
      "End of epoch 2, LoraLayer 14: Decay factor: 0.9999625086784363\n",
      "\n",
      "Testing loss: 2.302607297897339, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 50s 30ms/step - loss: 0.0607 - accuracy: 0.9807 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 3/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.0478 - accuracy: 0.9852\n",
      "End of epoch 3, LoraLayer 0: 5001 Step\n",
      "End of epoch 3, LoraLayer 0: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 1: 5001 Step\n",
      "End of epoch 3, LoraLayer 1: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 2: 5001 Step\n",
      "End of epoch 3, LoraLayer 2: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 3: 5001 Step\n",
      "End of epoch 3, LoraLayer 3: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 4: 5001 Step\n",
      "End of epoch 3, LoraLayer 4: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 5: 5001 Step\n",
      "End of epoch 3, LoraLayer 5: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 6: 5001 Step\n",
      "End of epoch 3, LoraLayer 6: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 7: 5001 Step\n",
      "End of epoch 3, LoraLayer 7: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 8: 5001 Step\n",
      "End of epoch 3, LoraLayer 8: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 9: 5001 Step\n",
      "End of epoch 3, LoraLayer 9: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 10: 5001 Step\n",
      "End of epoch 3, LoraLayer 10: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 11: 5001 Step\n",
      "End of epoch 3, LoraLayer 11: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 12: 5001 Step\n",
      "End of epoch 3, LoraLayer 12: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 13: 5001 Step\n",
      "End of epoch 3, LoraLayer 13: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 14: 5001 Step\n",
      "End of epoch 3, LoraLayer 14: Decay factor: 0.9374484419822693\n",
      "\n",
      "Testing loss: 2.302607297897339, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 49s 29ms/step - loss: 0.0478 - accuracy: 0.9852 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 4/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.0487 - accuracy: 0.9841\n",
      "End of epoch 4, LoraLayer 0: 6668 Step\n",
      "End of epoch 4, LoraLayer 0: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 1: 6668 Step\n",
      "End of epoch 4, LoraLayer 1: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 2: 6668 Step\n",
      "End of epoch 4, LoraLayer 2: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 3: 6668 Step\n",
      "End of epoch 4, LoraLayer 3: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 4: 6668 Step\n",
      "End of epoch 4, LoraLayer 4: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 5: 6668 Step\n",
      "End of epoch 4, LoraLayer 5: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 6: 6668 Step\n",
      "End of epoch 4, LoraLayer 6: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 7: 6668 Step\n",
      "End of epoch 4, LoraLayer 7: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 8: 6668 Step\n",
      "End of epoch 4, LoraLayer 8: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 9: 6668 Step\n",
      "End of epoch 4, LoraLayer 9: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 10: 6668 Step\n",
      "End of epoch 4, LoraLayer 10: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 11: 6668 Step\n",
      "End of epoch 4, LoraLayer 11: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 12: 6668 Step\n",
      "End of epoch 4, LoraLayer 12: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 13: 6668 Step\n",
      "End of epoch 4, LoraLayer 13: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 14: 6668 Step\n",
      "End of epoch 4, LoraLayer 14: Decay factor: 0.8749343752861023\n",
      "\n",
      "Testing loss: 2.302607297897339, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 48s 29ms/step - loss: 0.0487 - accuracy: 0.9841 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 5/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.0544 - accuracy: 0.9822\n",
      "End of epoch 5, LoraLayer 0: 8335 Step\n",
      "End of epoch 5, LoraLayer 0: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 1: 8335 Step\n",
      "End of epoch 5, LoraLayer 1: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 2: 8335 Step\n",
      "End of epoch 5, LoraLayer 2: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 3: 8335 Step\n",
      "End of epoch 5, LoraLayer 3: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 4: 8335 Step\n",
      "End of epoch 5, LoraLayer 4: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 5: 8335 Step\n",
      "End of epoch 5, LoraLayer 5: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 6: 8335 Step\n",
      "End of epoch 5, LoraLayer 6: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 7: 8335 Step\n",
      "End of epoch 5, LoraLayer 7: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 8: 8335 Step\n",
      "End of epoch 5, LoraLayer 8: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 9: 8335 Step\n",
      "End of epoch 5, LoraLayer 9: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 10: 8335 Step\n",
      "End of epoch 5, LoraLayer 10: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 11: 8335 Step\n",
      "End of epoch 5, LoraLayer 11: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 12: 8335 Step\n",
      "End of epoch 5, LoraLayer 12: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 13: 8335 Step\n",
      "End of epoch 5, LoraLayer 13: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 14: 8335 Step\n",
      "End of epoch 5, LoraLayer 14: Decay factor: 0.8124203085899353\n",
      "\n",
      "Testing loss: 2.302607297897339, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 48s 29ms/step - loss: 0.0544 - accuracy: 0.9822 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 6/20\n",
      "1665/1667 [============================>.] - ETA: 0s - loss: 0.0592 - accuracy: 0.9805\n",
      "End of epoch 6, LoraLayer 0: 10002 Step\n",
      "End of epoch 6, LoraLayer 0: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 1: 10002 Step\n",
      "End of epoch 6, LoraLayer 1: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 2: 10002 Step\n",
      "End of epoch 6, LoraLayer 2: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 3: 10002 Step\n",
      "End of epoch 6, LoraLayer 3: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 4: 10002 Step\n",
      "End of epoch 6, LoraLayer 4: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 5: 10002 Step\n",
      "End of epoch 6, LoraLayer 5: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 6: 10002 Step\n",
      "End of epoch 6, LoraLayer 6: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 7: 10002 Step\n",
      "End of epoch 6, LoraLayer 7: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 8: 10002 Step\n",
      "End of epoch 6, LoraLayer 8: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 9: 10002 Step\n",
      "End of epoch 6, LoraLayer 9: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 10: 10002 Step\n",
      "End of epoch 6, LoraLayer 10: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 11: 10002 Step\n",
      "End of epoch 6, LoraLayer 11: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 12: 10002 Step\n",
      "End of epoch 6, LoraLayer 12: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 13: 10002 Step\n",
      "End of epoch 6, LoraLayer 13: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 14: 10002 Step\n",
      "End of epoch 6, LoraLayer 14: Decay factor: 0.7499062418937683\n",
      "\n",
      "Testing loss: 2.302607297897339, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 50s 30ms/step - loss: 0.0592 - accuracy: 0.9805 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 7/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.0658 - accuracy: 0.9784\n",
      "End of epoch 7, LoraLayer 0: 11669 Step\n",
      "End of epoch 7, LoraLayer 0: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 1: 11669 Step\n",
      "End of epoch 7, LoraLayer 1: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 2: 11669 Step\n",
      "End of epoch 7, LoraLayer 2: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 3: 11669 Step\n",
      "End of epoch 7, LoraLayer 3: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 4: 11669 Step\n",
      "End of epoch 7, LoraLayer 4: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 5: 11669 Step\n",
      "End of epoch 7, LoraLayer 5: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 6: 11669 Step\n",
      "End of epoch 7, LoraLayer 6: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 7: 11669 Step\n",
      "End of epoch 7, LoraLayer 7: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 8: 11669 Step\n",
      "End of epoch 7, LoraLayer 8: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 9: 11669 Step\n",
      "End of epoch 7, LoraLayer 9: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 10: 11669 Step\n",
      "End of epoch 7, LoraLayer 10: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 11: 11669 Step\n",
      "End of epoch 7, LoraLayer 11: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 12: 11669 Step\n",
      "End of epoch 7, LoraLayer 12: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 13: 11669 Step\n",
      "End of epoch 7, LoraLayer 13: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 14: 11669 Step\n",
      "End of epoch 7, LoraLayer 14: Decay factor: 0.6873922348022461\n",
      "\n",
      "Testing loss: 2.302607297897339, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 50s 30ms/step - loss: 0.0658 - accuracy: 0.9784 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 8/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.0813 - accuracy: 0.9727\n",
      "End of epoch 8, LoraLayer 0: 13336 Step\n",
      "End of epoch 8, LoraLayer 0: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 1: 13336 Step\n",
      "End of epoch 8, LoraLayer 1: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 2: 13336 Step\n",
      "End of epoch 8, LoraLayer 2: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 3: 13336 Step\n",
      "End of epoch 8, LoraLayer 3: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 4: 13336 Step\n",
      "End of epoch 8, LoraLayer 4: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 5: 13336 Step\n",
      "End of epoch 8, LoraLayer 5: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 6: 13336 Step\n",
      "End of epoch 8, LoraLayer 6: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 7: 13336 Step\n",
      "End of epoch 8, LoraLayer 7: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 8: 13336 Step\n",
      "End of epoch 8, LoraLayer 8: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 9: 13336 Step\n",
      "End of epoch 8, LoraLayer 9: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 10: 13336 Step\n",
      "End of epoch 8, LoraLayer 10: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 11: 13336 Step\n",
      "End of epoch 8, LoraLayer 11: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 12: 13336 Step\n",
      "End of epoch 8, LoraLayer 12: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 13: 13336 Step\n",
      "End of epoch 8, LoraLayer 13: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 14: 13336 Step\n",
      "End of epoch 8, LoraLayer 14: Decay factor: 0.6248781681060791\n",
      "\n",
      "Testing loss: 2.302607297897339, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 49s 30ms/step - loss: 0.0813 - accuracy: 0.9727 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 9/20\n",
      "1665/1667 [============================>.] - ETA: 0s - loss: 0.0969 - accuracy: 0.9668\n",
      "End of epoch 9, LoraLayer 0: 15003 Step\n",
      "End of epoch 9, LoraLayer 0: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 1: 15003 Step\n",
      "End of epoch 9, LoraLayer 1: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 2: 15003 Step\n",
      "End of epoch 9, LoraLayer 2: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 3: 15003 Step\n",
      "End of epoch 9, LoraLayer 3: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 4: 15003 Step\n",
      "End of epoch 9, LoraLayer 4: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 5: 15003 Step\n",
      "End of epoch 9, LoraLayer 5: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 6: 15003 Step\n",
      "End of epoch 9, LoraLayer 6: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 7: 15003 Step\n",
      "End of epoch 9, LoraLayer 7: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 8: 15003 Step\n",
      "End of epoch 9, LoraLayer 8: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 9: 15003 Step\n",
      "End of epoch 9, LoraLayer 9: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 10: 15003 Step\n",
      "End of epoch 9, LoraLayer 10: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 11: 15003 Step\n",
      "End of epoch 9, LoraLayer 11: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 12: 15003 Step\n",
      "End of epoch 9, LoraLayer 12: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 13: 15003 Step\n",
      "End of epoch 9, LoraLayer 13: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 14: 15003 Step\n",
      "End of epoch 9, LoraLayer 14: Decay factor: 0.5623641014099121\n",
      "\n",
      "Testing loss: 2.302607297897339, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 49s 29ms/step - loss: 0.0969 - accuracy: 0.9668 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 10/20\n",
      "1665/1667 [============================>.] - ETA: 0s - loss: 0.1154 - accuracy: 0.9608\n",
      "End of epoch 10, LoraLayer 0: 16670 Step\n",
      "End of epoch 10, LoraLayer 0: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 1: 16670 Step\n",
      "End of epoch 10, LoraLayer 1: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 2: 16670 Step\n",
      "End of epoch 10, LoraLayer 2: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 3: 16670 Step\n",
      "End of epoch 10, LoraLayer 3: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 4: 16670 Step\n",
      "End of epoch 10, LoraLayer 4: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 5: 16670 Step\n",
      "End of epoch 10, LoraLayer 5: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 6: 16670 Step\n",
      "End of epoch 10, LoraLayer 6: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 7: 16670 Step\n",
      "End of epoch 10, LoraLayer 7: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 8: 16670 Step\n",
      "End of epoch 10, LoraLayer 8: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 9: 16670 Step\n",
      "End of epoch 10, LoraLayer 9: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 10: 16670 Step\n",
      "End of epoch 10, LoraLayer 10: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 11: 16670 Step\n",
      "End of epoch 10, LoraLayer 11: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 12: 16670 Step\n",
      "End of epoch 10, LoraLayer 12: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 13: 16670 Step\n",
      "End of epoch 10, LoraLayer 13: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 14: 16670 Step\n",
      "End of epoch 10, LoraLayer 14: Decay factor: 0.4998500347137451\n",
      "\n",
      "Testing loss: 2.3026070594787598, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 49s 30ms/step - loss: 0.1155 - accuracy: 0.9608 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 11/20\n",
      "1665/1667 [============================>.] - ETA: 0s - loss: 0.1421 - accuracy: 0.9506\n",
      "End of epoch 11, LoraLayer 0: 18337 Step\n",
      "End of epoch 11, LoraLayer 0: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 1: 18337 Step\n",
      "End of epoch 11, LoraLayer 1: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 2: 18337 Step\n",
      "End of epoch 11, LoraLayer 2: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 3: 18337 Step\n",
      "End of epoch 11, LoraLayer 3: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 4: 18337 Step\n",
      "End of epoch 11, LoraLayer 4: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 5: 18337 Step\n",
      "End of epoch 11, LoraLayer 5: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 6: 18337 Step\n",
      "End of epoch 11, LoraLayer 6: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 7: 18337 Step\n",
      "End of epoch 11, LoraLayer 7: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 8: 18337 Step\n",
      "End of epoch 11, LoraLayer 8: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 9: 18337 Step\n",
      "End of epoch 11, LoraLayer 9: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 10: 18337 Step\n",
      "End of epoch 11, LoraLayer 10: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 11: 18337 Step\n",
      "End of epoch 11, LoraLayer 11: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 12: 18337 Step\n",
      "End of epoch 11, LoraLayer 12: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 13: 18337 Step\n",
      "End of epoch 11, LoraLayer 13: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 14: 18337 Step\n",
      "End of epoch 11, LoraLayer 14: Decay factor: 0.4373359680175781\n",
      "\n",
      "Testing loss: 2.302603244781494, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 49s 30ms/step - loss: 0.1425 - accuracy: 0.9505 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 12/20\n",
      "1665/1667 [============================>.] - ETA: 0s - loss: 0.1740 - accuracy: 0.9401\n",
      "End of epoch 12, LoraLayer 0: 20004 Step\n",
      "End of epoch 12, LoraLayer 0: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 1: 20004 Step\n",
      "End of epoch 12, LoraLayer 1: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 2: 20004 Step\n",
      "End of epoch 12, LoraLayer 2: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 3: 20004 Step\n",
      "End of epoch 12, LoraLayer 3: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 4: 20004 Step\n",
      "End of epoch 12, LoraLayer 4: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 5: 20004 Step\n",
      "End of epoch 12, LoraLayer 5: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 6: 20004 Step\n",
      "End of epoch 12, LoraLayer 6: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 7: 20004 Step\n",
      "End of epoch 12, LoraLayer 7: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 8: 20004 Step\n",
      "End of epoch 12, LoraLayer 8: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 9: 20004 Step\n",
      "End of epoch 12, LoraLayer 9: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 10: 20004 Step\n",
      "End of epoch 12, LoraLayer 10: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 11: 20004 Step\n",
      "End of epoch 12, LoraLayer 11: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 12: 20004 Step\n",
      "End of epoch 12, LoraLayer 12: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 13: 20004 Step\n",
      "End of epoch 12, LoraLayer 13: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 14: 20004 Step\n",
      "End of epoch 12, LoraLayer 14: Decay factor: 0.37482190132141113\n",
      "\n",
      "Testing loss: 2.302602529525757, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 48s 29ms/step - loss: 0.1742 - accuracy: 0.9401 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 13/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.2155 - accuracy: 0.9266\n",
      "End of epoch 13, LoraLayer 0: 21671 Step\n",
      "End of epoch 13, LoraLayer 0: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 1: 21671 Step\n",
      "End of epoch 13, LoraLayer 1: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 2: 21671 Step\n",
      "End of epoch 13, LoraLayer 2: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 3: 21671 Step\n",
      "End of epoch 13, LoraLayer 3: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 4: 21671 Step\n",
      "End of epoch 13, LoraLayer 4: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 5: 21671 Step\n",
      "End of epoch 13, LoraLayer 5: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 6: 21671 Step\n",
      "End of epoch 13, LoraLayer 6: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 7: 21671 Step\n",
      "End of epoch 13, LoraLayer 7: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 8: 21671 Step\n",
      "End of epoch 13, LoraLayer 8: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 9: 21671 Step\n",
      "End of epoch 13, LoraLayer 9: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 10: 21671 Step\n",
      "End of epoch 13, LoraLayer 10: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 11: 21671 Step\n",
      "End of epoch 13, LoraLayer 11: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 12: 21671 Step\n",
      "End of epoch 13, LoraLayer 12: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 13: 21671 Step\n",
      "End of epoch 13, LoraLayer 13: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 14: 21671 Step\n",
      "End of epoch 13, LoraLayer 14: Decay factor: 0.31230783462524414\n",
      "\n",
      "Testing loss: 2.302597761154175, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 46s 28ms/step - loss: 0.2155 - accuracy: 0.9266 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 14/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.2563 - accuracy: 0.9111\n",
      "End of epoch 14, LoraLayer 0: 23338 Step\n",
      "End of epoch 14, LoraLayer 0: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 1: 23338 Step\n",
      "End of epoch 14, LoraLayer 1: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 2: 23338 Step\n",
      "End of epoch 14, LoraLayer 2: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 3: 23338 Step\n",
      "End of epoch 14, LoraLayer 3: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 4: 23338 Step\n",
      "End of epoch 14, LoraLayer 4: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 5: 23338 Step\n",
      "End of epoch 14, LoraLayer 5: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 6: 23338 Step\n",
      "End of epoch 14, LoraLayer 6: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 7: 23338 Step\n",
      "End of epoch 14, LoraLayer 7: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 8: 23338 Step\n",
      "End of epoch 14, LoraLayer 8: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 9: 23338 Step\n",
      "End of epoch 14, LoraLayer 9: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 10: 23338 Step\n",
      "End of epoch 14, LoraLayer 10: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 11: 23338 Step\n",
      "End of epoch 14, LoraLayer 11: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 12: 23338 Step\n",
      "End of epoch 14, LoraLayer 12: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 13: 23338 Step\n",
      "End of epoch 14, LoraLayer 13: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 14: 23338 Step\n",
      "End of epoch 14, LoraLayer 14: Decay factor: 0.24979376792907715\n",
      "\n",
      "Testing loss: 2.302576780319214, acc: 0.11129999905824661\n",
      "\n",
      "1667/1667 [==============================] - 47s 28ms/step - loss: 0.2563 - accuracy: 0.9111 - val_loss: 2.3026 - val_accuracy: 0.1114\n",
      "Epoch 15/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.3086 - accuracy: 0.8924\n",
      "End of epoch 15, LoraLayer 0: 25005 Step\n",
      "End of epoch 15, LoraLayer 0: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 1: 25005 Step\n",
      "End of epoch 15, LoraLayer 1: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 2: 25005 Step\n",
      "End of epoch 15, LoraLayer 2: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 3: 25005 Step\n",
      "End of epoch 15, LoraLayer 3: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 4: 25005 Step\n",
      "End of epoch 15, LoraLayer 4: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 5: 25005 Step\n",
      "End of epoch 15, LoraLayer 5: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 6: 25005 Step\n",
      "End of epoch 15, LoraLayer 6: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 7: 25005 Step\n",
      "End of epoch 15, LoraLayer 7: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 8: 25005 Step\n",
      "End of epoch 15, LoraLayer 8: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 9: 25005 Step\n",
      "End of epoch 15, LoraLayer 9: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 10: 25005 Step\n",
      "End of epoch 15, LoraLayer 10: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 11: 25005 Step\n",
      "End of epoch 15, LoraLayer 11: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 12: 25005 Step\n",
      "End of epoch 15, LoraLayer 12: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 13: 25005 Step\n",
      "End of epoch 15, LoraLayer 13: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 14: 25005 Step\n",
      "End of epoch 15, LoraLayer 14: Decay factor: 0.18727970123291016\n",
      "\n",
      "Testing loss: 2.3014438152313232, acc: 0.11670000106096268\n",
      "\n",
      "1667/1667 [==============================] - 45s 27ms/step - loss: 0.3087 - accuracy: 0.8924 - val_loss: 2.3014 - val_accuracy: 0.1167\n",
      "Epoch 16/20\n",
      "1665/1667 [============================>.] - ETA: 0s - loss: 0.3819 - accuracy: 0.8672\n",
      "End of epoch 16, LoraLayer 0: 26672 Step\n",
      "End of epoch 16, LoraLayer 0: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 1: 26672 Step\n",
      "End of epoch 16, LoraLayer 1: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 2: 26672 Step\n",
      "End of epoch 16, LoraLayer 2: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 3: 26672 Step\n",
      "End of epoch 16, LoraLayer 3: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 4: 26672 Step\n",
      "End of epoch 16, LoraLayer 4: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 5: 26672 Step\n",
      "End of epoch 16, LoraLayer 5: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 6: 26672 Step\n",
      "End of epoch 16, LoraLayer 6: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 7: 26672 Step\n",
      "End of epoch 16, LoraLayer 7: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 8: 26672 Step\n",
      "End of epoch 16, LoraLayer 8: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 9: 26672 Step\n",
      "End of epoch 16, LoraLayer 9: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 10: 26672 Step\n",
      "End of epoch 16, LoraLayer 10: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 11: 26672 Step\n",
      "End of epoch 16, LoraLayer 11: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 12: 26672 Step\n",
      "End of epoch 16, LoraLayer 12: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 13: 26672 Step\n",
      "End of epoch 16, LoraLayer 13: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 14: 26672 Step\n",
      "End of epoch 16, LoraLayer 14: Decay factor: 0.12476563453674316\n",
      "\n",
      "Testing loss: 2.2879252433776855, acc: 0.18140000104904175\n",
      "\n",
      "1667/1667 [==============================] - 46s 28ms/step - loss: 0.3821 - accuracy: 0.8672 - val_loss: 2.2879 - val_accuracy: 0.1812\n",
      "Epoch 17/20\n",
      "1665/1667 [============================>.] - ETA: 0s - loss: 0.4784 - accuracy: 0.8343\n",
      "End of epoch 17, LoraLayer 0: 28339 Step\n",
      "End of epoch 17, LoraLayer 0: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 1: 28339 Step\n",
      "End of epoch 17, LoraLayer 1: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 2: 28339 Step\n",
      "End of epoch 17, LoraLayer 2: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 3: 28339 Step\n",
      "End of epoch 17, LoraLayer 3: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 4: 28339 Step\n",
      "End of epoch 17, LoraLayer 4: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 5: 28339 Step\n",
      "End of epoch 17, LoraLayer 5: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 6: 28339 Step\n",
      "End of epoch 17, LoraLayer 6: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 7: 28339 Step\n",
      "End of epoch 17, LoraLayer 7: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 8: 28339 Step\n",
      "End of epoch 17, LoraLayer 8: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 9: 28339 Step\n",
      "End of epoch 17, LoraLayer 9: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 10: 28339 Step\n",
      "End of epoch 17, LoraLayer 10: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 11: 28339 Step\n",
      "End of epoch 17, LoraLayer 11: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 12: 28339 Step\n",
      "End of epoch 17, LoraLayer 12: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 13: 28339 Step\n",
      "End of epoch 17, LoraLayer 13: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 14: 28339 Step\n",
      "End of epoch 17, LoraLayer 14: Decay factor: 0.06225156784057617\n",
      "\n",
      "Testing loss: 2.0000150203704834, acc: 0.4909000098705292\n",
      "\n",
      "1667/1667 [==============================] - 48s 29ms/step - loss: 0.4785 - accuracy: 0.8343 - val_loss: 2.0000 - val_accuracy: 0.4907\n",
      "Epoch 18/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.5987 - accuracy: 0.7963\n",
      "End of epoch 18, LoraLayer 0: 30006 Step\n",
      "End of epoch 18, LoraLayer 0: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 1: 30006 Step\n",
      "End of epoch 18, LoraLayer 1: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 2: 30006 Step\n",
      "End of epoch 18, LoraLayer 2: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 3: 30006 Step\n",
      "End of epoch 18, LoraLayer 3: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 4: 30006 Step\n",
      "End of epoch 18, LoraLayer 4: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 5: 30006 Step\n",
      "End of epoch 18, LoraLayer 5: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 6: 30006 Step\n",
      "End of epoch 18, LoraLayer 6: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 7: 30006 Step\n",
      "End of epoch 18, LoraLayer 7: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 8: 30006 Step\n",
      "End of epoch 18, LoraLayer 8: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 9: 30006 Step\n",
      "End of epoch 18, LoraLayer 9: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 10: 30006 Step\n",
      "End of epoch 18, LoraLayer 10: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 11: 30006 Step\n",
      "End of epoch 18, LoraLayer 11: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 12: 30006 Step\n",
      "End of epoch 18, LoraLayer 12: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 13: 30006 Step\n",
      "End of epoch 18, LoraLayer 13: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 14: 30006 Step\n",
      "End of epoch 18, LoraLayer 14: Decay factor: 0.0\n",
      "\n",
      "Testing loss: 0.8019240498542786, acc: 0.7378000020980835\n",
      "\n",
      "1667/1667 [==============================] - 44s 26ms/step - loss: 0.5986 - accuracy: 0.7964 - val_loss: 0.8019 - val_accuracy: 0.7376\n",
      "Epoch 19/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.6102 - accuracy: 0.7947\n",
      "End of epoch 19, LoraLayer 0: 31673 Step\n",
      "End of epoch 19, LoraLayer 0: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 1: 31673 Step\n",
      "End of epoch 19, LoraLayer 1: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 2: 31673 Step\n",
      "End of epoch 19, LoraLayer 2: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 3: 31673 Step\n",
      "End of epoch 19, LoraLayer 3: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 4: 31673 Step\n",
      "End of epoch 19, LoraLayer 4: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 5: 31673 Step\n",
      "End of epoch 19, LoraLayer 5: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 6: 31673 Step\n",
      "End of epoch 19, LoraLayer 6: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 7: 31673 Step\n",
      "End of epoch 19, LoraLayer 7: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 8: 31673 Step\n",
      "End of epoch 19, LoraLayer 8: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 9: 31673 Step\n",
      "End of epoch 19, LoraLayer 9: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 10: 31673 Step\n",
      "End of epoch 19, LoraLayer 10: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 11: 31673 Step\n",
      "End of epoch 19, LoraLayer 11: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 12: 31673 Step\n",
      "End of epoch 19, LoraLayer 12: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 13: 31673 Step\n",
      "End of epoch 19, LoraLayer 13: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 14: 31673 Step\n",
      "End of epoch 19, LoraLayer 14: Decay factor: 0.0\n",
      "\n",
      "Testing loss: 0.7513076066970825, acc: 0.7512999773025513\n",
      "\n",
      "1667/1667 [==============================] - 46s 28ms/step - loss: 0.6102 - accuracy: 0.7947 - val_loss: 0.7513 - val_accuracy: 0.7515\n",
      "Epoch 20/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.4992 - accuracy: 0.8327\n",
      "End of epoch 20, LoraLayer 0: 33340 Step\n",
      "End of epoch 20, LoraLayer 0: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 1: 33340 Step\n",
      "End of epoch 20, LoraLayer 1: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 2: 33340 Step\n",
      "End of epoch 20, LoraLayer 2: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 3: 33340 Step\n",
      "End of epoch 20, LoraLayer 3: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 4: 33340 Step\n",
      "End of epoch 20, LoraLayer 4: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 5: 33340 Step\n",
      "End of epoch 20, LoraLayer 5: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 6: 33340 Step\n",
      "End of epoch 20, LoraLayer 6: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 7: 33340 Step\n",
      "End of epoch 20, LoraLayer 7: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 8: 33340 Step\n",
      "End of epoch 20, LoraLayer 8: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 9: 33340 Step\n",
      "End of epoch 20, LoraLayer 9: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 10: 33340 Step\n",
      "End of epoch 20, LoraLayer 10: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 11: 33340 Step\n",
      "End of epoch 20, LoraLayer 11: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 12: 33340 Step\n",
      "End of epoch 20, LoraLayer 12: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 13: 33340 Step\n",
      "End of epoch 20, LoraLayer 13: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 14: 33340 Step\n",
      "End of epoch 20, LoraLayer 14: Decay factor: 0.0\n",
      "\n",
      "Testing loss: 0.7153621315956116, acc: 0.7712000012397766\n",
      "\n",
      "1667/1667 [==============================] - 49s 29ms/step - loss: 0.4993 - accuracy: 0.8327 - val_loss: 0.7154 - val_accuracy: 0.7714\n"
     ]
    }
   ],
   "source": [
    "history_exp22 = exp22_lora_vgg16.fit(x_train, y_train, batch_size=30, epochs=20, validation_data=(x_test, y_test), callbacks=[print_step_callback, TestCallback((x_test, y_test))] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3tq5Lt1mf8vz",
    "outputId": "8e66bab1-efb5-452c-fa46-30f411411960"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 3s 10ms/step - loss: 0.7154 - accuracy: 0.7712\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7153621315956116, 0.7712000012397766]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 평가\n",
    "exp22_lora_vgg16.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "id": "NYKZmdk1fPCA",
    "outputId": "51e2bd34-3a2c-4ec9-9272-e19b5769da30"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 학습 및 검증 정확도 시각화\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# 정확도 그래프\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_exp22.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history_exp22.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# 손실 그래프\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_exp22.history['loss'], label='Training Loss')\n",
    "plt.plot(history_exp22.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n5MbMZ4hehMi"
   },
   "source": [
    "## 2-3. (32, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "8WLRVDyWej1q"
   },
   "outputs": [],
   "source": [
    "dataset_size = x_train.shape[0]\n",
    "batch_size = 30\n",
    "epochs = 20\n",
    "\n",
    "total_iteration= int(epochs * (dataset_size / batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "1fICgSqQqwIW"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout, InputLayer\n",
    "import tensorflow as tf\n",
    "\n",
    "# 새로운 레이어를 추가하기 위한 레이어 리스트 초기화\n",
    "new_layers = []\n",
    "\n",
    "# Dense 레이어 카운터\n",
    "dense_layer_count = 0\n",
    "\n",
    "# VGG 모델의 각 레이어 순회\n",
    "for layer in best_vgg16.layers:\n",
    "    if isinstance(layer, InputLayer):\n",
    "        continue\n",
    "    if isinstance(layer, Conv2D):\n",
    "        # Conv2D 레이어를 ConvLoRALayer로 대체\n",
    "        conv_lora_layer = ConvLoRALayer00_cdn2(original_conv_layer=layer, rank=32, alpha=32, total_iteration = total_iteration, trainable=True)\n",
    "        new_layers.append(conv_lora_layer)\n",
    "    elif isinstance(layer, Dense):\n",
    "        dense_layer_count += 1\n",
    "        # 첫 번째와 두 번째 Dense 레이어에만 LoraLayer 적용\n",
    "        if dense_layer_count in [1, 2]:\n",
    "            dense_lora_layer = LoraLayer(original_layer=layer, rank=64, alpha=32, total_iteration=total_iteration, trainable=True)\n",
    "            new_layers.append(dense_lora_layer)\n",
    "        else:\n",
    "            new_layers.append(layer)  # 그대로 유지함\n",
    "    else:\n",
    "        # 다른 유형의 레이어는 그대로 유지\n",
    "        new_layers.append(layer)\n",
    "\n",
    "# 새로운 입력 텐서 생성\n",
    "input_tensor = best_vgg16.input\n",
    "\n",
    "# 새로운 모델 구성\n",
    "x = input_tensor\n",
    "for layer in new_layers:\n",
    "    x = layer(x)\n",
    "\n",
    "# 새로운 모델 생성\n",
    "exp23_lora_vgg16 = Model(inputs=input_tensor, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "rDp5dkpgqx7G"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D\n",
    "\n",
    "# 새로 생성된 모델에서 Dense 레이어 중 LoRA가 적용되지 않은 레이어의 trainable을 False로 설정\n",
    "for layer in exp23_lora_vgg16.layers:\n",
    "    if isinstance(layer, Dense) and not isinstance(layer, LoraLayer):\n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DrXZDLTRq4LV",
    "outputId": "65255648-33de-4b3f-ac2a-744afa3bfe9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " block1_conv1 (ConvLoRALaye  (None, 32, 32, 64)        21090     \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block1_conv2 (ConvLoRALaye  (None, 32, 32, 64)        73794     \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " block2_conv1 (ConvLoRALaye  (None, 16, 16, 128)       129154    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block2_conv2 (ConvLoRALaye  (None, 16, 16, 128)       221314    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " block3_conv1 (ConvLoRALaye  (None, 8, 8, 256)         405762    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block3_conv2 (ConvLoRALaye  (None, 8, 8, 256)         737538    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block3_conv3 (ConvLoRALaye  (None, 8, 8, 256)         737538    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 4, 4, 256)         0         \n",
      "                                                                 \n",
      " block4_conv1 (ConvLoRALaye  (None, 4, 4, 512)         1401346   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block4_conv2 (ConvLoRALaye  (None, 4, 4, 512)         2654722   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block4_conv3 (ConvLoRALaye  (None, 4, 4, 512)         2654722   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      " block5_conv1 (ConvLoRALaye  (None, 2, 2, 512)         2654722   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block5_conv2 (ConvLoRALaye  (None, 2, 2, 512)         2654722   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block5_conv3 (ConvLoRALaye  (None, 2, 2, 512)         2654722   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 1, 1, 512)         0         \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_6 (LoraLayer)         (None, 4096)              2396162   \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_7 (LoraLayer)         (None, 512)               2392578   \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21795016 (83.14 MB)\n",
      "Trainable params: 2876256 (10.97 MB)\n",
      "Non-trainable params: 18918760 (72.17 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "exp23_lora_vgg16.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7BISzZtRq14v",
    "outputId": "33c7d8e4-b132-4cd0-8d0e-66e9495e7579"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: input_3\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block1_conv1\n",
      "  Trainable parameters: 19296\n",
      "  Non-trainable parameters: 1794\n",
      "Layer: block1_conv2\n",
      "  Trainable parameters: 36864\n",
      "  Non-trainable parameters: 36930\n",
      "Layer: block1_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block2_conv1\n",
      "  Trainable parameters: 55296\n",
      "  Non-trainable parameters: 73858\n",
      "Layer: block2_conv2\n",
      "  Trainable parameters: 73728\n",
      "  Non-trainable parameters: 147586\n",
      "Layer: block2_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block3_conv1\n",
      "  Trainable parameters: 110592\n",
      "  Non-trainable parameters: 295170\n",
      "Layer: block3_conv2\n",
      "  Trainable parameters: 147456\n",
      "  Non-trainable parameters: 590082\n",
      "Layer: block3_conv3\n",
      "  Trainable parameters: 147456\n",
      "  Non-trainable parameters: 590082\n",
      "Layer: block3_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block4_conv1\n",
      "  Trainable parameters: 221184\n",
      "  Non-trainable parameters: 1180162\n",
      "Layer: block4_conv2\n",
      "  Trainable parameters: 294912\n",
      "  Non-trainable parameters: 2359810\n",
      "Layer: block4_conv3\n",
      "  Trainable parameters: 294912\n",
      "  Non-trainable parameters: 2359810\n",
      "Layer: block4_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block5_conv1\n",
      "  Trainable parameters: 294912\n",
      "  Non-trainable parameters: 2359810\n",
      "Layer: block5_conv2\n",
      "  Trainable parameters: 294912\n",
      "  Non-trainable parameters: 2359810\n",
      "Layer: block5_conv3\n",
      "  Trainable parameters: 294912\n",
      "  Non-trainable parameters: 2359810\n",
      "Layer: block5_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: flatten_2\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: dense_6\n",
      "  Trainable parameters: 294912\n",
      "  Non-trainable parameters: 2101250\n",
      "Layer: dropout_4\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: dense_7\n",
      "  Trainable parameters: 294912\n",
      "  Non-trainable parameters: 2097666\n",
      "Layer: dropout_5\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: dense_8\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 5130\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#모델의 각 레이어를 순회하며 파라미터 수를 계산\n",
    "for layer in exp23_lora_vgg16.layers:\n",
    "    trainable_count = np.sum([tf.size(w).numpy() for w in layer.trainable_weights])\n",
    "    non_trainable_count = np.sum([tf.size(w).numpy() for w in layer.non_trainable_weights])\n",
    "\n",
    "    print(f\"Layer: {layer.name}\")\n",
    "    print(f\"  Trainable parameters: {trainable_count}\")\n",
    "    print(f\"  Non-trainable parameters: {non_trainable_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "_jH8Ng1iq8s9"
   },
   "outputs": [],
   "source": [
    "lora_layers = []  # lora 레이어들을 저장할 리스트\n",
    "\n",
    "for layer in exp23_lora_vgg16.layers:\n",
    "    if isinstance(layer, LoraLayer) or isinstance(layer, ConvLoRALayer00_cdn2):\n",
    "        lora_layers.append(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "6M9Ijmuhq_ql"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "class PrintCurrentStepCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, lora_layers):\n",
    "        super().__init__()\n",
    "        self.lora_layers = lora_layers\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print()\n",
    "        for i, lora_layer in enumerate(self.lora_layers):\n",
    "            current_step = lora_layer.current_step.value()\n",
    "            decay_factor = lora_layer.decay_factor.value()\n",
    "            print(f\"End of epoch {epoch + 1}, LoraLayer {i}: {current_step} Step\")\n",
    "            print(f\"End of epoch {epoch + 1}, LoraLayer {i}: Decay factor: {decay_factor}\")\n",
    "\n",
    "class TestCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, test_data):\n",
    "        self.test_data = test_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        x, y = self.test_data\n",
    "        loss, acc = self.model.evaluate(x, y, verbose=0)\n",
    "        print('\\nTesting loss: {}, acc: {}\\n'.format(loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "CdXjUGAUrCRa"
   },
   "outputs": [],
   "source": [
    "# 콜백 생성 시 lora_layers 딕셔너리의 값만 사용\n",
    "print_step_callback = PrintCurrentStepCallback(lora_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "ZMc-zQIQrEyM"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# 모델 컴파일\n",
    "exp23_lora_vgg16.compile(optimizer=Adam(learning_rate=0.00005),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dkn_4j2Y_B46"
   },
   "source": [
    "### 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "5Xzwh-2nrHaz",
    "outputId": "e31b4a46-a8c9-49d8-de53-23945d1097ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.0790 - accuracy: 0.9753\n",
      "End of epoch 1, LoraLayer 0: 1667 Step\n",
      "End of epoch 1, LoraLayer 0: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 1: 1667 Step\n",
      "End of epoch 1, LoraLayer 1: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 2: 1667 Step\n",
      "End of epoch 1, LoraLayer 2: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 3: 1667 Step\n",
      "End of epoch 1, LoraLayer 3: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 4: 1667 Step\n",
      "End of epoch 1, LoraLayer 4: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 5: 1667 Step\n",
      "End of epoch 1, LoraLayer 5: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 6: 1667 Step\n",
      "End of epoch 1, LoraLayer 6: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 7: 1667 Step\n",
      "End of epoch 1, LoraLayer 7: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 8: 1667 Step\n",
      "End of epoch 1, LoraLayer 8: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 9: 1667 Step\n",
      "End of epoch 1, LoraLayer 9: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 10: 1667 Step\n",
      "End of epoch 1, LoraLayer 10: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 11: 1667 Step\n",
      "End of epoch 1, LoraLayer 11: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 12: 1667 Step\n",
      "End of epoch 1, LoraLayer 12: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 13: 1667 Step\n",
      "End of epoch 1, LoraLayer 13: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 14: 1667 Step\n",
      "End of epoch 1, LoraLayer 14: Decay factor: 1.0\n",
      "\n",
      "Testing loss: 2.302607297897339, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 53s 27ms/step - loss: 0.0789 - accuracy: 0.9753 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 2/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.0578 - accuracy: 0.9818\n",
      "End of epoch 2, LoraLayer 0: 3334 Step\n",
      "End of epoch 2, LoraLayer 0: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 1: 3334 Step\n",
      "End of epoch 2, LoraLayer 1: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 2: 3334 Step\n",
      "End of epoch 2, LoraLayer 2: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 3: 3334 Step\n",
      "End of epoch 2, LoraLayer 3: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 4: 3334 Step\n",
      "End of epoch 2, LoraLayer 4: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 5: 3334 Step\n",
      "End of epoch 2, LoraLayer 5: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 6: 3334 Step\n",
      "End of epoch 2, LoraLayer 6: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 7: 3334 Step\n",
      "End of epoch 2, LoraLayer 7: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 8: 3334 Step\n",
      "End of epoch 2, LoraLayer 8: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 9: 3334 Step\n",
      "End of epoch 2, LoraLayer 9: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 10: 3334 Step\n",
      "End of epoch 2, LoraLayer 10: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 11: 3334 Step\n",
      "End of epoch 2, LoraLayer 11: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 12: 3334 Step\n",
      "End of epoch 2, LoraLayer 12: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 13: 3334 Step\n",
      "End of epoch 2, LoraLayer 13: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 14: 3334 Step\n",
      "End of epoch 2, LoraLayer 14: Decay factor: 0.9999625086784363\n",
      "\n",
      "Testing loss: 2.302607297897339, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 45s 27ms/step - loss: 0.0578 - accuracy: 0.9818 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 3/20\n",
      "1665/1667 [============================>.] - ETA: 0s - loss: 0.0456 - accuracy: 0.9852\n",
      "End of epoch 3, LoraLayer 0: 5001 Step\n",
      "End of epoch 3, LoraLayer 0: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 1: 5001 Step\n",
      "End of epoch 3, LoraLayer 1: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 2: 5001 Step\n",
      "End of epoch 3, LoraLayer 2: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 3: 5001 Step\n",
      "End of epoch 3, LoraLayer 3: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 4: 5001 Step\n",
      "End of epoch 3, LoraLayer 4: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 5: 5001 Step\n",
      "End of epoch 3, LoraLayer 5: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 6: 5001 Step\n",
      "End of epoch 3, LoraLayer 6: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 7: 5001 Step\n",
      "End of epoch 3, LoraLayer 7: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 8: 5001 Step\n",
      "End of epoch 3, LoraLayer 8: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 9: 5001 Step\n",
      "End of epoch 3, LoraLayer 9: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 10: 5001 Step\n",
      "End of epoch 3, LoraLayer 10: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 11: 5001 Step\n",
      "End of epoch 3, LoraLayer 11: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 12: 5001 Step\n",
      "End of epoch 3, LoraLayer 12: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 13: 5001 Step\n",
      "End of epoch 3, LoraLayer 13: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 14: 5001 Step\n",
      "End of epoch 3, LoraLayer 14: Decay factor: 0.9374484419822693\n",
      "\n",
      "Testing loss: 2.302607297897339, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 44s 26ms/step - loss: 0.0457 - accuracy: 0.9852 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 4/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.0453 - accuracy: 0.9856\n",
      "End of epoch 4, LoraLayer 0: 6668 Step\n",
      "End of epoch 4, LoraLayer 0: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 1: 6668 Step\n",
      "End of epoch 4, LoraLayer 1: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 2: 6668 Step\n",
      "End of epoch 4, LoraLayer 2: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 3: 6668 Step\n",
      "End of epoch 4, LoraLayer 3: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 4: 6668 Step\n",
      "End of epoch 4, LoraLayer 4: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 5: 6668 Step\n",
      "End of epoch 4, LoraLayer 5: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 6: 6668 Step\n",
      "End of epoch 4, LoraLayer 6: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 7: 6668 Step\n",
      "End of epoch 4, LoraLayer 7: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 8: 6668 Step\n",
      "End of epoch 4, LoraLayer 8: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 9: 6668 Step\n",
      "End of epoch 4, LoraLayer 9: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 10: 6668 Step\n",
      "End of epoch 4, LoraLayer 10: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 11: 6668 Step\n",
      "End of epoch 4, LoraLayer 11: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 12: 6668 Step\n",
      "End of epoch 4, LoraLayer 12: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 13: 6668 Step\n",
      "End of epoch 4, LoraLayer 13: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 14: 6668 Step\n",
      "End of epoch 4, LoraLayer 14: Decay factor: 0.8749343752861023\n",
      "\n",
      "Testing loss: 2.302607297897339, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 44s 26ms/step - loss: 0.0453 - accuracy: 0.9856 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 5/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.0478 - accuracy: 0.9843\n",
      "End of epoch 5, LoraLayer 0: 8335 Step\n",
      "End of epoch 5, LoraLayer 0: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 1: 8335 Step\n",
      "End of epoch 5, LoraLayer 1: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 2: 8335 Step\n",
      "End of epoch 5, LoraLayer 2: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 3: 8335 Step\n",
      "End of epoch 5, LoraLayer 3: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 4: 8335 Step\n",
      "End of epoch 5, LoraLayer 4: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 5: 8335 Step\n",
      "End of epoch 5, LoraLayer 5: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 6: 8335 Step\n",
      "End of epoch 5, LoraLayer 6: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 7: 8335 Step\n",
      "End of epoch 5, LoraLayer 7: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 8: 8335 Step\n",
      "End of epoch 5, LoraLayer 8: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 9: 8335 Step\n",
      "End of epoch 5, LoraLayer 9: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 10: 8335 Step\n",
      "End of epoch 5, LoraLayer 10: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 11: 8335 Step\n",
      "End of epoch 5, LoraLayer 11: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 12: 8335 Step\n",
      "End of epoch 5, LoraLayer 12: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 13: 8335 Step\n",
      "End of epoch 5, LoraLayer 13: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 14: 8335 Step\n",
      "End of epoch 5, LoraLayer 14: Decay factor: 0.8124203085899353\n",
      "\n",
      "Testing loss: 2.302607297897339, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 46s 27ms/step - loss: 0.0478 - accuracy: 0.9843 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 6/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.0539 - accuracy: 0.9819\n",
      "End of epoch 6, LoraLayer 0: 10002 Step\n",
      "End of epoch 6, LoraLayer 0: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 1: 10002 Step\n",
      "End of epoch 6, LoraLayer 1: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 2: 10002 Step\n",
      "End of epoch 6, LoraLayer 2: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 3: 10002 Step\n",
      "End of epoch 6, LoraLayer 3: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 4: 10002 Step\n",
      "End of epoch 6, LoraLayer 4: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 5: 10002 Step\n",
      "End of epoch 6, LoraLayer 5: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 6: 10002 Step\n",
      "End of epoch 6, LoraLayer 6: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 7: 10002 Step\n",
      "End of epoch 6, LoraLayer 7: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 8: 10002 Step\n",
      "End of epoch 6, LoraLayer 8: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 9: 10002 Step\n",
      "End of epoch 6, LoraLayer 9: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 10: 10002 Step\n",
      "End of epoch 6, LoraLayer 10: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 11: 10002 Step\n",
      "End of epoch 6, LoraLayer 11: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 12: 10002 Step\n",
      "End of epoch 6, LoraLayer 12: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 13: 10002 Step\n",
      "End of epoch 6, LoraLayer 13: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 14: 10002 Step\n",
      "End of epoch 6, LoraLayer 14: Decay factor: 0.7499062418937683\n",
      "\n",
      "Testing loss: 2.302607297897339, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 48s 29ms/step - loss: 0.0539 - accuracy: 0.9819 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 7/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.0619 - accuracy: 0.9794\n",
      "End of epoch 7, LoraLayer 0: 11669 Step\n",
      "End of epoch 7, LoraLayer 0: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 1: 11669 Step\n",
      "End of epoch 7, LoraLayer 1: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 2: 11669 Step\n",
      "End of epoch 7, LoraLayer 2: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 3: 11669 Step\n",
      "End of epoch 7, LoraLayer 3: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 4: 11669 Step\n",
      "End of epoch 7, LoraLayer 4: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 5: 11669 Step\n",
      "End of epoch 7, LoraLayer 5: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 6: 11669 Step\n",
      "End of epoch 7, LoraLayer 6: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 7: 11669 Step\n",
      "End of epoch 7, LoraLayer 7: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 8: 11669 Step\n",
      "End of epoch 7, LoraLayer 8: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 9: 11669 Step\n",
      "End of epoch 7, LoraLayer 9: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 10: 11669 Step\n",
      "End of epoch 7, LoraLayer 10: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 11: 11669 Step\n",
      "End of epoch 7, LoraLayer 11: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 12: 11669 Step\n",
      "End of epoch 7, LoraLayer 12: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 13: 11669 Step\n",
      "End of epoch 7, LoraLayer 13: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 14: 11669 Step\n",
      "End of epoch 7, LoraLayer 14: Decay factor: 0.6873922348022461\n",
      "\n",
      "Testing loss: 2.302607297897339, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 45s 27ms/step - loss: 0.0619 - accuracy: 0.9794 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 8/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.0721 - accuracy: 0.9753\n",
      "End of epoch 8, LoraLayer 0: 13336 Step\n",
      "End of epoch 8, LoraLayer 0: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 1: 13336 Step\n",
      "End of epoch 8, LoraLayer 1: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 2: 13336 Step\n",
      "End of epoch 8, LoraLayer 2: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 3: 13336 Step\n",
      "End of epoch 8, LoraLayer 3: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 4: 13336 Step\n",
      "End of epoch 8, LoraLayer 4: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 5: 13336 Step\n",
      "End of epoch 8, LoraLayer 5: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 6: 13336 Step\n",
      "End of epoch 8, LoraLayer 6: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 7: 13336 Step\n",
      "End of epoch 8, LoraLayer 7: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 8: 13336 Step\n",
      "End of epoch 8, LoraLayer 8: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 9: 13336 Step\n",
      "End of epoch 8, LoraLayer 9: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 10: 13336 Step\n",
      "End of epoch 8, LoraLayer 10: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 11: 13336 Step\n",
      "End of epoch 8, LoraLayer 11: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 12: 13336 Step\n",
      "End of epoch 8, LoraLayer 12: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 13: 13336 Step\n",
      "End of epoch 8, LoraLayer 13: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 14: 13336 Step\n",
      "End of epoch 8, LoraLayer 14: Decay factor: 0.6248781681060791\n",
      "\n",
      "Testing loss: 2.302607297897339, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 46s 27ms/step - loss: 0.0721 - accuracy: 0.9753 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 9/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.0872 - accuracy: 0.9703\n",
      "End of epoch 9, LoraLayer 0: 15003 Step\n",
      "End of epoch 9, LoraLayer 0: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 1: 15003 Step\n",
      "End of epoch 9, LoraLayer 1: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 2: 15003 Step\n",
      "End of epoch 9, LoraLayer 2: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 3: 15003 Step\n",
      "End of epoch 9, LoraLayer 3: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 4: 15003 Step\n",
      "End of epoch 9, LoraLayer 4: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 5: 15003 Step\n",
      "End of epoch 9, LoraLayer 5: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 6: 15003 Step\n",
      "End of epoch 9, LoraLayer 6: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 7: 15003 Step\n",
      "End of epoch 9, LoraLayer 7: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 8: 15003 Step\n",
      "End of epoch 9, LoraLayer 8: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 9: 15003 Step\n",
      "End of epoch 9, LoraLayer 9: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 10: 15003 Step\n",
      "End of epoch 9, LoraLayer 10: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 11: 15003 Step\n",
      "End of epoch 9, LoraLayer 11: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 12: 15003 Step\n",
      "End of epoch 9, LoraLayer 12: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 13: 15003 Step\n",
      "End of epoch 9, LoraLayer 13: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 14: 15003 Step\n",
      "End of epoch 9, LoraLayer 14: Decay factor: 0.5623641014099121\n",
      "\n",
      "Testing loss: 2.302607297897339, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 49s 29ms/step - loss: 0.0872 - accuracy: 0.9704 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 10/20\n",
      "1665/1667 [============================>.] - ETA: 0s - loss: 0.1070 - accuracy: 0.9630\n",
      "End of epoch 10, LoraLayer 0: 16670 Step\n",
      "End of epoch 10, LoraLayer 0: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 1: 16670 Step\n",
      "End of epoch 10, LoraLayer 1: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 2: 16670 Step\n",
      "End of epoch 10, LoraLayer 2: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 3: 16670 Step\n",
      "End of epoch 10, LoraLayer 3: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 4: 16670 Step\n",
      "End of epoch 10, LoraLayer 4: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 5: 16670 Step\n",
      "End of epoch 10, LoraLayer 5: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 6: 16670 Step\n",
      "End of epoch 10, LoraLayer 6: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 7: 16670 Step\n",
      "End of epoch 10, LoraLayer 7: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 8: 16670 Step\n",
      "End of epoch 10, LoraLayer 8: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 9: 16670 Step\n",
      "End of epoch 10, LoraLayer 9: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 10: 16670 Step\n",
      "End of epoch 10, LoraLayer 10: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 11: 16670 Step\n",
      "End of epoch 10, LoraLayer 11: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 12: 16670 Step\n",
      "End of epoch 10, LoraLayer 12: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 13: 16670 Step\n",
      "End of epoch 10, LoraLayer 13: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 14: 16670 Step\n",
      "End of epoch 10, LoraLayer 14: Decay factor: 0.4998500347137451\n",
      "\n",
      "Testing loss: 2.3026084899902344, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 44s 27ms/step - loss: 0.1070 - accuracy: 0.9630 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 11/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.1283 - accuracy: 0.9557\n",
      "End of epoch 11, LoraLayer 0: 18337 Step\n",
      "End of epoch 11, LoraLayer 0: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 1: 18337 Step\n",
      "End of epoch 11, LoraLayer 1: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 2: 18337 Step\n",
      "End of epoch 11, LoraLayer 2: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 3: 18337 Step\n",
      "End of epoch 11, LoraLayer 3: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 4: 18337 Step\n",
      "End of epoch 11, LoraLayer 4: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 5: 18337 Step\n",
      "End of epoch 11, LoraLayer 5: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 6: 18337 Step\n",
      "End of epoch 11, LoraLayer 6: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 7: 18337 Step\n",
      "End of epoch 11, LoraLayer 7: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 8: 18337 Step\n",
      "End of epoch 11, LoraLayer 8: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 9: 18337 Step\n",
      "End of epoch 11, LoraLayer 9: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 10: 18337 Step\n",
      "End of epoch 11, LoraLayer 10: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 11: 18337 Step\n",
      "End of epoch 11, LoraLayer 11: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 12: 18337 Step\n",
      "End of epoch 11, LoraLayer 12: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 13: 18337 Step\n",
      "End of epoch 11, LoraLayer 13: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 14: 18337 Step\n",
      "End of epoch 11, LoraLayer 14: Decay factor: 0.4373359680175781\n",
      "\n",
      "Testing loss: 2.3026084899902344, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 43s 26ms/step - loss: 0.1283 - accuracy: 0.9557 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 12/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.1633 - accuracy: 0.9424\n",
      "End of epoch 12, LoraLayer 0: 20004 Step\n",
      "End of epoch 12, LoraLayer 0: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 1: 20004 Step\n",
      "End of epoch 12, LoraLayer 1: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 2: 20004 Step\n",
      "End of epoch 12, LoraLayer 2: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 3: 20004 Step\n",
      "End of epoch 12, LoraLayer 3: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 4: 20004 Step\n",
      "End of epoch 12, LoraLayer 4: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 5: 20004 Step\n",
      "End of epoch 12, LoraLayer 5: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 6: 20004 Step\n",
      "End of epoch 12, LoraLayer 6: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 7: 20004 Step\n",
      "End of epoch 12, LoraLayer 7: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 8: 20004 Step\n",
      "End of epoch 12, LoraLayer 8: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 9: 20004 Step\n",
      "End of epoch 12, LoraLayer 9: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 10: 20004 Step\n",
      "End of epoch 12, LoraLayer 10: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 11: 20004 Step\n",
      "End of epoch 12, LoraLayer 11: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 12: 20004 Step\n",
      "End of epoch 12, LoraLayer 12: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 13: 20004 Step\n",
      "End of epoch 12, LoraLayer 13: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 14: 20004 Step\n",
      "End of epoch 12, LoraLayer 14: Decay factor: 0.37482190132141113\n",
      "\n",
      "Testing loss: 2.302623748779297, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 45s 27ms/step - loss: 0.1633 - accuracy: 0.9424 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 13/20\n",
      "1665/1667 [============================>.] - ETA: 0s - loss: 0.1937 - accuracy: 0.9310\n",
      "End of epoch 13, LoraLayer 0: 21671 Step\n",
      "End of epoch 13, LoraLayer 0: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 1: 21671 Step\n",
      "End of epoch 13, LoraLayer 1: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 2: 21671 Step\n",
      "End of epoch 13, LoraLayer 2: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 3: 21671 Step\n",
      "End of epoch 13, LoraLayer 3: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 4: 21671 Step\n",
      "End of epoch 13, LoraLayer 4: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 5: 21671 Step\n",
      "End of epoch 13, LoraLayer 5: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 6: 21671 Step\n",
      "End of epoch 13, LoraLayer 6: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 7: 21671 Step\n",
      "End of epoch 13, LoraLayer 7: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 8: 21671 Step\n",
      "End of epoch 13, LoraLayer 8: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 9: 21671 Step\n",
      "End of epoch 13, LoraLayer 9: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 10: 21671 Step\n",
      "End of epoch 13, LoraLayer 10: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 11: 21671 Step\n",
      "End of epoch 13, LoraLayer 11: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 12: 21671 Step\n",
      "End of epoch 13, LoraLayer 12: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 13: 21671 Step\n",
      "End of epoch 13, LoraLayer 13: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 14: 21671 Step\n",
      "End of epoch 13, LoraLayer 14: Decay factor: 0.31230783462524414\n",
      "\n",
      "Testing loss: 2.302616596221924, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 48s 29ms/step - loss: 0.1938 - accuracy: 0.9310 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 14/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.2405 - accuracy: 0.9154\n",
      "End of epoch 14, LoraLayer 0: 23338 Step\n",
      "End of epoch 14, LoraLayer 0: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 1: 23338 Step\n",
      "End of epoch 14, LoraLayer 1: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 2: 23338 Step\n",
      "End of epoch 14, LoraLayer 2: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 3: 23338 Step\n",
      "End of epoch 14, LoraLayer 3: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 4: 23338 Step\n",
      "End of epoch 14, LoraLayer 4: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 5: 23338 Step\n",
      "End of epoch 14, LoraLayer 5: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 6: 23338 Step\n",
      "End of epoch 14, LoraLayer 6: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 7: 23338 Step\n",
      "End of epoch 14, LoraLayer 7: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 8: 23338 Step\n",
      "End of epoch 14, LoraLayer 8: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 9: 23338 Step\n",
      "End of epoch 14, LoraLayer 9: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 10: 23338 Step\n",
      "End of epoch 14, LoraLayer 10: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 11: 23338 Step\n",
      "End of epoch 14, LoraLayer 11: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 12: 23338 Step\n",
      "End of epoch 14, LoraLayer 12: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 13: 23338 Step\n",
      "End of epoch 14, LoraLayer 13: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 14: 23338 Step\n",
      "End of epoch 14, LoraLayer 14: Decay factor: 0.24979376792907715\n",
      "\n",
      "Testing loss: 2.3024983406066895, acc: 0.10199999809265137\n",
      "\n",
      "1667/1667 [==============================] - 47s 28ms/step - loss: 0.2405 - accuracy: 0.9154 - val_loss: 2.3025 - val_accuracy: 0.1020\n",
      "Epoch 15/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.2975 - accuracy: 0.8965\n",
      "End of epoch 15, LoraLayer 0: 25005 Step\n",
      "End of epoch 15, LoraLayer 0: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 1: 25005 Step\n",
      "End of epoch 15, LoraLayer 1: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 2: 25005 Step\n",
      "End of epoch 15, LoraLayer 2: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 3: 25005 Step\n",
      "End of epoch 15, LoraLayer 3: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 4: 25005 Step\n",
      "End of epoch 15, LoraLayer 4: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 5: 25005 Step\n",
      "End of epoch 15, LoraLayer 5: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 6: 25005 Step\n",
      "End of epoch 15, LoraLayer 6: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 7: 25005 Step\n",
      "End of epoch 15, LoraLayer 7: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 8: 25005 Step\n",
      "End of epoch 15, LoraLayer 8: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 9: 25005 Step\n",
      "End of epoch 15, LoraLayer 9: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 10: 25005 Step\n",
      "End of epoch 15, LoraLayer 10: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 11: 25005 Step\n",
      "End of epoch 15, LoraLayer 11: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 12: 25005 Step\n",
      "End of epoch 15, LoraLayer 12: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 13: 25005 Step\n",
      "End of epoch 15, LoraLayer 13: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 14: 25005 Step\n",
      "End of epoch 15, LoraLayer 14: Decay factor: 0.18727970123291016\n",
      "\n",
      "Testing loss: 2.301131010055542, acc: 0.11410000175237656\n",
      "\n",
      "1667/1667 [==============================] - 47s 28ms/step - loss: 0.2975 - accuracy: 0.8965 - val_loss: 2.3011 - val_accuracy: 0.1140\n",
      "Epoch 16/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.3666 - accuracy: 0.8711\n",
      "End of epoch 16, LoraLayer 0: 26672 Step\n",
      "End of epoch 16, LoraLayer 0: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 1: 26672 Step\n",
      "End of epoch 16, LoraLayer 1: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 2: 26672 Step\n",
      "End of epoch 16, LoraLayer 2: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 3: 26672 Step\n",
      "End of epoch 16, LoraLayer 3: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 4: 26672 Step\n",
      "End of epoch 16, LoraLayer 4: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 5: 26672 Step\n",
      "End of epoch 16, LoraLayer 5: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 6: 26672 Step\n",
      "End of epoch 16, LoraLayer 6: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 7: 26672 Step\n",
      "End of epoch 16, LoraLayer 7: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 8: 26672 Step\n",
      "End of epoch 16, LoraLayer 8: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 9: 26672 Step\n",
      "End of epoch 16, LoraLayer 9: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 10: 26672 Step\n",
      "End of epoch 16, LoraLayer 10: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 11: 26672 Step\n",
      "End of epoch 16, LoraLayer 11: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 12: 26672 Step\n",
      "End of epoch 16, LoraLayer 12: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 13: 26672 Step\n",
      "End of epoch 16, LoraLayer 13: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 14: 26672 Step\n",
      "End of epoch 16, LoraLayer 14: Decay factor: 0.12476563453674316\n",
      "\n",
      "Testing loss: 2.2700114250183105, acc: 0.21299999952316284\n",
      "\n",
      "1667/1667 [==============================] - 48s 29ms/step - loss: 0.3670 - accuracy: 0.8710 - val_loss: 2.2700 - val_accuracy: 0.2129\n",
      "Epoch 17/20\n",
      "1665/1667 [============================>.] - ETA: 0s - loss: 0.4609 - accuracy: 0.8421\n",
      "End of epoch 17, LoraLayer 0: 28339 Step\n",
      "End of epoch 17, LoraLayer 0: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 1: 28339 Step\n",
      "End of epoch 17, LoraLayer 1: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 2: 28339 Step\n",
      "End of epoch 17, LoraLayer 2: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 3: 28339 Step\n",
      "End of epoch 17, LoraLayer 3: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 4: 28339 Step\n",
      "End of epoch 17, LoraLayer 4: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 5: 28339 Step\n",
      "End of epoch 17, LoraLayer 5: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 6: 28339 Step\n",
      "End of epoch 17, LoraLayer 6: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 7: 28339 Step\n",
      "End of epoch 17, LoraLayer 7: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 8: 28339 Step\n",
      "End of epoch 17, LoraLayer 8: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 9: 28339 Step\n",
      "End of epoch 17, LoraLayer 9: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 10: 28339 Step\n",
      "End of epoch 17, LoraLayer 10: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 11: 28339 Step\n",
      "End of epoch 17, LoraLayer 11: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 12: 28339 Step\n",
      "End of epoch 17, LoraLayer 12: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 13: 28339 Step\n",
      "End of epoch 17, LoraLayer 13: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 14: 28339 Step\n",
      "End of epoch 17, LoraLayer 14: Decay factor: 0.06225156784057617\n",
      "\n",
      "Testing loss: 1.8710811138153076, acc: 0.4722999930381775\n",
      "\n",
      "1667/1667 [==============================] - 48s 29ms/step - loss: 0.4609 - accuracy: 0.8422 - val_loss: 1.8711 - val_accuracy: 0.4720\n",
      "Epoch 18/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.5839 - accuracy: 0.8015\n",
      "End of epoch 18, LoraLayer 0: 30006 Step\n",
      "End of epoch 18, LoraLayer 0: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 1: 30006 Step\n",
      "End of epoch 18, LoraLayer 1: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 2: 30006 Step\n",
      "End of epoch 18, LoraLayer 2: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 3: 30006 Step\n",
      "End of epoch 18, LoraLayer 3: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 4: 30006 Step\n",
      "End of epoch 18, LoraLayer 4: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 5: 30006 Step\n",
      "End of epoch 18, LoraLayer 5: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 6: 30006 Step\n",
      "End of epoch 18, LoraLayer 6: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 7: 30006 Step\n",
      "End of epoch 18, LoraLayer 7: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 8: 30006 Step\n",
      "End of epoch 18, LoraLayer 8: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 9: 30006 Step\n",
      "End of epoch 18, LoraLayer 9: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 10: 30006 Step\n",
      "End of epoch 18, LoraLayer 10: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 11: 30006 Step\n",
      "End of epoch 18, LoraLayer 11: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 12: 30006 Step\n",
      "End of epoch 18, LoraLayer 12: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 13: 30006 Step\n",
      "End of epoch 18, LoraLayer 13: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 14: 30006 Step\n",
      "End of epoch 18, LoraLayer 14: Decay factor: 0.0\n",
      "\n",
      "Testing loss: 0.7966493368148804, acc: 0.736299991607666\n",
      "\n",
      "1667/1667 [==============================] - 47s 28ms/step - loss: 0.5839 - accuracy: 0.8015 - val_loss: 0.7966 - val_accuracy: 0.7361\n",
      "Epoch 19/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.5896 - accuracy: 0.8025\n",
      "End of epoch 19, LoraLayer 0: 31673 Step\n",
      "End of epoch 19, LoraLayer 0: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 1: 31673 Step\n",
      "End of epoch 19, LoraLayer 1: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 2: 31673 Step\n",
      "End of epoch 19, LoraLayer 2: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 3: 31673 Step\n",
      "End of epoch 19, LoraLayer 3: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 4: 31673 Step\n",
      "End of epoch 19, LoraLayer 4: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 5: 31673 Step\n",
      "End of epoch 19, LoraLayer 5: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 6: 31673 Step\n",
      "End of epoch 19, LoraLayer 6: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 7: 31673 Step\n",
      "End of epoch 19, LoraLayer 7: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 8: 31673 Step\n",
      "End of epoch 19, LoraLayer 8: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 9: 31673 Step\n",
      "End of epoch 19, LoraLayer 9: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 10: 31673 Step\n",
      "End of epoch 19, LoraLayer 10: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 11: 31673 Step\n",
      "End of epoch 19, LoraLayer 11: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 12: 31673 Step\n",
      "End of epoch 19, LoraLayer 12: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 13: 31673 Step\n",
      "End of epoch 19, LoraLayer 13: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 14: 31673 Step\n",
      "End of epoch 19, LoraLayer 14: Decay factor: 0.0\n",
      "\n",
      "Testing loss: 0.8155259490013123, acc: 0.7372999787330627\n",
      "\n",
      "1667/1667 [==============================] - 73s 44ms/step - loss: 0.5897 - accuracy: 0.8025 - val_loss: 0.8156 - val_accuracy: 0.7373\n",
      "Epoch 20/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.4862 - accuracy: 0.8349\n",
      "End of epoch 20, LoraLayer 0: 33340 Step\n",
      "End of epoch 20, LoraLayer 0: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 1: 33340 Step\n",
      "End of epoch 20, LoraLayer 1: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 2: 33340 Step\n",
      "End of epoch 20, LoraLayer 2: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 3: 33340 Step\n",
      "End of epoch 20, LoraLayer 3: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 4: 33340 Step\n",
      "End of epoch 20, LoraLayer 4: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 5: 33340 Step\n",
      "End of epoch 20, LoraLayer 5: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 6: 33340 Step\n",
      "End of epoch 20, LoraLayer 6: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 7: 33340 Step\n",
      "End of epoch 20, LoraLayer 7: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 8: 33340 Step\n",
      "End of epoch 20, LoraLayer 8: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 9: 33340 Step\n",
      "End of epoch 20, LoraLayer 9: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 10: 33340 Step\n",
      "End of epoch 20, LoraLayer 10: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 11: 33340 Step\n",
      "End of epoch 20, LoraLayer 11: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 12: 33340 Step\n",
      "End of epoch 20, LoraLayer 12: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 13: 33340 Step\n",
      "End of epoch 20, LoraLayer 13: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 14: 33340 Step\n",
      "End of epoch 20, LoraLayer 14: Decay factor: 0.0\n",
      "\n",
      "Testing loss: 0.7658989429473877, acc: 0.7544999718666077\n",
      "\n",
      "1667/1667 [==============================] - 92s 55ms/step - loss: 0.4862 - accuracy: 0.8349 - val_loss: 0.7659 - val_accuracy: 0.7543\n"
     ]
    }
   ],
   "source": [
    "history_exp23 = exp23_lora_vgg16.fit(x_train, y_train, batch_size=30, epochs=20, validation_data=(x_test, y_test), callbacks=[print_step_callback, TestCallback((x_test, y_test))] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "ndAMXGdSgAmG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 6s 20ms/step - loss: 0.7659 - accuracy: 0.7545\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7658989429473877, 0.7544999718666077]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 평가\n",
    "exp23_lora_vgg16.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "dEnYK6elrKvV"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 학습 및 검증 정확도 시각화\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# 정확도 그래프\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_exp23.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history_exp23.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# 손실 그래프\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_exp23.history['loss'], label='Training Loss')\n",
    "plt.plot(history_exp23.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3lR_5oviqMrk"
   },
   "source": [
    "## 2-4. (16, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "xshgj9pkqMrt"
   },
   "outputs": [],
   "source": [
    "dataset_size = x_train.shape[0]\n",
    "batch_size = 30\n",
    "epochs = 20\n",
    "\n",
    "total_iteration= int(epochs * (dataset_size / batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "xFEe2jFMqMrt"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout, InputLayer\n",
    "import tensorflow as tf\n",
    "\n",
    "# 새로운 레이어를 추가하기 위한 레이어 리스트 초기화\n",
    "new_layers = []\n",
    "\n",
    "# Dense 레이어 카운터\n",
    "dense_layer_count = 0\n",
    "\n",
    "# VGG 모델의 각 레이어 순회\n",
    "for layer in best_vgg16.layers:\n",
    "    if isinstance(layer, InputLayer):\n",
    "        continue\n",
    "    if isinstance(layer, Conv2D):\n",
    "        # Conv2D 레이어를 ConvLoRALayer로 대체\n",
    "        conv_lora_layer = ConvLoRALayer00_cdn2(original_conv_layer=layer, rank=16, alpha=32, total_iteration = total_iteration, trainable=True)\n",
    "        new_layers.append(conv_lora_layer)\n",
    "    elif isinstance(layer, Dense):\n",
    "        dense_layer_count += 1\n",
    "        # 첫 번째와 두 번째 Dense 레이어에만 LoraLayer 적용\n",
    "        if dense_layer_count in [1, 2]:\n",
    "            dense_lora_layer = LoraLayer(original_layer=layer, rank=16, alpha=32, total_iteration=total_iteration, trainable=True)\n",
    "            new_layers.append(dense_lora_layer)\n",
    "        else:\n",
    "            new_layers.append(layer)  # 그대로 유지함\n",
    "    else:\n",
    "        # 다른 유형의 레이어는 그대로 유지\n",
    "        new_layers.append(layer)\n",
    "\n",
    "# 새로운 입력 텐서 생성\n",
    "input_tensor = best_vgg16.input\n",
    "\n",
    "# 새로운 모델 구성\n",
    "x = input_tensor\n",
    "for layer in new_layers:\n",
    "    x = layer(x)\n",
    "\n",
    "# 새로운 모델 생성\n",
    "exp24_lora_vgg16 = Model(inputs=input_tensor, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "ff6yIKoKqMrt"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D\n",
    "\n",
    "# 새로 생성된 모델에서 Dense 레이어 중 LoRA가 적용되지 않은 레이어의 trainable을 False로 설정\n",
    "for layer in exp24_lora_vgg16.layers:\n",
    "    if isinstance(layer, Dense) and not isinstance(layer, LoraLayer):\n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZnmdfI4IqMrt",
    "outputId": "23d02fd4-4e40-45f0-b281-0e9eebcce05f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " block1_conv1 (ConvLoRALaye  (None, 32, 32, 64)        11442     \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block1_conv2 (ConvLoRALaye  (None, 32, 32, 64)        55362     \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " block2_conv1 (ConvLoRALaye  (None, 16, 16, 128)       101506    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block2_conv2 (ConvLoRALaye  (None, 16, 16, 128)       184450    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " block3_conv1 (ConvLoRALaye  (None, 8, 8, 256)         350466    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block3_conv2 (ConvLoRALaye  (None, 8, 8, 256)         663810    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block3_conv3 (ConvLoRALaye  (None, 8, 8, 256)         663810    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 4, 4, 256)         0         \n",
      "                                                                 \n",
      " block4_conv1 (ConvLoRALaye  (None, 4, 4, 512)         1290754   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block4_conv2 (ConvLoRALaye  (None, 4, 4, 512)         2507266   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block4_conv3 (ConvLoRALaye  (None, 4, 4, 512)         2507266   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      " block5_conv1 (ConvLoRALaye  (None, 2, 2, 512)         2507266   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block5_conv2 (ConvLoRALaye  (None, 2, 2, 512)         2507266   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block5_conv3 (ConvLoRALaye  (None, 2, 2, 512)         2507266   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 1, 1, 512)         0         \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_6 (LoraLayer)         (None, 4096)              2174978   \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_7 (LoraLayer)         (None, 512)               2171394   \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20209432 (77.09 MB)\n",
      "Trainable params: 1290672 (4.92 MB)\n",
      "Non-trainable params: 18918760 (72.17 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "exp24_lora_vgg16.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F00Ij82-qMrt",
    "outputId": "30affc13-5eb2-4c04-e7d2-7922b86656c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: input_3\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block1_conv1\n",
      "  Trainable parameters: 9648\n",
      "  Non-trainable parameters: 1794\n",
      "Layer: block1_conv2\n",
      "  Trainable parameters: 18432\n",
      "  Non-trainable parameters: 36930\n",
      "Layer: block1_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block2_conv1\n",
      "  Trainable parameters: 27648\n",
      "  Non-trainable parameters: 73858\n",
      "Layer: block2_conv2\n",
      "  Trainable parameters: 36864\n",
      "  Non-trainable parameters: 147586\n",
      "Layer: block2_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block3_conv1\n",
      "  Trainable parameters: 55296\n",
      "  Non-trainable parameters: 295170\n",
      "Layer: block3_conv2\n",
      "  Trainable parameters: 73728\n",
      "  Non-trainable parameters: 590082\n",
      "Layer: block3_conv3\n",
      "  Trainable parameters: 73728\n",
      "  Non-trainable parameters: 590082\n",
      "Layer: block3_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block4_conv1\n",
      "  Trainable parameters: 110592\n",
      "  Non-trainable parameters: 1180162\n",
      "Layer: block4_conv2\n",
      "  Trainable parameters: 147456\n",
      "  Non-trainable parameters: 2359810\n",
      "Layer: block4_conv3\n",
      "  Trainable parameters: 147456\n",
      "  Non-trainable parameters: 2359810\n",
      "Layer: block4_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block5_conv1\n",
      "  Trainable parameters: 147456\n",
      "  Non-trainable parameters: 2359810\n",
      "Layer: block5_conv2\n",
      "  Trainable parameters: 147456\n",
      "  Non-trainable parameters: 2359810\n",
      "Layer: block5_conv3\n",
      "  Trainable parameters: 147456\n",
      "  Non-trainable parameters: 2359810\n",
      "Layer: block5_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: flatten_2\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: dense_6\n",
      "  Trainable parameters: 73728\n",
      "  Non-trainable parameters: 2101250\n",
      "Layer: dropout_4\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: dense_7\n",
      "  Trainable parameters: 73728\n",
      "  Non-trainable parameters: 2097666\n",
      "Layer: dropout_5\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: dense_8\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 5130\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#모델의 각 레이어를 순회하며 파라미터 수를 계산\n",
    "for layer in exp24_lora_vgg16.layers:\n",
    "    trainable_count = np.sum([tf.size(w).numpy() for w in layer.trainable_weights])\n",
    "    non_trainable_count = np.sum([tf.size(w).numpy() for w in layer.non_trainable_weights])\n",
    "\n",
    "    print(f\"Layer: {layer.name}\")\n",
    "    print(f\"  Trainable parameters: {trainable_count}\")\n",
    "    print(f\"  Non-trainable parameters: {non_trainable_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "iXZC49h7qMru"
   },
   "outputs": [],
   "source": [
    "lora_layers = []  # lora 레이어들을 저장할 리스트\n",
    "\n",
    "for layer in exp24_lora_vgg16.layers:\n",
    "    if isinstance(layer, LoraLayer) or isinstance(layer, ConvLoRALayer00_cdn2):\n",
    "        lora_layers.append(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "HNE35A0EqMru"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "class PrintCurrentStepCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, lora_layers):\n",
    "        super().__init__()\n",
    "        self.lora_layers = lora_layers\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print()\n",
    "        for i, lora_layer in enumerate(self.lora_layers):\n",
    "            current_step = lora_layer.current_step.value()\n",
    "            decay_factor = lora_layer.decay_factor.value()\n",
    "            print(f\"End of epoch {epoch + 1}, LoraLayer {i}: {current_step} Step\")\n",
    "            print(f\"End of epoch {epoch + 1}, LoraLayer {i}: Decay factor: {decay_factor}\")\n",
    "\n",
    "class TestCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, test_data):\n",
    "        self.test_data = test_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        x, y = self.test_data\n",
    "        loss, acc = self.model.evaluate(x, y, verbose=0)\n",
    "        print('\\nTesting loss: {}, acc: {}\\n'.format(loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "zeLLoQGzqMru"
   },
   "outputs": [],
   "source": [
    "# 콜백 생성 시 lora_layers 딕셔너리의 값만 사용\n",
    "print_step_callback = PrintCurrentStepCallback(lora_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "e6Dj91ILqMru"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# 모델 컴파일\n",
    "exp24_lora_vgg16.compile(optimizer=Adam(learning_rate=0.00005),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3AQTPMJ_GOG"
   },
   "source": [
    "### 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gp_zFeAtqMru",
    "outputId": "ce31c6e2-6916-462a-f2f5-1a2bbcf534b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.1195 - accuracy: 0.9631\n",
      "End of epoch 1, LoraLayer 0: 1667 Step\n",
      "End of epoch 1, LoraLayer 0: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 1: 1667 Step\n",
      "End of epoch 1, LoraLayer 1: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 2: 1667 Step\n",
      "End of epoch 1, LoraLayer 2: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 3: 1667 Step\n",
      "End of epoch 1, LoraLayer 3: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 4: 1667 Step\n",
      "End of epoch 1, LoraLayer 4: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 5: 1667 Step\n",
      "End of epoch 1, LoraLayer 5: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 6: 1667 Step\n",
      "End of epoch 1, LoraLayer 6: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 7: 1667 Step\n",
      "End of epoch 1, LoraLayer 7: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 8: 1667 Step\n",
      "End of epoch 1, LoraLayer 8: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 9: 1667 Step\n",
      "End of epoch 1, LoraLayer 9: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 10: 1667 Step\n",
      "End of epoch 1, LoraLayer 10: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 11: 1667 Step\n",
      "End of epoch 1, LoraLayer 11: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 12: 1667 Step\n",
      "End of epoch 1, LoraLayer 12: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 13: 1667 Step\n",
      "End of epoch 1, LoraLayer 13: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 14: 1667 Step\n",
      "End of epoch 1, LoraLayer 14: Decay factor: 1.0\n",
      "\n",
      "Testing loss: 2.302607297897339, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 97s 53ms/step - loss: 0.1195 - accuracy: 0.9631 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 2/20\n",
      "1665/1667 [============================>.] - ETA: 0s - loss: 0.0922 - accuracy: 0.9707\n",
      "End of epoch 2, LoraLayer 0: 3334 Step\n",
      "End of epoch 2, LoraLayer 0: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 1: 3334 Step\n",
      "End of epoch 2, LoraLayer 1: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 2: 3334 Step\n",
      "End of epoch 2, LoraLayer 2: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 3: 3334 Step\n",
      "End of epoch 2, LoraLayer 3: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 4: 3334 Step\n",
      "End of epoch 2, LoraLayer 4: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 5: 3334 Step\n",
      "End of epoch 2, LoraLayer 5: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 6: 3334 Step\n",
      "End of epoch 2, LoraLayer 6: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 7: 3334 Step\n",
      "End of epoch 2, LoraLayer 7: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 8: 3334 Step\n",
      "End of epoch 2, LoraLayer 8: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 9: 3334 Step\n",
      "End of epoch 2, LoraLayer 9: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 10: 3334 Step\n",
      "End of epoch 2, LoraLayer 10: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 11: 3334 Step\n",
      "End of epoch 2, LoraLayer 11: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 12: 3334 Step\n",
      "End of epoch 2, LoraLayer 12: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 13: 3334 Step\n",
      "End of epoch 2, LoraLayer 13: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 14: 3334 Step\n",
      "End of epoch 2, LoraLayer 14: Decay factor: 0.9999625086784363\n",
      "\n",
      "Testing loss: 2.302607297897339, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 58s 35ms/step - loss: 0.0924 - accuracy: 0.9706 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 3/20\n",
      "1665/1667 [============================>.] - ETA: 0s - loss: 0.0823 - accuracy: 0.9741\n",
      "End of epoch 3, LoraLayer 0: 5001 Step\n",
      "End of epoch 3, LoraLayer 0: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 1: 5001 Step\n",
      "End of epoch 3, LoraLayer 1: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 2: 5001 Step\n",
      "End of epoch 3, LoraLayer 2: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 3: 5001 Step\n",
      "End of epoch 3, LoraLayer 3: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 4: 5001 Step\n",
      "End of epoch 3, LoraLayer 4: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 5: 5001 Step\n",
      "End of epoch 3, LoraLayer 5: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 6: 5001 Step\n",
      "End of epoch 3, LoraLayer 6: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 7: 5001 Step\n",
      "End of epoch 3, LoraLayer 7: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 8: 5001 Step\n",
      "End of epoch 3, LoraLayer 8: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 9: 5001 Step\n",
      "End of epoch 3, LoraLayer 9: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 10: 5001 Step\n",
      "End of epoch 3, LoraLayer 10: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 11: 5001 Step\n",
      "End of epoch 3, LoraLayer 11: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 12: 5001 Step\n",
      "End of epoch 3, LoraLayer 12: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 13: 5001 Step\n",
      "End of epoch 3, LoraLayer 13: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 14: 5001 Step\n",
      "End of epoch 3, LoraLayer 14: Decay factor: 0.9374484419822693\n",
      "\n",
      "Testing loss: 2.302607297897339, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 50s 30ms/step - loss: 0.0823 - accuracy: 0.9741 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 4/20\n",
      "1665/1667 [============================>.] - ETA: 0s - loss: 0.0802 - accuracy: 0.9732\n",
      "End of epoch 4, LoraLayer 0: 6668 Step\n",
      "End of epoch 4, LoraLayer 0: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 1: 6668 Step\n",
      "End of epoch 4, LoraLayer 1: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 2: 6668 Step\n",
      "End of epoch 4, LoraLayer 2: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 3: 6668 Step\n",
      "End of epoch 4, LoraLayer 3: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 4: 6668 Step\n",
      "End of epoch 4, LoraLayer 4: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 5: 6668 Step\n",
      "End of epoch 4, LoraLayer 5: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 6: 6668 Step\n",
      "End of epoch 4, LoraLayer 6: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 7: 6668 Step\n",
      "End of epoch 4, LoraLayer 7: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 8: 6668 Step\n",
      "End of epoch 4, LoraLayer 8: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 9: 6668 Step\n",
      "End of epoch 4, LoraLayer 9: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 10: 6668 Step\n",
      "End of epoch 4, LoraLayer 10: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 11: 6668 Step\n",
      "End of epoch 4, LoraLayer 11: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 12: 6668 Step\n",
      "End of epoch 4, LoraLayer 12: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 13: 6668 Step\n",
      "End of epoch 4, LoraLayer 13: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 14: 6668 Step\n",
      "End of epoch 4, LoraLayer 14: Decay factor: 0.8749343752861023\n",
      "\n",
      "Testing loss: 2.302607297897339, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 49s 29ms/step - loss: 0.0802 - accuracy: 0.9732 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 5/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.0827 - accuracy: 0.9731\n",
      "End of epoch 5, LoraLayer 0: 8335 Step\n",
      "End of epoch 5, LoraLayer 0: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 1: 8335 Step\n",
      "End of epoch 5, LoraLayer 1: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 2: 8335 Step\n",
      "End of epoch 5, LoraLayer 2: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 3: 8335 Step\n",
      "End of epoch 5, LoraLayer 3: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 4: 8335 Step\n",
      "End of epoch 5, LoraLayer 4: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 5: 8335 Step\n",
      "End of epoch 5, LoraLayer 5: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 6: 8335 Step\n",
      "End of epoch 5, LoraLayer 6: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 7: 8335 Step\n",
      "End of epoch 5, LoraLayer 7: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 8: 8335 Step\n",
      "End of epoch 5, LoraLayer 8: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 9: 8335 Step\n",
      "End of epoch 5, LoraLayer 9: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 10: 8335 Step\n",
      "End of epoch 5, LoraLayer 10: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 11: 8335 Step\n",
      "End of epoch 5, LoraLayer 11: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 12: 8335 Step\n",
      "End of epoch 5, LoraLayer 12: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 13: 8335 Step\n",
      "End of epoch 5, LoraLayer 13: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 14: 8335 Step\n",
      "End of epoch 5, LoraLayer 14: Decay factor: 0.8124203085899353\n",
      "\n",
      "Testing loss: 2.302607297897339, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 50s 30ms/step - loss: 0.0827 - accuracy: 0.9731 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 6/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.0951 - accuracy: 0.9696\n",
      "End of epoch 6, LoraLayer 0: 10002 Step\n",
      "End of epoch 6, LoraLayer 0: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 1: 10002 Step\n",
      "End of epoch 6, LoraLayer 1: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 2: 10002 Step\n",
      "End of epoch 6, LoraLayer 2: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 3: 10002 Step\n",
      "End of epoch 6, LoraLayer 3: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 4: 10002 Step\n",
      "End of epoch 6, LoraLayer 4: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 5: 10002 Step\n",
      "End of epoch 6, LoraLayer 5: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 6: 10002 Step\n",
      "End of epoch 6, LoraLayer 6: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 7: 10002 Step\n",
      "End of epoch 6, LoraLayer 7: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 8: 10002 Step\n",
      "End of epoch 6, LoraLayer 8: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 9: 10002 Step\n",
      "End of epoch 6, LoraLayer 9: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 10: 10002 Step\n",
      "End of epoch 6, LoraLayer 10: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 11: 10002 Step\n",
      "End of epoch 6, LoraLayer 11: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 12: 10002 Step\n",
      "End of epoch 6, LoraLayer 12: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 13: 10002 Step\n",
      "End of epoch 6, LoraLayer 13: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 14: 10002 Step\n",
      "End of epoch 6, LoraLayer 14: Decay factor: 0.7499062418937683\n",
      "\n",
      "Testing loss: 2.302607297897339, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 49s 30ms/step - loss: 0.0951 - accuracy: 0.9696 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 7/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.1074 - accuracy: 0.9644\n",
      "End of epoch 7, LoraLayer 0: 11669 Step\n",
      "End of epoch 7, LoraLayer 0: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 1: 11669 Step\n",
      "End of epoch 7, LoraLayer 1: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 2: 11669 Step\n",
      "End of epoch 7, LoraLayer 2: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 3: 11669 Step\n",
      "End of epoch 7, LoraLayer 3: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 4: 11669 Step\n",
      "End of epoch 7, LoraLayer 4: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 5: 11669 Step\n",
      "End of epoch 7, LoraLayer 5: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 6: 11669 Step\n",
      "End of epoch 7, LoraLayer 6: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 7: 11669 Step\n",
      "End of epoch 7, LoraLayer 7: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 8: 11669 Step\n",
      "End of epoch 7, LoraLayer 8: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 9: 11669 Step\n",
      "End of epoch 7, LoraLayer 9: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 10: 11669 Step\n",
      "End of epoch 7, LoraLayer 10: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 11: 11669 Step\n",
      "End of epoch 7, LoraLayer 11: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 12: 11669 Step\n",
      "End of epoch 7, LoraLayer 12: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 13: 11669 Step\n",
      "End of epoch 7, LoraLayer 13: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 14: 11669 Step\n",
      "End of epoch 7, LoraLayer 14: Decay factor: 0.6873922348022461\n",
      "\n",
      "Testing loss: 2.302607297897339, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 47s 28ms/step - loss: 0.1074 - accuracy: 0.9644 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 8/20\n",
      "1665/1667 [============================>.] - ETA: 0s - loss: 0.1254 - accuracy: 0.9575\n",
      "End of epoch 8, LoraLayer 0: 13336 Step\n",
      "End of epoch 8, LoraLayer 0: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 1: 13336 Step\n",
      "End of epoch 8, LoraLayer 1: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 2: 13336 Step\n",
      "End of epoch 8, LoraLayer 2: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 3: 13336 Step\n",
      "End of epoch 8, LoraLayer 3: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 4: 13336 Step\n",
      "End of epoch 8, LoraLayer 4: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 5: 13336 Step\n",
      "End of epoch 8, LoraLayer 5: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 6: 13336 Step\n",
      "End of epoch 8, LoraLayer 6: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 7: 13336 Step\n",
      "End of epoch 8, LoraLayer 7: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 8: 13336 Step\n",
      "End of epoch 8, LoraLayer 8: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 9: 13336 Step\n",
      "End of epoch 8, LoraLayer 9: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 10: 13336 Step\n",
      "End of epoch 8, LoraLayer 10: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 11: 13336 Step\n",
      "End of epoch 8, LoraLayer 11: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 12: 13336 Step\n",
      "End of epoch 8, LoraLayer 12: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 13: 13336 Step\n",
      "End of epoch 8, LoraLayer 13: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 14: 13336 Step\n",
      "End of epoch 8, LoraLayer 14: Decay factor: 0.6248781681060791\n",
      "\n",
      "Testing loss: 2.302607297897339, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 46s 27ms/step - loss: 0.1254 - accuracy: 0.9575 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 9/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.1433 - accuracy: 0.9531\n",
      "End of epoch 9, LoraLayer 0: 15003 Step\n",
      "End of epoch 9, LoraLayer 0: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 1: 15003 Step\n",
      "End of epoch 9, LoraLayer 1: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 2: 15003 Step\n",
      "End of epoch 9, LoraLayer 2: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 3: 15003 Step\n",
      "End of epoch 9, LoraLayer 3: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 4: 15003 Step\n",
      "End of epoch 9, LoraLayer 4: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 5: 15003 Step\n",
      "End of epoch 9, LoraLayer 5: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 6: 15003 Step\n",
      "End of epoch 9, LoraLayer 6: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 7: 15003 Step\n",
      "End of epoch 9, LoraLayer 7: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 8: 15003 Step\n",
      "End of epoch 9, LoraLayer 8: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 9: 15003 Step\n",
      "End of epoch 9, LoraLayer 9: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 10: 15003 Step\n",
      "End of epoch 9, LoraLayer 10: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 11: 15003 Step\n",
      "End of epoch 9, LoraLayer 11: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 12: 15003 Step\n",
      "End of epoch 9, LoraLayer 12: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 13: 15003 Step\n",
      "End of epoch 9, LoraLayer 13: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 14: 15003 Step\n",
      "End of epoch 9, LoraLayer 14: Decay factor: 0.5623641014099121\n",
      "\n",
      "Testing loss: 2.3026063442230225, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 44s 27ms/step - loss: 0.1433 - accuracy: 0.9531 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 10/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.1734 - accuracy: 0.9413\n",
      "End of epoch 10, LoraLayer 0: 16670 Step\n",
      "End of epoch 10, LoraLayer 0: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 1: 16670 Step\n",
      "End of epoch 10, LoraLayer 1: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 2: 16670 Step\n",
      "End of epoch 10, LoraLayer 2: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 3: 16670 Step\n",
      "End of epoch 10, LoraLayer 3: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 4: 16670 Step\n",
      "End of epoch 10, LoraLayer 4: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 5: 16670 Step\n",
      "End of epoch 10, LoraLayer 5: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 6: 16670 Step\n",
      "End of epoch 10, LoraLayer 6: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 7: 16670 Step\n",
      "End of epoch 10, LoraLayer 7: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 8: 16670 Step\n",
      "End of epoch 10, LoraLayer 8: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 9: 16670 Step\n",
      "End of epoch 10, LoraLayer 9: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 10: 16670 Step\n",
      "End of epoch 10, LoraLayer 10: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 11: 16670 Step\n",
      "End of epoch 10, LoraLayer 11: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 12: 16670 Step\n",
      "End of epoch 10, LoraLayer 12: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 13: 16670 Step\n",
      "End of epoch 10, LoraLayer 13: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 14: 16670 Step\n",
      "End of epoch 10, LoraLayer 14: Decay factor: 0.4998500347137451\n",
      "\n",
      "Testing loss: 2.3025989532470703, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 50s 30ms/step - loss: 0.1734 - accuracy: 0.9413 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 11/20\n",
      "1665/1667 [============================>.] - ETA: 0s - loss: 0.2128 - accuracy: 0.9274\n",
      "End of epoch 11, LoraLayer 0: 18337 Step\n",
      "End of epoch 11, LoraLayer 0: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 1: 18337 Step\n",
      "End of epoch 11, LoraLayer 1: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 2: 18337 Step\n",
      "End of epoch 11, LoraLayer 2: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 3: 18337 Step\n",
      "End of epoch 11, LoraLayer 3: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 4: 18337 Step\n",
      "End of epoch 11, LoraLayer 4: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 5: 18337 Step\n",
      "End of epoch 11, LoraLayer 5: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 6: 18337 Step\n",
      "End of epoch 11, LoraLayer 6: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 7: 18337 Step\n",
      "End of epoch 11, LoraLayer 7: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 8: 18337 Step\n",
      "End of epoch 11, LoraLayer 8: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 9: 18337 Step\n",
      "End of epoch 11, LoraLayer 9: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 10: 18337 Step\n",
      "End of epoch 11, LoraLayer 10: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 11: 18337 Step\n",
      "End of epoch 11, LoraLayer 11: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 12: 18337 Step\n",
      "End of epoch 11, LoraLayer 12: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 13: 18337 Step\n",
      "End of epoch 11, LoraLayer 13: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 14: 18337 Step\n",
      "End of epoch 11, LoraLayer 14: Decay factor: 0.4373359680175781\n",
      "\n",
      "Testing loss: 2.3026340007781982, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 45s 27ms/step - loss: 0.2130 - accuracy: 0.9274 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 12/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.2587 - accuracy: 0.9107\n",
      "End of epoch 12, LoraLayer 0: 20004 Step\n",
      "End of epoch 12, LoraLayer 0: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 1: 20004 Step\n",
      "End of epoch 12, LoraLayer 1: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 2: 20004 Step\n",
      "End of epoch 12, LoraLayer 2: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 3: 20004 Step\n",
      "End of epoch 12, LoraLayer 3: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 4: 20004 Step\n",
      "End of epoch 12, LoraLayer 4: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 5: 20004 Step\n",
      "End of epoch 12, LoraLayer 5: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 6: 20004 Step\n",
      "End of epoch 12, LoraLayer 6: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 7: 20004 Step\n",
      "End of epoch 12, LoraLayer 7: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 8: 20004 Step\n",
      "End of epoch 12, LoraLayer 8: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 9: 20004 Step\n",
      "End of epoch 12, LoraLayer 9: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 10: 20004 Step\n",
      "End of epoch 12, LoraLayer 10: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 11: 20004 Step\n",
      "End of epoch 12, LoraLayer 11: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 12: 20004 Step\n",
      "End of epoch 12, LoraLayer 12: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 13: 20004 Step\n",
      "End of epoch 12, LoraLayer 13: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 14: 20004 Step\n",
      "End of epoch 12, LoraLayer 14: Decay factor: 0.37482190132141113\n",
      "\n",
      "Testing loss: 2.302598714828491, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 47s 28ms/step - loss: 0.2587 - accuracy: 0.9107 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 13/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.3108 - accuracy: 0.8927\n",
      "End of epoch 13, LoraLayer 0: 21671 Step\n",
      "End of epoch 13, LoraLayer 0: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 1: 21671 Step\n",
      "End of epoch 13, LoraLayer 1: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 2: 21671 Step\n",
      "End of epoch 13, LoraLayer 2: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 3: 21671 Step\n",
      "End of epoch 13, LoraLayer 3: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 4: 21671 Step\n",
      "End of epoch 13, LoraLayer 4: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 5: 21671 Step\n",
      "End of epoch 13, LoraLayer 5: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 6: 21671 Step\n",
      "End of epoch 13, LoraLayer 6: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 7: 21671 Step\n",
      "End of epoch 13, LoraLayer 7: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 8: 21671 Step\n",
      "End of epoch 13, LoraLayer 8: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 9: 21671 Step\n",
      "End of epoch 13, LoraLayer 9: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 10: 21671 Step\n",
      "End of epoch 13, LoraLayer 10: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 11: 21671 Step\n",
      "End of epoch 13, LoraLayer 11: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 12: 21671 Step\n",
      "End of epoch 13, LoraLayer 12: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 13: 21671 Step\n",
      "End of epoch 13, LoraLayer 13: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 14: 21671 Step\n",
      "End of epoch 13, LoraLayer 14: Decay factor: 0.31230783462524414\n",
      "\n",
      "Testing loss: 2.3026421070098877, acc: 0.09860000014305115\n",
      "\n",
      "1667/1667 [==============================] - 47s 28ms/step - loss: 0.3108 - accuracy: 0.8927 - val_loss: 2.3026 - val_accuracy: 0.0986\n",
      "Epoch 14/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.3852 - accuracy: 0.8695\n",
      "End of epoch 14, LoraLayer 0: 23338 Step\n",
      "End of epoch 14, LoraLayer 0: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 1: 23338 Step\n",
      "End of epoch 14, LoraLayer 1: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 2: 23338 Step\n",
      "End of epoch 14, LoraLayer 2: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 3: 23338 Step\n",
      "End of epoch 14, LoraLayer 3: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 4: 23338 Step\n",
      "End of epoch 14, LoraLayer 4: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 5: 23338 Step\n",
      "End of epoch 14, LoraLayer 5: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 6: 23338 Step\n",
      "End of epoch 14, LoraLayer 6: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 7: 23338 Step\n",
      "End of epoch 14, LoraLayer 7: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 8: 23338 Step\n",
      "End of epoch 14, LoraLayer 8: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 9: 23338 Step\n",
      "End of epoch 14, LoraLayer 9: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 10: 23338 Step\n",
      "End of epoch 14, LoraLayer 10: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 11: 23338 Step\n",
      "End of epoch 14, LoraLayer 11: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 12: 23338 Step\n",
      "End of epoch 14, LoraLayer 12: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 13: 23338 Step\n",
      "End of epoch 14, LoraLayer 13: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 14: 23338 Step\n",
      "End of epoch 14, LoraLayer 14: Decay factor: 0.24979376792907715\n",
      "\n",
      "Testing loss: 2.302713632583618, acc: 0.10530000180006027\n",
      "\n",
      "1667/1667 [==============================] - 45s 27ms/step - loss: 0.3852 - accuracy: 0.8695 - val_loss: 2.3027 - val_accuracy: 0.1054\n",
      "Epoch 15/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.4597 - accuracy: 0.8436\n",
      "End of epoch 15, LoraLayer 0: 25005 Step\n",
      "End of epoch 15, LoraLayer 0: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 1: 25005 Step\n",
      "End of epoch 15, LoraLayer 1: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 2: 25005 Step\n",
      "End of epoch 15, LoraLayer 2: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 3: 25005 Step\n",
      "End of epoch 15, LoraLayer 3: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 4: 25005 Step\n",
      "End of epoch 15, LoraLayer 4: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 5: 25005 Step\n",
      "End of epoch 15, LoraLayer 5: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 6: 25005 Step\n",
      "End of epoch 15, LoraLayer 6: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 7: 25005 Step\n",
      "End of epoch 15, LoraLayer 7: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 8: 25005 Step\n",
      "End of epoch 15, LoraLayer 8: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 9: 25005 Step\n",
      "End of epoch 15, LoraLayer 9: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 10: 25005 Step\n",
      "End of epoch 15, LoraLayer 10: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 11: 25005 Step\n",
      "End of epoch 15, LoraLayer 11: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 12: 25005 Step\n",
      "End of epoch 15, LoraLayer 12: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 13: 25005 Step\n",
      "End of epoch 15, LoraLayer 13: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 14: 25005 Step\n",
      "End of epoch 15, LoraLayer 14: Decay factor: 0.18727970123291016\n",
      "\n",
      "Testing loss: 2.3005106449127197, acc: 0.12549999356269836\n",
      "\n",
      "1667/1667 [==============================] - 46s 27ms/step - loss: 0.4597 - accuracy: 0.8436 - val_loss: 2.3005 - val_accuracy: 0.1253\n",
      "Epoch 16/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.5469 - accuracy: 0.8149\n",
      "End of epoch 16, LoraLayer 0: 26672 Step\n",
      "End of epoch 16, LoraLayer 0: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 1: 26672 Step\n",
      "End of epoch 16, LoraLayer 1: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 2: 26672 Step\n",
      "End of epoch 16, LoraLayer 2: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 3: 26672 Step\n",
      "End of epoch 16, LoraLayer 3: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 4: 26672 Step\n",
      "End of epoch 16, LoraLayer 4: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 5: 26672 Step\n",
      "End of epoch 16, LoraLayer 5: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 6: 26672 Step\n",
      "End of epoch 16, LoraLayer 6: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 7: 26672 Step\n",
      "End of epoch 16, LoraLayer 7: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 8: 26672 Step\n",
      "End of epoch 16, LoraLayer 8: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 9: 26672 Step\n",
      "End of epoch 16, LoraLayer 9: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 10: 26672 Step\n",
      "End of epoch 16, LoraLayer 10: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 11: 26672 Step\n",
      "End of epoch 16, LoraLayer 11: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 12: 26672 Step\n",
      "End of epoch 16, LoraLayer 12: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 13: 26672 Step\n",
      "End of epoch 16, LoraLayer 13: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 14: 26672 Step\n",
      "End of epoch 16, LoraLayer 14: Decay factor: 0.12476563453674316\n",
      "\n",
      "Testing loss: 2.2836408615112305, acc: 0.1809999942779541\n",
      "\n",
      "1667/1667 [==============================] - 44s 27ms/step - loss: 0.5469 - accuracy: 0.8149 - val_loss: 2.2836 - val_accuracy: 0.1814\n",
      "Epoch 17/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.6148 - accuracy: 0.7903\n",
      "End of epoch 17, LoraLayer 0: 28339 Step\n",
      "End of epoch 17, LoraLayer 0: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 1: 28339 Step\n",
      "End of epoch 17, LoraLayer 1: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 2: 28339 Step\n",
      "End of epoch 17, LoraLayer 2: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 3: 28339 Step\n",
      "End of epoch 17, LoraLayer 3: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 4: 28339 Step\n",
      "End of epoch 17, LoraLayer 4: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 5: 28339 Step\n",
      "End of epoch 17, LoraLayer 5: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 6: 28339 Step\n",
      "End of epoch 17, LoraLayer 6: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 7: 28339 Step\n",
      "End of epoch 17, LoraLayer 7: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 8: 28339 Step\n",
      "End of epoch 17, LoraLayer 8: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 9: 28339 Step\n",
      "End of epoch 17, LoraLayer 9: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 10: 28339 Step\n",
      "End of epoch 17, LoraLayer 10: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 11: 28339 Step\n",
      "End of epoch 17, LoraLayer 11: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 12: 28339 Step\n",
      "End of epoch 17, LoraLayer 12: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 13: 28339 Step\n",
      "End of epoch 17, LoraLayer 13: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 14: 28339 Step\n",
      "End of epoch 17, LoraLayer 14: Decay factor: 0.06225156784057617\n",
      "\n",
      "Testing loss: 1.995307207107544, acc: 0.4442000091075897\n",
      "\n",
      "1667/1667 [==============================] - 48s 29ms/step - loss: 0.6149 - accuracy: 0.7903 - val_loss: 1.9953 - val_accuracy: 0.4439\n",
      "Epoch 18/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.7169 - accuracy: 0.7581\n",
      "End of epoch 18, LoraLayer 0: 30006 Step\n",
      "End of epoch 18, LoraLayer 0: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 1: 30006 Step\n",
      "End of epoch 18, LoraLayer 1: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 2: 30006 Step\n",
      "End of epoch 18, LoraLayer 2: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 3: 30006 Step\n",
      "End of epoch 18, LoraLayer 3: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 4: 30006 Step\n",
      "End of epoch 18, LoraLayer 4: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 5: 30006 Step\n",
      "End of epoch 18, LoraLayer 5: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 6: 30006 Step\n",
      "End of epoch 18, LoraLayer 6: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 7: 30006 Step\n",
      "End of epoch 18, LoraLayer 7: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 8: 30006 Step\n",
      "End of epoch 18, LoraLayer 8: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 9: 30006 Step\n",
      "End of epoch 18, LoraLayer 9: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 10: 30006 Step\n",
      "End of epoch 18, LoraLayer 10: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 11: 30006 Step\n",
      "End of epoch 18, LoraLayer 11: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 12: 30006 Step\n",
      "End of epoch 18, LoraLayer 12: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 13: 30006 Step\n",
      "End of epoch 18, LoraLayer 13: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 14: 30006 Step\n",
      "End of epoch 18, LoraLayer 14: Decay factor: 0.0\n",
      "\n",
      "Testing loss: 0.9181964993476868, acc: 0.6944000124931335\n",
      "\n",
      "1667/1667 [==============================] - 50s 30ms/step - loss: 0.7171 - accuracy: 0.7580 - val_loss: 0.9181 - val_accuracy: 0.6943\n",
      "Epoch 19/20\n",
      "1665/1667 [============================>.] - ETA: 0s - loss: 0.7179 - accuracy: 0.7615\n",
      "End of epoch 19, LoraLayer 0: 31673 Step\n",
      "End of epoch 19, LoraLayer 0: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 1: 31673 Step\n",
      "End of epoch 19, LoraLayer 1: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 2: 31673 Step\n",
      "End of epoch 19, LoraLayer 2: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 3: 31673 Step\n",
      "End of epoch 19, LoraLayer 3: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 4: 31673 Step\n",
      "End of epoch 19, LoraLayer 4: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 5: 31673 Step\n",
      "End of epoch 19, LoraLayer 5: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 6: 31673 Step\n",
      "End of epoch 19, LoraLayer 6: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 7: 31673 Step\n",
      "End of epoch 19, LoraLayer 7: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 8: 31673 Step\n",
      "End of epoch 19, LoraLayer 8: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 9: 31673 Step\n",
      "End of epoch 19, LoraLayer 9: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 10: 31673 Step\n",
      "End of epoch 19, LoraLayer 10: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 11: 31673 Step\n",
      "End of epoch 19, LoraLayer 11: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 12: 31673 Step\n",
      "End of epoch 19, LoraLayer 12: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 13: 31673 Step\n",
      "End of epoch 19, LoraLayer 13: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 14: 31673 Step\n",
      "End of epoch 19, LoraLayer 14: Decay factor: 0.0\n",
      "\n",
      "Testing loss: 0.7976213693618774, acc: 0.734499990940094\n",
      "\n",
      "1667/1667 [==============================] - 47s 28ms/step - loss: 0.7180 - accuracy: 0.7615 - val_loss: 0.7976 - val_accuracy: 0.7346\n",
      "Epoch 20/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.6305 - accuracy: 0.7887\n",
      "End of epoch 20, LoraLayer 0: 33340 Step\n",
      "End of epoch 20, LoraLayer 0: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 1: 33340 Step\n",
      "End of epoch 20, LoraLayer 1: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 2: 33340 Step\n",
      "End of epoch 20, LoraLayer 2: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 3: 33340 Step\n",
      "End of epoch 20, LoraLayer 3: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 4: 33340 Step\n",
      "End of epoch 20, LoraLayer 4: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 5: 33340 Step\n",
      "End of epoch 20, LoraLayer 5: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 6: 33340 Step\n",
      "End of epoch 20, LoraLayer 6: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 7: 33340 Step\n",
      "End of epoch 20, LoraLayer 7: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 8: 33340 Step\n",
      "End of epoch 20, LoraLayer 8: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 9: 33340 Step\n",
      "End of epoch 20, LoraLayer 9: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 10: 33340 Step\n",
      "End of epoch 20, LoraLayer 10: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 11: 33340 Step\n",
      "End of epoch 20, LoraLayer 11: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 12: 33340 Step\n",
      "End of epoch 20, LoraLayer 12: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 13: 33340 Step\n",
      "End of epoch 20, LoraLayer 13: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 14: 33340 Step\n",
      "End of epoch 20, LoraLayer 14: Decay factor: 0.0\n",
      "\n",
      "Testing loss: 0.7769083380699158, acc: 0.7461000084877014\n",
      "\n",
      "1667/1667 [==============================] - 48s 29ms/step - loss: 0.6305 - accuracy: 0.7887 - val_loss: 0.7769 - val_accuracy: 0.7461\n"
     ]
    }
   ],
   "source": [
    "history_exp24 = exp24_lora_vgg16.fit(x_train, y_train, batch_size=30, epochs=20, validation_data=(x_test, y_test), callbacks=[print_step_callback, TestCallback((x_test, y_test))] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "tNJCmN8QgDiO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 3s 9ms/step - loss: 0.7769 - accuracy: 0.7461\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7769083380699158, 0.7461000084877014]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 평가\n",
    "exp24_lora_vgg16.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "IrLS0MAbqMru"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 학습 및 검증 정확도 시각화\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# 정확도 그래프\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_exp24.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history_exp24.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# 손실 그래프\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_exp24.history['loss'], label='Training Loss')\n",
    "plt.plot(history_exp24.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "05hTVBWz_a7Q"
   },
   "source": [
    "## 2-5. (16, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "yAMaW8tq_a7a"
   },
   "outputs": [],
   "source": [
    "dataset_size = x_train.shape[0]\n",
    "batch_size = 30\n",
    "epochs = 20\n",
    "\n",
    "total_iteration= int(epochs * (dataset_size / batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "yQEGKcv1_a7a"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout, InputLayer\n",
    "import tensorflow as tf\n",
    "\n",
    "# 새로운 레이어를 추가하기 위한 레이어 리스트 초기화\n",
    "new_layers = []\n",
    "\n",
    "# Dense 레이어 카운터\n",
    "dense_layer_count = 0\n",
    "\n",
    "# VGG 모델의 각 레이어 순회\n",
    "for layer in best_vgg16.layers:\n",
    "    if isinstance(layer, InputLayer):\n",
    "        continue\n",
    "    if isinstance(layer, Conv2D):\n",
    "        # Conv2D 레이어를 ConvLoRALayer로 대체\n",
    "        conv_lora_layer = ConvLoRALayer00_cdn2(original_conv_layer=layer, rank=16, alpha=32, total_iteration = total_iteration, trainable=True)\n",
    "        new_layers.append(conv_lora_layer)\n",
    "    elif isinstance(layer, Dense):\n",
    "        dense_layer_count += 1\n",
    "        # 첫 번째와 두 번째 Dense 레이어에만 LoraLayer 적용\n",
    "        if dense_layer_count in [1, 2]:\n",
    "            dense_lora_layer = LoraLayer(original_layer=layer, rank=32, alpha=32, total_iteration=total_iteration, trainable=True)\n",
    "            new_layers.append(dense_lora_layer)\n",
    "        else:\n",
    "            new_layers.append(layer)  # 그대로 유지함\n",
    "    else:\n",
    "        # 다른 유형의 레이어는 그대로 유지\n",
    "        new_layers.append(layer)\n",
    "\n",
    "# 새로운 입력 텐서 생성\n",
    "input_tensor = best_vgg16.input\n",
    "\n",
    "# 새로운 모델 구성\n",
    "x = input_tensor\n",
    "for layer in new_layers:\n",
    "    x = layer(x)\n",
    "\n",
    "# 새로운 모델 생성\n",
    "exp25_lora_vgg16 = Model(inputs=input_tensor, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "_Nx7V_RG_a7a"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D\n",
    "\n",
    "# 새로 생성된 모델에서 Dense 레이어 중 LoRA가 적용되지 않은 레이어의 trainable을 False로 설정\n",
    "for layer in exp25_lora_vgg16.layers:\n",
    "    if isinstance(layer, Dense) and not isinstance(layer, LoraLayer):\n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QISgWsIB_a7b",
    "outputId": "63671162-e007-44da-f92a-49b173cff6c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " block1_conv1 (ConvLoRALaye  (None, 32, 32, 64)        11442     \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block1_conv2 (ConvLoRALaye  (None, 32, 32, 64)        55362     \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " block2_conv1 (ConvLoRALaye  (None, 16, 16, 128)       101506    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block2_conv2 (ConvLoRALaye  (None, 16, 16, 128)       184450    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " block3_conv1 (ConvLoRALaye  (None, 8, 8, 256)         350466    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block3_conv2 (ConvLoRALaye  (None, 8, 8, 256)         663810    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block3_conv3 (ConvLoRALaye  (None, 8, 8, 256)         663810    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 4, 4, 256)         0         \n",
      "                                                                 \n",
      " block4_conv1 (ConvLoRALaye  (None, 4, 4, 512)         1290754   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block4_conv2 (ConvLoRALaye  (None, 4, 4, 512)         2507266   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block4_conv3 (ConvLoRALaye  (None, 4, 4, 512)         2507266   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      " block5_conv1 (ConvLoRALaye  (None, 2, 2, 512)         2507266   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block5_conv2 (ConvLoRALaye  (None, 2, 2, 512)         2507266   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block5_conv3 (ConvLoRALaye  (None, 2, 2, 512)         2507266   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 1, 1, 512)         0         \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_6 (LoraLayer)         (None, 4096)              2248706   \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_7 (LoraLayer)         (None, 512)               2245122   \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20356888 (77.66 MB)\n",
      "Trainable params: 1438128 (5.49 MB)\n",
      "Non-trainable params: 18918760 (72.17 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "exp25_lora_vgg16.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3E9kf2WU_a7b",
    "outputId": "1a1abe8e-8ae7-4f7b-a46a-add76eb92b22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: input_3\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block1_conv1\n",
      "  Trainable parameters: 9648\n",
      "  Non-trainable parameters: 1794\n",
      "Layer: block1_conv2\n",
      "  Trainable parameters: 18432\n",
      "  Non-trainable parameters: 36930\n",
      "Layer: block1_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block2_conv1\n",
      "  Trainable parameters: 27648\n",
      "  Non-trainable parameters: 73858\n",
      "Layer: block2_conv2\n",
      "  Trainable parameters: 36864\n",
      "  Non-trainable parameters: 147586\n",
      "Layer: block2_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block3_conv1\n",
      "  Trainable parameters: 55296\n",
      "  Non-trainable parameters: 295170\n",
      "Layer: block3_conv2\n",
      "  Trainable parameters: 73728\n",
      "  Non-trainable parameters: 590082\n",
      "Layer: block3_conv3\n",
      "  Trainable parameters: 73728\n",
      "  Non-trainable parameters: 590082\n",
      "Layer: block3_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block4_conv1\n",
      "  Trainable parameters: 110592\n",
      "  Non-trainable parameters: 1180162\n",
      "Layer: block4_conv2\n",
      "  Trainable parameters: 147456\n",
      "  Non-trainable parameters: 2359810\n",
      "Layer: block4_conv3\n",
      "  Trainable parameters: 147456\n",
      "  Non-trainable parameters: 2359810\n",
      "Layer: block4_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block5_conv1\n",
      "  Trainable parameters: 147456\n",
      "  Non-trainable parameters: 2359810\n",
      "Layer: block5_conv2\n",
      "  Trainable parameters: 147456\n",
      "  Non-trainable parameters: 2359810\n",
      "Layer: block5_conv3\n",
      "  Trainable parameters: 147456\n",
      "  Non-trainable parameters: 2359810\n",
      "Layer: block5_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: flatten_2\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: dense_6\n",
      "  Trainable parameters: 147456\n",
      "  Non-trainable parameters: 2101250\n",
      "Layer: dropout_4\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: dense_7\n",
      "  Trainable parameters: 147456\n",
      "  Non-trainable parameters: 2097666\n",
      "Layer: dropout_5\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: dense_8\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 5130\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#모델의 각 레이어를 순회하며 파라미터 수를 계산\n",
    "for layer in exp25_lora_vgg16.layers:\n",
    "    trainable_count = np.sum([tf.size(w).numpy() for w in layer.trainable_weights])\n",
    "    non_trainable_count = np.sum([tf.size(w).numpy() for w in layer.non_trainable_weights])\n",
    "\n",
    "    print(f\"Layer: {layer.name}\")\n",
    "    print(f\"  Trainable parameters: {trainable_count}\")\n",
    "    print(f\"  Non-trainable parameters: {non_trainable_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "kuMPWKdD_a7b"
   },
   "outputs": [],
   "source": [
    "lora_layers = []  # lora 레이어들을 저장할 리스트\n",
    "\n",
    "for layer in exp25_lora_vgg16.layers:\n",
    "    if isinstance(layer, LoraLayer) or isinstance(layer, ConvLoRALayer00_cdn2):\n",
    "        lora_layers.append(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "2er5vDY4_a7b"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "class PrintCurrentStepCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, lora_layers):\n",
    "        super().__init__()\n",
    "        self.lora_layers = lora_layers\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print()\n",
    "        for i, lora_layer in enumerate(self.lora_layers):\n",
    "            current_step = lora_layer.current_step.value()\n",
    "            decay_factor = lora_layer.decay_factor.value()\n",
    "            print(f\"End of epoch {epoch + 1}, LoraLayer {i}: {current_step} Step\")\n",
    "            print(f\"End of epoch {epoch + 1}, LoraLayer {i}: Decay factor: {decay_factor}\")\n",
    "\n",
    "class TestCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, test_data):\n",
    "        self.test_data = test_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        x, y = self.test_data\n",
    "        loss, acc = self.model.evaluate(x, y, verbose=0)\n",
    "        print('\\nTesting loss: {}, acc: {}\\n'.format(loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "Pw6efjC2_a7c"
   },
   "outputs": [],
   "source": [
    "# 콜백 생성 시 lora_layers 딕셔너리의 값만 사용\n",
    "print_step_callback = PrintCurrentStepCallback(lora_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "ENCdYvNn_a7c"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# 모델 컴파일\n",
    "exp25_lora_vgg16.compile(optimizer=Adam(learning_rate=0.00005),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h_LWfZn7_a7c"
   },
   "source": [
    "### 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ubx3l9if_a7c",
    "outputId": "4c8bc6cb-79de-4456-d641-301e2d97ded9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.1140 - accuracy: 0.9640\n",
      "End of epoch 1, LoraLayer 0: 1667 Step\n",
      "End of epoch 1, LoraLayer 0: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 1: 1667 Step\n",
      "End of epoch 1, LoraLayer 1: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 2: 1667 Step\n",
      "End of epoch 1, LoraLayer 2: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 3: 1667 Step\n",
      "End of epoch 1, LoraLayer 3: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 4: 1667 Step\n",
      "End of epoch 1, LoraLayer 4: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 5: 1667 Step\n",
      "End of epoch 1, LoraLayer 5: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 6: 1667 Step\n",
      "End of epoch 1, LoraLayer 6: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 7: 1667 Step\n",
      "End of epoch 1, LoraLayer 7: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 8: 1667 Step\n",
      "End of epoch 1, LoraLayer 8: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 9: 1667 Step\n",
      "End of epoch 1, LoraLayer 9: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 10: 1667 Step\n",
      "End of epoch 1, LoraLayer 10: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 11: 1667 Step\n",
      "End of epoch 1, LoraLayer 11: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 12: 1667 Step\n",
      "End of epoch 1, LoraLayer 12: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 13: 1667 Step\n",
      "End of epoch 1, LoraLayer 13: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 14: 1667 Step\n",
      "End of epoch 1, LoraLayer 14: Decay factor: 1.0\n",
      "\n",
      "Testing loss: 2.302607297897339, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 54s 28ms/step - loss: 0.1141 - accuracy: 0.9640 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 2/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.0950 - accuracy: 0.9689\n",
      "End of epoch 2, LoraLayer 0: 3334 Step\n",
      "End of epoch 2, LoraLayer 0: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 1: 3334 Step\n",
      "End of epoch 2, LoraLayer 1: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 2: 3334 Step\n",
      "End of epoch 2, LoraLayer 2: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 3: 3334 Step\n",
      "End of epoch 2, LoraLayer 3: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 4: 3334 Step\n",
      "End of epoch 2, LoraLayer 4: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 5: 3334 Step\n",
      "End of epoch 2, LoraLayer 5: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 6: 3334 Step\n",
      "End of epoch 2, LoraLayer 6: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 7: 3334 Step\n",
      "End of epoch 2, LoraLayer 7: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 8: 3334 Step\n",
      "End of epoch 2, LoraLayer 8: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 9: 3334 Step\n",
      "End of epoch 2, LoraLayer 9: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 10: 3334 Step\n",
      "End of epoch 2, LoraLayer 10: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 11: 3334 Step\n",
      "End of epoch 2, LoraLayer 11: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 12: 3334 Step\n",
      "End of epoch 2, LoraLayer 12: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 13: 3334 Step\n",
      "End of epoch 2, LoraLayer 13: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 14: 3334 Step\n",
      "End of epoch 2, LoraLayer 14: Decay factor: 0.9999625086784363\n",
      "\n",
      "Testing loss: 2.302607297897339, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 48s 29ms/step - loss: 0.0950 - accuracy: 0.9689 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 3/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.0778 - accuracy: 0.9743\n",
      "End of epoch 3, LoraLayer 0: 5001 Step\n",
      "End of epoch 3, LoraLayer 0: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 1: 5001 Step\n",
      "End of epoch 3, LoraLayer 1: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 2: 5001 Step\n",
      "End of epoch 3, LoraLayer 2: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 3: 5001 Step\n",
      "End of epoch 3, LoraLayer 3: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 4: 5001 Step\n",
      "End of epoch 3, LoraLayer 4: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 5: 5001 Step\n",
      "End of epoch 3, LoraLayer 5: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 6: 5001 Step\n",
      "End of epoch 3, LoraLayer 6: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 7: 5001 Step\n",
      "End of epoch 3, LoraLayer 7: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 8: 5001 Step\n",
      "End of epoch 3, LoraLayer 8: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 9: 5001 Step\n",
      "End of epoch 3, LoraLayer 9: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 10: 5001 Step\n",
      "End of epoch 3, LoraLayer 10: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 11: 5001 Step\n",
      "End of epoch 3, LoraLayer 11: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 12: 5001 Step\n",
      "End of epoch 3, LoraLayer 12: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 13: 5001 Step\n",
      "End of epoch 3, LoraLayer 13: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 14: 5001 Step\n",
      "End of epoch 3, LoraLayer 14: Decay factor: 0.9374484419822693\n",
      "\n",
      "Testing loss: 2.302607297897339, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 47s 28ms/step - loss: 0.0778 - accuracy: 0.9743 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 4/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.0776 - accuracy: 0.9742\n",
      "End of epoch 4, LoraLayer 0: 6668 Step\n",
      "End of epoch 4, LoraLayer 0: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 1: 6668 Step\n",
      "End of epoch 4, LoraLayer 1: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 2: 6668 Step\n",
      "End of epoch 4, LoraLayer 2: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 3: 6668 Step\n",
      "End of epoch 4, LoraLayer 3: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 4: 6668 Step\n",
      "End of epoch 4, LoraLayer 4: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 5: 6668 Step\n",
      "End of epoch 4, LoraLayer 5: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 6: 6668 Step\n",
      "End of epoch 4, LoraLayer 6: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 7: 6668 Step\n",
      "End of epoch 4, LoraLayer 7: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 8: 6668 Step\n",
      "End of epoch 4, LoraLayer 8: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 9: 6668 Step\n",
      "End of epoch 4, LoraLayer 9: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 10: 6668 Step\n",
      "End of epoch 4, LoraLayer 10: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 11: 6668 Step\n",
      "End of epoch 4, LoraLayer 11: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 12: 6668 Step\n",
      "End of epoch 4, LoraLayer 12: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 13: 6668 Step\n",
      "End of epoch 4, LoraLayer 13: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 14: 6668 Step\n",
      "End of epoch 4, LoraLayer 14: Decay factor: 0.8749343752861023\n",
      "\n",
      "Testing loss: 2.302607297897339, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 44s 27ms/step - loss: 0.0777 - accuracy: 0.9742 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 5/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.0809 - accuracy: 0.9725\n",
      "End of epoch 5, LoraLayer 0: 8335 Step\n",
      "End of epoch 5, LoraLayer 0: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 1: 8335 Step\n",
      "End of epoch 5, LoraLayer 1: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 2: 8335 Step\n",
      "End of epoch 5, LoraLayer 2: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 3: 8335 Step\n",
      "End of epoch 5, LoraLayer 3: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 4: 8335 Step\n",
      "End of epoch 5, LoraLayer 4: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 5: 8335 Step\n",
      "End of epoch 5, LoraLayer 5: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 6: 8335 Step\n",
      "End of epoch 5, LoraLayer 6: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 7: 8335 Step\n",
      "End of epoch 5, LoraLayer 7: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 8: 8335 Step\n",
      "End of epoch 5, LoraLayer 8: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 9: 8335 Step\n",
      "End of epoch 5, LoraLayer 9: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 10: 8335 Step\n",
      "End of epoch 5, LoraLayer 10: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 11: 8335 Step\n",
      "End of epoch 5, LoraLayer 11: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 12: 8335 Step\n",
      "End of epoch 5, LoraLayer 12: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 13: 8335 Step\n",
      "End of epoch 5, LoraLayer 13: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 14: 8335 Step\n",
      "End of epoch 5, LoraLayer 14: Decay factor: 0.8124203085899353\n",
      "\n",
      "Testing loss: 2.302607297897339, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 44s 27ms/step - loss: 0.0809 - accuracy: 0.9725 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 6/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.0909 - accuracy: 0.9698\n",
      "End of epoch 6, LoraLayer 0: 10002 Step\n",
      "End of epoch 6, LoraLayer 0: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 1: 10002 Step\n",
      "End of epoch 6, LoraLayer 1: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 2: 10002 Step\n",
      "End of epoch 6, LoraLayer 2: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 3: 10002 Step\n",
      "End of epoch 6, LoraLayer 3: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 4: 10002 Step\n",
      "End of epoch 6, LoraLayer 4: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 5: 10002 Step\n",
      "End of epoch 6, LoraLayer 5: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 6: 10002 Step\n",
      "End of epoch 6, LoraLayer 6: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 7: 10002 Step\n",
      "End of epoch 6, LoraLayer 7: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 8: 10002 Step\n",
      "End of epoch 6, LoraLayer 8: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 9: 10002 Step\n",
      "End of epoch 6, LoraLayer 9: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 10: 10002 Step\n",
      "End of epoch 6, LoraLayer 10: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 11: 10002 Step\n",
      "End of epoch 6, LoraLayer 11: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 12: 10002 Step\n",
      "End of epoch 6, LoraLayer 12: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 13: 10002 Step\n",
      "End of epoch 6, LoraLayer 13: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 14: 10002 Step\n",
      "End of epoch 6, LoraLayer 14: Decay factor: 0.7499062418937683\n",
      "\n",
      "Testing loss: 2.302607297897339, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 46s 27ms/step - loss: 0.0909 - accuracy: 0.9698 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 7/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.1030 - accuracy: 0.9646\n",
      "End of epoch 7, LoraLayer 0: 11669 Step\n",
      "End of epoch 7, LoraLayer 0: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 1: 11669 Step\n",
      "End of epoch 7, LoraLayer 1: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 2: 11669 Step\n",
      "End of epoch 7, LoraLayer 2: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 3: 11669 Step\n",
      "End of epoch 7, LoraLayer 3: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 4: 11669 Step\n",
      "End of epoch 7, LoraLayer 4: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 5: 11669 Step\n",
      "End of epoch 7, LoraLayer 5: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 6: 11669 Step\n",
      "End of epoch 7, LoraLayer 6: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 7: 11669 Step\n",
      "End of epoch 7, LoraLayer 7: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 8: 11669 Step\n",
      "End of epoch 7, LoraLayer 8: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 9: 11669 Step\n",
      "End of epoch 7, LoraLayer 9: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 10: 11669 Step\n",
      "End of epoch 7, LoraLayer 10: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 11: 11669 Step\n",
      "End of epoch 7, LoraLayer 11: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 12: 11669 Step\n",
      "End of epoch 7, LoraLayer 12: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 13: 11669 Step\n",
      "End of epoch 7, LoraLayer 13: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 14: 11669 Step\n",
      "End of epoch 7, LoraLayer 14: Decay factor: 0.6873922348022461\n",
      "\n",
      "Testing loss: 2.3026070594787598, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 44s 27ms/step - loss: 0.1030 - accuracy: 0.9646 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 8/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.1183 - accuracy: 0.9595\n",
      "End of epoch 8, LoraLayer 0: 13336 Step\n",
      "End of epoch 8, LoraLayer 0: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 1: 13336 Step\n",
      "End of epoch 8, LoraLayer 1: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 2: 13336 Step\n",
      "End of epoch 8, LoraLayer 2: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 3: 13336 Step\n",
      "End of epoch 8, LoraLayer 3: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 4: 13336 Step\n",
      "End of epoch 8, LoraLayer 4: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 5: 13336 Step\n",
      "End of epoch 8, LoraLayer 5: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 6: 13336 Step\n",
      "End of epoch 8, LoraLayer 6: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 7: 13336 Step\n",
      "End of epoch 8, LoraLayer 7: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 8: 13336 Step\n",
      "End of epoch 8, LoraLayer 8: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 9: 13336 Step\n",
      "End of epoch 8, LoraLayer 9: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 10: 13336 Step\n",
      "End of epoch 8, LoraLayer 10: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 11: 13336 Step\n",
      "End of epoch 8, LoraLayer 11: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 12: 13336 Step\n",
      "End of epoch 8, LoraLayer 12: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 13: 13336 Step\n",
      "End of epoch 8, LoraLayer 13: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 14: 13336 Step\n",
      "End of epoch 8, LoraLayer 14: Decay factor: 0.6248781681060791\n",
      "\n",
      "Testing loss: 2.302607297897339, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 43s 26ms/step - loss: 0.1183 - accuracy: 0.9595 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 9/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.1385 - accuracy: 0.9516\n",
      "End of epoch 9, LoraLayer 0: 15003 Step\n",
      "End of epoch 9, LoraLayer 0: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 1: 15003 Step\n",
      "End of epoch 9, LoraLayer 1: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 2: 15003 Step\n",
      "End of epoch 9, LoraLayer 2: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 3: 15003 Step\n",
      "End of epoch 9, LoraLayer 3: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 4: 15003 Step\n",
      "End of epoch 9, LoraLayer 4: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 5: 15003 Step\n",
      "End of epoch 9, LoraLayer 5: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 6: 15003 Step\n",
      "End of epoch 9, LoraLayer 6: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 7: 15003 Step\n",
      "End of epoch 9, LoraLayer 7: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 8: 15003 Step\n",
      "End of epoch 9, LoraLayer 8: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 9: 15003 Step\n",
      "End of epoch 9, LoraLayer 9: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 10: 15003 Step\n",
      "End of epoch 9, LoraLayer 10: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 11: 15003 Step\n",
      "End of epoch 9, LoraLayer 11: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 12: 15003 Step\n",
      "End of epoch 9, LoraLayer 12: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 13: 15003 Step\n",
      "End of epoch 9, LoraLayer 13: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 14: 15003 Step\n",
      "End of epoch 9, LoraLayer 14: Decay factor: 0.5623641014099121\n",
      "\n",
      "Testing loss: 2.3026070594787598, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 44s 26ms/step - loss: 0.1388 - accuracy: 0.9516 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 10/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.1687 - accuracy: 0.9419\n",
      "End of epoch 10, LoraLayer 0: 16670 Step\n",
      "End of epoch 10, LoraLayer 0: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 1: 16670 Step\n",
      "End of epoch 10, LoraLayer 1: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 2: 16670 Step\n",
      "End of epoch 10, LoraLayer 2: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 3: 16670 Step\n",
      "End of epoch 10, LoraLayer 3: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 4: 16670 Step\n",
      "End of epoch 10, LoraLayer 4: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 5: 16670 Step\n",
      "End of epoch 10, LoraLayer 5: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 6: 16670 Step\n",
      "End of epoch 10, LoraLayer 6: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 7: 16670 Step\n",
      "End of epoch 10, LoraLayer 7: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 8: 16670 Step\n",
      "End of epoch 10, LoraLayer 8: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 9: 16670 Step\n",
      "End of epoch 10, LoraLayer 9: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 10: 16670 Step\n",
      "End of epoch 10, LoraLayer 10: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 11: 16670 Step\n",
      "End of epoch 10, LoraLayer 11: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 12: 16670 Step\n",
      "End of epoch 10, LoraLayer 12: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 13: 16670 Step\n",
      "End of epoch 10, LoraLayer 13: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 14: 16670 Step\n",
      "End of epoch 10, LoraLayer 14: Decay factor: 0.4998500347137451\n",
      "\n",
      "Testing loss: 2.302612781524658, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 43s 26ms/step - loss: 0.1687 - accuracy: 0.9419 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 11/20\n",
      "1665/1667 [============================>.] - ETA: 0s - loss: 0.2003 - accuracy: 0.9299\n",
      "End of epoch 11, LoraLayer 0: 18337 Step\n",
      "End of epoch 11, LoraLayer 0: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 1: 18337 Step\n",
      "End of epoch 11, LoraLayer 1: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 2: 18337 Step\n",
      "End of epoch 11, LoraLayer 2: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 3: 18337 Step\n",
      "End of epoch 11, LoraLayer 3: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 4: 18337 Step\n",
      "End of epoch 11, LoraLayer 4: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 5: 18337 Step\n",
      "End of epoch 11, LoraLayer 5: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 6: 18337 Step\n",
      "End of epoch 11, LoraLayer 6: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 7: 18337 Step\n",
      "End of epoch 11, LoraLayer 7: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 8: 18337 Step\n",
      "End of epoch 11, LoraLayer 8: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 9: 18337 Step\n",
      "End of epoch 11, LoraLayer 9: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 10: 18337 Step\n",
      "End of epoch 11, LoraLayer 10: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 11: 18337 Step\n",
      "End of epoch 11, LoraLayer 11: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 12: 18337 Step\n",
      "End of epoch 11, LoraLayer 12: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 13: 18337 Step\n",
      "End of epoch 11, LoraLayer 13: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 14: 18337 Step\n",
      "End of epoch 11, LoraLayer 14: Decay factor: 0.4373359680175781\n",
      "\n",
      "Testing loss: 2.3026058673858643, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 44s 26ms/step - loss: 0.2006 - accuracy: 0.9298 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 12/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.2472 - accuracy: 0.9138\n",
      "End of epoch 12, LoraLayer 0: 20004 Step\n",
      "End of epoch 12, LoraLayer 0: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 1: 20004 Step\n",
      "End of epoch 12, LoraLayer 1: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 2: 20004 Step\n",
      "End of epoch 12, LoraLayer 2: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 3: 20004 Step\n",
      "End of epoch 12, LoraLayer 3: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 4: 20004 Step\n",
      "End of epoch 12, LoraLayer 4: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 5: 20004 Step\n",
      "End of epoch 12, LoraLayer 5: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 6: 20004 Step\n",
      "End of epoch 12, LoraLayer 6: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 7: 20004 Step\n",
      "End of epoch 12, LoraLayer 7: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 8: 20004 Step\n",
      "End of epoch 12, LoraLayer 8: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 9: 20004 Step\n",
      "End of epoch 12, LoraLayer 9: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 10: 20004 Step\n",
      "End of epoch 12, LoraLayer 10: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 11: 20004 Step\n",
      "End of epoch 12, LoraLayer 11: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 12: 20004 Step\n",
      "End of epoch 12, LoraLayer 12: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 13: 20004 Step\n",
      "End of epoch 12, LoraLayer 13: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 14: 20004 Step\n",
      "End of epoch 12, LoraLayer 14: Decay factor: 0.37482190132141113\n",
      "\n",
      "Testing loss: 2.3026952743530273, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 45s 27ms/step - loss: 0.2472 - accuracy: 0.9138 - val_loss: 2.3027 - val_accuracy: 0.1000\n",
      "Epoch 13/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.3055 - accuracy: 0.8928\n",
      "End of epoch 13, LoraLayer 0: 21671 Step\n",
      "End of epoch 13, LoraLayer 0: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 1: 21671 Step\n",
      "End of epoch 13, LoraLayer 1: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 2: 21671 Step\n",
      "End of epoch 13, LoraLayer 2: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 3: 21671 Step\n",
      "End of epoch 13, LoraLayer 3: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 4: 21671 Step\n",
      "End of epoch 13, LoraLayer 4: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 5: 21671 Step\n",
      "End of epoch 13, LoraLayer 5: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 6: 21671 Step\n",
      "End of epoch 13, LoraLayer 6: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 7: 21671 Step\n",
      "End of epoch 13, LoraLayer 7: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 8: 21671 Step\n",
      "End of epoch 13, LoraLayer 8: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 9: 21671 Step\n",
      "End of epoch 13, LoraLayer 9: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 10: 21671 Step\n",
      "End of epoch 13, LoraLayer 10: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 11: 21671 Step\n",
      "End of epoch 13, LoraLayer 11: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 12: 21671 Step\n",
      "End of epoch 13, LoraLayer 12: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 13: 21671 Step\n",
      "End of epoch 13, LoraLayer 13: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 14: 21671 Step\n",
      "End of epoch 13, LoraLayer 14: Decay factor: 0.31230783462524414\n",
      "\n",
      "Testing loss: 2.302534818649292, acc: 0.09799999743700027\n",
      "\n",
      "1667/1667 [==============================] - 44s 26ms/step - loss: 0.3055 - accuracy: 0.8928 - val_loss: 2.3025 - val_accuracy: 0.0981\n",
      "Epoch 14/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.3676 - accuracy: 0.8725\n",
      "End of epoch 14, LoraLayer 0: 23338 Step\n",
      "End of epoch 14, LoraLayer 0: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 1: 23338 Step\n",
      "End of epoch 14, LoraLayer 1: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 2: 23338 Step\n",
      "End of epoch 14, LoraLayer 2: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 3: 23338 Step\n",
      "End of epoch 14, LoraLayer 3: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 4: 23338 Step\n",
      "End of epoch 14, LoraLayer 4: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 5: 23338 Step\n",
      "End of epoch 14, LoraLayer 5: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 6: 23338 Step\n",
      "End of epoch 14, LoraLayer 6: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 7: 23338 Step\n",
      "End of epoch 14, LoraLayer 7: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 8: 23338 Step\n",
      "End of epoch 14, LoraLayer 8: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 9: 23338 Step\n",
      "End of epoch 14, LoraLayer 9: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 10: 23338 Step\n",
      "End of epoch 14, LoraLayer 10: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 11: 23338 Step\n",
      "End of epoch 14, LoraLayer 11: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 12: 23338 Step\n",
      "End of epoch 14, LoraLayer 12: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 13: 23338 Step\n",
      "End of epoch 14, LoraLayer 13: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 14: 23338 Step\n",
      "End of epoch 14, LoraLayer 14: Decay factor: 0.24979376792907715\n",
      "\n",
      "Testing loss: 2.3027515411376953, acc: 0.10949999839067459\n",
      "\n",
      "1667/1667 [==============================] - 43s 26ms/step - loss: 0.3676 - accuracy: 0.8725 - val_loss: 2.3028 - val_accuracy: 0.1098\n",
      "Epoch 15/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.4370 - accuracy: 0.8488\n",
      "End of epoch 15, LoraLayer 0: 25005 Step\n",
      "End of epoch 15, LoraLayer 0: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 1: 25005 Step\n",
      "End of epoch 15, LoraLayer 1: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 2: 25005 Step\n",
      "End of epoch 15, LoraLayer 2: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 3: 25005 Step\n",
      "End of epoch 15, LoraLayer 3: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 4: 25005 Step\n",
      "End of epoch 15, LoraLayer 4: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 5: 25005 Step\n",
      "End of epoch 15, LoraLayer 5: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 6: 25005 Step\n",
      "End of epoch 15, LoraLayer 6: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 7: 25005 Step\n",
      "End of epoch 15, LoraLayer 7: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 8: 25005 Step\n",
      "End of epoch 15, LoraLayer 8: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 9: 25005 Step\n",
      "End of epoch 15, LoraLayer 9: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 10: 25005 Step\n",
      "End of epoch 15, LoraLayer 10: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 11: 25005 Step\n",
      "End of epoch 15, LoraLayer 11: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 12: 25005 Step\n",
      "End of epoch 15, LoraLayer 12: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 13: 25005 Step\n",
      "End of epoch 15, LoraLayer 13: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 14: 25005 Step\n",
      "End of epoch 15, LoraLayer 14: Decay factor: 0.18727970123291016\n",
      "\n",
      "Testing loss: 2.3004543781280518, acc: 0.11919999867677689\n",
      "\n",
      "1667/1667 [==============================] - 46s 28ms/step - loss: 0.4373 - accuracy: 0.8488 - val_loss: 2.3005 - val_accuracy: 0.1193\n",
      "Epoch 16/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.5245 - accuracy: 0.8200\n",
      "End of epoch 16, LoraLayer 0: 26672 Step\n",
      "End of epoch 16, LoraLayer 0: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 1: 26672 Step\n",
      "End of epoch 16, LoraLayer 1: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 2: 26672 Step\n",
      "End of epoch 16, LoraLayer 2: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 3: 26672 Step\n",
      "End of epoch 16, LoraLayer 3: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 4: 26672 Step\n",
      "End of epoch 16, LoraLayer 4: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 5: 26672 Step\n",
      "End of epoch 16, LoraLayer 5: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 6: 26672 Step\n",
      "End of epoch 16, LoraLayer 6: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 7: 26672 Step\n",
      "End of epoch 16, LoraLayer 7: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 8: 26672 Step\n",
      "End of epoch 16, LoraLayer 8: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 9: 26672 Step\n",
      "End of epoch 16, LoraLayer 9: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 10: 26672 Step\n",
      "End of epoch 16, LoraLayer 10: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 11: 26672 Step\n",
      "End of epoch 16, LoraLayer 11: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 12: 26672 Step\n",
      "End of epoch 16, LoraLayer 12: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 13: 26672 Step\n",
      "End of epoch 16, LoraLayer 13: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 14: 26672 Step\n",
      "End of epoch 16, LoraLayer 14: Decay factor: 0.12476563453674316\n",
      "\n",
      "Testing loss: 2.260749340057373, acc: 0.21050000190734863\n",
      "\n",
      "1667/1667 [==============================] - 45s 27ms/step - loss: 0.5248 - accuracy: 0.8199 - val_loss: 2.2608 - val_accuracy: 0.2104\n",
      "Epoch 17/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.6060 - accuracy: 0.7935\n",
      "End of epoch 17, LoraLayer 0: 28339 Step\n",
      "End of epoch 17, LoraLayer 0: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 1: 28339 Step\n",
      "End of epoch 17, LoraLayer 1: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 2: 28339 Step\n",
      "End of epoch 17, LoraLayer 2: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 3: 28339 Step\n",
      "End of epoch 17, LoraLayer 3: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 4: 28339 Step\n",
      "End of epoch 17, LoraLayer 4: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 5: 28339 Step\n",
      "End of epoch 17, LoraLayer 5: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 6: 28339 Step\n",
      "End of epoch 17, LoraLayer 6: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 7: 28339 Step\n",
      "End of epoch 17, LoraLayer 7: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 8: 28339 Step\n",
      "End of epoch 17, LoraLayer 8: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 9: 28339 Step\n",
      "End of epoch 17, LoraLayer 9: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 10: 28339 Step\n",
      "End of epoch 17, LoraLayer 10: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 11: 28339 Step\n",
      "End of epoch 17, LoraLayer 11: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 12: 28339 Step\n",
      "End of epoch 17, LoraLayer 12: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 13: 28339 Step\n",
      "End of epoch 17, LoraLayer 13: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 14: 28339 Step\n",
      "End of epoch 17, LoraLayer 14: Decay factor: 0.06225156784057617\n",
      "\n",
      "Testing loss: 1.9560397863388062, acc: 0.4309000074863434\n",
      "\n",
      "1667/1667 [==============================] - 45s 27ms/step - loss: 0.6059 - accuracy: 0.7936 - val_loss: 1.9561 - val_accuracy: 0.4308\n",
      "Epoch 18/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.6980 - accuracy: 0.7626\n",
      "End of epoch 18, LoraLayer 0: 30006 Step\n",
      "End of epoch 18, LoraLayer 0: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 1: 30006 Step\n",
      "End of epoch 18, LoraLayer 1: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 2: 30006 Step\n",
      "End of epoch 18, LoraLayer 2: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 3: 30006 Step\n",
      "End of epoch 18, LoraLayer 3: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 4: 30006 Step\n",
      "End of epoch 18, LoraLayer 4: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 5: 30006 Step\n",
      "End of epoch 18, LoraLayer 5: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 6: 30006 Step\n",
      "End of epoch 18, LoraLayer 6: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 7: 30006 Step\n",
      "End of epoch 18, LoraLayer 7: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 8: 30006 Step\n",
      "End of epoch 18, LoraLayer 8: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 9: 30006 Step\n",
      "End of epoch 18, LoraLayer 9: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 10: 30006 Step\n",
      "End of epoch 18, LoraLayer 10: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 11: 30006 Step\n",
      "End of epoch 18, LoraLayer 11: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 12: 30006 Step\n",
      "End of epoch 18, LoraLayer 12: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 13: 30006 Step\n",
      "End of epoch 18, LoraLayer 13: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 14: 30006 Step\n",
      "End of epoch 18, LoraLayer 14: Decay factor: 0.0\n",
      "\n",
      "Testing loss: 0.8685153126716614, acc: 0.7138000130653381\n",
      "\n",
      "1667/1667 [==============================] - 45s 27ms/step - loss: 0.6980 - accuracy: 0.7626 - val_loss: 0.8685 - val_accuracy: 0.7140\n",
      "Epoch 19/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.6967 - accuracy: 0.7690\n",
      "End of epoch 19, LoraLayer 0: 31673 Step\n",
      "End of epoch 19, LoraLayer 0: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 1: 31673 Step\n",
      "End of epoch 19, LoraLayer 1: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 2: 31673 Step\n",
      "End of epoch 19, LoraLayer 2: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 3: 31673 Step\n",
      "End of epoch 19, LoraLayer 3: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 4: 31673 Step\n",
      "End of epoch 19, LoraLayer 4: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 5: 31673 Step\n",
      "End of epoch 19, LoraLayer 5: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 6: 31673 Step\n",
      "End of epoch 19, LoraLayer 6: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 7: 31673 Step\n",
      "End of epoch 19, LoraLayer 7: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 8: 31673 Step\n",
      "End of epoch 19, LoraLayer 8: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 9: 31673 Step\n",
      "End of epoch 19, LoraLayer 9: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 10: 31673 Step\n",
      "End of epoch 19, LoraLayer 10: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 11: 31673 Step\n",
      "End of epoch 19, LoraLayer 11: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 12: 31673 Step\n",
      "End of epoch 19, LoraLayer 12: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 13: 31673 Step\n",
      "End of epoch 19, LoraLayer 13: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 14: 31673 Step\n",
      "End of epoch 19, LoraLayer 14: Decay factor: 0.0\n",
      "\n",
      "Testing loss: 0.782966136932373, acc: 0.7383999824523926\n",
      "\n",
      "1667/1667 [==============================] - 45s 27ms/step - loss: 0.6967 - accuracy: 0.7690 - val_loss: 0.7829 - val_accuracy: 0.7385\n",
      "Epoch 20/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.6110 - accuracy: 0.7946\n",
      "End of epoch 20, LoraLayer 0: 33340 Step\n",
      "End of epoch 20, LoraLayer 0: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 1: 33340 Step\n",
      "End of epoch 20, LoraLayer 1: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 2: 33340 Step\n",
      "End of epoch 20, LoraLayer 2: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 3: 33340 Step\n",
      "End of epoch 20, LoraLayer 3: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 4: 33340 Step\n",
      "End of epoch 20, LoraLayer 4: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 5: 33340 Step\n",
      "End of epoch 20, LoraLayer 5: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 6: 33340 Step\n",
      "End of epoch 20, LoraLayer 6: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 7: 33340 Step\n",
      "End of epoch 20, LoraLayer 7: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 8: 33340 Step\n",
      "End of epoch 20, LoraLayer 8: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 9: 33340 Step\n",
      "End of epoch 20, LoraLayer 9: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 10: 33340 Step\n",
      "End of epoch 20, LoraLayer 10: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 11: 33340 Step\n",
      "End of epoch 20, LoraLayer 11: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 12: 33340 Step\n",
      "End of epoch 20, LoraLayer 12: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 13: 33340 Step\n",
      "End of epoch 20, LoraLayer 13: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 14: 33340 Step\n",
      "End of epoch 20, LoraLayer 14: Decay factor: 0.0\n",
      "\n",
      "Testing loss: 0.7383261322975159, acc: 0.7605999708175659\n",
      "\n",
      "1667/1667 [==============================] - 45s 27ms/step - loss: 0.6110 - accuracy: 0.7946 - val_loss: 0.7383 - val_accuracy: 0.7605\n"
     ]
    }
   ],
   "source": [
    "history_exp25 = exp25_lora_vgg16.fit(x_train, y_train, batch_size=30, epochs=20, validation_data=(x_test, y_test), callbacks=[print_step_callback, TestCallback((x_test, y_test))] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LAh8O2E__a7c",
    "outputId": "7913fd46-3d5e-43c3-a21b-37e64438053b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 3s 10ms/step - loss: 0.7383 - accuracy: 0.7606\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7383261322975159, 0.7605999708175659]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 평가\n",
    "exp25_lora_vgg16.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "id": "GFAWHfeB_a7c",
    "outputId": "42b5f2b2-e3fe-4e09-81b9-84460b7ed130"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA+kAAAHWCAYAAAALjsguAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAACtXElEQVR4nOzdZ3QUZRuH8WvTNj2hpBFCR3ozdKUKIigKWBCVpthREXlVLIhYsICiomKh2CiKgAUFKVJFQSFI750UAqSTtpn3w5KVkAAJJJmU/++cOTv77JR7h+jMvU+zGIZhICIiIiIiIiKmczI7ABERERERERGxU5IuIiIiIiIiUkIoSRcREREREREpIZSki4iIiIiIiJQQStJFRERERERESggl6SIiIiIiIiIlhJJ0ERERERERkRJCSbqIiIiIiIhICaEkXURERERERKSEUJIuJcqQIUOoUaPGZe07duxYLBZL4QZUwhw8eBCLxcKMGTOK/dwWi4WxY8c63s+YMQOLxcLBgwcvuW+NGjUYMmRIocZzJX8rIiJSNui54eL03PAfPTdIaaIkXfLFYrHka1mxYoXZoZZ7jz/+OBaLhb17915wm+effx6LxcK///5bjJEV3PHjxxk7diwRERFmh5KnHTt2YLFYcHd3Jy4uzuxwRERKDD03lB56biha2T+UTJgwwexQpBRxMTsAKR2++uqrHO+//PJLlixZkqu8QYMGV3Sezz77jKysrMva94UXXuDZZ5+9ovOXBXfffTcffPABM2fOZMyYMXluM2vWLJo0aULTpk0v+zwDBw7kzjvvxGq1XvYxLuX48eO8/PLL1KhRg+bNm+f47Er+VgrL119/TXBwMKdPn2bu3LkMGzbM1HhEREoKPTeUHnpuECl5lKRLvtxzzz053v/5558sWbIkV/n5UlJS8PT0zPd5XF1dLys+ABcXF1xc9Cfdpk0b6tSpw6xZs/K82a5bt44DBw7wxhtvXNF5nJ2dcXZ2vqJjXIkr+VspDIZhMHPmTO666y4OHDjAN998U2KT9OTkZLy8vMwOQ0TKET03lB56bhApedTcXQpN586dady4Mf/88w8dO3bE09OT5557DoAffviBG2+8kSpVqmC1WqlduzavvPIKNpstxzHO7y90bhOhTz/9lNq1a2O1WmnVqhUbNmzIsW9efcssFgvDhw9nwYIFNG7cGKvVSqNGjVi0aFGu+FesWEHLli1xd3endu3afPLJJ/nur7Z69Wpuv/12qlWrhtVqJSwsjCeffJIzZ87k+n7e3t4cO3aMPn364O3tTUBAAKNGjcp1LeLi4hgyZAh+fn74+/szePDgfDepvvvuu9m5cycbN27M9dnMmTOxWCwMGDCA9PR0xowZQ3h4OH5+fnh5edGhQwd+//33S54jr75lhmHw6quvUrVqVTw9PenSpQvbtm3Lte+pU6cYNWoUTZo0wdvbG19fX3r27MnmzZsd26xYsYJWrVoBMHToUEfTyOx+dXn1LUtOTuapp54iLCwMq9VKvXr1mDBhAoZh5NiuIH8XF7J27VoOHjzInXfeyZ133smqVas4evRoru2ysrJ47733aNKkCe7u7gQEBHDDDTfw999/59ju66+/pnXr1nh6elKhQgU6duzIb7/9liPmc/v2ZTu/3172v8vKlSt55JFHCAwMpGrVqgAcOnSIRx55hHr16uHh4UGlSpW4/fbb8+wfGBcXx5NPPkmNGjWwWq1UrVqVQYMGERsbS1JSEl5eXjzxxBO59jt69CjOzs6MHz8+n1dSRMorPTfouaE8PTdcSkxMDPfddx9BQUG4u7vTrFkzvvjii1zbzZ49m/DwcHx8fPD19aVJkya89957js8zMjJ4+eWXqVu3Lu7u7lSqVIlrr72WJUuWFFqsUvT086EUqpMnT9KzZ0/uvPNO7rnnHoKCggD7/5i9vb0ZOXIk3t7eLF++nDFjxpCQkMDbb799yePOnDmTxMREHnzwQSwWC2+99Rb9+vVj//79l/xldM2aNcybN49HHnkEHx8f3n//fW699VYOHz5MpUqVANi0aRM33HADISEhvPzyy9hsNsaNG0dAQEC+vvd3331HSkoKDz/8MJUqVWL9+vV88MEHHD16lO+++y7HtjabjR49etCmTRsmTJjA0qVLmThxIrVr1+bhhx8G7DetW265hTVr1vDQQw/RoEED5s+fz+DBg/MVz913383LL7/MzJkzufrqq3Oc+9tvv6VDhw5Uq1aN2NhYPv/8cwYMGMD9999PYmIiU6dOpUePHqxfvz5XU7FLGTNmDK+++iq9evWiV69ebNy4keuvv5709PQc2+3fv58FCxZw++23U7NmTaKjo/nkk0/o1KkT27dvp0qVKjRo0IBx48YxZswYHnjgATp06ABA+/bt8zy3YRjcfPPN/P7779x33300b96cxYsX87///Y9jx47x7rvv5tg+P38XF/PNN99Qu3ZtWrVqRePGjfH09GTWrFn873//y7Hdfffdx4wZM+jZsyfDhg0jMzOT1atX8+eff9KyZUsAXn75ZcaOHUv79u0ZN24cbm5u/PXXXyxfvpzrr78+39f/XI888ggBAQGMGTOG5ORkADZs2MAff/zBnXfeSdWqVTl48CAff/wxnTt3Zvv27Y7aq6SkJDp06MCOHTu49957ufrqq4mNjeXHH3/k6NGjNG/enL59+zJnzhzeeeedHDUjs2bNwjAM7r777suKW0TKFz036LmhvDw3XMyZM2fo3Lkze/fuZfjw4dSsWZPvvvuOIUOGEBcX5/hRfMmSJQwYMIDrrruON998E7CPj7N27VrHNmPHjmX8+PEMGzaM1q1bk5CQwN9//83GjRvp3r37FcUpxcgQuQyPPvqocf6fT6dOnQzAmDJlSq7tU1JScpU9+OCDhqenp5GamuooGzx4sFG9enXH+wMHDhiAUalSJePUqVOO8h9++MEAjJ9++slR9tJLL+WKCTDc3NyMvXv3Oso2b95sAMYHH3zgKOvdu7fh6elpHDt2zFG2Z88ew8XFJdcx85LX9xs/frxhsViMQ4cO5fh+gDFu3Lgc27Zo0cIIDw93vF+wYIEBGG+99ZajLDMz0+jQoYMBGNOnT79kTK1atTKqVq1q2Gw2R9miRYsMwPjkk08cx0xLS8ux3+nTp42goCDj3nvvzVEOGC+99JLj/fTp0w3AOHDggGEYhhETE2O4ubkZN954o5GVleXY7rnnnjMAY/DgwY6y1NTUHHEZhv3f2mq15rg2GzZsuOD3Pf9vJfuavfrqqzm2u+222wyLxZLjbyC/fxcXkp6eblSqVMl4/vnnHWV33XWX0axZsxzbLV++3ACMxx9/PNcxsq/Rnj17DCcnJ6Nv3765rsm51/H865+tevXqOa5t9r/Ltddea2RmZubYNq+/03Xr1hmA8eWXXzrKxowZYwDGvHnzLhj34sWLDcD49ddfc3zetGlTo1OnTrn2E5HyTc8Nl/5+em6wK2vPDdl/k2+//fYFt5k0aZIBGF9//bWjLD093WjXrp3h7e1tJCQkGIZhGE888YTh6+ub6/5+rmbNmhk33njjRWOSkk/N3aVQWa1Whg4dmqvcw8PDsZ6YmEhsbCwdOnQgJSWFnTt3XvK4/fv3p0KFCo732b+O7t+//5L7duvWjdq1azveN23aFF9fX8e+NpuNpUuX0qdPH6pUqeLYrk6dOvTs2fOSx4ec3y85OZnY2Fjat2+PYRhs2rQp1/YPPfRQjvcdOnTI8V1++eUXXFxcHL+Qg70v12OPPZaveMDeH/Do0aOsWrXKUTZz5kzc3Ny4/fbbHcd0c3MD7M2yT506RWZmJi1btsyzydvFLF26lPT0dB577LEcTf1GjBiRa1ur1YqTk/1/PzabjZMnT+Lt7U29evUKfN5sv/zyC87Ozjz++OM5yp966ikMw+DXX3/NUX6pv4uL+fXXXzl58iQDBgxwlA0YMIDNmzfnaKb3/fffY7FYeOmll3IdI/saLViwgKysLMaMGeO4Judvcznuv//+XH3/zv07zcjI4OTJk9SpUwd/f/8c1/3777+nWbNm9O3b94Jxd+vWjSpVqvDNN984Ptu6dSv//vvvJfuciohk03ODnhvKw3NDfmIJDg7O8Vzh6urK448/TlJSEitXrgTA39+f5OTkizZd9/f3Z9u2bezZs+eK4xLzKEmXQhUaGur4n/e5tm3bRt++ffHz88PX15eAgADHg3x8fPwlj1utWrUc77NvvKdPny7wvtn7Z+8bExPDmTNnqFOnTq7t8irLy+HDhxkyZAgVK1Z09Bfr1KkTkPv7ZfdLvlA8YO87HBISgre3d47t6tWrl694AO68806cnZ2ZOXMmAKmpqcyfP5+ePXvmeHD54osvaNq0qaPfUkBAAAsXLszXv8u5Dh06BEDdunVzlAcEBOQ4H9hv7O+++y5169bFarVSuXJlAgIC+Pfffwt83nPPX6VKFXx8fHKUZ48cnB1ftkv9XVzM119/Tc2aNbFarezdu5e9e/dSu3ZtPD09cySt+/bto0qVKlSsWPGCx9q3bx9OTk40bNjwkuctiJo1a+YqO3PmDGPGjHH0vcu+7nFxcTmu+759+2jcuPFFj+/k5MTdd9/NggULSElJAexdANzd3R0PcyIil6LnBj03lIfnhvzEUrdu3Vw/1p8fyyOPPMJVV11Fz549qVq1Kvfee2+ufvHjxo0jLi6Oq666iiZNmvC///2vxE+dJ7kpSZdCde4vw9ni4uLo1KkTmzdvZty4cfz0008sWbLE0ZcmP9NhXGg0UOO8gT0Ke9/8sNlsdO/enYULF/LMM8+wYMEClixZ4hio5PzvV1wjmwYGBtK9e3e+//57MjIy+Omnn0hMTMzRV/jrr79myJAh1K5dm6lTp7Jo0SKWLFlC165di3Saktdff52RI0fSsWNHvv76axYvXsySJUto1KhRsU2Pcrl/FwkJCfz0008cOHCAunXrOpaGDRuSkpLCzJkzC+1vKz/OHzgoW17/LT722GO89tpr3HHHHXz77bf89ttvLFmyhEqVKl3WdR80aBBJSUksWLDAMdr9TTfdhJ+fX4GPJSLlk54b9NyQH6X5uaEwBQYGEhERwY8//ujoT9+zZ88cYw907NiRffv2MW3aNBo3bsznn3/O1Vdfzeeff15sccqV08BxUuRWrFjByZMnmTdvHh07dnSUHzhwwMSo/hMYGIi7uzt79+7N9VleZefbsmULu3fv5osvvmDQoEGO8isZRbN69eosW7aMpKSkHL+K79q1q0DHufvuu1m0aBG//vorM2fOxNfXl969ezs+nzt3LrVq1WLevHk5mprl1Tw7PzED7Nmzh1q1ajnKT5w4ketX5rlz59KlSxemTp2aozwuLo7KlSs73hekuXf16tVZunQpiYmJOX4Vz24WmR3flZo3bx6pqal8/PHHOWIF+7/PCy+8wNq1a7n22mupXbs2ixcv5tSpUxesTa9duzZZWVls3779ogPuVKhQIdcovenp6URGRuY79rlz5zJ48GAmTpzoKEtNTc113Nq1a7N169ZLHq9x48a0aNGCb775hqpVq3L48GE++OCDfMcjIpIXPTcUnJ4b7Eric0N+Y/n333/JysrKUZueVyxubm707t2b3r17k5WVxSOPPMInn3zCiy++6GjJUbFiRYYOHcrQoUNJSkqiY8eOjB07tsROFSu5qSZdilz2L4/n/tKYnp7ORx99ZFZIOTg7O9OtWzcWLFjA8ePHHeV79+7N1R/pQvtDzu9nGEaO6TAKqlevXmRmZvLxxx87ymw2W4EToD59+uDp6clHH33Er7/+Sr9+/XB3d79o7H/99Rfr1q0rcMzdunXD1dWVDz74IMfxJk2alGtbZ2fnXL88f/fddxw7dixHWfbc3vmZQqZXr17YbDYmT56co/zdd9/FYrHku5/gpXz99dfUqlWLhx56iNtuuy3HMmrUKLy9vR1N3m+99VYMw+Dll1/OdZzs79+nTx+cnJwYN25crtqAc69R7dq1c/QTBPj0008vWJOel7yu+wcffJDrGLfeeiubN29m/vz5F4w728CBA/ntt9+YNGkSlSpVKrTrLCLll54bCk7PDXYl8bkhP3r16kVUVBRz5sxxlGVmZvLBBx/g7e3t6Apx8uTJHPs5OTnRtGlTANLS0vLcxtvbmzp16jg+l9JBNelS5Nq3b0+FChUYPHgwjz/+OBaLha+++qpYmwddytixY/ntt9+45pprePjhhx3/027cuDEREREX3bd+/frUrl2bUaNGcezYMXx9ffn++++vqI9S7969ueaaa3j22Wc5ePAgDRs2ZN68eQXud+Xt7U2fPn0c/cvOnxbrpptuYt68efTt25cbb7yRAwcOMGXKFBo2bEhSUlKBzpU9b+v48eO56aab6NWrF5s2beLXX3/NVeN80003MW7cOIYOHUr79u3ZsmUL33zzTY5f0sGemPr7+zNlyhR8fHzw8vKiTZs2efa37t27N126dOH555/n4MGDNGvWjN9++40ffviBESNG5Bjs5XIdP36c33//PdcgM9msVis9evTgu+++4/3336dLly4MHDiQ999/nz179nDDDTeQlZXF6tWr6dKlC8OHD6dOnTo8//zzvPLKK3To0IF+/fphtVrZsGEDVapUccw3PmzYMB566CFuvfVWunfvzubNm1m8eHGua3sxN910E1999RV+fn40bNiQdevWsXTp0lxTx/zvf/9j7ty53H777dx7772Eh4dz6tQpfvzxR6ZMmUKzZs0c29511108/fTTzJ8/n4cffviSUxuJiFyKnhsKTs8NdiXtueFcy5YtIzU1NVd5nz59eOCBB/jkk08YMmQI//zzDzVq1GDu3LmsXbuWSZMmOWr6hw0bxqlTp+jatStVq1bl0KFDfPDBBzRv3tzRf71hw4Z07tyZ8PBwKlasyN9//83cuXMZPnx4oX4fKWLFMIK8lEEXmkqlUaNGeW6/du1ao23btoaHh4dRpUoV4+mnn3ZM4fT77787trvQVCp5TVvBeVN7XGgqlUcffTTXvudPW2UYhrFs2TKjRYsWhpubm1G7dm3j888/N5566inD3d39AlfhP9u3bze6detmeHt7G5UrVzbuv/9+x9Qc504DMnjwYMPLyyvX/nnFfvLkSWPgwIGGr6+v4efnZwwcONDYtGlTvqdSybZw4UIDMEJCQvKc4uv11183qlevblitVqNFixbGzz//nOvfwTAuPZWKYRiGzWYzXn75ZSMkJMTw8PAwOnfubGzdujXX9U5NTTWeeuopx3bXXHONsW7dOqNTp065pu/64YcfjIYNGzqmtcn+7nnFmJiYaDz55JNGlSpVDFdXV6Nu3brG22+/nWNql+zvkt+/i3NNnDjRAIxly5ZdcJsZM2YYgPHDDz8YhmGfrubtt9826tevb7i5uRkBAQFGz549jX/++SfHftOmTTNatGhhWK1Wo0KFCkanTp2MJUuWOD632WzGM888Y1SuXNnw9PQ0evToYezdu/eCU7Bt2LAhV2ynT582hg4dalSuXNnw9vY2evToYezcuTPP733y5Elj+PDhRmhoqOHm5mZUrVrVGDx4sBEbG5vruL169TIA448//rjgdRGR8k3PDTnpucGurD83GMZ/f5MXWr766ivDMAwjOjracY92c3MzmjRpkuvfbe7cucb1119vBAYGGm5ubka1atWMBx980IiMjHRs8+qrrxqtW7c2/P39DQ8PD6N+/frGa6+9ZqSnp180TilZLIZRgn6WFClh+vTpo2ksRC6hb9++bNmyJV99MUVEyjI9N4hIYVCfdJGzzpw5k+P9nj17+OWXX+jcubM5AYmUApGRkSxcuJCBAweaHYqISLHSc4OIFBXVpIucFRISwpAhQ6hVqxaHDh3i448/Ji0tjU2bNuWaw1OkvDtw4ABr167l888/Z8OGDezbt4/g4GCzwxIRKTZ6bhCRoqKB40TOuuGGG5g1axZRUVFYrVbatWvH66+/rhutSB5WrlzJ0KFDqVatGl988YUSdBEpd/TcICJFRTXpIiIiIiIiIiWE+qSLiIiIiIiIlBBK0kVERERERERKiHLXJz0rK4vjx4/j4+ODxWIxOxwREREMwyAxMZEqVarg5KTfzwuD7vciIlKSFOReX+6S9OPHjxMWFmZ2GCIiIrkcOXKEqlWrmh1GmaD7vYiIlET5udeXuyTdx8cHsF8cX19fk6MRERGBhIQEwsLCHPcouXK634uISElSkHt9uUvSs5u8+fr66qYtIiIlipplFx7d70VEpCTKz73e1I5vq1atonfv3lSpUgWLxcKCBQsuuc+KFSu4+uqrsVqt1KlThxkzZhR5nCIiIiIiIiLFwdQkPTk5mWbNmvHhhx/ma/sDBw5w44030qVLFyIiIhgxYgTDhg1j8eLFRRypiIiIiIiISNEztbl7z5496dmzZ763nzJlCjVr1mTixIkANGjQgDVr1vDuu+/So0ePogpTREREREREpFiUqnle1q1bR7du3XKU9ejRg3Xr1l1wn7S0NBISEnIsIiIiIiIiIiVRqUrSo6KiCAoKylEWFBREQkICZ86cyXOf8ePH4+fn51g0HYuIiIiIiIiUVKUqSb8co0ePJj4+3rEcOXLE7JBERERERERE8lSqpmALDg4mOjo6R1l0dDS+vr54eHjkuY/VasVqtRZHeCIiIiIiIiJXpFTVpLdr145ly5blKFuyZAnt2rUzKSIRERERERGRwmNqkp6UlERERAQRERGAfYq1iIgIDh8+DNibqg8aNMix/UMPPcT+/ft5+umn2blzJx999BHffvstTz75pBnhi4iIiIiIiBQqU5P0v//+mxYtWtCiRQsARo4cSYsWLRgzZgwAkZGRjoQdoGbNmixcuJAlS5bQrFkzJk6cyOeff67p10RERERERKRMsBiGYZgdRHFKSEjAz8+P+Ph4fH19zQ5HRERE96YioGsqIiIlSUHuS6WqT7qIiIiIiIhIWaYkXURERERERKSEKFVTsEneUjNsRCekEhWfStTZ18j4VKIT7K+pGbYL7muxWHK+z/FZ3uvOFgtWF2esrk64uzpjdcn56u7qhNUl52v259Zztzt7DA9XZ7ysLnhbXXB2yhmPiIiIqY6sh1/+Z3YUIqVEHr1o8+xZm9d2+Ti8JY83OZ5lL/DwyvnPl0YesZ1fZuR4ueB+Tk7g5AJOruDsCk7O9nUnl7PvXc5Zdz5nO5fzPjv7ecVaUOc6sPpc9FJI2aYkvQQzDIOE1ExHsh19Nvm2J+JniEpIIyr+DKdTMswOtdB4utkTdh+rC97u9sQ9z/dn1x2L+3+vPlZXPNyczf4qIiJSFqQlQGSE2VGISHni7AY1O0K9XvbFN8TsiKSYKUk3gWEYJKZlciIxjZiENE4kpRGTkMqJpDROJKQ5asOjElJJSb9wLfi53F2dCPZ1J9jP/eyrB8G+VoL93PG2uv537nN+Djz/h80cvyWe8+H5P2zabAZpmVmkZtgcr6mZNtIyshyvaee8Tz37Ps/XDPtrui0LgJR0GynpNk4kpuXre1+Il5szQWevRZBjsdrf+9nfB/pYcXVWjw8REbmIkOZw91yzoxApRS7QKvKijSXz05LSyHM192fnf3jeZ3nVsOdZlkd8ubYzwMgCWyZkZUJWRs71rMyCvc9Mh6Pr4eRe2LvUviwcCVWuhvq9oN6NENjgvDikLFKSXogybVmcSk4nJjHNnoAnpp59TXO8ZpelZmTl+7j+nq45EvAgX3dC/OzJZsjZMj8P11xN10uTtEwbyWk2klIzSUzLICk1k+T0TBJTM0lKy7S/T8sk8ex6Uto5yznbJKVnYhiQnG5j/4lk9p9IvuA5LRao5GV1JO+Bvtk/cFgd60G+7lTwLN3XVkREroBXZajb3ewoRKQ8ObEbdi2Enb/A0Q1wfKN9Wf4qVKhhT9br94KwtuCsdK4s0hRsV2DW+sMs2hrlSMBPJaeRVYCr6WN1IcDXSqCPlQAf97Ov1pyJuK+7mm4XQFaWQXK6vZVCVEIqMee0TIhJtL9GJ9h/LMmw5e8fy83FiUAfq6P2PcjXnYDz3gf6WPFXMi8il0nThRU+XVMRKRMSo2H3Itj1C+z7HWzntDb1qAhX9bA3ia/dFaze5sUpl1SQ+5J+erkCB2KTWbn7RI4yJwtU8rYn3tlJd6CP+9lXK4G+VgK87e+VfBc+JycLPu6u+Li7Uivgwv+jysoyOJ2STlSCfYC96IS0HIl8VIK9C8LJ5HTSM7M4evoMR0+fuei53Zyd7P/OvlaCfNztr3kk9KqZFxEREZF88QmC8MH2JT0Z9i2HnQvtifuZU7B5ln1xtkKtzvYa9qt62veTUks16Vdg0+HT7IlOOqc23EolL6tGKC9D0jJtxCRkd1n4rxY+OiGN6AR714XohNQCDd7n6mwhxM+D2gFe1A7wpk6gN7UDvakd4E1FL7ci/DYiUlKp1rfw6ZqKSJlmy4Qjf9qbxO9aCKcPnvOhBaq2tNewN+5nbyIvpivIfUlJukghSMu0/TfuQELq2Vd7Ah+T+F9CfzI5/aLHqeDpSu0Ae8JeO9DLsR5W0VM//oiUYbo3FT5dUxEpNwwDYnb814/9+Mb/PnPzgQdXQqXa5sUngJq7ixQ7q4szVSt4UrWC50W3S8/M4kRSGkdOpbDvRBL7YpLtryeSOHraPp3e34dO8/eh0zn2c3N2omZlrxyJe+0Ab2oFeOFl1X/GIiIiIuWWxQJBDe1Lx/9BwnHY9Sus/xRO7IS/PoFeb5kdpRSAnu5FipGbixOh/h6E+nvQtlalHJ+dSbexPzaJfSeS2ReTdDZ5T2b/iSTSMrPYFZ3IrujEXMcM8XOnTqA3DUN8aVjFl0ZVfKlZ2Vs17yIiIiLlkW8VaHUfVKwJX/WFiJlw3Ytg9TE7MsknJekiJYSHmzONqvjRqIpfjnJblsHxuDPsPZF0Nnm3177vP5FEbFI6kfGpRMansnpP7H/HcnWmfogPjar40jDEj0ZVfKkX7IO7qwYrFBERESkXanaGSnXh5B7YPBta3292RJJPStJFSjhnJwthFT0Jq+hJl3qBOT6LS0ln34lkdkcnsu14PNuOJ7AzMpEzGTY2HY5j0+G4HMepE+DtqG1vWMWXRiF++Hm6FvM3EhEREZEi5+RkT8x/fRrWfwathtmbxkuJpyRdpBTz93QjvLob4dUrOMpsWQYHYpPZdjye7ccT2HY8gW3H4zmdkuFoMj9/0zHH9qH+HjSq4nu2Ft+XRqG+BPu6a5o4ERERkdKu2QBYNg5id8GBVVCrk9kRST4oSRcpY5ydLNQJtE/tdkvzUAAMwyAqIZVtx+xJ+/ZIe6370dNnOBZnX37bHu04RkUvN66u5k/rmhVpVaMijUP9cHV2MusriYiIiMjlcPeFZnfChs/tA8kpSS8VlKSLlAMWi31u9hA/D7o1DHKUx6dksC3SXuOeXeu+90QSp5LTWbojhqU7YgB7H/erq/vTqkZFWteoSItqFfBwU/92ERERkRKv1f32JH3XLxB3BPzDzI5ILkFJukg55ufpSvvalWlfu7KjLDXDxvbIBP4+eIr1B06z4eAp4s9ksHbvSdbuPQmAi5OFJlX9aF3DXtPeskYF/D3dzPoaIiIiInIhgfWhZkd7c/e/p0G3l8yOSC7BYhiGYXYQxakgk8iLCGRlGeyJSWL9wVNsOHCK9QdOEZWQmmu7+sE+tKpRkVY17bXtwX7uJkQrUjrp3lT4dE1FRM6x4yeYcw94VoInt4OrntOKW0HuS6pJF5GLcnKyUC/Yh3rBPgxsWx3DMDh6+gzrzybsGw6eYn9sMjujEtkZlchXfx4CoFpFT3vz+JoVaFWjIjUre2kwOhEREREzXNUTfKtCwlHYNh+aDzA7IrkIJekiUiAWy39Twt0aXhWAE4lp/H3wFH+dTdp3RCZw+FQKh0+l8P3GowBUreBBj0bB9GgUTHj1Cjg7KWEXERERKRbOLtDqXvtI7+s/VZJewqm5u4gUuoTUDP45dJoNZ5P2zUfiSbdlOT6v7O1G94ZBXN8omPa1K2F10SB0Ur7p3lT4dE1FRM6THAvvNARbGgxbDlXDzY6oXFFzdxExla+7K13qBdKlXiAAKemZrNp9gsXbolm6I5rYpHRmrT/CrPVH8LG60KV+ID0aBdO5XgBeVv1vSURERKTQeVWGxrfC5pn22vSqn5gdkVyAatJFpFilZ2bx5/6TLN4WxW/bozmRmOb4zM3FiQ51KtOjUTDdGgZR0Usjxkv5oHtT4dM1FRHJw7GN8FkXcHazDyDnHWB2ROVGQe5LStJFxDRZWQabjsTx27YoFm+L4uDJFMdnThZoXbMiPRoFc32jYEL9PUyMVKRo6d5U+HRNRUQu4LPr4Njf0PVF6DjK7GjKDSXpF6GbtkjJZBgGu6ITWbw1msXbotgemZDj8yahftzQOJgejYKoE+hjUpQiRUP3psKnayoicgGb58D8B8A3FJ741z6onBQ5JekXoZu2SOlw5FSKvUn8tmg2HDrFuf+nqhXgxU1NqzCgdRghfqphl9JP96bCp2sqInIBmWn2AeRSYuGOL6HhLWZHVC4oSb8I3bRFSp8TiWks3WGvYV+7N5YMm/1/W85OFro1CGRg2xpcU6eS5mGXUkv3psKnayoichHLXoHVE6BGBxjys9nRlAtK0i9CN22R0i0xNYPlO2OYtf4wf+4/5SivVdmLu9tW57bwqvh5uJoYoUjB6d5U+HRNRUQuIv4oTGoKhg0eXgdBDc2OqMwryH3JqZhiEhEpFD7urtzSPJTZD7Tjtyc7MrhddbytLuyPTeaVn7fT5vWlPPv9v2w9Fm92qCIiIiIlk19VqH+jfX3DZ+bGIrmoJl1ESr2ktEwWbDrG138eYmdUoqO8RTV/BratTq8mIbi7OpsYocjF6d5U+HRNRUQu4cBq+OImcPWEkTvAw9/siMo01aSLSLnibXXhnrbV+fWJDnz3UDtublYFV2cLmw7HMfLbzbR/Yzlv/LqTI6dSLn0wERERkfKgxrUQ0AAyUmDzLLOjkXMoSReRMsNisdCqRkXeH9CCtc92ZdT1V1HFz51TyelMWbmPjm//zr0zNvD7zhhsWeWqEZGIiIhIThYLtL7fvr7+M8jKMjcecVCSLiJlUqCPO8O71mXV0134dGA4HepWxjBg+c4Yhs7YQOcJvzNl5T5OJaebHaqIiIiIOZr2B6svnNoH+5ebHY2cpSRdRMo0F2cnrm8UzFf3tWH5U52479qa+Lq7cOTUGd74dSdtxy9j5LcRbDx8mnI2RIeIiIiUd1ZvaH63fX29BpArKTRwnIiUO2fSbfy0+Thf/nmQrccSHOUNQny5q3UYt7QIxddd07hJ8dG9qfDpmoqI5FPsXpgcDljg8U1QsabZEZVJGjhOROQiPNycuaNVGD8Nv5YFj15Dv6tDcXNxYkdkAi/+sI02ry3j6bmbiTgSp9p1ERERKdsq14Ha1wEG/D3V7GgE1aSbHY6IlBBxKenM23iMmesPszcmyVHeIMSXu9pU45bmVVS7LkVG96bCp2sqIlIAuxbBrP7g7m+fjs3N0+yIyhzVpIuIFJC/pxv3XluTJU925LuH2tG3xTm16wu20ua1ZTwz91/VrouIiEjZU7c7+FeH1DjYOtfsaMo9JekiIufInsbt3f7NWf/cdbx4U0PqBHpzJsPGnL+P0OfDtdz4/hq++vMQiakZZocrIiIicuWcnKHVMPv6+k9BFRKmUnN3EZFLMAyDDQdPM2v9YRZuiSQ90z6PqKebMzc3q8KA1tVoWtUPi8VicqRSWuneVPh0TUVECijlFLzTADJT4d7FUK2t2RGVKWruLiJSiCwWC61r2mvX/xptr12vHeBFSrqN2RuOcMuHa7npgzV8rdp1ERERKa08K0KT2+3r6z81N5ZyTjXpIiKXIbt2feZfh/hla1Su2vW72lSjaVV/c4OUUkP3psKnayoichkiN8MnHcHJBZ7cBj7BZkdUZqgmXUSkiGXXrk+6swV/jb6OF25sQK1zatdvnryWO6as4/edMRpoTkREREqHkGYQ1hayMuGfGWZHU24pSRcRuUIVvNwY1qEWy0Z2Ys4DbbmleRVcnS2sP3iKoTM20PO91fwQcYxMW5bZoYqIiIhcXOv77a9/T4PMdHNjKaeUpIuIFBKLxUKbWpV4784WrH66Kw90rIWXmzM7oxJ5YnYEnSes4Kt1B0nNsJkdqoiIiEjeGtwM3kGQFA07fzI7mnJJSbqISBEI9nPnuV4N+OPZ6xh1/VVU9HLj6OkzvPjDNq55Yzkf/r6X+DMaZE5ERERKGBc3CB9qX1//mbmxlFNK0kVEipCfpyvDu9Zl7TNdGXdLI0L9PTiZnM7bi3dxzRvLef2XHUQnpJodpkiRGT9+PK1atcLHx4fAwED69OnDrl27Lrnfd999R/369XF3d6dJkyb88ssvxRCtiIgA0HKoffC4w+sg8l+zoyl3lKSLiBQDDzdnBrWrwYr/dWZS/+bUC/IhKS2TT1ftp8Obv/Ps9/+y/0SS2WGKFLqVK1fy6KOP8ueff7JkyRIyMjK4/vrrSU5OvuA+f/zxBwMGDOC+++5j06ZN9OnThz59+rB169ZijFxEpBzzCYaGt9jXN6g2vbhpCjYRERMYhsHvu2L4eMU+Nhw8DYDFAj0bB/Nwpzo0qepncoRSnMrTvenEiRMEBgaycuVKOnbsmOc2/fv3Jzk5mZ9//tlR1rZtW5o3b86UKVPydZ7ydE1FRIrE4T9hWg9w8YCR2+3zqMtl0xRsIiIlnMVioWv9IL57qD3fPdSO6+oHYhjwy5Yoek9ewz2f/8XavbGavk3KnPj4eAAqVrzww966devo1q1bjrIePXqwbt26C+6TlpZGQkJCjkVERK5AWBsIbgKZZyDiG7OjKVeUpIuImKxVjYpMHdKKRSM60LdFKM5OFtbsjeXuz//ilg/X8uuWSGxZStal9MvKymLEiBFcc801NG7c+ILbRUVFERQUlKMsKCiIqKioC+4zfvx4/Pz8HEtYWFihxS0iUi5ZLND6Afv6hs8hS7PTFBcl6SIiJUT9YF/e7d+cFaM6M7hdddxdnfj3aDwPf7OR7u+s5IeIY6pZl1Lt0UcfZevWrcyePbvQjz169Gji4+Mdy5EjRwr9HCIi5U7j28DdH04fhL1LzY6m3FCSLiJSwoRV9OTlWxqz9pmuPN61Dr7uLuyPTeaJ2RH0+/gPNh4+bXaIIgU2fPhwfv75Z37//XeqVq160W2Dg4OJjo7OURYdHU1wcPAF97Farfj6+uZYRETkCrl5wtUD7evrPzU3lnJESbqISAlVydvKyOvr8cfo63iq+1V4ujmz6XAc/T76gxGzN3E87ozZIYpckmEYDB8+nPnz57N8+XJq1qx5yX3atWvHsmXLcpQtWbKEdu3aFVWYIiJyIS3vAyz2mvST+8yOplxQki4iUsJ5W1147Lq6/D6qM7eHV8VigQURx+k6cQXvLtlNSnqm2SGKXNCjjz7K119/zcyZM/Hx8SEqKoqoqCjOnPnvR6ZBgwYxevRox/snnniCRYsWMXHiRHbu3MnYsWP5+++/GT58uBlfQUSkfKtYE67qYV/f8Lm5sZQTStJFREqJIF933r69GT8+ei2ta1QkNSOL95btoeuElczfdJQsDS4nJdDHH39MfHw8nTt3JiQkxLHMmTPHsc3hw4eJjIx0vG/fvj0zZ87k008/pVmzZsydO5cFCxZcdLA5EREpQq3vt79u+hrSksyNpRzQPOkiIqWQYRj8ujWK13/ZwdHT9hrJZmH+jLmpIeHVK5gcnRSU7k2FT9dURKQQZWXB5JZwah/0/QSa3Wl2RKWO5kkXESnjLBYLvZqEsHRkJ56+oR5ebs5sPhLHrR//weOzNnFM/dVFRESksDg5Qe2u9vXY3ebGUg4oSRcRKcXcXZ15pHMdfv9fZ/q3DMNigR83H6frhBVM/G0XyWnqry4iIiKFwO/szBzxR82NoxxQki4iUgYE+rjz5m1N+Wn4tbSpWZG0zCw+WL6XLhNWMPcf9VcXERGRK6QkvdgoSRcRKUMah/ox+4G2TLknnGoVPYlJTGPUd5vp89FaNhw8ZXZ4IiIiUlr5V7O/xh0xN45yQEm6iEgZY7FYuKFxMEtGdmR0z/p4W13492g8t09Zx6MzN3LkVIrZIYqIiEhpk12TnnAMsmzmxlLGKUkXESmjrC7OPNipNr+P6syA1tVwssDCfyO57p2VvL14J0nqry4iIiL55R0ETi5g2CAxyuxoyjQl6SIiZVyAj5Xx/Zrw82MdaF+7EumZWXz4+z46v/07U9ccIDVDv4aLiIjIJTg5g28V+3q8mrwXJSXpIiLlRMMqvnwzrA2fDWpJzcpexCal88rP2+kyYQUz/zpMhi3L7BBFRESkJPM72y9dg8cVKSXpIiLliMVioXvDIH57siNv9GtCiJ87kfGpPDd/C93eWcn8TUexaSR4ERERyYtjhHfVpBclJekiIuWQq7MTd7auxu+jOjPmpoZU9nbj0MkUnpyzmZ7vrWLR1kgMQ8m6iIiInEPTsBULJekiIuWYu6sz915bk1VPd+F/Perh6+7C7ugkHvp6IzdPXsuKXTFK1kVERMQuO0nXNGxFyvQk/cMPP6RGjRq4u7vTpk0b1q9ff9HtJ02aRL169fDw8CAsLIwnn3yS1NTUYopWRKRs8nRz4dEudVj9TFce61oHTzdnthyLZ8j0DfT/5E/+2n/S7BBFRETEbP5h9lfVpBcpU5P0OXPmMHLkSF566SU2btxIs2bN6NGjBzExMXluP3PmTJ599lleeuklduzYwdSpU5kzZw7PPfdcMUcuIlI2+Xm48tT19Vj9dBeGXVsTNxcn1h88Rf9P/2Tg1L/492ic2SGKiIiIWfyUpBcHU5P0d955h/vvv5+hQ4fSsGFDpkyZgqenJ9OmTctz+z/++INrrrmGu+66ixo1anD99dczYMCAS9a+i4hIwVTytvLCTQ1Z9b8u3N2mGi5OFlbvieXmyWt58Ku/2RWVaHaIIiIiUtyym7unxUNqvLmxlGGmJenp6en8888/dOvW7b9gnJzo1q0b69aty3Of9u3b888//ziS8v379/PLL7/Qq1evC54nLS2NhISEHIuIiORPsJ87r/VtwvKnOtPv6lCcLLB4WzQ3vLeKJ2Zv4mBsstkhioiISHFx8wKPivZ11aYXGdOS9NjYWGw2G0FBQTnKg4KCiIqKynOfu+66i3HjxnHttdfi6upK7dq16dy580Wbu48fPx4/Pz/HEhYWVqjfQ0SkPKhWyZN37mjO4hEd6dk4GMOAHyKOc907Kxk971+Ox50xO8Ridzo5nSOnUswOQ0REpHhphPciZ/rAcQWxYsUKXn/9dT766CM2btzIvHnzWLhwIa+88soF9xk9ejTx8fGO5cgRjUQoInK56gb58PE94fz82LV0rheALctg1vojdH57BeN+2k5CaobZIRa5uJR0JizeRYe3fuelH7eZHY6IiEjxcvRLV15VVFzMOnHlypVxdnYmOjo6R3l0dDTBwcF57vPiiy8ycOBAhg0bBkCTJk1ITk7mgQce4Pnnn8fJKfdvDlarFavVWvhfQESkHGsc6seMoa3ZcPAUExbv4q8Dp5i29gA/bj7Oizc14OZmVbBYLGaHWajiz2Qwdc0Bpq85QGJaJgBR8akkp2XiZTXtdioiIlK8NA1bkTOtJt3NzY3w8HCWLVvmKMvKymLZsmW0a9cuz31SUlJyJeLOzs4AmsdXRMQErWpUZPYDbfni3tbUquxFbFIaT8yO4J6pf7HvRJLZ4RWKhNQMJi3dzbVvLuf9ZXtITMukfrAPU862KFCCLiIi5YqmYStypj5ZjBw5ksGDB9OyZUtat27NpEmTSE5OZujQoQAMGjSI0NBQxo8fD0Dv3r155513aNGiBW3atGHv3r28+OKL9O7d25Gsi4hI8bJYLHS6KoBfR3Tg05X7mfz7XtbuPckNk1bxYMfaDO9aB3fX0vf/6MTUDGasPchnq/eTkGqvOb8qyJsR3a7ihkbBODmVrZYCIiIi+aI+6UXO1CS9f//+nDhxgjFjxhAVFUXz5s1ZtGiRYzC5w4cP56g5f+GFF7BYLLzwwgscO3aMgIAAevfuzWuvvWbWVxARkbOsLs48dl1dbmkeypgft7Ji1wkm/76XHzYf4+WbG9G1ftClD1ICJKVl8sUf9uQ8LsXex75OoDdPXFeXG5uEKDkXEZHyTXOlFzmLUc7aiSckJODn50d8fDy+vr5mhyMiUiYZhsHibVG8/NN2IuNTAejRKIiXejeiir+HydHlLTktky/XHeLTVfs4fTY5rxXgxRPX1eWmplVwLsLkXPemwqdrKiJSRBKjYGI9sDjBCzHg7Gp2RKVCQe5L6kgnIiKFzmKxcEPjEDrUDeC9ZXuYuuYAi7dFs3pPLE9cV5d7r62Jq3PJmGDkTLqNr/48yCcr93MyOR2AmpW9ePy6OtzcLLRIk3MREZFSxysQnN3Alg6JkeBfzeyIyhwl6SIiUmS8rC4816sB/a4O5YX5W/n70GnG/7qT7zce5dU+TWhds6JpsaVm2Pj6z0NMWbmP2CR7cl69kiePda1Ln+ZVcCkhPyKIiIiUKE5O4BsKpw/Ym7wrSS90StJFRKTI1Q/25dsH2zF341HG/7KD3dFJ3PHJOm4Lr8ronvWp5F18U2WmZtiY+ddhPl65jxOJaQCEVfTgsa516dsitMTU8IuIiJRYflX/S9Kl0ClJFxGRYuHkZOGOlmF0bxDEW4t3Mmv9Eeb+c5Ql26N55ob63NkqrEgHZUvNsDFnwxE+/H0vMWeT81B/Dx7rWodbw6sqORcREcmv7MHj4g6bG0cZpSRdRESKVQUvN8b3a8pt4WG8sGArOyITeG7+Fr775wiv9mlMoyp+V3T8+DMZHD2dwpFTZzh6OoWjp+2vm4/GO2rOq/i5M7xrXW4Lr4qbi5JzERGRAtFc6UVKSbqIiJgivHoFfhp+DV+sO8Q7v+1i0+E4en+whsHtazCy+1X4uOc9WmxyWiZHTqdw9NQZ++vpMxw59V8ynj2neV6Cfd15tGsd7mhZFatL6Zu7XUREpETQXOlFSkm6iIiYxsXZifuurcmNTUJ45eftLNwSyfS1B1n4bySPX1cXwzDsSfg5yXj29GgXU9nbjdAKnoRV8KBqBU+qVvCgeiVPWtWoiLurknMREZEr4kjSj5gbRxmlJF1EREwX7OfOh3dfzR27T/DSD1s5eDKFFxZsveD2/p6uVK3gQdjZBDysoqfjfWgFDzzdivj2lnEGtnwHrp7Q5LaiPZeIiEhJ43d2RPf4o2AYYNF0pYVJSbqIiJQYna4KYNGIjny6aj/Ld8YQ4GPNlYyHVvDA9wJN4YtcwnHY8Dn8PR3OnIIKNaBRX3BS7byIiJQjfqH21/QkSI0DjwqmhlPWKEkXEZESxd3Vmcevq8vj19U1O5T/HNkAf30M23+ArLN93v2qQath9vdK0kVEpDxx9QDPypASa69NV5JeqJSki4iI5MWWYU/K//wYjv39X3n1a6DNQ1CvFzjrNioiIuWUX1V7kh53BIKbmB1NmaKnCxERkXMln4R/psGGqZAYaS9zdoMmt0ObByGkmbnxiYiIlAT+YRAZoRHei4CSdBEREYDobfZa8y3fQWaqvcw7CFreBy2HgnegufGJiIiUJH7Zc6VrhPfCpiRdRETKrywb7F5kT84Prv6vPKQ5tH3EPiici5tp4YmIiJRYmiu9yChJFxGR8ic1ATZ9Des/gdMH7WUWZ2jQG9o+DGFtNJ2MiIjIxWiu9CKjJF1ERMqPk/vgr08g4hv7tDEA7v4QPhha3W/vXyciIiKX5mjurpr0wqYkXUREyr5D62DtJNi9GDDsZZXrQduHoGl/cPMyMzoREZHSJztJT4yCzHR1DytEStJFRKRsMgzYvwJWTYBDa/4rr3u9vUl7rS5q0i4iInK5vCqDsxVsaZBwDCrWNDuiMkNJuoiIlC2GYR8MbtWE/+Y3d3KF5ndB+8ehch1z4xMRESkLLBZ7v/RT++xN3pWkFxol6SIiUjZk2WDHj7BqIkRvsZe5uEP4EGj/2H8D3IiIiEjh8A/7L0mXQqMkXURESjdbJmydC6snQuxue5mbN7S6D9oN1/zmIiIiRUXTsBUJJekiIlI6ZabB5lmw5t3/plFz94M2D0ObB8GzoqnhiYiIlHmOEd4PmxtHGaMkXURESpeMM7DxS1j7nn2gGgDPSvZa81bDwN3X3PhERETKC9WkFwkl6SIiUjqkJcKGqbBuMiSfsJf5hNgHgwsfrGnUREREipvmSi8SStJFRKRkO3Ma/voU/vwIUuPsZf7V4JoR0PxucHU3MzoREZHy69yadMPQ1KaFREm6iIiUTMmxsO5DWP8ZpCfayyrVgQ5PQZPbwdnV3PhERETKO99Q+2tGCqScAq9K5sZTRihJFxGRkuff7+DHxyDzjP19YEPoOAoa9gEnZ1NDExERkbNc3cE7CJKiIf6IkvRCoiRdRERKnvWf2BP0kGbQ6Rm4qic4OZkdlYiIiJzPr+rZJP0oVGludjRlgp54RESkZMnKgujt9vV+n0H9G5Wgi4iIlFQa4b3Q6alHRERKlrhDkJEMzlaoWNvsaERERORiHCO8HzE3jjJESbqIiJQs0dvsrwH1wFm9skREREo0JemFTkm6iIiULNlJelBjc+MQERGRS1Nz90KnJF1EREqW6K3216BG5sYhIiIil5adpMepJr2wKEkXEZGSJebsoHFBDc2NQ0RERC4tu7l7cgxkpJobSxmhJF1EREqO9BQ4uc++rubuIiIiJZ9nRXD1tK8nHDM3ljJCSbqIiJQcJ3YABngFgHeg2dGIiIjIpVgs6pdeyJSki4hIyeEYNE790UVEREoNR5KufumFQUm6iIiUHBrZXUREpPRRTXqhUpIuIiIlR3aSHqhB40REREoNv2r2V9WkFwol6SIiUjIYhpq7i4iIlEaqSS9UStJFRKRkSIyCM6fA4gQB9c2ORkRERPJLc6UXKiXpIiJSMmTXoleqC67u5sYiIiIi+ed/dq70+KP2lnFyRZSki4hIyRCT3dRd/dFFRERKFZ8qgAVsaZAca3Y0pZ6SdBERKRnUH11ERKR0cnEDn2D7evxhc2MpA5Ski4hIyaDp10REREovDR5XaJSki4iI+WwZcGKXfV016SIiIqWP3zn90uWKKEkXERHzxe6BrAyw+v53k5cyYdWqVfTu3ZsqVapgsVhYsGDBRbdfsWIFFosl1xIVFVU8AYuIyOVRTXqhUZIuIiLmy27qHtgQLBZzY5FClZycTLNmzfjwww8LtN+uXbuIjIx0LIGBgUUUoYiIFIrsH9nj1Cf9SrmYHYCIiAjRW+2vaupe5vTs2ZOePXsWeL/AwED8/f3zvX1aWhppaWmO9wkJCQU+p4iIXAHVpBca1aSLiIj5NLK7nKd58+aEhITQvXt31q5de8ntx48fj5+fn2MJC1O3CRGRYuWvPumFRUm6iIiYTyO7y1khISFMmTKF77//nu+//56wsDA6d+7Mxo0bL7rf6NGjiY+PdyxHjhwppohFRAT4ryY9JRYyzpgbSymn5u4iImKulFOQeNy+HtjA3FjEdPXq1aNevXqO9+3bt2ffvn28++67fPXVVxfcz2q1YrVaiyNEERHJi7s/uHlDepK9Nr1yXbMjKrVUky4iIuaK2W5/9a8G7r7mxiIlUuvWrdm7d6/ZYYiIyMVYLOf0S1drpiuhJF1ERMylpu5yCREREYSEhJgdhoiIXIrmSi8Uau4uIiLm0sjuZVpSUlKOWvADBw4QERFBxYoVqVatGqNHj+bYsWN8+eWXAEyaNImaNWvSqFEjUlNT+fzzz1m+fDm//fabWV9BRETyK7smPU416VdCSbqIiJgr+mxz98CG5sYhReLvv/+mS5cujvcjR44EYPDgwcyYMYPIyEgOH/5vTt309HSeeuopjh07hqenJ02bNmXp0qU5jiEiIiWUpmErFErSRUTEPFlZ//VJV3P3Mqlz584YhnHBz2fMmJHj/dNPP83TTz9dxFGJiEiR8K9mf1Wf9CuiPukiImKe0wcgIwVc3KFiLbOjERERkSuhmvRCoSRdRETMkz1oXEB9cFbjLhERkVItO0lPOGZvLSeXRUm6iIiYxzGyuwaNExERKfV8QsDiBLZ0SI4xO5pSS0m6iIiYJ0ZJuoiISJnh7Ao+VezravJ+2ZSki4iIeVSTLiIiUrY4+qVr8LjLpSRdRETMkZYEpw7Y1zWyu4iISNmgudKvmJJ0ERExx4mdgAHeQeBV2exoREREpDBohPcrpiRdRETMkd3UPbChuXGIiIhI4fEPs78qSb9sStJFRMQc6o8uIiJS9vhlJ+mHzY2jFFOSLiIi5nAk6eqPLiIiUmaoufsVU5IuIiLFzzAgeqt9XTXpIiIiZUd2kn7mtH2QWCkwJekiIlL8EiMhNQ4szhBQz+xoREREpLC4+4HVz76ecMzcWEop05P0Dz/8kBo1auDu7k6bNm1Yv379RbePi4vj0UcfJSQkBKvVylVXXcUvv/xSTNGKiEihyG7qXrkuuFjNjUVEREQKl6ZhuyIFTtJr1KjBuHHjOHz4ygcCmDNnDiNHjuSll15i48aNNGvWjB49ehATE5Pn9unp6XTv3p2DBw8yd+5cdu3axWeffUZoaOgVxyIiIsVITd1FRETKLke/dCXpl6PASfqIESOYN28etWrVonv37syePZu0tLTLOvk777zD/fffz9ChQ2nYsCFTpkzB09OTadOm5bn9tGnTOHXqFAsWLOCaa66hRo0adOrUiWbNml3W+UVExCQa2V1ERKTs0jRsV+SykvSIiAjWr19PgwYNeOyxxwgJCWH48OFs3Lgx38dJT0/nn3/+oVu3bv8F4+REt27dWLduXZ77/Pjjj7Rr145HH32UoKAgGjduzOuvv47NZrvgedLS0khISMixiIiIyTSyu4iISNmlEd6vyGX3Sb/66qt5//33OX78OC+99BKff/45rVq1onnz5kybNg3DMC66f2xsLDabjaCgoBzlQUFBREVF5bnP/v37mTt3LjabjV9++YUXX3yRiRMn8uqrr17wPOPHj8fPz8+xhIWFFfzLiohI4clMh9jd9vXAhubGIiIiIoXPMVe6mrtfDpfL3TEjI4P58+czffp0lixZQtu2bbnvvvs4evQozz33HEuXLmXmzJmFGStZWVkEBgby6aef4uzsTHh4OMeOHePtt9/mpZdeynOf0aNHM3LkSMf7hISEfCXqNpuNjIyMQotdpKRwdXXF2dnZ7DCkPIvdDVmZ9pFfs39pFxERkbJDfdKvSIGT9I0bNzJ9+nRmzZqFk5MTgwYN4t1336V+/fqObfr27UurVq0uepzKlSvj7OxMdHR0jvLo6GiCg4Pz3CckJCRXgtGgQQOioqJIT0/Hzc0t1z5WqxWrNf8jBxuGQVRUFHFxcfneR6S08ff3Jzg4GIvFYnYoUh6d2x9df4MiIiJlT3ZNesJxyLKBkyqICqLASXqrVq3o3r07H3/8MX369MHV1TXXNjVr1uTOO++86HHc3NwIDw9n2bJl9OnTB7DXlC9btozhw4fnuc8111zDzJkzycrKwsnJ3lJ/9+7dhISE5JmgX47sBD0wMBBPT08lMVKmGIZBSkqKYwaFkJAQkyOSckkju4uIiJRtPsFgcba3nEuMAj/NxlUQBU7S9+/fT/Xq1S+6jZeXF9OnT7/ksUaOHMngwYNp2bIlrVu3ZtKkSSQnJzN06FAABg0aRGhoKOPHjwfg4YcfZvLkyTzxxBM89thj7Nmzh9dff53HH3+8oF8jTzabzZGgV6pUqVCOKVLSeHh4ABATE0NgYKCavkvxi9lufw1Sf3QREZEyyckZfEMh/rB98Dgl6QVS4CQ9JiaGqKgo2rRpk6P8r7/+wtnZmZYtW+b7WP379+fEiROMGTOGqKgomjdvzqJFixyDyR0+fNhRYw4QFhbG4sWLefLJJ2natCmhoaE88cQTPPPMMwX9GnnK7oPu6elZKMcTKamy/8YzMjKUpEvx08juIiIiZZ9f1bNJ+hGgzSU3l/8UOEl/9NFHefrpp3Ml6ceOHePNN9/kr7/+KtDxhg8ffsHm7StWrMhV1q5dO/78888CnaOg1MRdyjr9jYtpkk9CYqR9PbCBubGIiIhI0fEPg8NoGrbLUOAp2LZv387VV1+dq7xFixZs3769UIISEZEyKuZsLXqFGmD1MTUUERERKUIa4f2yFThJt1qtuUZkB4iMjMTF5bJndJMSqEaNGkyaNCnf269YsQKLxaKR8UXkwrKbugdq0DgREZEyzZGkqya9oAqcpF9//fWMHj2a+Ph4R1lcXBzPPfcc3bt3L9TgJH8sFstFl7Fjx17WcTds2MADDzyQ7+3bt29PZGQkfn5+l3W+y1G/fn2sVitRUVHFdk4RuQLnTr8mIiIiZVf2NGxK0guswFXfEyZMoGPHjlSvXp0WLVoAEBERQVBQEF999VWhByiXFhkZ6VifM2cOY8aMYdeuXY4yb29vx7phGNhstny1eggICChQHG5ubhec474orFmzhjNnznDbbbfxxRdfFNoAgpcrIyMjzykJReQcStJFRETKB0eSrubuBVXgmvTQ0FD+/fdf3nrrLRo2bEh4eDjvvfceW7ZsISwsrChilEsIDg52LH5+flgsFsf7nTt34uPjw6+//kp4eDhWq5U1a9awb98+brnlFoKCgvD29qZVq1YsXbo0x3HPb+5usVj4/PPP6du3L56entStW5cff/zR8fn5zd1nzJiBv78/ixcvpkGDBnh7e3PDDTfk+FEhMzOTxx9/HH9/fypVqsQzzzzD4MGD6dOnzyW/99SpU7nrrrsYOHAg06ZNy/X50aNHGTBgABUrVsTLy4uWLVvmGNjwp59+olWrVri7u1O5cmX69u2b47suWLAgx/H8/f2ZMWMGAAcPHsRisTBnzhw6deqEu7s733zzDSdPnmTAgAGEhobi6elJkyZNmDVrVo7jZGVl8dZbb1GnTh2sVivVqlXjtddeA6Br1665BlI8ceIEbm5uLFu27JLXRKREy7JBzA77ukZ2FxERKduyp11LjYfUBHNjKWUuqxO5l5dXgZpBl2aGYXAmw2bKuT1cnQttFO5nn32WCRMmUKtWLSpUqMCRI0fo1asXr732GlarlS+//JLevXuza9cuqlWrdsHjvPzyy7z11lu8/fbbfPDBB9x9990cOnSIihUr5rl9SkoKEyZM4KuvvsLJyYl77rmHUaNG8c033wDw5ptv8s033zB9+nQaNGjAe++9x4IFC+jSpctFv09iYiLfffcdf/31F/Xr1yc+Pp7Vq1fToUMHAJKSkujUqROhoaH8+OOPBAcHs3HjRrKysgBYuHAhffv25fnnn+fLL78kPT2dX3755bKu68SJE2nRogXu7u6kpqYSHh7OM888g6+vLwsXLmTgwIHUrl2b1q1bAzB69Gg+++wz3n33Xa699loiIyPZuXMnAMOGDWP48OFMnDgRq9UKwNdff01oaChdu3YtcHwiJcqpA5B5Blw8oGJNs6MRERGRomT1AXd/SI2zN3l3b2h2RKXGZY/0tn37dg4fPkx6enqO8ptvvvmKgypJzmTYaDhmsSnn3j6uB55uhTMY37hx43KMGVCxYkWaNWvmeP/KK68wf/58fvzxxwtOiQcwZMgQBgwYAMDrr7/O+++/z/r167nhhhvy3D4jI4MpU6ZQu3ZtwD7l3rhx4xyff/DBB4wePdpRiz158uR8JcuzZ8+mbt26NGpkbzJ75513MnXqVEeSPnPmTE6cOMGGDRscPyDUqVPHsf9rr73GnXfeycsvv+woO/d65NeIESPo169fjrJRo0Y51h977DEWL17Mt99+S+vWrUlMTOS9995j8uTJDB48GIDatWtz7bXXAtCvXz+GDx/ODz/8wB133AHYWyQMGTJE06ZJ6Zc9sntgfXByNjcWERERKXr+YRAVZ0/Sg5Sk51eBM8D9+/fTt29ftmzZgsViwTAM4L95l202c2qd5eJatmyZ431SUhJjx45l4cKFREZGkpmZyZkzZzh8+PBFj9O0aVPHupeXF76+vsTExFxwe09PT0eCDhASEuLYPj4+nujoaEcNM4CzszPh4eGOGu8LmTZtGvfcc4/j/T333EOnTp344IMP8PHxISIighYtWlywhj8iIoL777//oufIj/Ovq81m4/XXX+fbb7/l2LFjpKenk5aWhqenJwA7duwgLS2N6667Ls/jubu7O5rv33HHHWzcuJGtW7fm6FYgUmqpP3qpc+TIESwWC1Wr2kfoXb9+PTNnzqRhw4blpkWdiIhcAb8wiNoC8RfPMSSnAifpTzzxBDVr1mTZsmXUrFmT9evXc/LkSZ566ikmTJhQFDGaysPVme3jeph27sLi5eWV4/2oUaNYsmQJEyZMoE6dOnh4eHDbbbflahlxvvMHRrNYLBdNqPPaPvuHncu1fft2/vzzT9avX59jsDibzcbs2bO5//778fDwuOgxLvV5XnFmZGTk2u786/r222/z3nvvMWnSJJo0aYKXlxcjRoxwXNdLnRfsTd6bN2/O0aNHmT59Ol27dqV69eqX3E+kxHMk6eqPXlrcddddPPDAAwwcOJCoqCi6d+9Oo0aN+Oabb4iKimLMmDFmhygiIiWZpmG7LAUeOG7dunWMGzeOypUr4+TkhJOTE9deey3jx4/n8ccfL4oYTWWxWPB0czFlKcrmzWvXrmXIkCH07duXJk2aEBwczMGDB4vsfHnx8/MjKCiIDRs2OMpsNhsbN2686H5Tp06lY8eObN68mYiICMcycuRIpk6dCthr/CMiIjh16lSex2jatOlFB2ILCAjIMcDdnj17SElJueR3Wrt2Lbfccgv33HMPzZo1o1atWuzevdvxed26dfHw8LjouZs0aULLli357LPPmDlzJvfee+8lzytSKkRvtb+qJr3U2Lp1q6O107fffkvjxo35448/+OabbxwDaYqIiFyQkvTLUuAk3Waz4ePjA0DlypU5fvw4ANWrV88x7ZeUbHXr1mXevHlERESwefNm7rrrrks2MS8Kjz32GOPHj+eHH35g165dPPHEE5w+ffqCP1BkZGTw1VdfMWDAABo3bpxjGTZsGH/99Rfbtm1jwIABBAcH06dPH9auXcv+/fv5/vvvWbduHQAvvfQSs2bN4qWXXmLHjh1s2bKFN99803Gerl27MnnyZDZt2sTff//NQw89lK/p1erWrcuSJUv4448/2LFjBw8++CDR0dGOz93d3XnmmWd4+umn+fLLL9m3bx9//vmn48eFbMOGDeONN97AMIwco86LlFppSXD6oH09UEl6aZGRkeEYxHLp0qWOcWfq16+f44dMERGRPGmu9MtS4CS9cePGbN68GYA2bdrw1ltvsXbtWsaNG0etWrUKPUApGu+88w4VKlSgffv29O7dmx49enD11VcXexzPPPMMAwYMYNCgQbRr1w5vb2969OiBu7t7ntv/+OOPnDx5Ms/EtUGDBjRo0ICpU6fi5ubGb7/9RmBgIL169aJJkya88cYbODvbuxB07tyZ7777jh9//JHmzZvTtWtX1q9f7zjWxIkTCQsLo0OHDtx1112MGjXK0a/8Yl544QWuvvpqevToQefOnR0/FJzrxRdf5KmnnmLMmDE0aNCA/v375+rXP2DAAFxcXBgwYMAFr4VIqZI99Zp3MHhVMjcWybdGjRoxZcoUVq9ezZIlSxyDhB4/fpxKlfTvKCIil5CdpMdprvSCsBgF7CC8ePFikpOT6devH3v37uWmm25i9+7dVKpUiTlz5pT4aaISEhLw8/MjPj4eX1/fHJ+lpqZy4MABatasqcTIJFlZWTRo0IA77riDV155xexwTHPw4EFq167Nhg0biuTHE/2tS7H7ezr8PAJqXwcD55kdTYlzsXuTmVasWEHfvn1JSEhg8ODBTJs2DYDnnnuOnTt3Mm9eyf23LKnXVESkXEmIhHfqg8UJXjgBzoUzc1VpVJD7UoGvUo8e/w2iVqdOHXbu3MmpU6eoUKGCpoiSAjt06BC//fYbnTp1Ii0tjcmTJ3PgwAHuuusus0MzRUZGBidPnuSFF16gbdu2prRuECkSGtm9VOrcuTOxsbEkJCRQoUIFR/kDDzyQr9ZFIiJSznkHgZMrZGVAYqR9Sja5pAI1d8/IyMDFxYWtW7fmKK9YsaISdLksTk5OzJgxg1atWnHNNdewZcsWli5dSoMGDcwOzRRr164lJCSEDRs2MGXKFLPDESk8StJLpTNnzpCWluZI0A8dOsSkSZPYtWsXgYGBJkcnIiIlnpMT+IXa19UvPd8KVJPu6upKtWrVNBe6FJqwsDDWrl1rdhglRufOna94ijqREscwIEZJeml0yy230K9fPx566CHi4uJo06YNrq6uxMbG8s477/Dwww+bHaKIiJR0fmH2wWPjjwDtzI6mVCjwwHHPP/88zz333AWnthIREckh4RikxoOTC1S+yuxopAA2btxIhw4dAJg7dy5BQUEcOnSIL7/8kvfff9/k6EREpFRwTMOmwePyq8B90idPnszevXupUqUK1atXx8vLK8fnl5rjWkREypnspu6VrwIXq7mxSIGkpKQ4pl397bff6NevH05OTrRt25ZDhw6ZHJ2IiJQKmiu9wAqcpJ8/nZSIiMhFRZ8dx0RN3UudOnXqsGDBAvr27cvixYt58sknAYiJidGI6SIikj+ahq3ACpykv/TSS0URh4iIlFXR2+2vgQ3NjUMKbMyYMdx11108+eSTdO3alXbt7H0Jf/vtN1q0aGFydCIiUiqoJr3Ayu9EdSIiUjwcI7s3NjcOKbDbbruNa6+9lsjISJo1a+Yov+666+jbt6+JkYmISKmRXZMef8Q+mKxmBbukAifpTk5OF51uTSO/i4iIQ2YaxO62r6u5e6kUHBxMcHAwR4/aa0CqVq1K69atTY5KRERKjeya9PQk+0CyHv6mhlMaFHh09/nz5zNv3jzHMmfOHJ599llCQkL49NNPiyJGKSadO3dmxIgRjvc1atRg0qRJF93HYrGwYMGCKz53YR1HREqYE7vAsIG7P/hWMTsaKaCsrCzGjRuHn58f1atXp3r16vj7+/PKK6+QlZVldngiIlIauHmCZyX7ukZ4z5cC16Tfcsstucpuu+02GjVqxJw5c7jvvvsKJTDJv969e5ORkcGiRYtyfbZ69Wo6duzI5s2badq0aYGOu2HDhlyj91+psWPHsmDBAiIiInKUR0ZGUqFChUI914WcOXOG0NBQnJycOHbsGFarRpsWKTLR58yPruZtpc7zzz/P1KlTeeONN7jmmmsAWLNmDWPHjiU1NZXXXnvN5AhFRKRU8KsKKSft/dKDm5gdTYlX4Jr0C2nbti3Lli0rrMNJAdx3330sWbLE0RTxXNOnT6dly5YFTtABAgIC8PT0LIwQLyk4OLjYkuXvv/+eRo0aUb9+fdNr7w3DIDMz09QYRIpUzDlJupQ6X3zxBZ9//jkPP/wwTZs2pWnTpjzyyCN89tlnzJgxw+zwRESktHD0S9fgcflRKEn6mTNneP/99wkNDS2Mw0kB3XTTTQQEBOR6YEpKSuK7777jvvvu4+TJkwwYMIDQ0FA8PT1p0qQJs2bNuuhxz2/uvmfPHjp27Ii7uzsNGzZkyZIlufZ55plnuOqqq/D09KRWrVq8+OKLZGRkADBjxgxefvllNm/ejMViwWKxOGI+v7n7li1b6Nq1Kx4eHlSqVIkHHniApKQkx+dDhgyhT58+TJgwgZCQECpVqsSjjz7qONfFTJ06lXvuuYd77rmHqVOn5vp827Zt3HTTTfj6+uLj40OHDh3Yt2+f4/Np06bRqFEjrFYrISEhDB8+HICDBw9isVhytBKIi4vDYrGwYsUKAFasWIHFYuHXX38lPDwcq9XKmjVr2LdvH7fccgtBQUF4e3vTqlUrli5dmiOutLQ0nnnmGcLCwrBardSpU4epU6diGAZ16tRhwoQJObaPiIjAYrGwd+/eS14TkSITrSS9NDt16hT169fPVV6/fn1OnTplQkQiIlIqnTt4nFxSgZu7V6hQIcfAcYZhkJiYiKenJ19//XWhBlciGAZkpJhzblfPfDUPdXFxYdCgQcyYMYPnn3/e8e/z3XffYbPZGDBgAElJSYSHh/PMM8/g6+vLwoULGThwILVr187XAEBZWVn069ePoKAg/vrrL+Lj43P0X8/m4+PDjBkzqFKlClu2bOH+++/Hx8eHp59+mv79+7N161YWLVrkSED9/PxyHSM5OZkePXrQrl07NmzYQExMDMOGDWP48OE5foj4/fffCQkJ4ffff2fv3r3079+f5s2bc//991/we+zbt49169Yxb948DMPgySef5NChQ1SvXh2AY8eO0bFjRzp37szy5cvx9fVl7dq1jtrujz/+mJEjR/LGG2/Qs2dP4uPjWbt27SWv3/meffZZJkyYQK1atahQoQJHjhyhV69evPbaa1itVr788kt69+7Nrl27qFatGgCDBg1i3bp1vP/++zRr1owDBw4QGxuLxWLh3nvvZfr06YwaNcpxjunTp9OxY0fq1KlT4PhECo1Gdi/VmjVrxuTJk3n//fdzlE+ePPmyWmiJiEg5lT14nOZKz5cCJ+nvvvtujiTdycmJgIAA2rRpU2x9iotVRgq8btJgR88dB7f89Qm/9957efvtt1m5ciWdO3cG7Enarbfeip+fH35+fjkSuMcee4zFixfz7bff5itJX7p0KTt37mTx4sVUqWK/Hq+//jo9e/bMsd0LL7zgWK9RowajRo1i9uzZPP3003h4eODt7Y2LiwvBwcEXPNfMmTNJTU3lyy+/dPSJnzx5Mr179+bNN98kKCgIsP9gNHnyZJydnalfvz433ngjy5Ytu2iSPm3aNHr27On4W+3RowfTp09n7NixAHz44Yf4+fkxe/ZsXF1dAbjqqqsc+7/66qs89dRTPPHEE46yVq1aXfL6nW/cuHF0797d8b5ixYo5pjd65ZVXmD9/Pj/++CPDhw9n9+7dfPvttyxZsoRu3boBUKtWLcf2Q4YMYcyYMaxfv57WrVuTkZHBzJkzc9WuixSr5FhIigYsEJC7NlZKvrfeeosbb7yRpUuXOuZIX7duHUeOHOGXX34xOToRESk1NFd6gRQ4SR8yZEgRhCFXqn79+rRv355p06bRuXNn9u7dy+rVqxk3bhxgnxrv9ddf59tvv+XYsWOkp6eTlpaW7z7nO3bsICwszJGgA44HtnPNmTOH999/n3379pGUlERmZia+vr4F+i47duygWbNmOQatu+aaa8jKymLXrl2OJL1Ro0Y4Ozs7tgkJCWHLli0XPK7NZuOLL77gvffec5Tdc889jBo1ijFjxuDk5ERERAQdOnRwJOjniomJ4fjx41x33XUF+j55admyZY73SUlJjB07loULFxIZGUlmZiZnzpzh8OHDgL3purOzM506dcrzeFWqVOHGG29k2rRptG7dmp9++om0tDRuv/32K45V5LJl16JXqAFWb1NDkcvTqVMndu/ezYcffsjOnTsB6NevHw888ACvvvoqHTp0MDlCEREpFdQnvUAKnKRPnz4db2/vXA//3333HSkpKQwePLjQgisRXD3tNdpmnbsA7rvvPh577DE+/PBDpk+fTu3atR1J3dtvv817773HpEmTaNKkCV5eXowYMYL09PRCC3fdunXcfffdvPzyy/To0cNRIz1x4sRCO8e5zk+kLRbLRacEWrx4MceOHaN///45ym02G8uWLaN79+54eHhccP+LfQb2ViVg7wKS7UJ95M8fNX/UqFEsWbKECRMmUKdOHTw8PLjtttsc/z6XOjfAsGHDGDhwIO+++y7Tp0+nf//+xTbwn0ie1B+9TKhSpUquUdw3b97M1KlTNfWqiIjkj//ZJD0xEjLTwcXN3HhKuAIPHDd+/HgqV66cqzwwMJDXX3+9UIIqUSwWe5NzM5YCTld0xx134OTkxMyZM/nyyy+59957HV0T1q5dyy233MI999xDs2bNqFWrFrt37873sRs0aMCRI0eIjIx0lP355585tvnjjz+oXr06zz//PC1btqRu3bocOnQoxzZubm7YbLZLnmvz5s0kJyc7ytauXYuTkxP16tXLd8znmzp1KnfeeScRERE5ljvvvNMxgFzTpk1ZvXp1nsm1j48PNWrUuOAsBgEBAQA5rtH5U81dyNq1axkyZAh9+/alSZMmBAcHc/DgQcfnTZo0ISsri5UrV17wGL169cLLy4uPP/6YRYsWce+99+br3CJFRv3RRUREBMCzMjhbAQMSTaoALUUKnKQfPnyYmjVr5iqvXr26o2mumMPb25v+/fszevRoIiMjc3RNqFu3LkuWLOGPP/5gx44dPPjgg0RHR+f72N26deOqq65i8ODBbN68mdWrV/P888/n2KZu3bocPnyY2bNns2/fPt5//33mz5+fY5saNWpw4MABIiIiiI2NJS0tLde57r77btzd3Rk8eDBbt27l999/57HHHmPgwIGOpu4FdeLECX766ScGDx5M48aNcyyDBg1iwYIFnDp1iuHDh5OQkMCdd97J33//zZ49e/jqq6/YtWsXYJ/nfeLEibz//vvs2bOHjRs38sEHHwD22u62bdvyxhtvsGPHDlauXJmjj/7F1K1bl3nz5hEREcHmzZu56667crQKqFGjBoMHD+bee+9lwYIFHDhwgBUrVvDtt986tnF2dmbIkCGMHj2aunXr5tkdQaRYRW+1v6omXUREpHxzcgK/szOBqcn7JRU4SQ8MDOTff//NVb5582YqVapUKEHJ5bvvvvs4ffo0PXr0yNF//IUXXuDqq6+mR48edO7cmeDgYPr06ZPv4zo5OTF//nzOnDlD69atGTZsWK7mjzfffDNPPvkkw4cPp3nz5vzxxx+8+OKLOba59dZbueGGG+jSpQsBAQF5TgPn6enJ4sWLOXXqFK1ateK2227juuuuY/LkyQW7GOfIHoQur/7k1113HR4eHnz99ddUqlSJ5cuXk5SURKdOnQgPD+ezzz5zNK0fPHgwkyZN4qOPPqJRo0bcdNNN7Nmzx3GsadOmkZmZSXh4OCNGjODVV1/NV3zvvPMOFSpUoH379vTu3ZsePXpw9dVX59jm448/5rbbbuORRx6hfv363H///TlaG4D93z89PZ2hQ4cW9BKJFK4sG5yw92FWki4iIiIaPC7/LMa5HWjz4ZlnnmHOnDmO6Z0AVq5cyb333sttt91W4keTTkhIwM/Pj/j4+FwDmqWmpnLgwAFq1qyJu7u7SRGKXL7Vq1dz3XXXceTIkYu2OtDfuhS52D0wuaV9bI3RR8HJ+dL7lGMXuzeZoV+/fhf9PC4ujpUrV16y+5KZSto1FREp9xY8ChFfQ5cXoNP/zI6m2BXkvlTggeNeeeUVDh48yHXXXYeLi333rKwsBg0aVDb7pIuUAmlpaZw4cYKxY8dy++23X3a3AJFCk93UPbCBEvRSyM/P75KfDxo0qJiiERGRMsFRk6650i+lwEm6m5sbc+bM4dVXXyUiIgIPDw+aNGlC9erViyI+EcmHWbNmcd9999G8eXO+/PJLs8MR0cjupdz06dPNDkFERMoaNXfPtwIn6dnq1q1L3bp1CzMWEblMQ4YMyTFQoIjpspP0QCXpIiIigpL0AijwwHG33norb775Zq7yt956K9fc6SIiUk6pJl1ERETO5V/N/hp/BAo2LFq5U+AkfdWqVfTq1StXec+ePVm1alWhBGW2Ao6lJ1Lq6G9cilRqAsQdsq8rSRcREREA37MzT2WkwJnT5sZSwhU4SU9KSsLNzS1XuaurKwkJCYUSlFmyp9lKSUkxORKRopX9N579Ny9SqGJ22F99qoBnRXNjERERkZLB1QO8AuzrGjzuogrcJ71JkybMmTOHMWPG5CifPXs2DRs2LLTAzODs7Iy/vz8xMTGAfb5ui8ViclQihccwDFJSUoiJicHf3x9nZ426LUUge2R31aKLiIjIufzCIPkExB2BkGZmR1NiFThJf/HFF+nXrx/79u2ja9euACxbtoyZM2cyd+7cQg+wuAUHBwM4EnWRssjf39/xty5S6GK221+DSvcPtyIiIlLI/KrC8Y0aPO4SCpyk9+7dmwULFvD6668zd+5cPDw8aNasGcuXL6dixdLfrNFisRASEkJgYCAZGRlmhyNS6FxdXVWDLkXLMWhcY3PjEBERkZLFL8z+qubuF3VZU7DdeOON3HjjjQAkJCQwa9YsRo0axT///IPNZivUAM3i7OysREZEpKAMQyO7i4iISN40DVu+FHjguGyrVq1i8ODBVKlShYkTJ9K1a1f+/PPPwoxNRERKm/gjkJYATq5Qqa7Z0UgJsGrVKnr37k2VKlWwWCwsWLDgkvusWLGCq6++GqvVSp06dZgxY0aRxykiIsXAXzXp+VGgJD0qKoo33niDunXrcvvtt+Pr60taWhoLFizgjTfeoFWrVkUVp4iIlAbZteiVrwKX3DOBSPmTnJxMs2bN+PDDD/O1/YEDB7jxxhvp0qULERERjBgxgmHDhrF48eIijlRERIqcatLzJd/N3Xv37s2qVau48cYbmTRpEjfccAPOzs5MmTKlKOMTEZHSRE3d5Tw9e/akZ8+e+d5+ypQp1KxZk4kTJwLQoEED1qxZw7vvvkuPHj2KKkwRESkO2X3Sk6IhMw1crObGU0LlO0n/9ddfefzxx3n44YepW1dNGEVEJA9K0uUKrVu3jm7duuUo69GjByNGjLjofmlpaaSlpTneJyQkFEV4IiJyJTwrgYsHZJ6BhGNQsZbZEZVI+W7uvmbNGhITEwkPD6dNmzZMnjyZ2NjYooxNRERKG43sLlcoKiqKoKCgHGVBQUEkJCRw5syZC+43fvx4/Pz8HEtYWFhRhyoiIgVlsfzX5D1O/dIvJN9Jetu2bfnss8+IjIzkwQcfZPbs2VSpUoWsrCyWLFlCYmJiUcYpIiIlXUYqnNxrX9cc6VLMRo8eTXx8vGM5ckQPfyIiJZL6pV9SgUd39/Ly4t5772XNmjVs2bKFp556ijfeeIPAwEBuvvnmoohRRERKg9hdYNjAowL4hJgdjZRSwcHBREdH5yiLjo7G19cXDw+PC+5ntVrx9fXNsYiISAmkJP2SLnsKNoB69erx1ltvcfToUWbNmlVYMYmISGl0blN3i8XcWKTUateuHcuWLctRtmTJEtq1a2dSRCIiUqiyB4+LP2xuHCXYFSXp2ZydnenTpw8//vhjYRxORERKIw0aJ3lISkoiIiKCiIgIwD7FWkREBIcP2x/ORo8ezaBBgxzbP/TQQ+zfv5+nn36anTt38tFHH/Htt9/y5JNPmhG+iIgUNsdc6apJv5BCSdJFRESI3mp/VZIu5/j7779p0aIFLVq0AGDkyJG0aNGCMWPGABAZGelI2AFq1qzJwoULWbJkCc2aNWPixIl8/vnnmn5NRKSsUHP3S8r3FGwiIiIXFb3d/hqoJF3+07lzZwzDuODnM2bMyHOfTZs2FWFUIiJimnOTdMNQF7k8qCZdRESuXFIMJMcAFgisb3Y0IiIiUlL5hgIWyEyFZE3pnRcl6SIicuWy+6NXrAVuXubGIiIiIiWXixW8g+zr8ZouMy9K0kVE5Mpp0DgRERHJL/VLvygl6SIicuWUpIuIiEh+KUm/KCXpIiJy5WKUpIuIiEg+OaZhU3P3vChJFxGRK5ORCjE77OtBjc2NRUREREo+PyXpF6MkXURErkxkBNjSwSsAKtQwOxoREREp6dTc/aKUpIuIyJU5/Kf9NayN5joVERGRS8tO0uNUk54XJekiInJljvxlf63W1tw4REREpHTIbu6eEgsZZ8yNpQRSki4iIpfPMP5L0sPamBuLiIiIlA4eFcDVy74ef8zcWEogJekiInL5Tu6DlJPgbIWQZmZHIyIiIqWBxXJOv3Q1eT+fknQREbl8R872Rw+9Glys5sYiIiIipYeS9AtSki4iIpfv3EHjRERERPLLMVe6Rng/n5J0ERG5fBo0TkRERC5Hdk36lrlwPMLUUEqaEpGkf/jhh9SoUQN3d3fatGnD+vXr87Xf7NmzsVgs9OnTp2gDFBGR3FJOQexu+3rV1ubGIiIiIqVLkzvAsxKc2gefdYXlr0JmutlRlQimJ+lz5sxh5MiRvPTSS2zcuJFmzZrRo0cPYmJiLrrfwYMHGTVqFB06dCimSEVEJIfsWvRKdcGrkrmxiIiISOlSoTo8uh4a9gHDBqvehk87w/FNZkdmOtOT9HfeeYf777+foUOH0rBhQ6ZMmYKnpyfTpk274D42m427776bl19+mVq1ahVjtCIi4uBo6q7+6CIiInIZvCrDHV/A7TPAszLEbIPProNlr0BmmtnRmcbUJD09PZ1//vmHbt26OcqcnJzo1q0b69atu+B+48aNIzAwkPvuu++S50hLSyMhISHHIiIiheBw9vzo6o8uIiIiV6BRX3j0L/urYYPVE8p1rbqpSXpsbCw2m42goKAc5UFBQURFReW5z5o1a5g6dSqfffZZvs4xfvx4/Pz8HEtYWNgVxy0iUu5lpsPxjfZ1DRonIiIiV8qrsr1G/fYvztaqby+3teqmN3cviMTERAYOHMhnn31G5cqV87XP6NGjiY+PdyxHjmgePhGRKxa5GTJTwaMiVKpjdjQiIiJSVjTqk3et+rGNZkdWbFzMPHnlypVxdnYmOjo6R3l0dDTBwcG5tt+3bx8HDx6kd+/ejrKsrCwAXFxc2LVrF7Vr186xj9VqxWq1FkH0IiLl2JFz5ke3WMyNRURERMqW7Fr1hn1g4VP2WvXPu8G1I6DTM+BStvM7U2vS3dzcCA8PZ9myZY6yrKwsli1bRrt27XJtX79+fbZs2UJERIRjufnmm+nSpQsRERFqyi4iUlw0aJyIiIgUtUZ97CPAN+p3tlZ9InzSqczXqptakw4wcuRIBg8eTMuWLWndujWTJk0iOTmZoUOHAjBo0CBCQ0MZP3487u7uNG7cOMf+/v7+ALnKRUSkiBiGBo0TERGR4uFVCW6fbk/YFz4FJ3bYa9WveQI6P1sma9VNT9L79+/PiRMnGDNmDFFRUTRv3pxFixY5BpM7fPgwTk6lquu8iEjZdvoAJMeAsxtUaWF2NCIiIlIeNLwFql8Lv/4Ptn4Pa96BXb9An48gNNzs6AqVxTAMw+wgilNCQgJ+fn7Ex8fj6+trdjgiIqVPxCxY8BBUbQ3DlpgdTZmge1Ph0zUVESnDtv8IC0dC8gmwOJeKWvWC3JdURS0iIgXjGDSutblxiIiISPnU8GZ45C9ofKu9r/qad+CTjnDsH7MjKxRK0kVEpGCy+6NrfnQRERExi1cluG0a3PEVeAXAiZ32vuo/PgYH18DZWcBKIyXpIiKSf2fi7AO2gH36NREREREzOWrVbwMjCzZ+CTNuhHcbweLn7SPBl7Ie3krSRUQk/45usL9WrAXegebGIiIiIgJna9WnwuCfofk9YPWDxOOwbjJ81gU+uBqWvwoxO82ONF9MH91dRERKkcPZ/dHV1F1ERERKmJod7MtN78CeJfZR4Hf9Cqf2w6q37UtQY3tf9sa3QoXqZkecJyXpIiKSf0ey50fXoHEiIiJSQrlYocFN9iUtyZ6ob50Le5dB9Fb7suxlqNrK3ky+UV/wCTI7agcl6SIikj+2DDj6t31dg8aJiIhIaWD1hqa325eUU7DjJ3vCfmC1vRvf0Q2weDTU6ABNboMGvcGjgqkhK0kXEZH8idoCmWfA3Q8q1zM7GhEREZGC8awI4YPtS2IUbJtvbxJ/dAMcWGlffh4JdbrZE/Z6PcHNq9jDVJIuIiL542jq3gacNO6oiIiIlGI+wdD2Yfty+qA9Wd/yPcRsg92/2hdXT3ui3uV5qFS72ELTU5aIiOSPY9A4Tb0mIiIiZUiFGtDhKXjkD3jkT+j4P6hQEzJSYOs8cPUo1nBUky4iIpdmGDlr0kVERETKosAG0PUFe+358Y328Xh8qxRrCErSRUTk0uIOQ2IkOLlAaLjZ0YiIiIgULYvF/sxjwnOPmruLiMilHVlvfw1uCm6e5sYiIiIiUoYpSRcRkUs7crY/uqZeExERESlSau4uIiKXdlj90UVERKTwpGbY+G17NMlpmThZwMliwdnJgpPFgpOTBWeLxV6eve503jaO9f+2cXayUCfQG3dXZ7O/3hVRki4iIheXmmCfjgSUpIuIiMgV2xWVyBOzN7EzKrHQjx3oY2V8vyZc1yCo0I9dXJSki4jIxR3dAEYW+FcD3xCzoxEREZFSyjAMvvrzEK8t3EFaZhaVvNy4unoFsrIMbIZBloF9Pcsgy7Av9nVyrju2N8g6W2bLMkhMzSAmMY37vvibfi1Ceal3I/w8Xc3+2gWmJF1ERC7OMfWa+qOLiIjI5TmZlMbTc/9l2c4YADrXC+Dt25oR4GMttHOkZth4Z8luPlu9n3mbjrFmbyyv921Ct4alq1ZdA8eJiMjFZSfp1dTUXURERApu1e4T3PDeapbtjMHN2YmXejdk+pBWhZqgA7i7OvNcrwbMfag9tQK8iElMY9iXfzNyTgTxKRmFeq6ipCRdREQuzJYJR/+2r6smXURERAogLdPGKz9vZ9C09ZxITKNuoDc/DL+GodfUxGKxFNl5w6tX4JfHO/BAx1o4WWDepmN0f3clS7dHF9k5C5OSdBERubCYbZCeBFZfCGxgdjQiIiJSSuyNSaTPh38wdc0BAAa1q85Pj11LgxDfYjl/dq36d+fVqj85J4K4lPRiieFyKUkXEZELy556rWpLcCrd05mIiIhI0TMMg6//PMRNH6xhR2QCFb3c+HxQS8bd0tiUqdGya9UfPFurPn/TMbq/u6pE16orSRcRkQs78qf9VU3dRURE5BJOJafzwFf/8MKCraRmZNGhbmUWPdHB9IHb3F2dGd2rAXMftteqnyjhtepK0kVE5MKOrLe/atA4ERERuYg1e2K5YdIqlmyPxtXZwgs3NuCLoa0J9HU3OzSHq6vlXau+pITVqitJFxGRvMUfg/gjYHGG0JZmRyMiIiIlUHpmFuN/2cE9U/8iJjGN2gFezH/kGoZ1qIWTU9ENDne5zq1Vr322Vv3+ElarriRdRETylt3UPbgxWL3NjUVERERKnH0nkuj38Vo+WbUfgLvaVOPnxzrQONTP5Mgu7epqFVj4eAce7FTyatWVpIuISN6yB40LU1N3ERER+Y9hGMxef5ib3l/D1mMJ+Hu68snAcF7v2wQPt9Iz0Ky7qzOje+auVR8xe5OptepK0kVEJG+OQeOUpIuIiIhdXEo6D3+9kWfnbeFMho32tSux6ImO9GgUbHZol+38WvUFEcfp9s4qftsWZUo8StJFRCS3tCSI2mpfr6aR3UVERAT+2BfLDZNWs2hbFC5OFkb3rM/X97Uh2K/kDA53ubJr1b8/W6sem5TGA1/9Y0qtupJ0ERHJ7dg/YNjAtyr4VTU7GhERETFRclomr/y8nbs//4uohFRqVbYPDvdgp9olcnC4K9HivFr1Hzcf5+DJlGKNwaVYzyYiIqXDkbP90TX1moiISLm2fGc0Ly7YxrG4MwDc2SqMMb0b4ulWdlPJ7Fr1GxoFs/lIHM3D/Iv1/GX3yoqIyOU7rP7oIiIi5VlMYiov/7Sdhf9GAhDq78GrfRvTpV6gyZEVnxbVKtCiWoViP6+SdBERySnLBkc32NeVpIuIiJQrWVkGszccYfyvO0hMzcTJAvddW5Mnu19VpmvPSxJdZRERySlmB6QlgKsXBDU2OxoREREpJntjEhk9bwsbDp4GoEmoH+P7NSkV856XJUrSRUQkp+z+6FVbgrNuEyIiImVdaoaNj1bs4+MVe8mwGXi6OfPU9fUY3K46Ls4aa7y46elLRERycgwap6nXREREyrp1+07y/Pwt7I9NBuC6+oGM69OYUH8PkyMrv5Ski4hITho0TkREpMyLS0nn9V928O3fRwEI8LHy8s2N6Nk4GIulbE2rVtooSRcRkf8kRkHcIcBib+4uIiIiZYphGPy4+Tiv/Lyd2KR0AO5qU41nbqiPn4erydEJKEkXEZFzZdeiBzUCdw0SIyIiUpYcOZXC8wu2smr3CQDqBnrzer8mtKpR0eTI5FxK0kVE5D9H1ttf1dRdRESkzMiwZTFtzQHeXbqb1Iws3FyceKxLHR7sVBs3Fw0MV9IoSRcRkf8cOVuTrkHjREREyoTNR+J4dt4WdkQmANC2VkVe79uEWgHeJkcmF6IkXURE7NJTIHKzfV016SIiIqVaUlomExbv4st1B8kywN/Tled6NeD28KoaGK6EU5IuIiJ2xzdCViZ4B4N/NbOjERERkcv0x95Y/jf3X47FnQGgT/MqvHBTQyp7W02OTPJDHRBERMQue9C4am1Av7BLIfvwww+pUaMG7u7utGnThvXr119w2xkzZmCxWHIs7u7uxRitiEjpdCbdxtgft3HX539xLO4MVSt48OW9rZl0Zwsl6KWIatJFRMTOMWic+qNL4ZozZw4jR45kypQptGnThkmTJtGjRw927dpFYGBgnvv4+vqya9cux3s1zRQRubh/Dp1m1HebORCbDMDdbarxXK8GeFmV8pU2+hcTERHIyoIjf9nXq6k/uhSud955h/vvv5+hQ4cCMGXKFBYuXMi0adN49tln89zHYrEQHBxcnGGKiJRKaZk2Ji3dwycr95FlQLCvO2/e1pROVwWYHZpcJjV3FxERiN0NqXHg4gHBTc2ORsqQ9PR0/vnnH7p16+Yoc3Jyolu3bqxbt+6C+yUlJVG9enXCwsK45ZZb2LZt20XPk5aWRkJCQo5FRKSs23osnlsmr+XjFfYEvV+LUBaP6KgEvZRTki4iIv9NvRYaDs6u5sYiZUpsbCw2m42goKAc5UFBQURFReW5T7169Zg2bRo//PADX3/9NVlZWbRv356jR49e8Dzjx4/Hz8/PsYSFhRXq9xARKUkybVm8v2wPfT5cy86oRCp5uTHlnnDe6d8cP0/dx0s7NXcXERE4rKbuUnK0a9eOdu3aOd63b9+eBg0a8Mknn/DKK6/kuc/o0aMZOXKk431CQoISdREpk/bGJPLUt5vZfDQegBsaBfNa38ZU0sBwZYaSdBER+a8mXYPGSSGrXLkyzs7OREdH5yiPjo7Od59zV1dXWrRowd69ey+4jdVqxWrVA6qIlF1ZWQbT1h7grcW7SM/MwtfdhXG3NOaW5lU0uGYZo+buIiLlXdIJOLXfvh7WytxYpMxxc3MjPDycZcuWOcqysrJYtmxZjtryi7HZbGzZsoWQkJCiClNEpEQ7fDKFOz/7k1cX7iA9M4tOVwXw25Od6NMiVAl6GaSadBGR8i57VPeABuBRwdxYpEwaOXIkgwcPpmXLlrRu3ZpJkyaRnJzsGO190KBBhIaGMn78eADGjRtH27ZtqVOnDnFxcbz99tscOnSIYcOGmfk1RESKnWEYzFx/mNcW7iAl3YaXmzMv3NSQO1uFKTkvw5Ski4iUd46m7q3NjUPKrP79+3PixAnGjBlDVFQUzZs3Z9GiRY7B5A4fPoyT03+N+06fPs39999PVFQUFSpUIDw8nD/++IOGDRua9RVERIpdZPwZnp77L6v3xALQpmZFJtzejLCKniZHJkXNYhiGYXYQxSkhIQE/Pz/i4+Px9fU1OxwREfN93h2Oroc+H0Pzu8yOplzSvanw6ZqKSGllGAbzNx3jpR+3kZiaidXFiadvqM/Q9jVwclLteWlVkPuSatJFRMqzjFSIjLCvh2lkdxERETPFJqXx/PwtLN5mH2yzWZg/E29vRp1Ab5Mjk+KkJF1EpDyLjABbOngFQMVaZkcjIiJSLhmGwS9bonjxh62cSk7H1dnCiG5X8WDHWrg4a6zv8kZJuohIeXY4uz96G9AANCIiIsXun0OneePXHWw4eBqA+sE+vHNHcxpWUVed8kpJuohIeZY9sruauouIiBSrfSeSeGvRTkfTdndXJx7oUItHu9bB6uJscnRiJiXpIiLllWH8l6RXa2tuLCIiIuVETEIqk5btYc6GI9iyDJwscHt4GE92v4pgP3ezw5MSQEm6iEh5dXIvpJwEZyuENDM7GhERkTItMTWDT1ft5/PVBziTYQOgW4MgnrmhHnWDfEyOTkoSJekiIuVVdi166NXgYjU3FhERkTIqPTOLmX8d4v3lezmVnA5Ai2r+jO7ZgNY1K5ocnZREStJFRMqrcweNExERkUKVlWXw85ZIJizexeFTKQDUquzF0zfUo0ejYCwasFUuQEm6iEh5pf7oIiIiReKPvbGM/3UnW47FAxDgY2VEt7rc0TIMV02pJpegJF1EpDxKOQWxu+3rVVubG4uIiEgZsSMygTd+3cnK3ScA8HJz5sFOtRnWoSaebkq9JH/0lyIiUh5l16JXqgtelcyNRUREpJQ7ejqFd37bzfyIYxgGuDhZuKdtdYZ3rUNlb437IgWjJF1EpDzK7o9eTf3RRURELldcSjof/r6XL/44RLotC4Cbmobwvx71qF7Jy+TopLRSki4iUh4dWW9/DVN/dBERkYJKSc/kiz8O8dGKvSSmZgLQrlYlnu1Zn2Zh/uYGJ6WeknQRkfImMx2Ob7Sva9A4ERGRfEtIzeDLPw4ydc0BTqdkAFA/2Idnetan81UBGrFdCoWSdBGR8ubfOZCZCl4BUKmO2dGIiIiUeKeS05m+9gAz/jjoqDmvXsmTx7vWpU+LUJydlJxL4SkR4/9/+OGH1KhRA3d3d9q0acP69esvuO1nn31Ghw4dqFChAhUqVKBbt24X3V5ERM6RlgTLX7WvXzMC9Iu/iIjIBcUkpPLawu1c++ZyPlhub9peN9Cb9+5szrKRnbg1vKoSdCl0ptekz5kzh5EjRzJlyhTatGnDpEmT6NGjB7t27SIwMDDX9itWrGDAgAG0b98ed3d33nzzTa6//nq2bdtGaGioCd9ARKQUWTcZkqLAvzq0vt/saEREREqkY3Fn+GTlPmZvOEJ6pn1AuMahvgzvUofrGwbjpMRcipDFMAzDzADatGlDq1atmDx5MgBZWVmEhYXx2GOP8eyzz15yf5vNRoUKFZg8eTKDBg265PYJCQn4+fkRHx+Pr6/vFccvIlJqJEbB+y0gIwVumw6N+5kdkZyle1Ph0zUVkctxIDaZj1fsZd7GY2Rm2dOk8OoVGN61jvqcyxUpyH3J1Jr09PR0/vnnH0aPHu0oc3Jyolu3bqxbty5fx0hJSSEjI4OKFSvm+XlaWhppaWmO9wkJCVcWtIhIabX8VXuCXrUVNOprdjQiIiIlxq6oRD78fS8//3ucs7k519SpxPAudWlbq6KScylWpibpsbGx2Gw2goKCcpQHBQWxc+fOfB3jmWeeoUqVKnTr1i3Pz8ePH8/LL798xbGKiJRqUVth09f29etfU190ERERYMvReCb/vofF26IdZV3rB/JolzqEV69gYmRSnpneJ/1KvPHGG8yePZsVK1bg7u6e5zajR49m5MiRjvcJCQmEhYUVV4giIiXDkjGAAQ1vgWptzI5GRETEVH8fPMUHy/eycvcJwP7bdc/GwTzSuQ6NQ/1Mjk7KO1OT9MqVK+Ps7Ex0dHSO8ujoaIKDgy+674QJE3jjjTdYunQpTZs2veB2VqsVq9VaKPGKiJRKe5fCvmXg5ArdxpodjYiIiCkMw2Dt3pNM/n0Pf+4/BYCzk4VbmlXhkS61qRPoY3KEInamJulubm6Eh4ezbNky+vTpA9gHjlu2bBnDhw+/4H5vvfUWr732GosXL6Zly5bFFK2ISCmUZYPfxtjXWz8AFWuZG4+IiEgxs2UZLN0Rzccr9hFxJA4AV2cLt4VX5aFOtaleycvcAEXOY3pz95EjRzJ48GBatmxJ69atmTRpEsnJyQwdOhSAQYMGERoayvjx4wF48803GTNmDDNnzqRGjRpERUUB4O3tjbe3t2nfQ0SkRIr4BmK2gbsfdBxldjQiIiLFJjXDxtx/jjJ1zQEOxCYDYHVxYkDragy7tjqVPe2pUGpqqplhShni5uaGk5PTFR/H9CS9f//+nDhxgjFjxhAVFUXz5s1ZtGiRYzC5w4cP5/iiH3/8Menp6dx22205jvPSSy8xduzY4gxdRKRkS0uC5a/Z1zs+DZ55z4IhIiJSlpxMSuPLdYf46s9DnEpOB8DPw5W721RjSPsa2JJPExd9hEST45Syx8nJiZo1a+Lm5nZFxzF9nvTipnlTRaTcWPEGrBgPFWrAo+vBReNzlFS6NxU+XVOR8mf/iSQ+X3OA7/85SlpmFgBVK3gw7Nqa3N4yDC+rC5GRkcTFxREYGIinp6emVpNCk5WVxfHjx3F1daVatWq5/rZKzTzpIiJSRBIiYe179vVuY5Wgi4hImWQYBn8fOs2nq/azdEc02dWPzar68UDH2vRoFISLs71Vrs1mcyTolSpVMjFqKasCAgI4fvw4mZmZuLq6XvZxlKSLiJRFv78GGSlQtTU07GN2NCIiIoXKlmXw27YoPl29n02H4xzl3RoEcn+HWrSuWTFXTWZGRgYAnp6exRmqlCPZzdxtNpuSdBEROUfUVtj0tX29x2v2yV9FRETKgJT0TOb+c5TPVx/g8KkUANxcnLj16lDuu7YWdQIvPZC0mrhLUSmsvy0l6SIiZc2SFwHDXoMe1trsaERERK7YicQ0vlx3kK/+PERcir1G3N/TlUFtqzOwXQ0CfNStS8oOJekiImXJ3qWwbzk4uUK3l8yORkRE5IrsjUnk89UHmLfxGOk2+2Bw1St5MuzamtwWHoaHm7PJEZZONWrUYMSIEYwYMSJf269YsYIuXbpw+vRp/P39izQ2UZIuIlJ2ZNngtxft660fgIq1zI1HRETkMmTaslizN5av1h1i2c4YR3mLav482LEW3RsG4+xUPpqsX6r59OVOQ71hwwa8vLzyvX379u2JjIzEz8+vwOcqCP0YYKckXUSkrIj4BmK2g7s/dBxldjQiIiL5ZhgG244nMH/TMX6IOE5sUhpgH1bl+oZBPNCxFuHVK5ocZfGLjIx0rM+ZM4cxY8awa9cuR5m393998A3DwGaz/b+9e4+Lqtr7B/6ZGWBghrvcFQURFU3RFEm7WMkJ0AyM0jycxCfNowdJj8cyK1PrKZ+Ol0ztZ52ehOPpYnleaT5p3shLec9LUSrHC4KogCK34TYws35/jIyOMNwEZs/web9e+zUze6+991qzZvjynbUvsLNrOsXz9vZuUT0cHBzg5+fXonWo9eSWrgAREbWBag3ww38bno98BVB1vn9kiIjI+lwprsT/23seT7y/H0+u/gmf/pSFG5pqeKodMHlEEH7426P4+Pmh7ZKgCyFQoa21yCTq7hXXBD8/P+Pk5uYGmUxmfH327Fm4uLjg+++/x5AhQ6BUKvHTTz/hwoULiIuLg6+vL5ydnREREYHdu3ebbDcoKAgrV640vpbJZPjf//1fjBs3DiqVCqGhodiyZYtx+d69eyGTyVBcXAwASEtLg7u7O3bs2IGwsDA4OzsjJibG5EeF2tpavPTSS3B3d0eXLl0wb948JCUlIT4+vtV9VlRUhEmTJsHDwwMqlQqxsbE4d+6ccXl2djbGjh0LDw8PqNVq9O/fH9u2bTOum5iYCG9vbzg5OSE0NBSpqamtrkt74kg6EZEtOLga0OQDHkFAxFRL14aIiMis0qoabM/Iwzcnc3Ek66bx3uYOdnL8oZ8vnh7cFY/09oa9on3HEytrdOj35o523Yc5p9+KhsqhbVKxV199FcuWLUPPnj3h4eGBy5cvY/To0XjnnXegVCqxfv16jB07FpmZmejevbvZ7SxevBh///vfsXTpUqxevRqJiYnIzs6Gp2fDP5BUVFRg2bJl+Ne//gW5XI4//elPmDt3Lj7//HMAwHvvvYfPP/8cqampCAsLwwcffIDNmzfjsccea3VbJ0+ejHPnzmHLli1wdXXFvHnzMHr0aJw+fRr29vZITk6GVqvF/v37oVarcfr0aePRBgsWLMDp06fx/fffw8vLC+fPn0dlZWWr69KemKQTEVm70mvAwVWG51GLATte4ZaIiKSlRqfH/v9cxzcnr2D36XxU1+qNyx7o6YmnB3dDzAA/uDq2/t7SndVbb72FP/zhD8bXnp6eCA8PN75+++23sWnTJmzZsgUzZ840u53Jkydj4sSJAIB3330Xq1atwtGjRxETE9Ng+ZqaGnz00UcICQkBAMycORNvvfWWcfnq1asxf/58jBs3DgCwZs0a46h2a9Ql5wcOHMCIESMAAJ9//jkCAwOxefNmPPvss8jJyUFCQgIGDBgAAOjZ8/b1eXJycjB48GAMHToUgOFoAqlikk5EZO32vAPUVADdhgH94ixdGyIiIgCGw8l/yS3BphO5+L9fr+Fmuda4LNTHGePu74q4QV3R1d3JIvVzslfg9FvRFtt3W6lLOutoNBosWrQIW7duxbVr11BbW4vKykrk5OQ0up2BAwcan6vVari6uqKgoMBseZVKZUzQAcDf399YvqSkBPn5+Rg27PatYBUKBYYMGQK9Xl9vW81x5swZ2NnZITIy0jivS5cu6NOnD86cOQMAeOmllzBjxgzs3LkTUVFRSEhIMLZrxowZSEhIwIkTJ/DEE08gPj7emOxLDZN0IiJrlvcbcPIzw/PodwxX2CEiIrKgyzcrsOnkFWw+eQUXb5Qb53s5K/FUeACevr8r+ge4Nnnl8vYmk8na7JBzS7r7Ku1z587Frl27sGzZMvTq1QtOTk545plnoNVqzWzBwN7e9CgGmUzWaELdUPnmnmvfXqZOnYro6Ghs3boVO3fuxJIlS7B8+XKkpKQgNjYW2dnZ2LZtG3bt2oVRo0YhOTkZy5Yts2idG2L9n0oios5s1wIAAugXDwQOa6o0ERFRu7hZrsX3v13DphNX8HN2kXG+o70c0f39MG5wVzzUywt27XyeOQEHDhzA5MmTjYeZazQaXLp0qUPr4ObmBl9fXxw7dgyPPPIIAECn0+HEiRMYNGhQq7YZFhaG2tpaHDlyxDgCXlhYiMzMTPTr189YLjAwENOnT8f06dMxf/58fPLJJ0hJSQFguKp9UlISkpKS8PDDD+Pll19mkk5ERG3o/G7gwg+A3B6IWmTp2hARUSdys1yLo1mFOHzxJg5fLMTZvDLjMpkMeDDEC+MGd0X0fX5wVjLl6EihoaH45ptvMHbsWMhkMixYsKDVh5jfi5SUFCxZsgS9evVC3759sXr1ahQVFTXrCIqMjAy4uLgYX8tkMoSHhyMuLg4vvvgiPv74Y7i4uODVV19F165dERdnON1v9uzZiI2NRe/evVFUVIQ9e/YgLCwMAPDmm29iyJAh6N+/P6qrq/Hdd98Zl0kNvzFERNZIrwN2LjA8j/wz4Bls2foQEZFNu6GpxtEsQ0J+5OJNZOaX1SsT5u+KcYMD8FR4V/i5OVqglgQAK1aswAsvvIARI0bAy8sL8+bNQ2lpaYfXY968ecjLy8OkSZOgUCgwbdo0REdHQ6Fo+nz8utH3OgqFArW1tUhNTcWsWbPw5JNPQqvV4pFHHsG2bduMh97rdDokJycjNzcXrq6uiImJwfvvvw/AcK/3+fPn49KlS3BycsLDDz+MDRs2tH3D24BMWPrEgQ5WWloKNzc3lJSUwNXV1dLVISJqneP/BP7vJcDRHXjpJO+LbuUYm9oe31Oie3O9rBpHsgwJ+eGLhThXoKlXprevMx7o2QWRwV0wLNgT3i7SvrtIVVUVsrKyEBwcDEdH/ojQ0fR6PcLCwjB+/Hi8/fbblq5Ou2jsM9aSuMSRdCIia1OtMVzRHQBGzmOCTkRE96ygtAqHs27iyMVCHL5YiAvXy+uV6evncisp98SwYE90cZZ2Uk6WlZ2djZ07d2LkyJGorq7GmjVrkJWVhT/+8Y+WrprkMUknIrI2B1cDmnzAIxiImGrp2hARkRXKK6nCkVvnlB+5WGhyFXbAcF55Xz9XPNDTE5HBhsTcQ+1godqSNZLL5UhLS8PcuXMhhMB9992H3bt3S/Y8cClhkk5EZE1KrwEHVxmeRy0C7PgPExERNU6vFzhXoMGxSzdxPLsIxy7dRG5RpUkZmQzo5+9qMlLurmKModYLDAzEgQMHLF0Nq8QknYjImuz5b6CmAgiMBPrFWbo2REQkQVU1OvyaW4Jjl27i51uJeWlVrUkZuQzoH+BmHCmPCPaEm5O9mS0SUUdikk5EZC3yfgNOfm54/sQ7hmEPIiLq9Ao11TieXYSfs4vw86WbyLhSghqd6bWhVQ4KDO7ujqE9PDE0yAODu3vw1mhEEsVvJhGRNRAC2PkGAAH0HwcERli6RkREZAFCCFwqrDAcun6pCMeyb+JiAxd583FRIiLIE0N6eCAiyBNh/i6wU8gtUGMiaikm6URE1uB8OnBxD6BwAEYttHRtiIioA+j1AtdKq3C+QIPMvFIczy7C8ewi3NBo65UN9XHG0CBPRAR5YGgPTwR6OkHGI66IrBKTdCIiqdMUALsWGJ4PmwZ4Blu2PkRE1Ka0tXpkF5bjfIEGF65rcL5Ag/PXNbhQUI7KGl298g4KOcID3TCkhyEpH9LDgxd5I7IhTNKJiKRCrwNuXgTyfgXyMm5PmnzDckd34JG5Fq0iERG1XllVDS5eLzcm4XVJeXZhBXR60eA6dnIZgrzU6OXtjEHd3TG0hwfu6+oGR3tFB9eeiDoKk3QiIkvQVgAFp00T8vzfDVdur0cGePUGot8BnDw6vKpERNR8NTo98kurkHOzAhcKNLhQl5QXaJBXWmV2PbWDAr18nBHi44wQb2f08jFM3T1VsOe55HSPHn30UQwaNAgrV64EAAQFBWH27NmYPXu22XVkMhk2bdqE+Pj4e9p3W22nM2GSTkTU3jQF9UfHC88DQl+/rJ0T4Nsf8BtgmPzDAZ8wwEHd8fUmIlwtrsTqH86jp5cawV5qBHurEeihgoMdk6bOSlNdiytFlbhaXIncYsPj1eJK47y80iqYGRQHAHi7KNHL2xkhPobR8V4+LgjxUcPP1ZHnkFM9Y8eORU1NDbZv315v2Y8//ohHHnkEv/zyCwYOHNii7R47dgxqddv+b7Fo0SJs3rwZp06dMpl/7do1eHi07yBDWloaZs+ejeLi4nbdT0dhkk5E1JbKbwCXjwC5x+ofrn43tTfgN/B2Qu43EOgSAsh5CCORVJzNK8WXR3NM5inkMgR6OBmSdi9nBHurjUm8n6sj5HImWtZKrxe4rqnGlbsS7yvFlbhSXIUrRRX17jfeEAeFHAHujsYRcePouLcz3FS8Fzk135QpU5CQkIDc3Fx069bNZFlqaiqGDh3a4gQdALy9vduqik3y8/PrsH3ZCibpREStJQRw4xxw+TCQc8TwWHi+gYIyoEsv02TcbwDg4tvhVSailunuqcJLo0Jx8boGWTfKkXWjHBVaHS4VVuBSYQX2ZF43Ke9oL0dQFzV6eqtvJ/FehiTeQ80Le1mKTi9QqKlGQVk1rmuqcb3sjunW67ySKlwrqax3f/GGuDnZo6u7EwLcndDNwwkB7o7o6q4yPHo4wUut5I811kAIM6eZdQB7FdCMIyeefPJJeHt7Iy0tDW+88YZxvkajwcaNG7F06VIUFhZi5syZ2L9/P4qKihASEoLXXnsNEydONLvduw93P3fuHKZMmYKjR4+iZ8+e+OCDD+qtM2/ePGzatAm5ubnw8/NDYmIi3nzzTdjb2yMtLQ2LFy8GAOMRIampqZg8eXK9w90zMjIwa9YsHDp0CCqVCgkJCVixYgWcnZ0BAJMnT0ZxcTEeeughLF++HFqtFs899xxWrlwJe/vW/ciVk5ODlJQUpKenQy6XIyYmBqtXr4avr+F/sV9++QWzZ8/Gzz//DJlMhtDQUHz88ccYOnQosrOzMXPmTPz000/QarUICgrC0qVLMXr06FbVpTmYpBMRNVdtNXD1JJBz2DBannMYqLxZv5xXHyBwGBAw2JCQ+/bj4epEVqqXjwvm/MHF+FoIgYKyaly8Xn4raTck7xdvlCOnsAJVNXqczSvD2byyettyV9nfStzVCO6iRvcuKgR6qtDdU4Uuagce6txCQgiUVdfielk1CkrNJ9/Xy6pxs7y60UPQ76SQy+Dn6ngr8TYk4l09biXk7k7wd3eCs5L/QtuEmgrg3QDL7Pu1q83638DOzg6TJk1CWloaXn/9dePfiY0bN0Kn02HixInQaDQYMmQI5s2bB1dXV2zduhXPP/88QkJCMGzYsCb3odfr8fTTT8PX1xdHjhxBSUlJg+equ7i4IC0tDQEBAcjIyMCLL74IFxcXvPLKK5gwYQJ+++03bN++Hbt37wYAuLm51dtGeXk5oqOjMXz4cBw7dgwFBQWYOnUqZs6cibS0NGO5PXv2wN/fH3v27MH58+cxYcIEDBo0CC+++GKT7WmofXFxcXB2dsa+fftQW1uL5ORkTJgwAXv37gUAJCYmYvDgwVi7di0UCgVOnTpl/EEgOTkZWq0W+/fvh1qtxunTp40/KLQX/oUhIjKnvNCQjF8+bEjIr54EdHfdm9bOEQi4H+geCQQ+YEjOVZ6WqS8RtTuZTAZfV0f4ujpieEgXk2W1Oj1yiyqNSXtdAp91vRxXS6pQXFGDkznFOJlTXG+7ageFMWHv7qkySeC7eThBaWf7p8Foa/UortDiZoUWN8u1KCqvwc0KLYrKb72u0KKoosb4+oamGtW1DVzbwwy5DOjirIS3sxLeLrcnH+OjYRTc10UJO16ojSTkhRdewNKlS7Fv3z48+uijAAyj1AkJCXBzc4Obmxvmzr1995eUlBTs2LEDX3/9dbOS9N27d+Ps2bPYsWMHAgIMP1q8++67iI2NNSl350h+UFAQ5s6diw0bNuCVV16Bk5MTnJ2dYWdn1+jh7V988QWqqqqwfv164znxa9aswdixY/Hee+8ZR7Y9PDywZs0aKBQK9O3bF2PGjEF6enqrkvT09HRkZGQgKysLgYGBAID169ejf//+OHbsGCIiIpCTk4OXX34Zffv2BQCEhoYa18/JyUFCQgIGDBgAAOjZs2eL69BSTNLvxektQNZ+S9eCqHOS2wH2TqaT3V2v7VWGJNpeBdjXPToZ5t09YiWE4VD1nMO3D18vPFd/vyovoPsDhinwAcOF3ex4CCsRAXYKOYK81AjyUuOxu5ZVanW4VGgYfb9465ZbOTcrcPlmBa6VVqFcqzM7Ai+TAf6ujmaT+I4ehRdCoEYnoNXpUVOrR41Oj+pbj4Z5hmXaunm1hvkllTW3km/t7eT7VtJdVK5FWXXT53o3xEVpZ5J0GyfnOxNxR3iqHaDgIeh0J3uVYUTbUvtupr59+2LEiBFYt24dHn30UZw/fx4//vgj3nrrLQCATqfDu+++i6+//hpXrlyBVqtFdXU1VKrm7ePMmTMIDAw0JugAMHz48HrlvvrqK6xatQoXLlyARqNBbW0tXF1dm92Oun2Fh4ebXLTuwQcfhF6vR2ZmpjFJ79+/PxSK2z9O+vv7IyMjo0X7unOfgYGBxgQdAPr16wd3d3ecOXMGERERmDNnDqZOnYp//etfiIqKwrPPPouQkBAAwEsvvYQZM2Zg586diIqKQkJCQquuA9ASTNLvxeUjwLFPLF0LImoNO6fbibudI1BVDFQU1i/n1ef2KHn3BwDPns06h4yI6E5ODgqE+bsizL/+P7TVtTrkFlUak/acWwl83VSh1eFqSRWullThSFb9U2zUDgr4uDqa/Gm6+6/UnUm8zGT+XeVuLRW4lYibJN+3HptxznZryWWAu8oBHip7eKod4KFyMDyqHeCpuvWotoe7ygHezkp4OSvh5GD7RxlQO5HJrOZ0tClTpiAlJQUffvghUlNTERISgpEjRwIAli5dig8++AArV67EgAEDoFarMXv2bGi12ia22nyHDh1CYmIiFi9ejOjoaLi5uWHDhg1Yvnx5m+3jTnefey6TyaDXN//ImZZatGgR/vjHP2Lr1q34/vvvsXDhQmzYsAHjxo3D1KlTER0dja1bt2Lnzp1YsmQJli9fjpSUlHarD5P0e9HzMav5YhPZHF0NUFtlOJ+spvL21OC8W493Hqpee2t+ZdHteQol0PV+IDDy1kh5JA9dJ6J2p7RTIMTbcPXvuwkhUFiuNZvA590ahc+6UW6BmhvIZYCDnRz2CjkcFPLbz43zZHCwk8PNyd5s0l0339XRnhdcI2rA+PHjMWvWLHzxxRdYv349ZsyYYfzx7cCBA4iLi8Of/vQnAIZzsP/zn/+gX79+zdp2WFgYLl++jGvXrsHf3x8AcPjwYZMyBw8eRI8ePfD6668b52VnZ5uUcXBwgE6na3JfaWlpKC8vN46mHzhwAHK5HH369GlWfVuqrn2XL182jqafPn0axcXFJu9R79690bt3b/z1r3/FxIkTkZqainHjxgEAAgMDMX36dEyfPh3z58/HJ598wiRdskKjDBMRWQe9rn7iXlMB1FQBCgfA7z7ATmnpWhIRGclkMnjdGjG+v3v9+wxX1ehwpbgShZrbP0IKYRjpvnO8W9zxQsDkRUNPAQD2CjnsbyXYdybfdQl43TweQk7U/pydnTFhwgTMnz8fpaWlmDx5snFZaGgo/v3vf+PgwYPw8PDAihUrkJ+f3+wkPSoqCr1790ZSUhKWLl2K0tJSk2S8bh85OTnYsGEDIiIisHXrVmzatMmkTFBQELKysnDq1Cl069YNLi4uUCpN/69KTEzEwoULkZSUhEWLFuH69etISUnB888/bzzUvbV0Ol29e7QrlUpERUVhwIABSExMxMqVK1FbW4u//OUvGDlyJIYOHYrKykq8/PLLeOaZZxAcHIzc3FwcO3YMCQkJAIDZs2cjNjYWvXv3RlFREfbs2YOwsLB7qmtTmKQTUechVwBKZ8NERGQDHO3rRuEtXRMiam9TpkzBp59+itGjR5ucP/7GG2/g4sWLiI6OhkqlwrRp0xAfH4+SkpJmbVcul2PTpk2YMmUKhg0bhqCgIKxatQoxMTHGMk899RT++te/YubMmaiursaYMWOwYMECLFq0yFgmISEB33zzDR577DEUFxcbb8F2J5VKhR07dmDWrFmIiIgwuQXbvdJoNBg8eLDJvJCQEJw/fx7ffvstUlJS8Mgjj5jcgg0AFAoFCgsLMWnSJOTn58PLywtPP/208ZZyOp0OycnJyM3NhaurK2JiYvD+++/fc30bIxNCtN+JRRJUWloKNzc3lJSUtPhCB0RERO2Bsant8T0lortVVVUhKysLwcHBcHR0tHR1yAY19hlrSVzi/SWIiIiIiIiIJIJJOhEREREREZFEMEknIiIiIiIikggm6UREREREREQSwSSdiIiIiIg6jU523WzqQG312WKSTkRERO3uww8/RFBQEBwdHREZGYmjR482Wn7jxo3o27cvHB0dMWDAAGzbtq2DakpEtsre3h4AUFFRYeGakK3SarUADLd1uxe8TzoRERG1q6+++gpz5szBRx99hMjISKxcuRLR0dHIzMyEj49PvfIHDx7ExIkTsWTJEjz55JP44osvEB8fjxMnTuC+++6zQAuIyBYoFAq4u7ujoKAAgOGe3TKZzMK1Iluh1+tx/fp1qFQq2NndW5rN+6QTERFZmK3HpsjISERERGDNmjUADP/IBAYGIiUlBa+++mq98hMmTEB5eTm+++4747wHHngAgwYNwkcffdTgPqqrq1FdXW18XVpaisDAQJt9T4modYQQyMvLQ3FxsaWrQjZILpcjODgYDg4O9Za1JNZzJJ2IiIjajVarxfHjxzF//nzjPLlcjqioKBw6dKjBdQ4dOoQ5c+aYzIuOjsbmzZvN7mfJkiVYvHhxm9SZiGyXTCaDv78/fHx8UFNTY+nqkI1xcHCAXH7vZ5QzSSciIqJ2c+PGDeh0Ovj6+prM9/X1xdmzZxtcJy8vr8HyeXl5Zvczf/58k8S+biSdiKghCoXins8bJmovTNKJiIjI6imVSiiVSktXg4iI6J7x6u5ERETUbry8vKBQKJCfn28yPz8/H35+fg2u4+fn16LyREREtoRJOhEREbUbBwcHDBkyBOnp6cZ5er0e6enpGD58eIPrDB8+3KQ8AOzatctseSIiIlvS6Q53r7uYfWlpqYVrQkREZFAXk2z1hitz5sxBUlIShg4dimHDhmHlypUoLy/Hf/3XfwEAJk2ahK5du2LJkiUAgFmzZmHkyJFYvnw5xowZgw0bNuDnn3/GP/7xj2bvk/GeiIikpCWxvtMl6WVlZQDAi8kQEZHklJWVwc3NzdLVaHMTJkzA9evX8eabbyIvLw+DBg3C9u3bjReHy8nJMbka7ogRI/DFF1/gjTfewGuvvYbQ0FBs3ry5RfdIZ7wnIiIpak6s73T3Sdfr9bh69SpcXFwgk8nuaVt1V469fPmy1d+DlW2RHltpB2A7bbGVdgC20xZbaYcQAmVlZQgICGiTW7cQ431DbKUdgO20xVbaAbAtUmQr7QBsoy0tifWdbiRdLpejW7dubbpNV1dXq/2w3I1tkR5baQdgO22xlXYAttMWW2iHLY6gWxLjvXm20g7AdtpiK+0A2BYpspV2ANbflubGev5cT0RERERERCQRTNKJiIiIiIiIJIJJ+j1QKpVYuHAhlEqlpatyz9gW6bGVdgC20xZbaQdgO22xlXaQtNnK58xW2gHYTltspR0A2yJFttIOwLba0hyd7sJxRERERERERFLFkXQiIiIiIiIiiWCSTkRERERERCQRTNKJiIiIiIiIJIJJOhEREREREZFEMElvwocffoigoCA4OjoiMjISR48ebbT8xo0b0bdvXzg6OmLAgAHYtm1bB9XUvCVLliAiIgIuLi7w8fFBfHw8MjMzG10nLS0NMpnMZHJ0dOygGpu3aNGievXq27dvo+tIsU+CgoLqtUMmkyE5ObnB8lLqj/3792Ps2LEICAiATCbD5s2bTZYLIfDmm2/C398fTk5OiIqKwrlz55rcbku/a22hsbbU1NRg3rx5GDBgANRqNQICAjBp0iRcvXq10W225jPanu0AgMmTJ9erU0xMTJPblVqfAGjweyOTybB06VKz27REn5D1sfZ4z1gvrf6oY63xnrGesb49MdY3jUl6I7766ivMmTMHCxcuxIkTJxAeHo7o6GgUFBQ0WP7gwYOYOHEipkyZgpMnTyI+Ph7x8fH47bffOrjmpvbt24fk5GQcPnwYu3btQk1NDZ544gmUl5c3up6rqyuuXbtmnLKzszuoxo3r37+/Sb1++ukns2Wl2ifHjh0zacOuXbsAAM8++6zZdaTSH+Xl5QgPD8eHH37Y4PK///3vWLVqFT766CMcOXIEarUa0dHRqKqqMrvNln7X2kpjbamoqMCJEyewYMECnDhxAt988w0yMzPx1FNPNbndlnxG20JTfQIAMTExJnX68ssvG92mFPsEgEkbrl27hnXr1kEmkyEhIaHR7XZ0n5B1sYV4z1gvrf6oY63xnrGesb49MdY3gyCzhg0bJpKTk42vdTqdCAgIEEuWLGmw/Pjx48WYMWNM5kVGRoo///nP7VrPliooKBAAxL59+8yWSU1NFW5ubh1XqWZauHChCA8Pb3Z5a+mTWbNmiZCQEKHX6xtcLtX+ACA2bdpkfK3X64Wfn59YunSpcV5xcbFQKpXiyy+/NLudln7X2sPdbWnI0aNHBQCRnZ1ttkxLP6NtraF2JCUlibi4uBZtx1r6JC4uTjz++OONlrF0n5D02WK8Z6yXVn/UscZ4z1hfn6XjCmN9fZbuk7bGkXQztFotjh8/jqioKOM8uVyOqKgoHDp0qMF1Dh06ZFIeAKKjo82Wt5SSkhIAgKenZ6PlNBoNevTogcDAQMTFxeH333/viOo16dy5cwgICEDPnj2RmJiInJwcs2WtoU+0Wi0+++wzvPDCC5DJZGbLSbU/7pSVlYW8vDyT99zNzQ2RkZFm3/PWfNcspaSkBDKZDO7u7o2Wa8lntKPs3bsXPj4+6NOnD2bMmIHCwkKzZa2lT/Lz87F161ZMmTKlybJS7BOSBluN94z10uoPwHbiPWO9gRTjCmO99PqktZikm3Hjxg3odDr4+vqazPf19UVeXl6D6+Tl5bWovCXo9XrMnj0bDz74IO677z6z5fr06YN169bh22+/xWeffQa9Xo8RI0YgNze3A2tbX2RkJNLS0rB9+3asXbsWWVlZePjhh1FWVtZgeWvok82bN6O4uBiTJ082W0aq/XG3uve1Je95a75rllBVVYV58+Zh4sSJcHV1NVuupZ/RjhATE4P169cjPT0d7733Hvbt24fY2FjodLoGy1tLn/zzn/+Ei4sLnn766UbLSbFPSDpsMd4z1kurP+rYSrxnrJdmXGGsl16f3As7S1eAOlZycjJ+++23Js/RGD58OIYPH258PWLECISFheHjjz/G22+/3d7VNCs2Ntb4fODAgYiMjESPHj3w9ddfN+sXNin69NNPERsbi4CAALNlpNofnUVNTQ3Gjx8PIQTWrl3baFkpfkafe+454/MBAwZg4MCBCAkJwd69ezFq1CiL1KktrFu3DomJiU1eVEmKfULUnhjrpYnxXtoY66Wps8Z6jqSb4eXlBYVCgfz8fJP5+fn58PPza3AdPz+/FpXvaDNnzsR3332HPXv2oFu3bi1a197eHoMHD8b58+fbqXat4+7ujt69e5utl9T7JDs7G7t378bUqVNbtJ5U+6PufW3Je96a71pHqgva2dnZ2LVrV6O/rDekqc+oJfTs2RNeXl5m6yT1PgGAH3/8EZmZmS3+7gDS7BOyHFuL94z1BlLpjzq2FO8Z6+uTYlxhrJden7QEk3QzHBwcMGTIEKSnpxvn6fV6pKenm/zCeafhw4eblAeAXbt2mS3fUYQQmDlzJjZt2oQffvgBwcHBLd6GTqdDRkYG/P3926GGrafRaHDhwgWz9ZJqn9RJTU2Fj48PxowZ06L1pNofwcHB8PPzM3nPS0tLceTIEbPveWu+ax2lLmifO3cOu3fvRpcuXVq8jaY+o5aQm5uLwsJCs3WScp/U+fTTTzFkyBCEh4e3eF0p9glZjq3Ee8Z6afXH3Wwp3jPW1yfFuMJYL70+aRHLXrdO2jZs2CCUSqVIS0sTp0+fFtOmTRPu7u4iLy9PCCHE888/L1599VVj+QMHDgg7OzuxbNkycebMGbFw4UJhb28vMjIyLNUEIYQQM2bMEG5ubmLv3r3i2rVrxqmiosJY5u62LF68WOzYsUNcuHBBHD9+XDz33HPC0dFR/P7775ZogtHf/vY3sXfvXpGVlSUOHDggoqKihJeXlygoKBBCWE+fCGG4gmb37t3FvHnz6i2Tcn+UlZWJkydPipMnTwoAYsWKFeLkyZPGq6D+z//8j3B3dxfffvut+PXXX0VcXJwIDg4WlZWVxm08/vjjYvXq1cbXTX3XLNEWrVYrnnrqKdGtWzdx6tQpk+9OdXW12bY09Rnt6HaUlZWJuXPnikOHDomsrCyxe/ducf/994vQ0FBRVVVlth1S7JM6JSUlQqVSibVr1za4DSn0CVkXW4j3jPXS6o87WWO8Z6xnrG9PjPVNY5LehNWrV4vu3bsLBwcHMWzYMHH48GHjspEjR4qkpCST8l9//bXo3bu3cHBwEP379xdbt27t4BrXB6DBKTU11Vjm7rbMnj3b2G5fX18xevRoceLEiY6v/F0mTJgg/P39hYODg+jatauYMGGCOH/+vHG5tfSJEELs2LFDABCZmZn1lkm5P/bs2dPg56muvnq9XixYsED4+voKpVIpRo0aVa+NPXr0EAsXLjSZ19h3zRJtycrKMvvd2bNnj9m2NPUZ7eh2VFRUiCeeeEJ4e3sLe3t70aNHD/Hiiy/WC8DW0Cd1Pv74Y+Hk5CSKi4sb3IYU+oSsj7XHe8Z6afXHnawx3jPWM9Zbqi11OnuslwkhRGtH4YmIiIiIiIio7fCcdCIiIiIiIiKJYJJOREREREREJBFM0omIiIiIiIgkgkk6ERERERERkUQwSSciIiIiIiKSCCbpRERERERERBLBJJ2IiIiIiIhIIpikExEREREREUkEk3Qi6nAymQybN2+2dDWIiIionTDWE7Uek3SiTmby5MmQyWT1ppiYGEtXjYiIiNoAYz2RdbOzdAWIqOPFxMQgNTXVZJ5SqbRQbYiIiKitMdYTWS+OpBN1QkqlEn5+fiaTh4cHAMPhaWvXrkVsbCycnJzQs2dP/Pvf/zZZPyMjA48//jicnJzQpUsXTJs2DRqNxqTMunXr0L9/fyiVSvj7+2PmzJkmy2/cuIFx48ZBpVIhNDQUW7Zsad9GExERdSKM9UTWi0k6EdWzYMECJCQk4JdffkFiYiKee+45nDlzBgBQXl6O6OhoeHh44NixY9i4cSN2795tEpjXrl2L5ORkTJs2DRkZGdiyZQt69eplso/Fixdj/Pjx+PXXXzF69GgkJibi5s2bHdpOIiKizoqxnkjCBBF1KklJSUKhUAi1Wm0yvfPOO0IIIQCI6dOnm6wTGRkpZsyYIYQQ4h//+Ifw8PAQGo3GuHzr1q1CLpeLvLw8IYQQAQEB4vXXXzdbBwDijTfeML7WaDQCgPj+++/brJ1ERESdFWM9kXXjOelEndBjjz2GtWvXmszz9PQ0Ph8+fLjJsuHDh+PUqVMAgDNnziA8PBxqtdq4/MEHH4Rer0dmZiZkMhmuXr2KUaNGNVqHgQMHGp+r1Wq4urqioKCgtU0iIiKiOzDWE1kvJulEnZBara53SFpbcXJyalY5e3t7k9cymQx6vb49qkRERNTpMNYTWS+ek05E9Rw+fLje67CwMABAWFgYfvnlF5SXlxuXHzhwAHK5HH369IGLiwuCgoKQnp7eoXUmIiKi5mOsJ5IujqQTdULV1dXIy8szmWdnZwcvLy8AwMaNGzF06FA89NBD+Pzzz3H06FF8+umnAIDExEQsXLgQSUlJWLRoEa5fv46UlBQ8//zz8PX1BQAsWrQI06dPh4+PD2JjY1FWVoYDBw4gJSWlYxtKRETUSTHWE1kvJulEndD27dvh7+9vMq9Pnz44e/YsAMPVWDds2IC//OUv8Pf3x5dffol+/foBAFQqFXbs2IFZs2YhIiICKpUKCQkJWLFihXFbSUlJqKqqwvvvv4+5c+fCy8sLzzzzTMc1kIiIqJNjrCeyXjIhhLB0JYhIOmQyGTZt2oT4+HhLV4WIiIjaAWM9kbTxnHQiIiIiIiIiiWCSTkRERERERCQRPNydiIiIiIiISCI4kk5EREREREQkEUzSiYiIiIiIiCSCSToRERERERGRRDBJJyIiIiIiIpIIJulEREREREREEsEknYiIiIiIiEgimKQTERERERERSQSTdCIiIiIiIiKJ+P/qU1KuOioaaQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 학습 및 검증 정확도 시각화\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# 정확도 그래프\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_exp25.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history_exp25.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# 손실 그래프\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_exp25.history['loss'], label='Training Loss')\n",
    "plt.plot(history_exp25.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9mqm8sMZACK3"
   },
   "source": [
    "## 2-6. (16, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "wj2k2FjkACK_"
   },
   "outputs": [],
   "source": [
    "dataset_size = x_train.shape[0]\n",
    "batch_size = 30\n",
    "epochs = 20\n",
    "\n",
    "total_iteration= int(epochs * (dataset_size / batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "VvfLtCIcACK_"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout, InputLayer\n",
    "import tensorflow as tf\n",
    "\n",
    "# 새로운 레이어를 추가하기 위한 레이어 리스트 초기화\n",
    "new_layers = []\n",
    "\n",
    "# Dense 레이어 카운터\n",
    "dense_layer_count = 0\n",
    "\n",
    "# VGG 모델의 각 레이어 순회\n",
    "for layer in best_vgg16.layers:\n",
    "    if isinstance(layer, InputLayer):\n",
    "        continue\n",
    "    if isinstance(layer, Conv2D):\n",
    "        # Conv2D 레이어를 ConvLoRALayer로 대체\n",
    "        conv_lora_layer = ConvLoRALayer00_cdn2(original_conv_layer=layer, rank=16, alpha=32, total_iteration = total_iteration, trainable=True)\n",
    "        new_layers.append(conv_lora_layer)\n",
    "    elif isinstance(layer, Dense):\n",
    "        dense_layer_count += 1\n",
    "        # 첫 번째와 두 번째 Dense 레이어에만 LoraLayer 적용\n",
    "        if dense_layer_count in [1, 2]:\n",
    "            dense_lora_layer = LoraLayer(original_layer=layer, rank=64, alpha=32, total_iteration=total_iteration, trainable=True)\n",
    "            new_layers.append(dense_lora_layer)\n",
    "        else:\n",
    "            new_layers.append(layer)  # 그대로 유지함\n",
    "    else:\n",
    "        # 다른 유형의 레이어는 그대로 유지\n",
    "        new_layers.append(layer)\n",
    "\n",
    "# 새로운 입력 텐서 생성\n",
    "input_tensor = best_vgg16.input\n",
    "\n",
    "# 새로운 모델 구성\n",
    "x = input_tensor\n",
    "for layer in new_layers:\n",
    "    x = layer(x)\n",
    "\n",
    "# 새로운 모델 생성\n",
    "exp26_lora_vgg16 = Model(inputs=input_tensor, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "yZSvkVUZACK_"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D\n",
    "\n",
    "# 새로 생성된 모델에서 Dense 레이어 중 LoRA가 적용되지 않은 레이어의 trainable을 False로 설정\n",
    "for layer in exp26_lora_vgg16.layers:\n",
    "    if isinstance(layer, Dense) and not isinstance(layer, LoraLayer):\n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "587UuheQACK_",
    "outputId": "63671162-e007-44da-f92a-49b173cff6c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " block1_conv1 (ConvLoRALaye  (None, 32, 32, 64)        11442     \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block1_conv2 (ConvLoRALaye  (None, 32, 32, 64)        55362     \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " block2_conv1 (ConvLoRALaye  (None, 16, 16, 128)       101506    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block2_conv2 (ConvLoRALaye  (None, 16, 16, 128)       184450    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " block3_conv1 (ConvLoRALaye  (None, 8, 8, 256)         350466    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block3_conv2 (ConvLoRALaye  (None, 8, 8, 256)         663810    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block3_conv3 (ConvLoRALaye  (None, 8, 8, 256)         663810    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 4, 4, 256)         0         \n",
      "                                                                 \n",
      " block4_conv1 (ConvLoRALaye  (None, 4, 4, 512)         1290754   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block4_conv2 (ConvLoRALaye  (None, 4, 4, 512)         2507266   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block4_conv3 (ConvLoRALaye  (None, 4, 4, 512)         2507266   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      " block5_conv1 (ConvLoRALaye  (None, 2, 2, 512)         2507266   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block5_conv2 (ConvLoRALaye  (None, 2, 2, 512)         2507266   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block5_conv3 (ConvLoRALaye  (None, 2, 2, 512)         2507266   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 1, 1, 512)         0         \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_6 (LoraLayer)         (None, 4096)              2396162   \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_7 (LoraLayer)         (None, 512)               2392578   \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20651800 (78.78 MB)\n",
      "Trainable params: 1733040 (6.61 MB)\n",
      "Non-trainable params: 18918760 (72.17 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "exp26_lora_vgg16.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ugu_3sPBACK_",
    "outputId": "1a1abe8e-8ae7-4f7b-a46a-add76eb92b22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: input_3\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block1_conv1\n",
      "  Trainable parameters: 9648\n",
      "  Non-trainable parameters: 1794\n",
      "Layer: block1_conv2\n",
      "  Trainable parameters: 18432\n",
      "  Non-trainable parameters: 36930\n",
      "Layer: block1_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block2_conv1\n",
      "  Trainable parameters: 27648\n",
      "  Non-trainable parameters: 73858\n",
      "Layer: block2_conv2\n",
      "  Trainable parameters: 36864\n",
      "  Non-trainable parameters: 147586\n",
      "Layer: block2_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block3_conv1\n",
      "  Trainable parameters: 55296\n",
      "  Non-trainable parameters: 295170\n",
      "Layer: block3_conv2\n",
      "  Trainable parameters: 73728\n",
      "  Non-trainable parameters: 590082\n",
      "Layer: block3_conv3\n",
      "  Trainable parameters: 73728\n",
      "  Non-trainable parameters: 590082\n",
      "Layer: block3_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block4_conv1\n",
      "  Trainable parameters: 110592\n",
      "  Non-trainable parameters: 1180162\n",
      "Layer: block4_conv2\n",
      "  Trainable parameters: 147456\n",
      "  Non-trainable parameters: 2359810\n",
      "Layer: block4_conv3\n",
      "  Trainable parameters: 147456\n",
      "  Non-trainable parameters: 2359810\n",
      "Layer: block4_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block5_conv1\n",
      "  Trainable parameters: 147456\n",
      "  Non-trainable parameters: 2359810\n",
      "Layer: block5_conv2\n",
      "  Trainable parameters: 147456\n",
      "  Non-trainable parameters: 2359810\n",
      "Layer: block5_conv3\n",
      "  Trainable parameters: 147456\n",
      "  Non-trainable parameters: 2359810\n",
      "Layer: block5_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: flatten_2\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: dense_6\n",
      "  Trainable parameters: 294912\n",
      "  Non-trainable parameters: 2101250\n",
      "Layer: dropout_4\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: dense_7\n",
      "  Trainable parameters: 294912\n",
      "  Non-trainable parameters: 2097666\n",
      "Layer: dropout_5\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: dense_8\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 5130\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#모델의 각 레이어를 순회하며 파라미터 수를 계산\n",
    "for layer in exp26_lora_vgg16.layers:\n",
    "    trainable_count = np.sum([tf.size(w).numpy() for w in layer.trainable_weights])\n",
    "    non_trainable_count = np.sum([tf.size(w).numpy() for w in layer.non_trainable_weights])\n",
    "\n",
    "    print(f\"Layer: {layer.name}\")\n",
    "    print(f\"  Trainable parameters: {trainable_count}\")\n",
    "    print(f\"  Non-trainable parameters: {non_trainable_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "QIRBl6_HACLA"
   },
   "outputs": [],
   "source": [
    "lora_layers = []  # lora 레이어들을 저장할 리스트\n",
    "\n",
    "for layer in exp26_lora_vgg16.layers:\n",
    "    if isinstance(layer, LoraLayer) or isinstance(layer, ConvLoRALayer00_cdn2):\n",
    "        lora_layers.append(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "rQLsY-jCACLA"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "class PrintCurrentStepCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, lora_layers):\n",
    "        super().__init__()\n",
    "        self.lora_layers = lora_layers\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print()\n",
    "        for i, lora_layer in enumerate(self.lora_layers):\n",
    "            current_step = lora_layer.current_step.value()\n",
    "            decay_factor = lora_layer.decay_factor.value()\n",
    "            print(f\"End of epoch {epoch + 1}, LoraLayer {i}: {current_step} Step\")\n",
    "            print(f\"End of epoch {epoch + 1}, LoraLayer {i}: Decay factor: {decay_factor}\")\n",
    "\n",
    "class TestCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, test_data):\n",
    "        self.test_data = test_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        x, y = self.test_data\n",
    "        loss, acc = self.model.evaluate(x, y, verbose=0)\n",
    "        print('\\nTesting loss: {}, acc: {}\\n'.format(loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "i5VDOlyHACLA"
   },
   "outputs": [],
   "source": [
    "# 콜백 생성 시 lora_layers 딕셔너리의 값만 사용\n",
    "print_step_callback = PrintCurrentStepCallback(lora_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "mYnT-xGlACLA"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# 모델 컴파일\n",
    "exp26_lora_vgg16.compile(optimizer=Adam(learning_rate=0.00005),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sU49T8ueACLA"
   },
   "source": [
    "### 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vw3LrTXoACLA",
    "outputId": "4c8bc6cb-79de-4456-d641-301e2d97ded9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.1146 - accuracy: 0.9632\n",
      "End of epoch 1, LoraLayer 0: 1667 Step\n",
      "End of epoch 1, LoraLayer 0: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 1: 1667 Step\n",
      "End of epoch 1, LoraLayer 1: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 2: 1667 Step\n",
      "End of epoch 1, LoraLayer 2: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 3: 1667 Step\n",
      "End of epoch 1, LoraLayer 3: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 4: 1667 Step\n",
      "End of epoch 1, LoraLayer 4: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 5: 1667 Step\n",
      "End of epoch 1, LoraLayer 5: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 6: 1667 Step\n",
      "End of epoch 1, LoraLayer 6: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 7: 1667 Step\n",
      "End of epoch 1, LoraLayer 7: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 8: 1667 Step\n",
      "End of epoch 1, LoraLayer 8: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 9: 1667 Step\n",
      "End of epoch 1, LoraLayer 9: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 10: 1667 Step\n",
      "End of epoch 1, LoraLayer 10: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 11: 1667 Step\n",
      "End of epoch 1, LoraLayer 11: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 12: 1667 Step\n",
      "End of epoch 1, LoraLayer 12: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 13: 1667 Step\n",
      "End of epoch 1, LoraLayer 13: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 14: 1667 Step\n",
      "End of epoch 1, LoraLayer 14: Decay factor: 1.0\n",
      "\n",
      "Testing loss: 2.302607297897339, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 55s 28ms/step - loss: 0.1146 - accuracy: 0.9632 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 2/20\n",
      "1665/1667 [============================>.] - ETA: 0s - loss: 0.0902 - accuracy: 0.9704\n",
      "End of epoch 2, LoraLayer 0: 3334 Step\n",
      "End of epoch 2, LoraLayer 0: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 1: 3334 Step\n",
      "End of epoch 2, LoraLayer 1: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 2: 3334 Step\n",
      "End of epoch 2, LoraLayer 2: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 3: 3334 Step\n",
      "End of epoch 2, LoraLayer 3: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 4: 3334 Step\n",
      "End of epoch 2, LoraLayer 4: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 5: 3334 Step\n",
      "End of epoch 2, LoraLayer 5: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 6: 3334 Step\n",
      "End of epoch 2, LoraLayer 6: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 7: 3334 Step\n",
      "End of epoch 2, LoraLayer 7: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 8: 3334 Step\n",
      "End of epoch 2, LoraLayer 8: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 9: 3334 Step\n",
      "End of epoch 2, LoraLayer 9: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 10: 3334 Step\n",
      "End of epoch 2, LoraLayer 10: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 11: 3334 Step\n",
      "End of epoch 2, LoraLayer 11: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 12: 3334 Step\n",
      "End of epoch 2, LoraLayer 12: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 13: 3334 Step\n",
      "End of epoch 2, LoraLayer 13: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 14: 3334 Step\n",
      "End of epoch 2, LoraLayer 14: Decay factor: 0.9999625086784363\n",
      "\n",
      "Testing loss: 2.302607297897339, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 46s 27ms/step - loss: 0.0902 - accuracy: 0.9704 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 3/20\n",
      "1665/1667 [============================>.] - ETA: 0s - loss: 0.0762 - accuracy: 0.9750\n",
      "End of epoch 3, LoraLayer 0: 5001 Step\n",
      "End of epoch 3, LoraLayer 0: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 1: 5001 Step\n",
      "End of epoch 3, LoraLayer 1: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 2: 5001 Step\n",
      "End of epoch 3, LoraLayer 2: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 3: 5001 Step\n",
      "End of epoch 3, LoraLayer 3: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 4: 5001 Step\n",
      "End of epoch 3, LoraLayer 4: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 5: 5001 Step\n",
      "End of epoch 3, LoraLayer 5: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 6: 5001 Step\n",
      "End of epoch 3, LoraLayer 6: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 7: 5001 Step\n",
      "End of epoch 3, LoraLayer 7: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 8: 5001 Step\n",
      "End of epoch 3, LoraLayer 8: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 9: 5001 Step\n",
      "End of epoch 3, LoraLayer 9: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 10: 5001 Step\n",
      "End of epoch 3, LoraLayer 10: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 11: 5001 Step\n",
      "End of epoch 3, LoraLayer 11: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 12: 5001 Step\n",
      "End of epoch 3, LoraLayer 12: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 13: 5001 Step\n",
      "End of epoch 3, LoraLayer 13: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 14: 5001 Step\n",
      "End of epoch 3, LoraLayer 14: Decay factor: 0.9374484419822693\n",
      "\n",
      "Testing loss: 2.302607297897339, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 45s 27ms/step - loss: 0.0762 - accuracy: 0.9750 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 4/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.0766 - accuracy: 0.9747\n",
      "End of epoch 4, LoraLayer 0: 6668 Step\n",
      "End of epoch 4, LoraLayer 0: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 1: 6668 Step\n",
      "End of epoch 4, LoraLayer 1: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 2: 6668 Step\n",
      "End of epoch 4, LoraLayer 2: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 3: 6668 Step\n",
      "End of epoch 4, LoraLayer 3: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 4: 6668 Step\n",
      "End of epoch 4, LoraLayer 4: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 5: 6668 Step\n",
      "End of epoch 4, LoraLayer 5: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 6: 6668 Step\n",
      "End of epoch 4, LoraLayer 6: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 7: 6668 Step\n",
      "End of epoch 4, LoraLayer 7: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 8: 6668 Step\n",
      "End of epoch 4, LoraLayer 8: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 9: 6668 Step\n",
      "End of epoch 4, LoraLayer 9: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 10: 6668 Step\n",
      "End of epoch 4, LoraLayer 10: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 11: 6668 Step\n",
      "End of epoch 4, LoraLayer 11: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 12: 6668 Step\n",
      "End of epoch 4, LoraLayer 12: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 13: 6668 Step\n",
      "End of epoch 4, LoraLayer 13: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 14: 6668 Step\n",
      "End of epoch 4, LoraLayer 14: Decay factor: 0.8749343752861023\n",
      "\n",
      "Testing loss: 2.302607297897339, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 45s 27ms/step - loss: 0.0766 - accuracy: 0.9747 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 5/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.0836 - accuracy: 0.9724\n",
      "End of epoch 5, LoraLayer 0: 8335 Step\n",
      "End of epoch 5, LoraLayer 0: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 1: 8335 Step\n",
      "End of epoch 5, LoraLayer 1: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 2: 8335 Step\n",
      "End of epoch 5, LoraLayer 2: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 3: 8335 Step\n",
      "End of epoch 5, LoraLayer 3: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 4: 8335 Step\n",
      "End of epoch 5, LoraLayer 4: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 5: 8335 Step\n",
      "End of epoch 5, LoraLayer 5: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 6: 8335 Step\n",
      "End of epoch 5, LoraLayer 6: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 7: 8335 Step\n",
      "End of epoch 5, LoraLayer 7: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 8: 8335 Step\n",
      "End of epoch 5, LoraLayer 8: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 9: 8335 Step\n",
      "End of epoch 5, LoraLayer 9: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 10: 8335 Step\n",
      "End of epoch 5, LoraLayer 10: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 11: 8335 Step\n",
      "End of epoch 5, LoraLayer 11: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 12: 8335 Step\n",
      "End of epoch 5, LoraLayer 12: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 13: 8335 Step\n",
      "End of epoch 5, LoraLayer 13: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 14: 8335 Step\n",
      "End of epoch 5, LoraLayer 14: Decay factor: 0.8124203085899353\n",
      "\n",
      "Testing loss: 2.302607297897339, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 45s 27ms/step - loss: 0.0836 - accuracy: 0.9724 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 6/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.0877 - accuracy: 0.9704\n",
      "End of epoch 6, LoraLayer 0: 10002 Step\n",
      "End of epoch 6, LoraLayer 0: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 1: 10002 Step\n",
      "End of epoch 6, LoraLayer 1: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 2: 10002 Step\n",
      "End of epoch 6, LoraLayer 2: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 3: 10002 Step\n",
      "End of epoch 6, LoraLayer 3: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 4: 10002 Step\n",
      "End of epoch 6, LoraLayer 4: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 5: 10002 Step\n",
      "End of epoch 6, LoraLayer 5: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 6: 10002 Step\n",
      "End of epoch 6, LoraLayer 6: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 7: 10002 Step\n",
      "End of epoch 6, LoraLayer 7: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 8: 10002 Step\n",
      "End of epoch 6, LoraLayer 8: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 9: 10002 Step\n",
      "End of epoch 6, LoraLayer 9: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 10: 10002 Step\n",
      "End of epoch 6, LoraLayer 10: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 11: 10002 Step\n",
      "End of epoch 6, LoraLayer 11: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 12: 10002 Step\n",
      "End of epoch 6, LoraLayer 12: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 13: 10002 Step\n",
      "End of epoch 6, LoraLayer 13: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 14: 10002 Step\n",
      "End of epoch 6, LoraLayer 14: Decay factor: 0.7499062418937683\n",
      "\n",
      "Testing loss: 2.302607297897339, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 45s 27ms/step - loss: 0.0877 - accuracy: 0.9704 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 7/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.0970 - accuracy: 0.9677\n",
      "End of epoch 7, LoraLayer 0: 11669 Step\n",
      "End of epoch 7, LoraLayer 0: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 1: 11669 Step\n",
      "End of epoch 7, LoraLayer 1: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 2: 11669 Step\n",
      "End of epoch 7, LoraLayer 2: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 3: 11669 Step\n",
      "End of epoch 7, LoraLayer 3: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 4: 11669 Step\n",
      "End of epoch 7, LoraLayer 4: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 5: 11669 Step\n",
      "End of epoch 7, LoraLayer 5: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 6: 11669 Step\n",
      "End of epoch 7, LoraLayer 6: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 7: 11669 Step\n",
      "End of epoch 7, LoraLayer 7: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 8: 11669 Step\n",
      "End of epoch 7, LoraLayer 8: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 9: 11669 Step\n",
      "End of epoch 7, LoraLayer 9: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 10: 11669 Step\n",
      "End of epoch 7, LoraLayer 10: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 11: 11669 Step\n",
      "End of epoch 7, LoraLayer 11: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 12: 11669 Step\n",
      "End of epoch 7, LoraLayer 12: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 13: 11669 Step\n",
      "End of epoch 7, LoraLayer 13: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 14: 11669 Step\n",
      "End of epoch 7, LoraLayer 14: Decay factor: 0.6873922348022461\n",
      "\n",
      "Testing loss: 2.302607297897339, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 45s 27ms/step - loss: 0.0970 - accuracy: 0.9677 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 8/20\n",
      "1665/1667 [============================>.] - ETA: 0s - loss: 0.1127 - accuracy: 0.9623\n",
      "End of epoch 8, LoraLayer 0: 13336 Step\n",
      "End of epoch 8, LoraLayer 0: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 1: 13336 Step\n",
      "End of epoch 8, LoraLayer 1: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 2: 13336 Step\n",
      "End of epoch 8, LoraLayer 2: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 3: 13336 Step\n",
      "End of epoch 8, LoraLayer 3: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 4: 13336 Step\n",
      "End of epoch 8, LoraLayer 4: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 5: 13336 Step\n",
      "End of epoch 8, LoraLayer 5: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 6: 13336 Step\n",
      "End of epoch 8, LoraLayer 6: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 7: 13336 Step\n",
      "End of epoch 8, LoraLayer 7: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 8: 13336 Step\n",
      "End of epoch 8, LoraLayer 8: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 9: 13336 Step\n",
      "End of epoch 8, LoraLayer 9: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 10: 13336 Step\n",
      "End of epoch 8, LoraLayer 10: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 11: 13336 Step\n",
      "End of epoch 8, LoraLayer 11: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 12: 13336 Step\n",
      "End of epoch 8, LoraLayer 12: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 13: 13336 Step\n",
      "End of epoch 8, LoraLayer 13: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 14: 13336 Step\n",
      "End of epoch 8, LoraLayer 14: Decay factor: 0.6248781681060791\n",
      "\n",
      "Testing loss: 2.302607536315918, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 46s 27ms/step - loss: 0.1127 - accuracy: 0.9623 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 9/20\n",
      "1665/1667 [============================>.] - ETA: 0s - loss: 0.1361 - accuracy: 0.9533\n",
      "End of epoch 9, LoraLayer 0: 15003 Step\n",
      "End of epoch 9, LoraLayer 0: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 1: 15003 Step\n",
      "End of epoch 9, LoraLayer 1: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 2: 15003 Step\n",
      "End of epoch 9, LoraLayer 2: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 3: 15003 Step\n",
      "End of epoch 9, LoraLayer 3: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 4: 15003 Step\n",
      "End of epoch 9, LoraLayer 4: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 5: 15003 Step\n",
      "End of epoch 9, LoraLayer 5: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 6: 15003 Step\n",
      "End of epoch 9, LoraLayer 6: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 7: 15003 Step\n",
      "End of epoch 9, LoraLayer 7: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 8: 15003 Step\n",
      "End of epoch 9, LoraLayer 8: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 9: 15003 Step\n",
      "End of epoch 9, LoraLayer 9: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 10: 15003 Step\n",
      "End of epoch 9, LoraLayer 10: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 11: 15003 Step\n",
      "End of epoch 9, LoraLayer 11: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 12: 15003 Step\n",
      "End of epoch 9, LoraLayer 12: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 13: 15003 Step\n",
      "End of epoch 9, LoraLayer 13: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 14: 15003 Step\n",
      "End of epoch 9, LoraLayer 14: Decay factor: 0.5623641014099121\n",
      "\n",
      "Testing loss: 2.302607774734497, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 45s 27ms/step - loss: 0.1362 - accuracy: 0.9533 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 10/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.1574 - accuracy: 0.9461\n",
      "End of epoch 10, LoraLayer 0: 16670 Step\n",
      "End of epoch 10, LoraLayer 0: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 1: 16670 Step\n",
      "End of epoch 10, LoraLayer 1: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 2: 16670 Step\n",
      "End of epoch 10, LoraLayer 2: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 3: 16670 Step\n",
      "End of epoch 10, LoraLayer 3: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 4: 16670 Step\n",
      "End of epoch 10, LoraLayer 4: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 5: 16670 Step\n",
      "End of epoch 10, LoraLayer 5: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 6: 16670 Step\n",
      "End of epoch 10, LoraLayer 6: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 7: 16670 Step\n",
      "End of epoch 10, LoraLayer 7: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 8: 16670 Step\n",
      "End of epoch 10, LoraLayer 8: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 9: 16670 Step\n",
      "End of epoch 10, LoraLayer 9: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 10: 16670 Step\n",
      "End of epoch 10, LoraLayer 10: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 11: 16670 Step\n",
      "End of epoch 10, LoraLayer 11: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 12: 16670 Step\n",
      "End of epoch 10, LoraLayer 12: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 13: 16670 Step\n",
      "End of epoch 10, LoraLayer 13: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 14: 16670 Step\n",
      "End of epoch 10, LoraLayer 14: Decay factor: 0.4998500347137451\n",
      "\n",
      "Testing loss: 2.3026123046875, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 46s 28ms/step - loss: 0.1575 - accuracy: 0.9461 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 11/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.1931 - accuracy: 0.9324\n",
      "End of epoch 11, LoraLayer 0: 18337 Step\n",
      "End of epoch 11, LoraLayer 0: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 1: 18337 Step\n",
      "End of epoch 11, LoraLayer 1: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 2: 18337 Step\n",
      "End of epoch 11, LoraLayer 2: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 3: 18337 Step\n",
      "End of epoch 11, LoraLayer 3: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 4: 18337 Step\n",
      "End of epoch 11, LoraLayer 4: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 5: 18337 Step\n",
      "End of epoch 11, LoraLayer 5: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 6: 18337 Step\n",
      "End of epoch 11, LoraLayer 6: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 7: 18337 Step\n",
      "End of epoch 11, LoraLayer 7: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 8: 18337 Step\n",
      "End of epoch 11, LoraLayer 8: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 9: 18337 Step\n",
      "End of epoch 11, LoraLayer 9: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 10: 18337 Step\n",
      "End of epoch 11, LoraLayer 10: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 11: 18337 Step\n",
      "End of epoch 11, LoraLayer 11: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 12: 18337 Step\n",
      "End of epoch 11, LoraLayer 12: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 13: 18337 Step\n",
      "End of epoch 11, LoraLayer 13: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 14: 18337 Step\n",
      "End of epoch 11, LoraLayer 14: Decay factor: 0.4373359680175781\n",
      "\n",
      "Testing loss: 2.3026108741760254, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 45s 27ms/step - loss: 0.1931 - accuracy: 0.9324 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 12/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.2407 - accuracy: 0.9157\n",
      "End of epoch 12, LoraLayer 0: 20004 Step\n",
      "End of epoch 12, LoraLayer 0: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 1: 20004 Step\n",
      "End of epoch 12, LoraLayer 1: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 2: 20004 Step\n",
      "End of epoch 12, LoraLayer 2: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 3: 20004 Step\n",
      "End of epoch 12, LoraLayer 3: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 4: 20004 Step\n",
      "End of epoch 12, LoraLayer 4: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 5: 20004 Step\n",
      "End of epoch 12, LoraLayer 5: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 6: 20004 Step\n",
      "End of epoch 12, LoraLayer 6: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 7: 20004 Step\n",
      "End of epoch 12, LoraLayer 7: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 8: 20004 Step\n",
      "End of epoch 12, LoraLayer 8: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 9: 20004 Step\n",
      "End of epoch 12, LoraLayer 9: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 10: 20004 Step\n",
      "End of epoch 12, LoraLayer 10: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 11: 20004 Step\n",
      "End of epoch 12, LoraLayer 11: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 12: 20004 Step\n",
      "End of epoch 12, LoraLayer 12: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 13: 20004 Step\n",
      "End of epoch 12, LoraLayer 13: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 14: 20004 Step\n",
      "End of epoch 12, LoraLayer 14: Decay factor: 0.37482190132141113\n",
      "\n",
      "Testing loss: 2.302586078643799, acc: 0.09889999777078629\n",
      "\n",
      "1667/1667 [==============================] - 47s 28ms/step - loss: 0.2407 - accuracy: 0.9157 - val_loss: 2.3026 - val_accuracy: 0.0989\n",
      "Epoch 13/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.2951 - accuracy: 0.8958\n",
      "End of epoch 13, LoraLayer 0: 21671 Step\n",
      "End of epoch 13, LoraLayer 0: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 1: 21671 Step\n",
      "End of epoch 13, LoraLayer 1: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 2: 21671 Step\n",
      "End of epoch 13, LoraLayer 2: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 3: 21671 Step\n",
      "End of epoch 13, LoraLayer 3: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 4: 21671 Step\n",
      "End of epoch 13, LoraLayer 4: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 5: 21671 Step\n",
      "End of epoch 13, LoraLayer 5: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 6: 21671 Step\n",
      "End of epoch 13, LoraLayer 6: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 7: 21671 Step\n",
      "End of epoch 13, LoraLayer 7: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 8: 21671 Step\n",
      "End of epoch 13, LoraLayer 8: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 9: 21671 Step\n",
      "End of epoch 13, LoraLayer 9: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 10: 21671 Step\n",
      "End of epoch 13, LoraLayer 10: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 11: 21671 Step\n",
      "End of epoch 13, LoraLayer 11: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 12: 21671 Step\n",
      "End of epoch 13, LoraLayer 12: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 13: 21671 Step\n",
      "End of epoch 13, LoraLayer 13: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 14: 21671 Step\n",
      "End of epoch 13, LoraLayer 14: Decay factor: 0.31230783462524414\n",
      "\n",
      "Testing loss: 2.3027515411376953, acc: 0.09989999979734421\n",
      "\n",
      "1667/1667 [==============================] - 47s 28ms/step - loss: 0.2951 - accuracy: 0.8958 - val_loss: 2.3027 - val_accuracy: 0.1001\n",
      "Epoch 14/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.3569 - accuracy: 0.8758\n",
      "End of epoch 14, LoraLayer 0: 23338 Step\n",
      "End of epoch 14, LoraLayer 0: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 1: 23338 Step\n",
      "End of epoch 14, LoraLayer 1: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 2: 23338 Step\n",
      "End of epoch 14, LoraLayer 2: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 3: 23338 Step\n",
      "End of epoch 14, LoraLayer 3: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 4: 23338 Step\n",
      "End of epoch 14, LoraLayer 4: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 5: 23338 Step\n",
      "End of epoch 14, LoraLayer 5: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 6: 23338 Step\n",
      "End of epoch 14, LoraLayer 6: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 7: 23338 Step\n",
      "End of epoch 14, LoraLayer 7: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 8: 23338 Step\n",
      "End of epoch 14, LoraLayer 8: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 9: 23338 Step\n",
      "End of epoch 14, LoraLayer 9: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 10: 23338 Step\n",
      "End of epoch 14, LoraLayer 10: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 11: 23338 Step\n",
      "End of epoch 14, LoraLayer 11: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 12: 23338 Step\n",
      "End of epoch 14, LoraLayer 12: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 13: 23338 Step\n",
      "End of epoch 14, LoraLayer 13: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 14: 23338 Step\n",
      "End of epoch 14, LoraLayer 14: Decay factor: 0.24979376792907715\n",
      "\n",
      "Testing loss: 2.3017427921295166, acc: 0.1096000000834465\n",
      "\n",
      "1667/1667 [==============================] - 46s 27ms/step - loss: 0.3569 - accuracy: 0.8758 - val_loss: 2.3017 - val_accuracy: 0.1095\n",
      "Epoch 15/20\n",
      "1665/1667 [============================>.] - ETA: 0s - loss: 0.4370 - accuracy: 0.8487\n",
      "End of epoch 15, LoraLayer 0: 25005 Step\n",
      "End of epoch 15, LoraLayer 0: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 1: 25005 Step\n",
      "End of epoch 15, LoraLayer 1: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 2: 25005 Step\n",
      "End of epoch 15, LoraLayer 2: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 3: 25005 Step\n",
      "End of epoch 15, LoraLayer 3: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 4: 25005 Step\n",
      "End of epoch 15, LoraLayer 4: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 5: 25005 Step\n",
      "End of epoch 15, LoraLayer 5: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 6: 25005 Step\n",
      "End of epoch 15, LoraLayer 6: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 7: 25005 Step\n",
      "End of epoch 15, LoraLayer 7: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 8: 25005 Step\n",
      "End of epoch 15, LoraLayer 8: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 9: 25005 Step\n",
      "End of epoch 15, LoraLayer 9: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 10: 25005 Step\n",
      "End of epoch 15, LoraLayer 10: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 11: 25005 Step\n",
      "End of epoch 15, LoraLayer 11: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 12: 25005 Step\n",
      "End of epoch 15, LoraLayer 12: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 13: 25005 Step\n",
      "End of epoch 15, LoraLayer 13: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 14: 25005 Step\n",
      "End of epoch 15, LoraLayer 14: Decay factor: 0.18727970123291016\n",
      "\n",
      "Testing loss: 2.2969963550567627, acc: 0.12160000205039978\n",
      "\n",
      "1667/1667 [==============================] - 46s 27ms/step - loss: 0.4369 - accuracy: 0.8487 - val_loss: 2.2970 - val_accuracy: 0.1217\n",
      "Epoch 16/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.5188 - accuracy: 0.8193\n",
      "End of epoch 16, LoraLayer 0: 26672 Step\n",
      "End of epoch 16, LoraLayer 0: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 1: 26672 Step\n",
      "End of epoch 16, LoraLayer 1: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 2: 26672 Step\n",
      "End of epoch 16, LoraLayer 2: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 3: 26672 Step\n",
      "End of epoch 16, LoraLayer 3: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 4: 26672 Step\n",
      "End of epoch 16, LoraLayer 4: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 5: 26672 Step\n",
      "End of epoch 16, LoraLayer 5: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 6: 26672 Step\n",
      "End of epoch 16, LoraLayer 6: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 7: 26672 Step\n",
      "End of epoch 16, LoraLayer 7: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 8: 26672 Step\n",
      "End of epoch 16, LoraLayer 8: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 9: 26672 Step\n",
      "End of epoch 16, LoraLayer 9: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 10: 26672 Step\n",
      "End of epoch 16, LoraLayer 10: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 11: 26672 Step\n",
      "End of epoch 16, LoraLayer 11: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 12: 26672 Step\n",
      "End of epoch 16, LoraLayer 12: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 13: 26672 Step\n",
      "End of epoch 16, LoraLayer 13: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 14: 26672 Step\n",
      "End of epoch 16, LoraLayer 14: Decay factor: 0.12476563453674316\n",
      "\n",
      "Testing loss: 2.240123748779297, acc: 0.18549999594688416\n",
      "\n",
      "1667/1667 [==============================] - 46s 28ms/step - loss: 0.5188 - accuracy: 0.8193 - val_loss: 2.2401 - val_accuracy: 0.1855\n",
      "Epoch 17/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.6013 - accuracy: 0.7932\n",
      "End of epoch 17, LoraLayer 0: 28339 Step\n",
      "End of epoch 17, LoraLayer 0: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 1: 28339 Step\n",
      "End of epoch 17, LoraLayer 1: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 2: 28339 Step\n",
      "End of epoch 17, LoraLayer 2: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 3: 28339 Step\n",
      "End of epoch 17, LoraLayer 3: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 4: 28339 Step\n",
      "End of epoch 17, LoraLayer 4: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 5: 28339 Step\n",
      "End of epoch 17, LoraLayer 5: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 6: 28339 Step\n",
      "End of epoch 17, LoraLayer 6: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 7: 28339 Step\n",
      "End of epoch 17, LoraLayer 7: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 8: 28339 Step\n",
      "End of epoch 17, LoraLayer 8: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 9: 28339 Step\n",
      "End of epoch 17, LoraLayer 9: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 10: 28339 Step\n",
      "End of epoch 17, LoraLayer 10: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 11: 28339 Step\n",
      "End of epoch 17, LoraLayer 11: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 12: 28339 Step\n",
      "End of epoch 17, LoraLayer 12: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 13: 28339 Step\n",
      "End of epoch 17, LoraLayer 13: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 14: 28339 Step\n",
      "End of epoch 17, LoraLayer 14: Decay factor: 0.06225156784057617\n",
      "\n",
      "Testing loss: 1.8376857042312622, acc: 0.4323999881744385\n",
      "\n",
      "1667/1667 [==============================] - 46s 28ms/step - loss: 0.6013 - accuracy: 0.7932 - val_loss: 1.8377 - val_accuracy: 0.4324\n",
      "Epoch 18/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.6944 - accuracy: 0.7638\n",
      "End of epoch 18, LoraLayer 0: 30006 Step\n",
      "End of epoch 18, LoraLayer 0: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 1: 30006 Step\n",
      "End of epoch 18, LoraLayer 1: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 2: 30006 Step\n",
      "End of epoch 18, LoraLayer 2: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 3: 30006 Step\n",
      "End of epoch 18, LoraLayer 3: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 4: 30006 Step\n",
      "End of epoch 18, LoraLayer 4: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 5: 30006 Step\n",
      "End of epoch 18, LoraLayer 5: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 6: 30006 Step\n",
      "End of epoch 18, LoraLayer 6: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 7: 30006 Step\n",
      "End of epoch 18, LoraLayer 7: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 8: 30006 Step\n",
      "End of epoch 18, LoraLayer 8: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 9: 30006 Step\n",
      "End of epoch 18, LoraLayer 9: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 10: 30006 Step\n",
      "End of epoch 18, LoraLayer 10: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 11: 30006 Step\n",
      "End of epoch 18, LoraLayer 11: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 12: 30006 Step\n",
      "End of epoch 18, LoraLayer 12: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 13: 30006 Step\n",
      "End of epoch 18, LoraLayer 13: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 14: 30006 Step\n",
      "End of epoch 18, LoraLayer 14: Decay factor: 0.0\n",
      "\n",
      "Testing loss: 0.8631668090820312, acc: 0.7131999731063843\n",
      "\n",
      "1667/1667 [==============================] - 44s 26ms/step - loss: 0.6944 - accuracy: 0.7638 - val_loss: 0.8632 - val_accuracy: 0.7133\n",
      "Epoch 19/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.6890 - accuracy: 0.7693\n",
      "End of epoch 19, LoraLayer 0: 31673 Step\n",
      "End of epoch 19, LoraLayer 0: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 1: 31673 Step\n",
      "End of epoch 19, LoraLayer 1: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 2: 31673 Step\n",
      "End of epoch 19, LoraLayer 2: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 3: 31673 Step\n",
      "End of epoch 19, LoraLayer 3: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 4: 31673 Step\n",
      "End of epoch 19, LoraLayer 4: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 5: 31673 Step\n",
      "End of epoch 19, LoraLayer 5: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 6: 31673 Step\n",
      "End of epoch 19, LoraLayer 6: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 7: 31673 Step\n",
      "End of epoch 19, LoraLayer 7: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 8: 31673 Step\n",
      "End of epoch 19, LoraLayer 8: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 9: 31673 Step\n",
      "End of epoch 19, LoraLayer 9: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 10: 31673 Step\n",
      "End of epoch 19, LoraLayer 10: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 11: 31673 Step\n",
      "End of epoch 19, LoraLayer 11: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 12: 31673 Step\n",
      "End of epoch 19, LoraLayer 12: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 13: 31673 Step\n",
      "End of epoch 19, LoraLayer 13: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 14: 31673 Step\n",
      "End of epoch 19, LoraLayer 14: Decay factor: 0.0\n",
      "\n",
      "Testing loss: 0.7362777590751648, acc: 0.7602999806404114\n",
      "\n",
      "1667/1667 [==============================] - 46s 27ms/step - loss: 0.6890 - accuracy: 0.7693 - val_loss: 0.7363 - val_accuracy: 0.7605\n",
      "Epoch 20/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.6018 - accuracy: 0.7958\n",
      "End of epoch 20, LoraLayer 0: 33340 Step\n",
      "End of epoch 20, LoraLayer 0: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 1: 33340 Step\n",
      "End of epoch 20, LoraLayer 1: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 2: 33340 Step\n",
      "End of epoch 20, LoraLayer 2: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 3: 33340 Step\n",
      "End of epoch 20, LoraLayer 3: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 4: 33340 Step\n",
      "End of epoch 20, LoraLayer 4: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 5: 33340 Step\n",
      "End of epoch 20, LoraLayer 5: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 6: 33340 Step\n",
      "End of epoch 20, LoraLayer 6: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 7: 33340 Step\n",
      "End of epoch 20, LoraLayer 7: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 8: 33340 Step\n",
      "End of epoch 20, LoraLayer 8: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 9: 33340 Step\n",
      "End of epoch 20, LoraLayer 9: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 10: 33340 Step\n",
      "End of epoch 20, LoraLayer 10: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 11: 33340 Step\n",
      "End of epoch 20, LoraLayer 11: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 12: 33340 Step\n",
      "End of epoch 20, LoraLayer 12: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 13: 33340 Step\n",
      "End of epoch 20, LoraLayer 13: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 14: 33340 Step\n",
      "End of epoch 20, LoraLayer 14: Decay factor: 0.0\n",
      "\n",
      "Testing loss: 0.7439601421356201, acc: 0.7519000172615051\n",
      "\n",
      "1667/1667 [==============================] - 46s 27ms/step - loss: 0.6017 - accuracy: 0.7958 - val_loss: 0.7439 - val_accuracy: 0.7517\n"
     ]
    }
   ],
   "source": [
    "history_exp26 = exp26_lora_vgg16.fit(x_train, y_train, batch_size=30, epochs=20, validation_data=(x_test, y_test), callbacks=[print_step_callback, TestCallback((x_test, y_test))] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p2oLoaoZACLB",
    "outputId": "7913fd46-3d5e-43c3-a21b-37e64438053b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 3s 9ms/step - loss: 0.7440 - accuracy: 0.7519\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7439601421356201, 0.7519000172615051]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 평가\n",
    "exp26_lora_vgg16.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "id": "7lmu1SLyACLB",
    "outputId": "42b5f2b2-e3fe-4e09-81b9-84460b7ed130"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 학습 및 검증 정확도 시각화\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# 정확도 그래프\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_exp26.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history_exp26.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# 손실 그래프\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_exp26.history['loss'], label='Training Loss')\n",
    "plt.plot(history_exp26.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
