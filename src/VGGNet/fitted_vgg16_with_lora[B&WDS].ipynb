{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "McBYcpzkTZ9_",
    "outputId": "a04948c3-a789-4ffb-cebc-551911fafe9f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-30 09:13:07.110621: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-30 09:13:07.110676: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-30 09:13:07.110728: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-30 09:13:07.119084: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.14.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x3H9TbtJTO5k"
   },
   "source": [
    "# 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9GWr9Ep7TOZi",
    "outputId": "61f248ce-932e-4ab3-d215-c621d53787f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data\n",
      "(50000, 32, 32, 3)\n",
      "(50000, 1)\n",
      "test data\n",
      "(10000, 32, 32, 3)\n",
      "(10000, 1)\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "print('train data')\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print('test data')\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "5IyhwDy0TcOf"
   },
   "outputs": [],
   "source": [
    "# 데이터 전처리: 정규화\n",
    "x_train, x_test = x_train / 255.0,  x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s1xQRMYeTg3_",
    "outputId": "53e8f401-3c97-40b1-de46-fcc56e52e0ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data\n",
      "(50000, 32, 32, 3)\n",
      "(50000, 10)\n",
      "test data\n",
      "(10000, 32, 32, 3)\n",
      "(10000, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-30 09:13:10.833610: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18452 MB memory:  -> device: 0, name: NVIDIA RTX 4000 SFF Ada Generation, pci bus id: 0000:8b:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "# scalar 형태의 레이블(0-9)을 one-hot encoding 형태로 변환합니다\n",
    "\n",
    "y_train = tf.squeeze(tf.one_hot(y_train, 10),axis=1)\n",
    "y_test = tf.squeeze(tf.one_hot(y_test, 10), axis=1)\n",
    "\n",
    "print('train data')\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print('test data')\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nj-DhZZZTRxC"
   },
   "source": [
    "# 모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mfhhV71RMn2Y",
    "outputId": "ad1dfc62-9138-439c-9369-1dc02bf4b978"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "_LJi10btMdaM"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "best_vgg16 = load_model(\"./best_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "olYdP2YUM_Pi",
    "outputId": "d612fc3b-5117-4cde-cbf6-00c8d1327175"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 32, 32, 64)        1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 32, 32, 64)        36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 16, 16, 128)       73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 16, 16, 128)       147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 8, 8, 256)         295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 8, 8, 256)         590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 8, 8, 256)         590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 4, 4, 256)         0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 4, 4, 512)         1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 4, 4, 512)         2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 4, 4, 512)         2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 2, 2, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 2, 2, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 2, 2, 512)         2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 1, 1, 512)         0         \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 4096)              2101248   \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 512)               2097664   \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 18918730 (72.17 MB)\n",
      "Trainable params: 18918730 (72.17 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "best_vgg16.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zoj2Gx6bfMfH"
   },
   "source": [
    "# inference time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fJf4zX5-fGCf",
    "outputId": "8f9a98fb-01cf-42b0-9197-9890ae336575"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-30 09:13:12.772160: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8905\n",
      "2023-11-30 09:13:12.928267: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  8/313 [..............................] - ETA: 2s - loss: 0.5679 - accuracy: 0.8516  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-30 09:13:13.382949: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 4s 8ms/step - loss: 0.5326 - accuracy: 0.8536\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5325812697410583, 0.853600025177002]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 평가\n",
    "best_vgg16.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hv-4CTjZPIoJ"
   },
   "source": [
    "## 가중치 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s1m-0r85R0TO",
    "outputId": "baf4a76f-e104-43e1-8a8d-0a4b541c6971"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: input_3\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block1_conv1\n",
      "  Trainable parameters: 1792\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block1_conv2\n",
      "  Trainable parameters: 36928\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block1_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block2_conv1\n",
      "  Trainable parameters: 73856\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block2_conv2\n",
      "  Trainable parameters: 147584\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block2_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block3_conv1\n",
      "  Trainable parameters: 295168\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block3_conv2\n",
      "  Trainable parameters: 590080\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block3_conv3\n",
      "  Trainable parameters: 590080\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block3_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block4_conv1\n",
      "  Trainable parameters: 1180160\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block4_conv2\n",
      "  Trainable parameters: 2359808\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block4_conv3\n",
      "  Trainable parameters: 2359808\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block4_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block5_conv1\n",
      "  Trainable parameters: 2359808\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block5_conv2\n",
      "  Trainable parameters: 2359808\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block5_conv3\n",
      "  Trainable parameters: 2359808\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block5_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: flatten_2\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: dense_6\n",
      "  Trainable parameters: 2101248\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: dropout_4\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: dense_7\n",
      "  Trainable parameters: 2097664\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: dropout_5\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: dense_8\n",
      "  Trainable parameters: 5130\n",
      "  Non-trainable parameters: 0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "#모델의 각 레이어를 순회하며 파라미터 수를 계산\n",
    "for layer in best_vgg16.layers:\n",
    "    trainable_count = np.sum([tf.size(w).numpy() for w in layer.trainable_weights])\n",
    "    non_trainable_count = np.sum([tf.size(w).numpy() for w in layer.non_trainable_weights])\n",
    "\n",
    "    print(f\"Layer: {layer.name}\")\n",
    "    print(f\"  Trainable parameters: {trainable_count}\")\n",
    "    print(f\"  Non-trainable parameters: {non_trainable_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MX5RRcHq9CDt"
   },
   "source": [
    "# [Code] LoRA\n",
    "\n",
    "아래의 lora 코드에는 scheduling factor와 noise가 포함되어 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UylB2lpL9Gy5"
   },
   "source": [
    "## ConvLoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "npn-lPaP9Hev"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, initializers\n",
    "from tensorflow.keras.layers import Conv2D, Conv1D, Conv3D\n",
    "\n",
    "class ConvLoRALayer00_cdn2(layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        original_conv_layer,\n",
    "        total_iteration = 1000 ,  # Total number of iterations for the decay\n",
    "        start_percent=0.1,  # The percentage of total_iteration when decay starts\n",
    "        end_percent=0.9,  # The percentage of total_iteration when decay ends\n",
    "        min_decay_factor=0,  # The minimum value that decay factor can take\n",
    "        rank=32,\n",
    "        alpha=32,\n",
    "        trainable=True,\n",
    "        **kwargs\n",
    "    ):\n",
    "        # Capture the original layer's configuration.\n",
    "        original_layer_config = original_conv_layer.get_config()\n",
    "        name = original_layer_config[\"name\"]\n",
    "        kwargs.pop(\"name\", None)\n",
    "\n",
    "        super().__init__(name=name, trainable=trainable, **kwargs)\n",
    "\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self._scale = alpha / rank\n",
    "\n",
    "        # The original convolutional layer is set to non-trainable to freeze its weights.\n",
    "        self.original_conv_layer = original_conv_layer\n",
    "        self.original_conv_layer.trainable = False\n",
    "\n",
    "        self.kernel = None\n",
    "        self.filters = original_conv_layer.filters #\n",
    "        self.kernel_size = original_conv_layer.kernel_size[0] #\n",
    "        self.in_channels = None\n",
    "\n",
    "        self.total_iteration = total_iteration\n",
    "        self.start_step = int(total_iteration * start_percent)\n",
    "        self.end_step = int(total_iteration * end_percent)\n",
    "        self.min_decay_factor = min_decay_factor\n",
    "\n",
    "        #trainable=False, 이 변수가 텐서플로우의 자동 미분 및 최적화 과정에 의해 업데이트되지 않는다는 뜻\n",
    "        #수동으로 업데이트될 수 있습니다. 예를 들어, 반복문 안에서 이 변수의 값을 업데이트하는 로직을 작성할 수 있음!\n",
    "        self.current_step = tf.Variable(0, dtype=tf.int32, trainable=False)\n",
    "        self.decay_factor = tf.Variable(1.0, dtype=tf.float32, trainable=False)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Ensure the original convolutional layer is built.\n",
    "        #if not self.original_conv_layer.built:\n",
    "        #    self.original_conv_layer.build(input_shape)\n",
    "\n",
    "        # Calculate the shape for LoRA weights A and B.\n",
    "        #self.kernel = self.original_conv_layer.kernel\n",
    "        self.in_channels = input_shape[-1]\n",
    "\n",
    "        in_channels = self.in_channels\n",
    "        out_channels = self.filters\n",
    "        kernel_size = self.original_conv_layer.kernel_size[0]\n",
    "\n",
    "        # LoRA weights A and B.\n",
    "        self.A_weight = self.add_weight(\n",
    "            name=\"lora_A_weight\",\n",
    "            shape=(self.rank*kernel_size, in_channels*kernel_size),\n",
    "            initializer=initializers.VarianceScaling(scale=1.0, mode='fan_in', distribution='uniform'),\n",
    "            trainable=self.trainable\n",
    "        )\n",
    "\n",
    "        self.B_weight = self.add_weight(\n",
    "            name=\"lora_B_weight\",\n",
    "            shape=(out_channels*kernel_size, self.rank*kernel_size),\n",
    "            initializer=\"zeros\",\n",
    "            trainable=self.trainable\n",
    "        )\n",
    "\n",
    "        bias_shape = self.original_conv_layer.bias.shape\n",
    "        self.C_weight = self.add_weight(\n",
    "            name=\"lora_C_weight\",\n",
    "            shape=bias_shape,\n",
    "            initializer=\"zeros\",\n",
    "            trainable=self.trainable\n",
    "        )\n",
    "\n",
    "        super().build(input_shape)\n",
    "\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        if training is None:\n",
    "                training = self.trainable\n",
    "\n",
    "        # Calculate the linear decay factor\n",
    "        if self.current_step < self.start_step:\n",
    "            self.decay_factor.assign(1.0)  # Decay has not started yet\n",
    "        elif self.current_step > self.end_step:\n",
    "            self.decay_factor.assign(tf.cast(self.min_decay_factor, dtype=tf.float32))  # Ensure float32 type for consistency\n",
    "        else:\n",
    "            # Linear decay between start_step and end_step\n",
    "            self.decay_factor.assign(1.0 - ((tf.cast(self.current_step, dtype=tf.float32) - self.start_step) /\n",
    "                                    (self.end_step - self.start_step) *\n",
    "                                    (1.0 - tf.cast(self.min_decay_factor, dtype=tf.float32))))\n",
    "\n",
    "        lora_BA = (self.B_weight@self.A_weight)\n",
    "\n",
    "        kernel_size = self.original_conv_layer.kernel_size[0]\n",
    "        in_channels = self.in_channels\n",
    "        out_channels = self.filters\n",
    "\n",
    "           # lora_BA의 형태 변환\n",
    "           # lora_BA가 (out_channels*kernel_size*kernel_size, in_channels*kernel_size*kernel_size) 형태라고 가정\n",
    "           # 이를 (kernel_size, kernel_size, in_channels, out_channels)로 변환\n",
    "        lora_BA_reshaped = tf.reshape(lora_BA, (out_channels, kernel_size, kernel_size, in_channels))\n",
    "        lora_BA_reshaped = tf.transpose(lora_BA_reshaped, [1, 2, 3, 0])\n",
    "        lora_output = tf.nn.conv2d(inputs, lora_BA_reshaped, strides=[1, 1, 1, 1], padding='SAME') * self._scale\n",
    "\n",
    "        # original_output = self.original_conv_layer(inputs) * self.decay_factor\n",
    "\n",
    "        if training:\n",
    "            original_output = self.original_conv_layer(inputs)\n",
    "            # 평균과 표준편차 계산\n",
    "            original_weight_matrix = self.original_conv_layer.weights[0]\n",
    "            original_mean = tf.reduce_mean(original_weight_matrix)\n",
    "            original_variance = tf.reduce_mean(tf.square(original_weight_matrix - original_mean))\n",
    "            original_stddev = tf.sqrt(original_variance)\n",
    "\n",
    "            # decay_factor가 0.3보다 작으면 noise_mean과 noise_std를 0으로 설정\n",
    "            noise_mean = tf.where(self.decay_factor < 0.3, 0.0, original_mean * (1 - self.decay_factor))\n",
    "            noise_std = tf.where(self.decay_factor < 0.3, 0.0, original_stddev * tf.sqrt(1 - tf.square(self.decay_factor)))\n",
    "            noise = tf.random.normal(tf.shape(original_weight_matrix), mean=noise_mean, stddev=noise_std)\n",
    "            noise_output = tf.nn.conv2d(inputs, noise, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "            self.current_step.assign_add(1)\n",
    "\n",
    "            return original_output * self.decay_factor + lora_output + self.C_weight\n",
    "\n",
    "        else:\n",
    "            # 추론 모드에서는 LoRA 출력만 반환\n",
    "            return lora_output + self.C_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CLLXZoFi9UgM"
   },
   "source": [
    "## DenseLoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "b3zto3tY9Wu8"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from tensorflow import keras\n",
    "\n",
    "class LoraLayer(keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        original_layer,\n",
    "        total_iteration = 1000 ,  # Total number of iterations for the decay\n",
    "        start_percent=0.1,  # The percentage of total_iteration when decay starts\n",
    "        end_percent=0.9,  # The percentage of total_iteration when decay ends\n",
    "        min_decay_factor=0,  # The minimum value that decay factor can take\n",
    "        rank=32,\n",
    "        alpha=32,\n",
    "        trainable=True,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        original_layer_config = original_layer.get_config()\n",
    "        name = original_layer_config[\"name\"]\n",
    "        kwargs.pop(\"name\", None)\n",
    "\n",
    "        super().__init__(name=name, trainable=trainable, **kwargs)\n",
    "\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self._scale = alpha / rank\n",
    "\n",
    "        self.original_layer = original_layer\n",
    "        self.original_layer.trainable = False\n",
    "\n",
    "\n",
    "        self.total_iteration = total_iteration\n",
    "        self.start_step = int(total_iteration * start_percent)\n",
    "        self.end_step = int(total_iteration * end_percent)\n",
    "        self.min_decay_factor = min_decay_factor\n",
    "\n",
    "        #trainable=False, 이 변수가 텐서플로우의 자동 미분 및 최적화 과정에 의해 업데이트되지 않는다는 뜻\n",
    "        #수동으로 업데이트될 수 있습니다. 예를 들어, 반복문 안에서 이 변수의 값을 업데이트하는 로직을 작성할 수 있음!\n",
    "        self.current_step = tf.Variable(0, dtype=tf.int32, trainable=False)\n",
    "        self.decay_factor = tf.Variable(1.0, dtype=tf.float32, trainable=False)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # LoRA weights.\n",
    "        kernel_shape = self.original_layer.kernel.shape\n",
    "        self.A_weight = self.add_weight(\n",
    "            name=\"lora_A_weight\",\n",
    "            shape=(self.rank, kernel_shape[0]),\n",
    "            initializer=keras.initializers.VarianceScaling(\n",
    "                scale=math.sqrt(5), mode=\"fan_in\", distribution=\"uniform\"\n",
    "            ),\n",
    "            trainable=self.trainable,\n",
    "        )\n",
    "\n",
    "        self.B_weight = self.add_weight(\n",
    "            name=\"lora_B_weight\",\n",
    "            shape=(self.original_layer.units, self.rank),\n",
    "            initializer='zeros',\n",
    "            trainable=self.trainable,\n",
    "        )\n",
    "\n",
    "        self.C_weight = self.add_weight(\n",
    "            name=\"lora_C_weight\",\n",
    "            shape=(self.original_layer.units,),\n",
    "            initializer='zeros',\n",
    "            trainable=self.trainable,\n",
    "        )\n",
    "\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "            if training is None:\n",
    "                training = self.trainable\n",
    "\n",
    "            # Calculate the linear decay factor\n",
    "            if self.current_step < self.start_step:\n",
    "                self.decay_factor.assign(1.0)  # Decay has not started yet\n",
    "            elif self.current_step > self.end_step:\n",
    "                self.decay_factor.assign(tf.cast(self.min_decay_factor, dtype=tf.float32))  # Ensure float32 type for consistency\n",
    "            else:\n",
    "                # Linear decay between start_step and end_step\n",
    "                self.decay_factor.assign(1.0 - ((tf.cast(self.current_step, dtype=tf.float32) - self.start_step) /\n",
    "                                        (self.end_step - self.start_step) *\n",
    "                                        (1.0 - tf.cast(self.min_decay_factor, dtype=tf.float32))))\n",
    "\n",
    "            # Matrix multiplication for A and B weights with inputs\n",
    "            lora_A_output = tf.matmul(self.A_weight, tf.transpose(inputs))  # Ax\n",
    "            lora_output = tf.transpose(tf.matmul(self.B_weight, lora_A_output) * self._scale)  # BAx Transpose back to [batch_size, original_layer.units]\n",
    "\n",
    "            #lora_output *= (1 - self.decay_factor) # 멘토링 때 나온 의견\n",
    "\n",
    "            if training:\n",
    "                original_output = self.original_layer(inputs)\n",
    "                # 평균과 표준편차 계산\n",
    "                original_weight_matrix = self.original_layer.weights[0]\n",
    "                original_mean = tf.reduce_mean(original_weight_matrix, axis=0)\n",
    "                original_variance = tf.reduce_mean(tf.square(original_weight_matrix - original_mean), axis=0)\n",
    "                original_stddev = tf.sqrt(original_variance)\n",
    "\n",
    "                # decay_factor가 0.3보다 작으면 noise_mean과 noise_std를 0으로 설정\n",
    "                noise_mean = tf.where(self.decay_factor < 0.3, 0.0, original_mean * (1 - self.decay_factor))\n",
    "                noise_std = tf.where(self.decay_factor < 0.3, 0.0, original_stddev * tf.sqrt(1 - tf.square(self.decay_factor)))\n",
    "                noise = tf.random.normal(tf.shape(original_weight_matrix), mean=noise_mean, stddev=noise_std)\n",
    "\n",
    "                self.current_step.assign_add(1)\n",
    "\n",
    "                return original_output * self.decay_factor + lora_output + self.C_weight\n",
    "\n",
    "            else:\n",
    "                # 추론 모드에서는 LoRA 출력만 반환\n",
    "                return lora_output + self.C_weight\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KBM9sOz3_Yt1"
   },
   "source": [
    "# LoRA 적용. Exp1: Convlora + Denselayer는 ❄"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pww7eFLm30pR"
   },
   "source": [
    "## 1-1. (16, -)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "zuTOhngBFiYl"
   },
   "outputs": [],
   "source": [
    "dataset_size = x_train.shape[0]\n",
    "batch_size = 30\n",
    "epochs = 20\n",
    "\n",
    "total_iteration= int(epochs * (dataset_size / batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "BPKqW1Hk_X6m"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout, InputLayer\n",
    "import tensorflow as tf\n",
    "\n",
    "# 새로운 레이어를 추가하기 위한 레이어 리스트 초기화\n",
    "new_layers = []\n",
    "\n",
    "# VGG 모델의 각 레이어 순회\n",
    "for layer in best_vgg16.layers:\n",
    "    if isinstance(layer, InputLayer):\n",
    "        continue\n",
    "\n",
    "    if isinstance(layer, Conv2D):\n",
    "        # Conv2D 레이어를 ConvLoRALayer로 대체\n",
    "        conv_lora_layer = ConvLoRALayer00_cdn2(original_conv_layer=layer, rank=16, alpha=32, total_iteration = total_iteration, trainable=True)\n",
    "        new_layers.append(conv_lora_layer)\n",
    "\n",
    "    else:\n",
    "        # 다른 유형의 레이어는 그대로 유지\n",
    "        new_layers.append(layer)\n",
    "\n",
    "# 새로운 입력 텐서 생성\n",
    "input_tensor = best_vgg16.input\n",
    "\n",
    "# 새로운 모델 구성\n",
    "x = input_tensor\n",
    "for layer in new_layers:\n",
    "    x = layer(x)\n",
    "\n",
    "# 새로운 모델 생성\n",
    "exp11_lora_vgg16 = Model(inputs=input_tensor, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "uFd65OKPpW9T"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D\n",
    "\n",
    "# 새로 생성된 모델에서 Dense 레이어 중 LoRA가 적용되지 않은 레이어의 trainable을 False로 설정\n",
    "for layer in exp11_lora_vgg16.layers:\n",
    "    if isinstance(layer, Dense):\n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fP-6DUC7FopE",
    "outputId": "ffcabf22-e139-49d0-d76a-1fba020e7e1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " block1_conv1 (ConvLoRALaye  (None, 32, 32, 64)        11506     \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block1_conv2 (ConvLoRALaye  (None, 32, 32, 64)        55426     \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " block2_conv1 (ConvLoRALaye  (None, 16, 16, 128)       101634    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block2_conv2 (ConvLoRALaye  (None, 16, 16, 128)       184578    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " block3_conv1 (ConvLoRALaye  (None, 8, 8, 256)         350722    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block3_conv2 (ConvLoRALaye  (None, 8, 8, 256)         664066    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block3_conv3 (ConvLoRALaye  (None, 8, 8, 256)         664066    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 4, 4, 256)         0         \n",
      "                                                                 \n",
      " block4_conv1 (ConvLoRALaye  (None, 4, 4, 512)         1291266   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block4_conv2 (ConvLoRALaye  (None, 4, 4, 512)         2507778   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block4_conv3 (ConvLoRALaye  (None, 4, 4, 512)         2507778   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      " block5_conv1 (ConvLoRALaye  (None, 2, 2, 512)         2507778   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block5_conv2 (ConvLoRALaye  (None, 2, 2, 512)         2507778   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block5_conv3 (ConvLoRALaye  (None, 2, 2, 512)         2507778   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 1, 1, 512)         0         \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 4096)              2101248   \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 512)               2097664   \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20066196 (76.55 MB)\n",
      "Trainable params: 1147440 (4.38 MB)\n",
      "Non-trainable params: 18918756 (72.17 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "exp11_lora_vgg16.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4gdRpVfKPM3t"
   },
   "source": [
    "## 가중치 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VEO3IP_PRqSl",
    "outputId": "a19ebea2-61a9-4f67-9d53-619bc2692331"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: input_3\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block1_conv1\n",
      "  Trainable parameters: 9712\n",
      "  Non-trainable parameters: 1794\n",
      "Layer: block1_conv2\n",
      "  Trainable parameters: 18496\n",
      "  Non-trainable parameters: 36930\n",
      "Layer: block1_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block2_conv1\n",
      "  Trainable parameters: 27776\n",
      "  Non-trainable parameters: 73858\n",
      "Layer: block2_conv2\n",
      "  Trainable parameters: 36992\n",
      "  Non-trainable parameters: 147586\n",
      "Layer: block2_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block3_conv1\n",
      "  Trainable parameters: 55552\n",
      "  Non-trainable parameters: 295170\n",
      "Layer: block3_conv2\n",
      "  Trainable parameters: 73984\n",
      "  Non-trainable parameters: 590082\n",
      "Layer: block3_conv3\n",
      "  Trainable parameters: 73984\n",
      "  Non-trainable parameters: 590082\n",
      "Layer: block3_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block4_conv1\n",
      "  Trainable parameters: 111104\n",
      "  Non-trainable parameters: 1180162\n",
      "Layer: block4_conv2\n",
      "  Trainable parameters: 147968\n",
      "  Non-trainable parameters: 2359810\n",
      "Layer: block4_conv3\n",
      "  Trainable parameters: 147968\n",
      "  Non-trainable parameters: 2359810\n",
      "Layer: block4_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block5_conv1\n",
      "  Trainable parameters: 147968\n",
      "  Non-trainable parameters: 2359810\n",
      "Layer: block5_conv2\n",
      "  Trainable parameters: 147968\n",
      "  Non-trainable parameters: 2359810\n",
      "Layer: block5_conv3\n",
      "  Trainable parameters: 147968\n",
      "  Non-trainable parameters: 2359810\n",
      "Layer: block5_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: flatten_2\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: dense_6\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 2101248\n",
      "Layer: dropout_4\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: dense_7\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 2097664\n",
      "Layer: dropout_5\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: dense_8\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 5130\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "#모델의 각 레이어를 순회하며 파라미터 수를 계산\n",
    "for layer in exp11_lora_vgg16.layers:\n",
    "    trainable_count = np.sum([tf.size(w).numpy() for w in layer.trainable_weights])\n",
    "    non_trainable_count = np.sum([tf.size(w).numpy() for w in layer.non_trainable_weights])\n",
    "\n",
    "    print(f\"Layer: {layer.name}\")\n",
    "    print(f\"  Trainable parameters: {trainable_count}\")\n",
    "    print(f\"  Non-trainable parameters: {non_trainable_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mMG9bhGIU2ch"
   },
   "source": [
    "## 스케줄링 및 노이즈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "Knnl1gljU8aT"
   },
   "outputs": [],
   "source": [
    "lora_layers = []  # lora 레이어들을 저장할 리스트\n",
    "\n",
    "for layer in exp11_lora_vgg16.layers:\n",
    "    if isinstance(layer, ConvLoRALayer00_cdn2) or isinstance(layer, LoraLayer):\n",
    "        lora_layers.append(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "ESXAgLxgZVzR"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "class PrintCurrentStepCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, lora_layers):\n",
    "        super().__init__()\n",
    "        self.lora_layers = lora_layers\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print()\n",
    "        for i, lora_layer in enumerate(self.lora_layers):\n",
    "            current_step = lora_layer.current_step.value()\n",
    "            decay_factor = lora_layer.decay_factor.value()\n",
    "            print(f\"End of epoch {epoch + 1}, LoraLayer {i}: {current_step} Step\")\n",
    "            print(f\"End of epoch {epoch + 1}, LoraLayer {i}: Decay factor: {decay_factor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "f70WB6oBaKLk"
   },
   "outputs": [],
   "source": [
    "# 콜백 생성 시 lora_layers 딕셔너리의 값만 사용\n",
    "print_step_callback = PrintCurrentStepCallback(lora_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "PhpCpsu_aMtY"
   },
   "outputs": [],
   "source": [
    "class TestCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, test_data):\n",
    "        self.test_data = test_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        x, y = self.test_data\n",
    "        loss, acc = self.model.evaluate(x, y, verbose=0)\n",
    "        print('\\nTesting loss: {}, acc: {}\\n'.format(loss, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bKWXrQnopKB6"
   },
   "source": [
    "##학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "pnAakdZ8aRCW"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# 모델 컴파일\n",
    "exp11_lora_vgg16.compile(optimizer=Adam(learning_rate=0.0001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XQ1mcREqapIR",
    "outputId": "1413af14-7277-4ffd-c91d-8dfbd2443aea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " block1_conv1 (ConvLoRALaye  (None, 32, 32, 64)        11506     \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block1_conv2 (ConvLoRALaye  (None, 32, 32, 64)        55426     \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " block2_conv1 (ConvLoRALaye  (None, 16, 16, 128)       101634    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block2_conv2 (ConvLoRALaye  (None, 16, 16, 128)       184578    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " block3_conv1 (ConvLoRALaye  (None, 8, 8, 256)         350722    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block3_conv2 (ConvLoRALaye  (None, 8, 8, 256)         664066    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block3_conv3 (ConvLoRALaye  (None, 8, 8, 256)         664066    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 4, 4, 256)         0         \n",
      "                                                                 \n",
      " block4_conv1 (ConvLoRALaye  (None, 4, 4, 512)         1291266   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block4_conv2 (ConvLoRALaye  (None, 4, 4, 512)         2507778   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block4_conv3 (ConvLoRALaye  (None, 4, 4, 512)         2507778   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      " block5_conv1 (ConvLoRALaye  (None, 2, 2, 512)         2507778   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block5_conv2 (ConvLoRALaye  (None, 2, 2, 512)         2507778   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block5_conv3 (ConvLoRALaye  (None, 2, 2, 512)         2507778   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 1, 1, 512)         0         \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 4096)              2101248   \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 512)               2097664   \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20066196 (76.55 MB)\n",
      "Trainable params: 1147440 (4.38 MB)\n",
      "Non-trainable params: 18918756 (72.17 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "exp11_lora_vgg16.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PQWxiR4Qas7b",
    "outputId": "d75c95be-0610-4c6e-f0f1-a8c64ade3cbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-30 09:13:26.092418: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fc42f023ed0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-11-30 09:13:26.092452: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA RTX 4000 SFF Ada Generation, Compute Capability 8.9\n",
      "2023-11-30 09:13:26.097969: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-11-30 09:13:26.251396: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1667/1667 [==============================] - ETA: 0s - loss: 0.2196 - accuracy: 0.9306\n",
      "End of epoch 1, LoraLayer 0: 1667 Step\n",
      "End of epoch 1, LoraLayer 0: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 1: 1667 Step\n",
      "End of epoch 1, LoraLayer 1: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 2: 1667 Step\n",
      "End of epoch 1, LoraLayer 2: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 3: 1667 Step\n",
      "End of epoch 1, LoraLayer 3: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 4: 1667 Step\n",
      "End of epoch 1, LoraLayer 4: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 5: 1667 Step\n",
      "End of epoch 1, LoraLayer 5: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 6: 1667 Step\n",
      "End of epoch 1, LoraLayer 6: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 7: 1667 Step\n",
      "End of epoch 1, LoraLayer 7: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 8: 1667 Step\n",
      "End of epoch 1, LoraLayer 8: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 9: 1667 Step\n",
      "End of epoch 1, LoraLayer 9: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 10: 1667 Step\n",
      "End of epoch 1, LoraLayer 10: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 11: 1667 Step\n",
      "End of epoch 1, LoraLayer 11: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 12: 1667 Step\n",
      "End of epoch 1, LoraLayer 12: Decay factor: 1.0\n",
      "\n",
      "Testing loss: 2.303476572036743, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 62s 30ms/step - loss: 0.2196 - accuracy: 0.9306 - val_loss: 2.3035 - val_accuracy: 0.1000\n",
      "Epoch 2/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.1985 - accuracy: 0.9370\n",
      "End of epoch 2, LoraLayer 0: 3334 Step\n",
      "End of epoch 2, LoraLayer 0: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 1: 3334 Step\n",
      "End of epoch 2, LoraLayer 1: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 2: 3334 Step\n",
      "End of epoch 2, LoraLayer 2: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 3: 3334 Step\n",
      "End of epoch 2, LoraLayer 3: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 4: 3334 Step\n",
      "End of epoch 2, LoraLayer 4: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 5: 3334 Step\n",
      "End of epoch 2, LoraLayer 5: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 6: 3334 Step\n",
      "End of epoch 2, LoraLayer 6: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 7: 3334 Step\n",
      "End of epoch 2, LoraLayer 7: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 8: 3334 Step\n",
      "End of epoch 2, LoraLayer 8: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 9: 3334 Step\n",
      "End of epoch 2, LoraLayer 9: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 10: 3334 Step\n",
      "End of epoch 2, LoraLayer 10: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 11: 3334 Step\n",
      "End of epoch 2, LoraLayer 11: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 12: 3334 Step\n",
      "End of epoch 2, LoraLayer 12: Decay factor: 0.9999625086784363\n",
      "\n",
      "Testing loss: 2.3038108348846436, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 47s 28ms/step - loss: 0.1985 - accuracy: 0.9370 - val_loss: 2.3038 - val_accuracy: 0.1000\n",
      "Epoch 3/20\n",
      "1665/1667 [============================>.] - ETA: 0s - loss: 0.1707 - accuracy: 0.9450\n",
      "End of epoch 3, LoraLayer 0: 5001 Step\n",
      "End of epoch 3, LoraLayer 0: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 1: 5001 Step\n",
      "End of epoch 3, LoraLayer 1: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 2: 5001 Step\n",
      "End of epoch 3, LoraLayer 2: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 3: 5001 Step\n",
      "End of epoch 3, LoraLayer 3: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 4: 5001 Step\n",
      "End of epoch 3, LoraLayer 4: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 5: 5001 Step\n",
      "End of epoch 3, LoraLayer 5: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 6: 5001 Step\n",
      "End of epoch 3, LoraLayer 6: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 7: 5001 Step\n",
      "End of epoch 3, LoraLayer 7: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 8: 5001 Step\n",
      "End of epoch 3, LoraLayer 8: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 9: 5001 Step\n",
      "End of epoch 3, LoraLayer 9: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 10: 5001 Step\n",
      "End of epoch 3, LoraLayer 10: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 11: 5001 Step\n",
      "End of epoch 3, LoraLayer 11: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 12: 5001 Step\n",
      "End of epoch 3, LoraLayer 12: Decay factor: 0.9374484419822693\n",
      "\n",
      "Testing loss: 2.3038430213928223, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 48s 29ms/step - loss: 0.1708 - accuracy: 0.9449 - val_loss: 2.3038 - val_accuracy: 0.1000\n",
      "Epoch 4/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.1649 - accuracy: 0.9459\n",
      "End of epoch 4, LoraLayer 0: 6668 Step\n",
      "End of epoch 4, LoraLayer 0: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 1: 6668 Step\n",
      "End of epoch 4, LoraLayer 1: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 2: 6668 Step\n",
      "End of epoch 4, LoraLayer 2: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 3: 6668 Step\n",
      "End of epoch 4, LoraLayer 3: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 4: 6668 Step\n",
      "End of epoch 4, LoraLayer 4: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 5: 6668 Step\n",
      "End of epoch 4, LoraLayer 5: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 6: 6668 Step\n",
      "End of epoch 4, LoraLayer 6: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 7: 6668 Step\n",
      "End of epoch 4, LoraLayer 7: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 8: 6668 Step\n",
      "End of epoch 4, LoraLayer 8: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 9: 6668 Step\n",
      "End of epoch 4, LoraLayer 9: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 10: 6668 Step\n",
      "End of epoch 4, LoraLayer 10: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 11: 6668 Step\n",
      "End of epoch 4, LoraLayer 11: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 12: 6668 Step\n",
      "End of epoch 4, LoraLayer 12: Decay factor: 0.8749343752861023\n",
      "\n",
      "Testing loss: 2.304030179977417, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 46s 27ms/step - loss: 0.1649 - accuracy: 0.9459 - val_loss: 2.3040 - val_accuracy: 0.1000\n",
      "Epoch 5/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.1630 - accuracy: 0.9461\n",
      "End of epoch 5, LoraLayer 0: 8335 Step\n",
      "End of epoch 5, LoraLayer 0: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 1: 8335 Step\n",
      "End of epoch 5, LoraLayer 1: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 2: 8335 Step\n",
      "End of epoch 5, LoraLayer 2: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 3: 8335 Step\n",
      "End of epoch 5, LoraLayer 3: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 4: 8335 Step\n",
      "End of epoch 5, LoraLayer 4: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 5: 8335 Step\n",
      "End of epoch 5, LoraLayer 5: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 6: 8335 Step\n",
      "End of epoch 5, LoraLayer 6: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 7: 8335 Step\n",
      "End of epoch 5, LoraLayer 7: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 8: 8335 Step\n",
      "End of epoch 5, LoraLayer 8: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 9: 8335 Step\n",
      "End of epoch 5, LoraLayer 9: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 10: 8335 Step\n",
      "End of epoch 5, LoraLayer 10: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 11: 8335 Step\n",
      "End of epoch 5, LoraLayer 11: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 12: 8335 Step\n",
      "End of epoch 5, LoraLayer 12: Decay factor: 0.8124203085899353\n",
      "\n",
      "Testing loss: 2.304033041000366, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 53s 32ms/step - loss: 0.1630 - accuracy: 0.9461 - val_loss: 2.3040 - val_accuracy: 0.1000\n",
      "Epoch 6/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.1699 - accuracy: 0.9428\n",
      "End of epoch 6, LoraLayer 0: 10002 Step\n",
      "End of epoch 6, LoraLayer 0: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 1: 10002 Step\n",
      "End of epoch 6, LoraLayer 1: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 2: 10002 Step\n",
      "End of epoch 6, LoraLayer 2: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 3: 10002 Step\n",
      "End of epoch 6, LoraLayer 3: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 4: 10002 Step\n",
      "End of epoch 6, LoraLayer 4: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 5: 10002 Step\n",
      "End of epoch 6, LoraLayer 5: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 6: 10002 Step\n",
      "End of epoch 6, LoraLayer 6: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 7: 10002 Step\n",
      "End of epoch 6, LoraLayer 7: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 8: 10002 Step\n",
      "End of epoch 6, LoraLayer 8: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 9: 10002 Step\n",
      "End of epoch 6, LoraLayer 9: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 10: 10002 Step\n",
      "End of epoch 6, LoraLayer 10: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 11: 10002 Step\n",
      "End of epoch 6, LoraLayer 11: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 12: 10002 Step\n",
      "End of epoch 6, LoraLayer 12: Decay factor: 0.7499062418937683\n",
      "\n",
      "Testing loss: 2.3054146766662598, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 54s 32ms/step - loss: 0.1699 - accuracy: 0.9428 - val_loss: 2.3054 - val_accuracy: 0.1000\n",
      "Epoch 7/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.1780 - accuracy: 0.9397\n",
      "End of epoch 7, LoraLayer 0: 11669 Step\n",
      "End of epoch 7, LoraLayer 0: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 1: 11669 Step\n",
      "End of epoch 7, LoraLayer 1: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 2: 11669 Step\n",
      "End of epoch 7, LoraLayer 2: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 3: 11669 Step\n",
      "End of epoch 7, LoraLayer 3: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 4: 11669 Step\n",
      "End of epoch 7, LoraLayer 4: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 5: 11669 Step\n",
      "End of epoch 7, LoraLayer 5: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 6: 11669 Step\n",
      "End of epoch 7, LoraLayer 6: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 7: 11669 Step\n",
      "End of epoch 7, LoraLayer 7: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 8: 11669 Step\n",
      "End of epoch 7, LoraLayer 8: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 9: 11669 Step\n",
      "End of epoch 7, LoraLayer 9: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 10: 11669 Step\n",
      "End of epoch 7, LoraLayer 10: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 11: 11669 Step\n",
      "End of epoch 7, LoraLayer 11: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 12: 11669 Step\n",
      "End of epoch 7, LoraLayer 12: Decay factor: 0.6873922348022461\n",
      "\n",
      "Testing loss: 2.30804443359375, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 55s 33ms/step - loss: 0.1780 - accuracy: 0.9397 - val_loss: 2.3080 - val_accuracy: 0.1000\n",
      "Epoch 8/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.2034 - accuracy: 0.9302\n",
      "End of epoch 8, LoraLayer 0: 13336 Step\n",
      "End of epoch 8, LoraLayer 0: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 1: 13336 Step\n",
      "End of epoch 8, LoraLayer 1: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 2: 13336 Step\n",
      "End of epoch 8, LoraLayer 2: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 3: 13336 Step\n",
      "End of epoch 8, LoraLayer 3: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 4: 13336 Step\n",
      "End of epoch 8, LoraLayer 4: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 5: 13336 Step\n",
      "End of epoch 8, LoraLayer 5: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 6: 13336 Step\n",
      "End of epoch 8, LoraLayer 6: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 7: 13336 Step\n",
      "End of epoch 8, LoraLayer 7: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 8: 13336 Step\n",
      "End of epoch 8, LoraLayer 8: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 9: 13336 Step\n",
      "End of epoch 8, LoraLayer 9: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 10: 13336 Step\n",
      "End of epoch 8, LoraLayer 10: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 11: 13336 Step\n",
      "End of epoch 8, LoraLayer 11: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 12: 13336 Step\n",
      "End of epoch 8, LoraLayer 12: Decay factor: 0.6248781681060791\n",
      "\n",
      "Testing loss: 2.3097333908081055, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 49s 30ms/step - loss: 0.2034 - accuracy: 0.9301 - val_loss: 2.3097 - val_accuracy: 0.1000\n",
      "Epoch 9/20\n",
      "1665/1667 [============================>.] - ETA: 0s - loss: 0.2292 - accuracy: 0.9218\n",
      "End of epoch 9, LoraLayer 0: 15003 Step\n",
      "End of epoch 9, LoraLayer 0: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 1: 15003 Step\n",
      "End of epoch 9, LoraLayer 1: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 2: 15003 Step\n",
      "End of epoch 9, LoraLayer 2: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 3: 15003 Step\n",
      "End of epoch 9, LoraLayer 3: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 4: 15003 Step\n",
      "End of epoch 9, LoraLayer 4: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 5: 15003 Step\n",
      "End of epoch 9, LoraLayer 5: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 6: 15003 Step\n",
      "End of epoch 9, LoraLayer 6: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 7: 15003 Step\n",
      "End of epoch 9, LoraLayer 7: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 8: 15003 Step\n",
      "End of epoch 9, LoraLayer 8: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 9: 15003 Step\n",
      "End of epoch 9, LoraLayer 9: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 10: 15003 Step\n",
      "End of epoch 9, LoraLayer 10: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 11: 15003 Step\n",
      "End of epoch 9, LoraLayer 11: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 12: 15003 Step\n",
      "End of epoch 9, LoraLayer 12: Decay factor: 0.5623641014099121\n",
      "\n",
      "Testing loss: 2.3115622997283936, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 46s 28ms/step - loss: 0.2292 - accuracy: 0.9218 - val_loss: 2.3116 - val_accuracy: 0.1000\n",
      "Epoch 10/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.2674 - accuracy: 0.9102\n",
      "End of epoch 10, LoraLayer 0: 16670 Step\n",
      "End of epoch 10, LoraLayer 0: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 1: 16670 Step\n",
      "End of epoch 10, LoraLayer 1: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 2: 16670 Step\n",
      "End of epoch 10, LoraLayer 2: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 3: 16670 Step\n",
      "End of epoch 10, LoraLayer 3: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 4: 16670 Step\n",
      "End of epoch 10, LoraLayer 4: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 5: 16670 Step\n",
      "End of epoch 10, LoraLayer 5: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 6: 16670 Step\n",
      "End of epoch 10, LoraLayer 6: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 7: 16670 Step\n",
      "End of epoch 10, LoraLayer 7: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 8: 16670 Step\n",
      "End of epoch 10, LoraLayer 8: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 9: 16670 Step\n",
      "End of epoch 10, LoraLayer 9: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 10: 16670 Step\n",
      "End of epoch 10, LoraLayer 10: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 11: 16670 Step\n",
      "End of epoch 10, LoraLayer 11: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 12: 16670 Step\n",
      "End of epoch 10, LoraLayer 12: Decay factor: 0.4998500347137451\n",
      "\n",
      "Testing loss: 2.3085198402404785, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 51s 30ms/step - loss: 0.2674 - accuracy: 0.9102 - val_loss: 2.3085 - val_accuracy: 0.1000\n",
      "Epoch 11/20\n",
      "1665/1667 [============================>.] - ETA: 0s - loss: 0.3082 - accuracy: 0.8951\n",
      "End of epoch 11, LoraLayer 0: 18337 Step\n",
      "End of epoch 11, LoraLayer 0: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 1: 18337 Step\n",
      "End of epoch 11, LoraLayer 1: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 2: 18337 Step\n",
      "End of epoch 11, LoraLayer 2: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 3: 18337 Step\n",
      "End of epoch 11, LoraLayer 3: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 4: 18337 Step\n",
      "End of epoch 11, LoraLayer 4: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 5: 18337 Step\n",
      "End of epoch 11, LoraLayer 5: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 6: 18337 Step\n",
      "End of epoch 11, LoraLayer 6: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 7: 18337 Step\n",
      "End of epoch 11, LoraLayer 7: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 8: 18337 Step\n",
      "End of epoch 11, LoraLayer 8: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 9: 18337 Step\n",
      "End of epoch 11, LoraLayer 9: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 10: 18337 Step\n",
      "End of epoch 11, LoraLayer 10: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 11: 18337 Step\n",
      "End of epoch 11, LoraLayer 11: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 12: 18337 Step\n",
      "End of epoch 11, LoraLayer 12: Decay factor: 0.4373359680175781\n",
      "\n",
      "Testing loss: 2.320451021194458, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 46s 28ms/step - loss: 0.3084 - accuracy: 0.8950 - val_loss: 2.3205 - val_accuracy: 0.1000\n",
      "Epoch 12/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.3571 - accuracy: 0.8777\n",
      "End of epoch 12, LoraLayer 0: 20004 Step\n",
      "End of epoch 12, LoraLayer 0: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 1: 20004 Step\n",
      "End of epoch 12, LoraLayer 1: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 2: 20004 Step\n",
      "End of epoch 12, LoraLayer 2: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 3: 20004 Step\n",
      "End of epoch 12, LoraLayer 3: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 4: 20004 Step\n",
      "End of epoch 12, LoraLayer 4: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 5: 20004 Step\n",
      "End of epoch 12, LoraLayer 5: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 6: 20004 Step\n",
      "End of epoch 12, LoraLayer 6: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 7: 20004 Step\n",
      "End of epoch 12, LoraLayer 7: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 8: 20004 Step\n",
      "End of epoch 12, LoraLayer 8: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 9: 20004 Step\n",
      "End of epoch 12, LoraLayer 9: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 10: 20004 Step\n",
      "End of epoch 12, LoraLayer 10: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 11: 20004 Step\n",
      "End of epoch 12, LoraLayer 11: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 12: 20004 Step\n",
      "End of epoch 12, LoraLayer 12: Decay factor: 0.37482190132141113\n",
      "\n",
      "Testing loss: 2.332934856414795, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 52s 31ms/step - loss: 0.3570 - accuracy: 0.8777 - val_loss: 2.3329 - val_accuracy: 0.1000\n",
      "Epoch 13/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.4126 - accuracy: 0.8598\n",
      "End of epoch 13, LoraLayer 0: 21671 Step\n",
      "End of epoch 13, LoraLayer 0: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 1: 21671 Step\n",
      "End of epoch 13, LoraLayer 1: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 2: 21671 Step\n",
      "End of epoch 13, LoraLayer 2: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 3: 21671 Step\n",
      "End of epoch 13, LoraLayer 3: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 4: 21671 Step\n",
      "End of epoch 13, LoraLayer 4: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 5: 21671 Step\n",
      "End of epoch 13, LoraLayer 5: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 6: 21671 Step\n",
      "End of epoch 13, LoraLayer 6: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 7: 21671 Step\n",
      "End of epoch 13, LoraLayer 7: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 8: 21671 Step\n",
      "End of epoch 13, LoraLayer 8: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 9: 21671 Step\n",
      "End of epoch 13, LoraLayer 9: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 10: 21671 Step\n",
      "End of epoch 13, LoraLayer 10: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 11: 21671 Step\n",
      "End of epoch 13, LoraLayer 11: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 12: 21671 Step\n",
      "End of epoch 13, LoraLayer 12: Decay factor: 0.31230783462524414\n",
      "\n",
      "Testing loss: 2.3319950103759766, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 53s 32ms/step - loss: 0.4125 - accuracy: 0.8599 - val_loss: 2.3320 - val_accuracy: 0.1000\n",
      "Epoch 14/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.4664 - accuracy: 0.8415\n",
      "End of epoch 14, LoraLayer 0: 23338 Step\n",
      "End of epoch 14, LoraLayer 0: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 1: 23338 Step\n",
      "End of epoch 14, LoraLayer 1: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 2: 23338 Step\n",
      "End of epoch 14, LoraLayer 2: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 3: 23338 Step\n",
      "End of epoch 14, LoraLayer 3: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 4: 23338 Step\n",
      "End of epoch 14, LoraLayer 4: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 5: 23338 Step\n",
      "End of epoch 14, LoraLayer 5: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 6: 23338 Step\n",
      "End of epoch 14, LoraLayer 6: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 7: 23338 Step\n",
      "End of epoch 14, LoraLayer 7: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 8: 23338 Step\n",
      "End of epoch 14, LoraLayer 8: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 9: 23338 Step\n",
      "End of epoch 14, LoraLayer 9: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 10: 23338 Step\n",
      "End of epoch 14, LoraLayer 10: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 11: 23338 Step\n",
      "End of epoch 14, LoraLayer 11: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 12: 23338 Step\n",
      "End of epoch 14, LoraLayer 12: Decay factor: 0.24979376792907715\n",
      "\n",
      "Testing loss: 2.3441760540008545, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 54s 33ms/step - loss: 0.4665 - accuracy: 0.8415 - val_loss: 2.3442 - val_accuracy: 0.1000\n",
      "Epoch 15/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.5299 - accuracy: 0.8195\n",
      "End of epoch 15, LoraLayer 0: 25005 Step\n",
      "End of epoch 15, LoraLayer 0: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 1: 25005 Step\n",
      "End of epoch 15, LoraLayer 1: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 2: 25005 Step\n",
      "End of epoch 15, LoraLayer 2: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 3: 25005 Step\n",
      "End of epoch 15, LoraLayer 3: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 4: 25005 Step\n",
      "End of epoch 15, LoraLayer 4: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 5: 25005 Step\n",
      "End of epoch 15, LoraLayer 5: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 6: 25005 Step\n",
      "End of epoch 15, LoraLayer 6: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 7: 25005 Step\n",
      "End of epoch 15, LoraLayer 7: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 8: 25005 Step\n",
      "End of epoch 15, LoraLayer 8: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 9: 25005 Step\n",
      "End of epoch 15, LoraLayer 9: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 10: 25005 Step\n",
      "End of epoch 15, LoraLayer 10: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 11: 25005 Step\n",
      "End of epoch 15, LoraLayer 11: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 12: 25005 Step\n",
      "End of epoch 15, LoraLayer 12: Decay factor: 0.18727970123291016\n",
      "\n",
      "Testing loss: 2.352308511734009, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 50s 30ms/step - loss: 0.5300 - accuracy: 0.8195 - val_loss: 2.3523 - val_accuracy: 0.1000\n",
      "Epoch 16/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.6025 - accuracy: 0.7968\n",
      "End of epoch 16, LoraLayer 0: 26672 Step\n",
      "End of epoch 16, LoraLayer 0: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 1: 26672 Step\n",
      "End of epoch 16, LoraLayer 1: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 2: 26672 Step\n",
      "End of epoch 16, LoraLayer 2: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 3: 26672 Step\n",
      "End of epoch 16, LoraLayer 3: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 4: 26672 Step\n",
      "End of epoch 16, LoraLayer 4: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 5: 26672 Step\n",
      "End of epoch 16, LoraLayer 5: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 6: 26672 Step\n",
      "End of epoch 16, LoraLayer 6: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 7: 26672 Step\n",
      "End of epoch 16, LoraLayer 7: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 8: 26672 Step\n",
      "End of epoch 16, LoraLayer 8: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 9: 26672 Step\n",
      "End of epoch 16, LoraLayer 9: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 10: 26672 Step\n",
      "End of epoch 16, LoraLayer 10: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 11: 26672 Step\n",
      "End of epoch 16, LoraLayer 11: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 12: 26672 Step\n",
      "End of epoch 16, LoraLayer 12: Decay factor: 0.12476563453674316\n",
      "\n",
      "Testing loss: 2.3270013332366943, acc: 0.1071000024676323\n",
      "\n",
      "1667/1667 [==============================] - 52s 31ms/step - loss: 0.6025 - accuracy: 0.7968 - val_loss: 2.3270 - val_accuracy: 0.1071\n",
      "Epoch 17/20\n",
      "1665/1667 [============================>.] - ETA: 0s - loss: 0.6835 - accuracy: 0.7710\n",
      "End of epoch 17, LoraLayer 0: 28339 Step\n",
      "End of epoch 17, LoraLayer 0: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 1: 28339 Step\n",
      "End of epoch 17, LoraLayer 1: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 2: 28339 Step\n",
      "End of epoch 17, LoraLayer 2: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 3: 28339 Step\n",
      "End of epoch 17, LoraLayer 3: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 4: 28339 Step\n",
      "End of epoch 17, LoraLayer 4: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 5: 28339 Step\n",
      "End of epoch 17, LoraLayer 5: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 6: 28339 Step\n",
      "End of epoch 17, LoraLayer 6: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 7: 28339 Step\n",
      "End of epoch 17, LoraLayer 7: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 8: 28339 Step\n",
      "End of epoch 17, LoraLayer 8: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 9: 28339 Step\n",
      "End of epoch 17, LoraLayer 9: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 10: 28339 Step\n",
      "End of epoch 17, LoraLayer 10: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 11: 28339 Step\n",
      "End of epoch 17, LoraLayer 11: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 12: 28339 Step\n",
      "End of epoch 17, LoraLayer 12: Decay factor: 0.06225156784057617\n",
      "\n",
      "Testing loss: 1.8811745643615723, acc: 0.30959999561309814\n",
      "\n",
      "1667/1667 [==============================] - 50s 30ms/step - loss: 0.6833 - accuracy: 0.7710 - val_loss: 1.8813 - val_accuracy: 0.3096\n",
      "Epoch 18/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.7473 - accuracy: 0.7499\n",
      "End of epoch 18, LoraLayer 0: 30006 Step\n",
      "End of epoch 18, LoraLayer 0: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 1: 30006 Step\n",
      "End of epoch 18, LoraLayer 1: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 2: 30006 Step\n",
      "End of epoch 18, LoraLayer 2: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 3: 30006 Step\n",
      "End of epoch 18, LoraLayer 3: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 4: 30006 Step\n",
      "End of epoch 18, LoraLayer 4: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 5: 30006 Step\n",
      "End of epoch 18, LoraLayer 5: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 6: 30006 Step\n",
      "End of epoch 18, LoraLayer 6: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 7: 30006 Step\n",
      "End of epoch 18, LoraLayer 7: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 8: 30006 Step\n",
      "End of epoch 18, LoraLayer 8: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 9: 30006 Step\n",
      "End of epoch 18, LoraLayer 9: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 10: 30006 Step\n",
      "End of epoch 18, LoraLayer 10: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 11: 30006 Step\n",
      "End of epoch 18, LoraLayer 11: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 12: 30006 Step\n",
      "End of epoch 18, LoraLayer 12: Decay factor: 0.0\n",
      "\n",
      "Testing loss: 0.8368344902992249, acc: 0.7164000272750854\n",
      "\n",
      "1667/1667 [==============================] - 50s 30ms/step - loss: 0.7472 - accuracy: 0.7500 - val_loss: 0.8368 - val_accuracy: 0.7164\n",
      "Epoch 19/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.7271 - accuracy: 0.7565\n",
      "End of epoch 19, LoraLayer 0: 31673 Step\n",
      "End of epoch 19, LoraLayer 0: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 1: 31673 Step\n",
      "End of epoch 19, LoraLayer 1: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 2: 31673 Step\n",
      "End of epoch 19, LoraLayer 2: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 3: 31673 Step\n",
      "End of epoch 19, LoraLayer 3: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 4: 31673 Step\n",
      "End of epoch 19, LoraLayer 4: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 5: 31673 Step\n",
      "End of epoch 19, LoraLayer 5: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 6: 31673 Step\n",
      "End of epoch 19, LoraLayer 6: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 7: 31673 Step\n",
      "End of epoch 19, LoraLayer 7: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 8: 31673 Step\n",
      "End of epoch 19, LoraLayer 8: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 9: 31673 Step\n",
      "End of epoch 19, LoraLayer 9: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 10: 31673 Step\n",
      "End of epoch 19, LoraLayer 10: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 11: 31673 Step\n",
      "End of epoch 19, LoraLayer 11: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 12: 31673 Step\n",
      "End of epoch 19, LoraLayer 12: Decay factor: 0.0\n",
      "\n",
      "Testing loss: 0.7955965399742126, acc: 0.7343999743461609\n",
      "\n",
      "1667/1667 [==============================] - 51s 31ms/step - loss: 0.7271 - accuracy: 0.7565 - val_loss: 0.7956 - val_accuracy: 0.7342\n",
      "Epoch 20/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.6636 - accuracy: 0.7787\n",
      "End of epoch 20, LoraLayer 0: 33340 Step\n",
      "End of epoch 20, LoraLayer 0: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 1: 33340 Step\n",
      "End of epoch 20, LoraLayer 1: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 2: 33340 Step\n",
      "End of epoch 20, LoraLayer 2: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 3: 33340 Step\n",
      "End of epoch 20, LoraLayer 3: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 4: 33340 Step\n",
      "End of epoch 20, LoraLayer 4: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 5: 33340 Step\n",
      "End of epoch 20, LoraLayer 5: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 6: 33340 Step\n",
      "End of epoch 20, LoraLayer 6: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 7: 33340 Step\n",
      "End of epoch 20, LoraLayer 7: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 8: 33340 Step\n",
      "End of epoch 20, LoraLayer 8: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 9: 33340 Step\n",
      "End of epoch 20, LoraLayer 9: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 10: 33340 Step\n",
      "End of epoch 20, LoraLayer 10: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 11: 33340 Step\n",
      "End of epoch 20, LoraLayer 11: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 12: 33340 Step\n",
      "End of epoch 20, LoraLayer 12: Decay factor: 0.0\n",
      "\n",
      "Testing loss: 0.7834004163742065, acc: 0.7409999966621399\n",
      "\n",
      "1667/1667 [==============================] - 49s 30ms/step - loss: 0.6636 - accuracy: 0.7787 - val_loss: 0.7834 - val_accuracy: 0.7410\n"
     ]
    }
   ],
   "source": [
    "history11 = exp11_lora_vgg16.fit(x_train, y_train, batch_size=30, epochs=20, validation_data=(x_test, y_test), callbacks=[print_step_callback, TestCallback((x_test, y_test))] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bxbUXaLxfZIF"
   },
   "source": [
    "## inference time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fj8riYOifa42",
    "outputId": "f5b18b66-2306-4971-d08f-fa4cf52d5fbc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 3s 11ms/step - loss: 0.7834 - accuracy: 0.7410\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7834004163742065, 0.7409999966621399]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 평가\n",
    "exp11_lora_vgg16.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "id": "xLQEo46Gbt-o",
    "outputId": "0a3818f4-3558-434b-fcf9-ab6cc8536ccc"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 학습 및 검증 정확도 시각화\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# 정확도 그래프\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history11.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history11.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# 손실 그래프\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history11.history['loss'], label='Training Loss')\n",
    "plt.plot(history11.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GvAH25Td4TPl"
   },
   "source": [
    "## 1-2. (32, -)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "oX1SFBOl4f0i"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout, InputLayer\n",
    "import tensorflow as tf\n",
    "\n",
    "# 새로운 레이어를 추가하기 위한 레이어 리스트 초기화\n",
    "new_layers = []\n",
    "\n",
    "# VGG 모델의 각 레이어 순회\n",
    "for layer in best_vgg16.layers:\n",
    "    if isinstance(layer, InputLayer):\n",
    "        continue\n",
    "\n",
    "    if isinstance(layer, Conv2D):\n",
    "        # Conv2D 레이어를 ConvLoRALayer로 대체\n",
    "        conv_lora_layer = ConvLoRALayer00_cdn2(original_conv_layer=layer, rank=32, alpha=32, total_iteration = total_iteration, trainable=True)\n",
    "        new_layers.append(conv_lora_layer)\n",
    "\n",
    "    else:\n",
    "        # 다른 유형의 레이어는 그대로 유지\n",
    "        new_layers.append(layer)\n",
    "\n",
    "# 새로운 입력 텐서 생성\n",
    "input_tensor = best_vgg16.input\n",
    "\n",
    "# 새로운 모델 구성\n",
    "x = input_tensor\n",
    "for layer in new_layers:\n",
    "    x = layer(x)\n",
    "\n",
    "# 새로운 모델 생성\n",
    "exp12_lora_vgg16 = Model(inputs=input_tensor, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "wk9goxZs4f0i"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D\n",
    "\n",
    "# 새로 생성된 모델에서 Dense 레이어 중 LoRA가 적용되지 않은 레이어의 trainable을 False로 설정\n",
    "for layer in exp12_lora_vgg16.layers:\n",
    "    if isinstance(layer, Dense):\n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OaPm45ig4f0i",
    "outputId": "ffcabf22-e139-49d0-d76a-1fba020e7e1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " block1_conv1 (ConvLoRALaye  (None, 32, 32, 64)        21154     \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block1_conv2 (ConvLoRALaye  (None, 32, 32, 64)        73858     \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " block2_conv1 (ConvLoRALaye  (None, 16, 16, 128)       129282    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block2_conv2 (ConvLoRALaye  (None, 16, 16, 128)       221442    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " block3_conv1 (ConvLoRALaye  (None, 8, 8, 256)         406018    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block3_conv2 (ConvLoRALaye  (None, 8, 8, 256)         737794    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block3_conv3 (ConvLoRALaye  (None, 8, 8, 256)         737794    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 4, 4, 256)         0         \n",
      "                                                                 \n",
      " block4_conv1 (ConvLoRALaye  (None, 4, 4, 512)         1401858   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block4_conv2 (ConvLoRALaye  (None, 4, 4, 512)         2655234   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block4_conv3 (ConvLoRALaye  (None, 4, 4, 512)         2655234   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      " block5_conv1 (ConvLoRALaye  (None, 2, 2, 512)         2655234   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block5_conv2 (ConvLoRALaye  (None, 2, 2, 512)         2655234   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block5_conv3 (ConvLoRALaye  (None, 2, 2, 512)         2655234   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 1, 1, 512)         0         \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 4096)              2101248   \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 512)               2097664   \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21209412 (80.91 MB)\n",
      "Trainable params: 2290656 (8.74 MB)\n",
      "Non-trainable params: 18918756 (72.17 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "exp12_lora_vgg16.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F77s-EYN4vcq",
    "outputId": "a19ebea2-61a9-4f67-9d53-619bc2692331"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: input_3\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block1_conv1\n",
      "  Trainable parameters: 19360\n",
      "  Non-trainable parameters: 1794\n",
      "Layer: block1_conv2\n",
      "  Trainable parameters: 36928\n",
      "  Non-trainable parameters: 36930\n",
      "Layer: block1_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block2_conv1\n",
      "  Trainable parameters: 55424\n",
      "  Non-trainable parameters: 73858\n",
      "Layer: block2_conv2\n",
      "  Trainable parameters: 73856\n",
      "  Non-trainable parameters: 147586\n",
      "Layer: block2_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block3_conv1\n",
      "  Trainable parameters: 110848\n",
      "  Non-trainable parameters: 295170\n",
      "Layer: block3_conv2\n",
      "  Trainable parameters: 147712\n",
      "  Non-trainable parameters: 590082\n",
      "Layer: block3_conv3\n",
      "  Trainable parameters: 147712\n",
      "  Non-trainable parameters: 590082\n",
      "Layer: block3_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block4_conv1\n",
      "  Trainable parameters: 221696\n",
      "  Non-trainable parameters: 1180162\n",
      "Layer: block4_conv2\n",
      "  Trainable parameters: 295424\n",
      "  Non-trainable parameters: 2359810\n",
      "Layer: block4_conv3\n",
      "  Trainable parameters: 295424\n",
      "  Non-trainable parameters: 2359810\n",
      "Layer: block4_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block5_conv1\n",
      "  Trainable parameters: 295424\n",
      "  Non-trainable parameters: 2359810\n",
      "Layer: block5_conv2\n",
      "  Trainable parameters: 295424\n",
      "  Non-trainable parameters: 2359810\n",
      "Layer: block5_conv3\n",
      "  Trainable parameters: 295424\n",
      "  Non-trainable parameters: 2359810\n",
      "Layer: block5_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: flatten_2\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: dense_6\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 2101248\n",
      "Layer: dropout_4\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: dense_7\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 2097664\n",
      "Layer: dropout_5\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: dense_8\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 5130\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "#모델의 각 레이어를 순회하며 파라미터 수를 계산\n",
    "for layer in exp12_lora_vgg16.layers:\n",
    "    trainable_count = np.sum([tf.size(w).numpy() for w in layer.trainable_weights])\n",
    "    non_trainable_count = np.sum([tf.size(w).numpy() for w in layer.non_trainable_weights])\n",
    "\n",
    "    print(f\"Layer: {layer.name}\")\n",
    "    print(f\"  Trainable parameters: {trainable_count}\")\n",
    "    print(f\"  Non-trainable parameters: {non_trainable_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "OK4liexw4zdy"
   },
   "outputs": [],
   "source": [
    "lora_layers = []  # lora 레이어들을 저장할 리스트\n",
    "\n",
    "for layer in exp12_lora_vgg16.layers:\n",
    "    if isinstance(layer, ConvLoRALayer00_cdn2) or isinstance(layer, LoraLayer):\n",
    "        lora_layers.append(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "1iKvTdFN4zdy"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "class PrintCurrentStepCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, lora_layers):\n",
    "        super().__init__()\n",
    "        self.lora_layers = lora_layers\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print()\n",
    "        for i, lora_layer in enumerate(self.lora_layers):\n",
    "            current_step = lora_layer.current_step.value()\n",
    "            decay_factor = lora_layer.decay_factor.value()\n",
    "            print(f\"End of epoch {epoch + 1}, LoraLayer {i}: {current_step} Step\")\n",
    "            print(f\"End of epoch {epoch + 1}, LoraLayer {i}: Decay factor: {decay_factor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "lL8z4Ky84zdz"
   },
   "outputs": [],
   "source": [
    "# 콜백 생성 시 lora_layers 딕셔너리의 값만 사용\n",
    "print_step_callback = PrintCurrentStepCallback(lora_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "vmjY3HT-4zdz"
   },
   "outputs": [],
   "source": [
    "class TestCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, test_data):\n",
    "        self.test_data = test_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        x, y = self.test_data\n",
    "        loss, acc = self.model.evaluate(x, y, verbose=0)\n",
    "        print('\\nTesting loss: {}, acc: {}\\n'.format(loss, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TtqOiU8844Ta"
   },
   "source": [
    "##학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "pCKYMJdu44Tb"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# 모델 컴파일\n",
    "exp12_lora_vgg16.compile(optimizer=Adam(learning_rate=0.0001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YSTgDSLX44Tb",
    "outputId": "1413af14-7277-4ffd-c91d-8dfbd2443aea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " block1_conv1 (ConvLoRALaye  (None, 32, 32, 64)        21154     \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block1_conv2 (ConvLoRALaye  (None, 32, 32, 64)        73858     \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " block2_conv1 (ConvLoRALaye  (None, 16, 16, 128)       129282    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block2_conv2 (ConvLoRALaye  (None, 16, 16, 128)       221442    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " block3_conv1 (ConvLoRALaye  (None, 8, 8, 256)         406018    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block3_conv2 (ConvLoRALaye  (None, 8, 8, 256)         737794    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block3_conv3 (ConvLoRALaye  (None, 8, 8, 256)         737794    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 4, 4, 256)         0         \n",
      "                                                                 \n",
      " block4_conv1 (ConvLoRALaye  (None, 4, 4, 512)         1401858   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block4_conv2 (ConvLoRALaye  (None, 4, 4, 512)         2655234   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block4_conv3 (ConvLoRALaye  (None, 4, 4, 512)         2655234   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      " block5_conv1 (ConvLoRALaye  (None, 2, 2, 512)         2655234   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block5_conv2 (ConvLoRALaye  (None, 2, 2, 512)         2655234   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block5_conv3 (ConvLoRALaye  (None, 2, 2, 512)         2655234   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 1, 1, 512)         0         \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 4096)              2101248   \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 512)               2097664   \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21209412 (80.91 MB)\n",
      "Trainable params: 2290656 (8.74 MB)\n",
      "Non-trainable params: 18918756 (72.17 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "exp12_lora_vgg16.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HW3wKPlB44Tb",
    "outputId": "d75c95be-0610-4c6e-f0f1-a8c64ade3cbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.1437 - accuracy: 0.9537\n",
      "End of epoch 1, LoraLayer 0: 1667 Step\n",
      "End of epoch 1, LoraLayer 0: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 1: 1667 Step\n",
      "End of epoch 1, LoraLayer 1: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 2: 1667 Step\n",
      "End of epoch 1, LoraLayer 2: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 3: 1667 Step\n",
      "End of epoch 1, LoraLayer 3: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 4: 1667 Step\n",
      "End of epoch 1, LoraLayer 4: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 5: 1667 Step\n",
      "End of epoch 1, LoraLayer 5: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 6: 1667 Step\n",
      "End of epoch 1, LoraLayer 6: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 7: 1667 Step\n",
      "End of epoch 1, LoraLayer 7: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 8: 1667 Step\n",
      "End of epoch 1, LoraLayer 8: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 9: 1667 Step\n",
      "End of epoch 1, LoraLayer 9: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 10: 1667 Step\n",
      "End of epoch 1, LoraLayer 10: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 11: 1667 Step\n",
      "End of epoch 1, LoraLayer 11: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 12: 1667 Step\n",
      "End of epoch 1, LoraLayer 12: Decay factor: 1.0\n",
      "\n",
      "Testing loss: 2.303812026977539, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 62s 32ms/step - loss: 0.1437 - accuracy: 0.9537 - val_loss: 2.3038 - val_accuracy: 0.1000\n",
      "Epoch 2/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.1082 - accuracy: 0.9662\n",
      "End of epoch 2, LoraLayer 0: 3334 Step\n",
      "End of epoch 2, LoraLayer 0: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 1: 3334 Step\n",
      "End of epoch 2, LoraLayer 1: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 2: 3334 Step\n",
      "End of epoch 2, LoraLayer 2: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 3: 3334 Step\n",
      "End of epoch 2, LoraLayer 3: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 4: 3334 Step\n",
      "End of epoch 2, LoraLayer 4: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 5: 3334 Step\n",
      "End of epoch 2, LoraLayer 5: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 6: 3334 Step\n",
      "End of epoch 2, LoraLayer 6: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 7: 3334 Step\n",
      "End of epoch 2, LoraLayer 7: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 8: 3334 Step\n",
      "End of epoch 2, LoraLayer 8: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 9: 3334 Step\n",
      "End of epoch 2, LoraLayer 9: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 10: 3334 Step\n",
      "End of epoch 2, LoraLayer 10: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 11: 3334 Step\n",
      "End of epoch 2, LoraLayer 11: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 12: 3334 Step\n",
      "End of epoch 2, LoraLayer 12: Decay factor: 0.9999625086784363\n",
      "\n",
      "Testing loss: 2.3035526275634766, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 49s 30ms/step - loss: 0.1081 - accuracy: 0.9662 - val_loss: 2.3036 - val_accuracy: 0.1000\n",
      "Epoch 3/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.0967 - accuracy: 0.9685\n",
      "End of epoch 3, LoraLayer 0: 5001 Step\n",
      "End of epoch 3, LoraLayer 0: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 1: 5001 Step\n",
      "End of epoch 3, LoraLayer 1: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 2: 5001 Step\n",
      "End of epoch 3, LoraLayer 2: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 3: 5001 Step\n",
      "End of epoch 3, LoraLayer 3: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 4: 5001 Step\n",
      "End of epoch 3, LoraLayer 4: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 5: 5001 Step\n",
      "End of epoch 3, LoraLayer 5: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 6: 5001 Step\n",
      "End of epoch 3, LoraLayer 6: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 7: 5001 Step\n",
      "End of epoch 3, LoraLayer 7: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 8: 5001 Step\n",
      "End of epoch 3, LoraLayer 8: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 9: 5001 Step\n",
      "End of epoch 3, LoraLayer 9: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 10: 5001 Step\n",
      "End of epoch 3, LoraLayer 10: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 11: 5001 Step\n",
      "End of epoch 3, LoraLayer 11: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 12: 5001 Step\n",
      "End of epoch 3, LoraLayer 12: Decay factor: 0.9374484419822693\n",
      "\n",
      "Testing loss: 2.3039705753326416, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 54s 32ms/step - loss: 0.0967 - accuracy: 0.9685 - val_loss: 2.3040 - val_accuracy: 0.1000\n",
      "Epoch 4/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.0866 - accuracy: 0.9707\n",
      "End of epoch 4, LoraLayer 0: 6668 Step\n",
      "End of epoch 4, LoraLayer 0: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 1: 6668 Step\n",
      "End of epoch 4, LoraLayer 1: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 2: 6668 Step\n",
      "End of epoch 4, LoraLayer 2: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 3: 6668 Step\n",
      "End of epoch 4, LoraLayer 3: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 4: 6668 Step\n",
      "End of epoch 4, LoraLayer 4: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 5: 6668 Step\n",
      "End of epoch 4, LoraLayer 5: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 6: 6668 Step\n",
      "End of epoch 4, LoraLayer 6: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 7: 6668 Step\n",
      "End of epoch 4, LoraLayer 7: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 8: 6668 Step\n",
      "End of epoch 4, LoraLayer 8: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 9: 6668 Step\n",
      "End of epoch 4, LoraLayer 9: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 10: 6668 Step\n",
      "End of epoch 4, LoraLayer 10: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 11: 6668 Step\n",
      "End of epoch 4, LoraLayer 11: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 12: 6668 Step\n",
      "End of epoch 4, LoraLayer 12: Decay factor: 0.8749343752861023\n",
      "\n",
      "Testing loss: 2.3045339584350586, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 52s 31ms/step - loss: 0.0866 - accuracy: 0.9707 - val_loss: 2.3045 - val_accuracy: 0.1000\n",
      "Epoch 5/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.0874 - accuracy: 0.9709\n",
      "End of epoch 5, LoraLayer 0: 8335 Step\n",
      "End of epoch 5, LoraLayer 0: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 1: 8335 Step\n",
      "End of epoch 5, LoraLayer 1: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 2: 8335 Step\n",
      "End of epoch 5, LoraLayer 2: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 3: 8335 Step\n",
      "End of epoch 5, LoraLayer 3: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 4: 8335 Step\n",
      "End of epoch 5, LoraLayer 4: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 5: 8335 Step\n",
      "End of epoch 5, LoraLayer 5: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 6: 8335 Step\n",
      "End of epoch 5, LoraLayer 6: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 7: 8335 Step\n",
      "End of epoch 5, LoraLayer 7: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 8: 8335 Step\n",
      "End of epoch 5, LoraLayer 8: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 9: 8335 Step\n",
      "End of epoch 5, LoraLayer 9: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 10: 8335 Step\n",
      "End of epoch 5, LoraLayer 10: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 11: 8335 Step\n",
      "End of epoch 5, LoraLayer 11: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 12: 8335 Step\n",
      "End of epoch 5, LoraLayer 12: Decay factor: 0.8124203085899353\n",
      "\n",
      "Testing loss: 2.3053793907165527, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 47s 28ms/step - loss: 0.0874 - accuracy: 0.9710 - val_loss: 2.3054 - val_accuracy: 0.1000\n",
      "Epoch 6/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.0910 - accuracy: 0.9699\n",
      "End of epoch 6, LoraLayer 0: 10002 Step\n",
      "End of epoch 6, LoraLayer 0: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 1: 10002 Step\n",
      "End of epoch 6, LoraLayer 1: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 2: 10002 Step\n",
      "End of epoch 6, LoraLayer 2: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 3: 10002 Step\n",
      "End of epoch 6, LoraLayer 3: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 4: 10002 Step\n",
      "End of epoch 6, LoraLayer 4: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 5: 10002 Step\n",
      "End of epoch 6, LoraLayer 5: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 6: 10002 Step\n",
      "End of epoch 6, LoraLayer 6: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 7: 10002 Step\n",
      "End of epoch 6, LoraLayer 7: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 8: 10002 Step\n",
      "End of epoch 6, LoraLayer 8: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 9: 10002 Step\n",
      "End of epoch 6, LoraLayer 9: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 10: 10002 Step\n",
      "End of epoch 6, LoraLayer 10: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 11: 10002 Step\n",
      "End of epoch 6, LoraLayer 11: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 12: 10002 Step\n",
      "End of epoch 6, LoraLayer 12: Decay factor: 0.7499062418937683\n",
      "\n",
      "Testing loss: 2.306338310241699, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 48s 29ms/step - loss: 0.0910 - accuracy: 0.9699 - val_loss: 2.3063 - val_accuracy: 0.1000\n",
      "Epoch 7/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.0947 - accuracy: 0.9689\n",
      "End of epoch 7, LoraLayer 0: 11669 Step\n",
      "End of epoch 7, LoraLayer 0: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 1: 11669 Step\n",
      "End of epoch 7, LoraLayer 1: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 2: 11669 Step\n",
      "End of epoch 7, LoraLayer 2: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 3: 11669 Step\n",
      "End of epoch 7, LoraLayer 3: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 4: 11669 Step\n",
      "End of epoch 7, LoraLayer 4: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 5: 11669 Step\n",
      "End of epoch 7, LoraLayer 5: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 6: 11669 Step\n",
      "End of epoch 7, LoraLayer 6: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 7: 11669 Step\n",
      "End of epoch 7, LoraLayer 7: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 8: 11669 Step\n",
      "End of epoch 7, LoraLayer 8: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 9: 11669 Step\n",
      "End of epoch 7, LoraLayer 9: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 10: 11669 Step\n",
      "End of epoch 7, LoraLayer 10: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 11: 11669 Step\n",
      "End of epoch 7, LoraLayer 11: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 12: 11669 Step\n",
      "End of epoch 7, LoraLayer 12: Decay factor: 0.6873922348022461\n",
      "\n",
      "Testing loss: 2.3086938858032227, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 52s 31ms/step - loss: 0.0947 - accuracy: 0.9689 - val_loss: 2.3087 - val_accuracy: 0.1000\n",
      "Epoch 8/20\n",
      "1665/1667 [============================>.] - ETA: 0s - loss: 0.1137 - accuracy: 0.9614\n",
      "End of epoch 8, LoraLayer 0: 13336 Step\n",
      "End of epoch 8, LoraLayer 0: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 1: 13336 Step\n",
      "End of epoch 8, LoraLayer 1: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 2: 13336 Step\n",
      "End of epoch 8, LoraLayer 2: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 3: 13336 Step\n",
      "End of epoch 8, LoraLayer 3: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 4: 13336 Step\n",
      "End of epoch 8, LoraLayer 4: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 5: 13336 Step\n",
      "End of epoch 8, LoraLayer 5: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 6: 13336 Step\n",
      "End of epoch 8, LoraLayer 6: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 7: 13336 Step\n",
      "End of epoch 8, LoraLayer 7: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 8: 13336 Step\n",
      "End of epoch 8, LoraLayer 8: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 9: 13336 Step\n",
      "End of epoch 8, LoraLayer 9: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 10: 13336 Step\n",
      "End of epoch 8, LoraLayer 10: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 11: 13336 Step\n",
      "End of epoch 8, LoraLayer 11: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 12: 13336 Step\n",
      "End of epoch 8, LoraLayer 12: Decay factor: 0.6248781681060791\n",
      "\n",
      "Testing loss: 2.31062912940979, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 46s 28ms/step - loss: 0.1138 - accuracy: 0.9613 - val_loss: 2.3106 - val_accuracy: 0.1000\n",
      "Epoch 9/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.1208 - accuracy: 0.9602\n",
      "End of epoch 9, LoraLayer 0: 15003 Step\n",
      "End of epoch 9, LoraLayer 0: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 1: 15003 Step\n",
      "End of epoch 9, LoraLayer 1: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 2: 15003 Step\n",
      "End of epoch 9, LoraLayer 2: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 3: 15003 Step\n",
      "End of epoch 9, LoraLayer 3: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 4: 15003 Step\n",
      "End of epoch 9, LoraLayer 4: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 5: 15003 Step\n",
      "End of epoch 9, LoraLayer 5: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 6: 15003 Step\n",
      "End of epoch 9, LoraLayer 6: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 7: 15003 Step\n",
      "End of epoch 9, LoraLayer 7: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 8: 15003 Step\n",
      "End of epoch 9, LoraLayer 8: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 9: 15003 Step\n",
      "End of epoch 9, LoraLayer 9: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 10: 15003 Step\n",
      "End of epoch 9, LoraLayer 10: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 11: 15003 Step\n",
      "End of epoch 9, LoraLayer 11: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 12: 15003 Step\n",
      "End of epoch 9, LoraLayer 12: Decay factor: 0.5623641014099121\n",
      "\n",
      "Testing loss: 2.3199355602264404, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 50s 30ms/step - loss: 0.1208 - accuracy: 0.9602 - val_loss: 2.3199 - val_accuracy: 0.1000\n",
      "Epoch 10/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.1403 - accuracy: 0.9514\n",
      "End of epoch 10, LoraLayer 0: 16670 Step\n",
      "End of epoch 10, LoraLayer 0: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 1: 16670 Step\n",
      "End of epoch 10, LoraLayer 1: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 2: 16670 Step\n",
      "End of epoch 10, LoraLayer 2: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 3: 16670 Step\n",
      "End of epoch 10, LoraLayer 3: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 4: 16670 Step\n",
      "End of epoch 10, LoraLayer 4: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 5: 16670 Step\n",
      "End of epoch 10, LoraLayer 5: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 6: 16670 Step\n",
      "End of epoch 10, LoraLayer 6: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 7: 16670 Step\n",
      "End of epoch 10, LoraLayer 7: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 8: 16670 Step\n",
      "End of epoch 10, LoraLayer 8: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 9: 16670 Step\n",
      "End of epoch 10, LoraLayer 9: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 10: 16670 Step\n",
      "End of epoch 10, LoraLayer 10: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 11: 16670 Step\n",
      "End of epoch 10, LoraLayer 11: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 12: 16670 Step\n",
      "End of epoch 10, LoraLayer 12: Decay factor: 0.4998500347137451\n",
      "\n",
      "Testing loss: 2.3344762325286865, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 49s 30ms/step - loss: 0.1404 - accuracy: 0.9514 - val_loss: 2.3345 - val_accuracy: 0.1000\n",
      "Epoch 11/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.1590 - accuracy: 0.9462\n",
      "End of epoch 11, LoraLayer 0: 18337 Step\n",
      "End of epoch 11, LoraLayer 0: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 1: 18337 Step\n",
      "End of epoch 11, LoraLayer 1: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 2: 18337 Step\n",
      "End of epoch 11, LoraLayer 2: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 3: 18337 Step\n",
      "End of epoch 11, LoraLayer 3: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 4: 18337 Step\n",
      "End of epoch 11, LoraLayer 4: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 5: 18337 Step\n",
      "End of epoch 11, LoraLayer 5: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 6: 18337 Step\n",
      "End of epoch 11, LoraLayer 6: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 7: 18337 Step\n",
      "End of epoch 11, LoraLayer 7: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 8: 18337 Step\n",
      "End of epoch 11, LoraLayer 8: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 9: 18337 Step\n",
      "End of epoch 11, LoraLayer 9: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 10: 18337 Step\n",
      "End of epoch 11, LoraLayer 10: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 11: 18337 Step\n",
      "End of epoch 11, LoraLayer 11: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 12: 18337 Step\n",
      "End of epoch 11, LoraLayer 12: Decay factor: 0.4373359680175781\n",
      "\n",
      "Testing loss: 2.4195597171783447, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 51s 30ms/step - loss: 0.1590 - accuracy: 0.9462 - val_loss: 2.4196 - val_accuracy: 0.1000\n",
      "Epoch 12/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.1950 - accuracy: 0.9341\n",
      "End of epoch 12, LoraLayer 0: 20004 Step\n",
      "End of epoch 12, LoraLayer 0: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 1: 20004 Step\n",
      "End of epoch 12, LoraLayer 1: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 2: 20004 Step\n",
      "End of epoch 12, LoraLayer 2: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 3: 20004 Step\n",
      "End of epoch 12, LoraLayer 3: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 4: 20004 Step\n",
      "End of epoch 12, LoraLayer 4: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 5: 20004 Step\n",
      "End of epoch 12, LoraLayer 5: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 6: 20004 Step\n",
      "End of epoch 12, LoraLayer 6: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 7: 20004 Step\n",
      "End of epoch 12, LoraLayer 7: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 8: 20004 Step\n",
      "End of epoch 12, LoraLayer 8: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 9: 20004 Step\n",
      "End of epoch 12, LoraLayer 9: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 10: 20004 Step\n",
      "End of epoch 12, LoraLayer 10: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 11: 20004 Step\n",
      "End of epoch 12, LoraLayer 11: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 12: 20004 Step\n",
      "End of epoch 12, LoraLayer 12: Decay factor: 0.37482190132141113\n",
      "\n",
      "Testing loss: 2.4373550415039062, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 48s 29ms/step - loss: 0.1950 - accuracy: 0.9341 - val_loss: 2.4374 - val_accuracy: 0.1000\n",
      "Epoch 13/20\n",
      "1665/1667 [============================>.] - ETA: 0s - loss: 0.2279 - accuracy: 0.9219\n",
      "End of epoch 13, LoraLayer 0: 21671 Step\n",
      "End of epoch 13, LoraLayer 0: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 1: 21671 Step\n",
      "End of epoch 13, LoraLayer 1: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 2: 21671 Step\n",
      "End of epoch 13, LoraLayer 2: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 3: 21671 Step\n",
      "End of epoch 13, LoraLayer 3: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 4: 21671 Step\n",
      "End of epoch 13, LoraLayer 4: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 5: 21671 Step\n",
      "End of epoch 13, LoraLayer 5: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 6: 21671 Step\n",
      "End of epoch 13, LoraLayer 6: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 7: 21671 Step\n",
      "End of epoch 13, LoraLayer 7: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 8: 21671 Step\n",
      "End of epoch 13, LoraLayer 8: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 9: 21671 Step\n",
      "End of epoch 13, LoraLayer 9: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 10: 21671 Step\n",
      "End of epoch 13, LoraLayer 10: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 11: 21671 Step\n",
      "End of epoch 13, LoraLayer 11: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 12: 21671 Step\n",
      "End of epoch 13, LoraLayer 12: Decay factor: 0.31230783462524414\n",
      "\n",
      "Testing loss: 2.4726107120513916, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 47s 28ms/step - loss: 0.2279 - accuracy: 0.9218 - val_loss: 2.4726 - val_accuracy: 0.1000\n",
      "Epoch 14/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.2748 - accuracy: 0.9059\n",
      "End of epoch 14, LoraLayer 0: 23338 Step\n",
      "End of epoch 14, LoraLayer 0: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 1: 23338 Step\n",
      "End of epoch 14, LoraLayer 1: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 2: 23338 Step\n",
      "End of epoch 14, LoraLayer 2: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 3: 23338 Step\n",
      "End of epoch 14, LoraLayer 3: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 4: 23338 Step\n",
      "End of epoch 14, LoraLayer 4: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 5: 23338 Step\n",
      "End of epoch 14, LoraLayer 5: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 6: 23338 Step\n",
      "End of epoch 14, LoraLayer 6: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 7: 23338 Step\n",
      "End of epoch 14, LoraLayer 7: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 8: 23338 Step\n",
      "End of epoch 14, LoraLayer 8: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 9: 23338 Step\n",
      "End of epoch 14, LoraLayer 9: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 10: 23338 Step\n",
      "End of epoch 14, LoraLayer 10: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 11: 23338 Step\n",
      "End of epoch 14, LoraLayer 11: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 12: 23338 Step\n",
      "End of epoch 14, LoraLayer 12: Decay factor: 0.24979376792907715\n",
      "\n",
      "Testing loss: 2.599996566772461, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 52s 31ms/step - loss: 0.2748 - accuracy: 0.9059 - val_loss: 2.6000 - val_accuracy: 0.1000\n",
      "Epoch 15/20\n",
      "1665/1667 [============================>.] - ETA: 0s - loss: 0.3358 - accuracy: 0.8858\n",
      "End of epoch 15, LoraLayer 0: 25005 Step\n",
      "End of epoch 15, LoraLayer 0: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 1: 25005 Step\n",
      "End of epoch 15, LoraLayer 1: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 2: 25005 Step\n",
      "End of epoch 15, LoraLayer 2: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 3: 25005 Step\n",
      "End of epoch 15, LoraLayer 3: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 4: 25005 Step\n",
      "End of epoch 15, LoraLayer 4: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 5: 25005 Step\n",
      "End of epoch 15, LoraLayer 5: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 6: 25005 Step\n",
      "End of epoch 15, LoraLayer 6: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 7: 25005 Step\n",
      "End of epoch 15, LoraLayer 7: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 8: 25005 Step\n",
      "End of epoch 15, LoraLayer 8: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 9: 25005 Step\n",
      "End of epoch 15, LoraLayer 9: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 10: 25005 Step\n",
      "End of epoch 15, LoraLayer 10: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 11: 25005 Step\n",
      "End of epoch 15, LoraLayer 11: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 12: 25005 Step\n",
      "End of epoch 15, LoraLayer 12: Decay factor: 0.18727970123291016\n",
      "\n",
      "Testing loss: 2.5544450283050537, acc: 0.10019999742507935\n",
      "\n",
      "1667/1667 [==============================] - 48s 29ms/step - loss: 0.3358 - accuracy: 0.8858 - val_loss: 2.5544 - val_accuracy: 0.1000\n",
      "Epoch 16/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.4134 - accuracy: 0.8595\n",
      "End of epoch 16, LoraLayer 0: 26672 Step\n",
      "End of epoch 16, LoraLayer 0: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 1: 26672 Step\n",
      "End of epoch 16, LoraLayer 1: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 2: 26672 Step\n",
      "End of epoch 16, LoraLayer 2: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 3: 26672 Step\n",
      "End of epoch 16, LoraLayer 3: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 4: 26672 Step\n",
      "End of epoch 16, LoraLayer 4: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 5: 26672 Step\n",
      "End of epoch 16, LoraLayer 5: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 6: 26672 Step\n",
      "End of epoch 16, LoraLayer 6: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 7: 26672 Step\n",
      "End of epoch 16, LoraLayer 7: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 8: 26672 Step\n",
      "End of epoch 16, LoraLayer 8: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 9: 26672 Step\n",
      "End of epoch 16, LoraLayer 9: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 10: 26672 Step\n",
      "End of epoch 16, LoraLayer 10: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 11: 26672 Step\n",
      "End of epoch 16, LoraLayer 11: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 12: 26672 Step\n",
      "End of epoch 16, LoraLayer 12: Decay factor: 0.12476563453674316\n",
      "\n",
      "Testing loss: 2.704838514328003, acc: 0.12240000069141388\n",
      "\n",
      "1667/1667 [==============================] - 47s 28ms/step - loss: 0.4134 - accuracy: 0.8595 - val_loss: 2.7048 - val_accuracy: 0.1225\n",
      "Epoch 17/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.5088 - accuracy: 0.8283\n",
      "End of epoch 17, LoraLayer 0: 28339 Step\n",
      "End of epoch 17, LoraLayer 0: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 1: 28339 Step\n",
      "End of epoch 17, LoraLayer 1: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 2: 28339 Step\n",
      "End of epoch 17, LoraLayer 2: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 3: 28339 Step\n",
      "End of epoch 17, LoraLayer 3: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 4: 28339 Step\n",
      "End of epoch 17, LoraLayer 4: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 5: 28339 Step\n",
      "End of epoch 17, LoraLayer 5: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 6: 28339 Step\n",
      "End of epoch 17, LoraLayer 6: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 7: 28339 Step\n",
      "End of epoch 17, LoraLayer 7: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 8: 28339 Step\n",
      "End of epoch 17, LoraLayer 8: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 9: 28339 Step\n",
      "End of epoch 17, LoraLayer 9: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 10: 28339 Step\n",
      "End of epoch 17, LoraLayer 10: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 11: 28339 Step\n",
      "End of epoch 17, LoraLayer 11: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 12: 28339 Step\n",
      "End of epoch 17, LoraLayer 12: Decay factor: 0.06225156784057617\n",
      "\n",
      "Testing loss: 1.7721755504608154, acc: 0.39980000257492065\n",
      "\n",
      "1667/1667 [==============================] - 48s 29ms/step - loss: 0.5088 - accuracy: 0.8283 - val_loss: 1.7722 - val_accuracy: 0.3994\n",
      "Epoch 18/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.6221 - accuracy: 0.7910\n",
      "End of epoch 18, LoraLayer 0: 30006 Step\n",
      "End of epoch 18, LoraLayer 0: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 1: 30006 Step\n",
      "End of epoch 18, LoraLayer 1: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 2: 30006 Step\n",
      "End of epoch 18, LoraLayer 2: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 3: 30006 Step\n",
      "End of epoch 18, LoraLayer 3: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 4: 30006 Step\n",
      "End of epoch 18, LoraLayer 4: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 5: 30006 Step\n",
      "End of epoch 18, LoraLayer 5: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 6: 30006 Step\n",
      "End of epoch 18, LoraLayer 6: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 7: 30006 Step\n",
      "End of epoch 18, LoraLayer 7: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 8: 30006 Step\n",
      "End of epoch 18, LoraLayer 8: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 9: 30006 Step\n",
      "End of epoch 18, LoraLayer 9: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 10: 30006 Step\n",
      "End of epoch 18, LoraLayer 10: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 11: 30006 Step\n",
      "End of epoch 18, LoraLayer 11: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 12: 30006 Step\n",
      "End of epoch 18, LoraLayer 12: Decay factor: 0.0\n",
      "\n",
      "Testing loss: 0.7419387698173523, acc: 0.753600001335144\n",
      "\n",
      "1667/1667 [==============================] - 49s 30ms/step - loss: 0.6222 - accuracy: 0.7910 - val_loss: 0.7419 - val_accuracy: 0.7538\n",
      "Epoch 19/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.5957 - accuracy: 0.8018\n",
      "End of epoch 19, LoraLayer 0: 31673 Step\n",
      "End of epoch 19, LoraLayer 0: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 1: 31673 Step\n",
      "End of epoch 19, LoraLayer 1: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 2: 31673 Step\n",
      "End of epoch 19, LoraLayer 2: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 3: 31673 Step\n",
      "End of epoch 19, LoraLayer 3: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 4: 31673 Step\n",
      "End of epoch 19, LoraLayer 4: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 5: 31673 Step\n",
      "End of epoch 19, LoraLayer 5: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 6: 31673 Step\n",
      "End of epoch 19, LoraLayer 6: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 7: 31673 Step\n",
      "End of epoch 19, LoraLayer 7: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 8: 31673 Step\n",
      "End of epoch 19, LoraLayer 8: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 9: 31673 Step\n",
      "End of epoch 19, LoraLayer 9: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 10: 31673 Step\n",
      "End of epoch 19, LoraLayer 10: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 11: 31673 Step\n",
      "End of epoch 19, LoraLayer 11: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 12: 31673 Step\n",
      "End of epoch 19, LoraLayer 12: Decay factor: 0.0\n",
      "\n",
      "Testing loss: 0.7232495546340942, acc: 0.7609000205993652\n",
      "\n",
      "1667/1667 [==============================] - 47s 28ms/step - loss: 0.5957 - accuracy: 0.8018 - val_loss: 0.7233 - val_accuracy: 0.7610\n",
      "Epoch 20/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.5021 - accuracy: 0.8312\n",
      "End of epoch 20, LoraLayer 0: 33340 Step\n",
      "End of epoch 20, LoraLayer 0: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 1: 33340 Step\n",
      "End of epoch 20, LoraLayer 1: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 2: 33340 Step\n",
      "End of epoch 20, LoraLayer 2: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 3: 33340 Step\n",
      "End of epoch 20, LoraLayer 3: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 4: 33340 Step\n",
      "End of epoch 20, LoraLayer 4: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 5: 33340 Step\n",
      "End of epoch 20, LoraLayer 5: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 6: 33340 Step\n",
      "End of epoch 20, LoraLayer 6: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 7: 33340 Step\n",
      "End of epoch 20, LoraLayer 7: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 8: 33340 Step\n",
      "End of epoch 20, LoraLayer 8: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 9: 33340 Step\n",
      "End of epoch 20, LoraLayer 9: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 10: 33340 Step\n",
      "End of epoch 20, LoraLayer 10: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 11: 33340 Step\n",
      "End of epoch 20, LoraLayer 11: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 12: 33340 Step\n",
      "End of epoch 20, LoraLayer 12: Decay factor: 0.0\n",
      "\n",
      "Testing loss: 0.6971790194511414, acc: 0.7735000252723694\n",
      "\n",
      "1667/1667 [==============================] - 49s 29ms/step - loss: 0.5021 - accuracy: 0.8312 - val_loss: 0.6972 - val_accuracy: 0.7737\n"
     ]
    }
   ],
   "source": [
    "history12 = exp12_lora_vgg16.fit(x_train, y_train, batch_size=30, epochs=20, validation_data=(x_test, y_test), callbacks=[print_step_callback, TestCallback((x_test, y_test))] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4iVURdjC48v9",
    "outputId": "f5b18b66-2306-4971-d08f-fa4cf52d5fbc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 3s 10ms/step - loss: 0.6972 - accuracy: 0.7735\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6971790194511414, 0.7735000252723694]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 평가\n",
    "exp12_lora_vgg16.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "oMHh5D1X48v-",
    "outputId": "0a3818f4-3558-434b-fcf9-ab6cc8536ccc"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 학습 및 검증 정확도 시각화\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# 정확도 그래프\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history12.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history12.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# 손실 그래프\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history12.history['loss'], label='Training Loss')\n",
    "plt.plot(history12.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1iiF1k-9D0hd"
   },
   "source": [
    "---\n",
    "#Experiments 2 : 모든 Convlora + dense6,7에 denselora + dense8 ❄"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DmYS1wIAD5x8"
   },
   "source": [
    "## 2-1. (32, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "GFXXUUlKFeK7"
   },
   "outputs": [],
   "source": [
    "dataset_size = x_train.shape[0]\n",
    "batch_size = 30\n",
    "epochs = 20\n",
    "\n",
    "total_iteration= int(epochs * (dataset_size / batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "Z2OFtKnqD4bc"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout, InputLayer\n",
    "import tensorflow as tf\n",
    "\n",
    "# 새로운 레이어를 추가하기 위한 레이어 리스트 초기화\n",
    "new_layers = []\n",
    "\n",
    "# Dense 레이어 카운터\n",
    "dense_layer_count = 0\n",
    "\n",
    "# VGG 모델의 각 레이어 순회\n",
    "for layer in best_vgg16.layers:\n",
    "    if isinstance(layer, InputLayer):\n",
    "        continue\n",
    "    if isinstance(layer, Conv2D):\n",
    "        # Conv2D 레이어를 ConvLoRALayer로 대체\n",
    "        conv_lora_layer = ConvLoRALayer00_cdn2(original_conv_layer=layer, rank=32, alpha=32, total_iteration = total_iteration, trainable=True)\n",
    "        new_layers.append(conv_lora_layer)\n",
    "    elif isinstance(layer, Dense):\n",
    "        dense_layer_count += 1\n",
    "        # 첫 번째와 두 번째 Dense 레이어에만 LoraLayer 적용\n",
    "        if dense_layer_count in [1, 2]:\n",
    "            dense_lora_layer = LoraLayer(original_layer=layer, rank=32, alpha=32, total_iteration=total_iteration, trainable=True)\n",
    "            new_layers.append(dense_lora_layer)\n",
    "        else:\n",
    "            new_layers.append(layer)  # 그대로 유지함\n",
    "    else:\n",
    "        # 다른 유형의 레이어는 그대로 유지\n",
    "        new_layers.append(layer)\n",
    "\n",
    "# 새로운 입력 텐서 생성\n",
    "input_tensor = best_vgg16.input\n",
    "\n",
    "# 새로운 모델 구성\n",
    "x = input_tensor\n",
    "for layer in new_layers:\n",
    "    x = layer(x)\n",
    "\n",
    "# 새로운 모델 생성\n",
    "exp21_lora_vgg16 = Model(inputs=input_tensor, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "GbLnmfaLELbE"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D\n",
    "\n",
    "# 새로 생성된 모델에서 Dense 레이어 중 LoRA가 적용되지 않은 레이어의 trainable을 False로 설정\n",
    "for layer in exp21_lora_vgg16.layers:\n",
    "    if isinstance(layer, Dense) and not isinstance(layer, LoraLayer):\n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aP_ss6DrEcBt",
    "outputId": "63671162-e007-44da-f92a-49b173cff6c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " block1_conv1 (ConvLoRALaye  (None, 32, 32, 64)        21154     \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block1_conv2 (ConvLoRALaye  (None, 32, 32, 64)        73858     \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " block2_conv1 (ConvLoRALaye  (None, 16, 16, 128)       129282    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block2_conv2 (ConvLoRALaye  (None, 16, 16, 128)       221442    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " block3_conv1 (ConvLoRALaye  (None, 8, 8, 256)         406018    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block3_conv2 (ConvLoRALaye  (None, 8, 8, 256)         737794    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block3_conv3 (ConvLoRALaye  (None, 8, 8, 256)         737794    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 4, 4, 256)         0         \n",
      "                                                                 \n",
      " block4_conv1 (ConvLoRALaye  (None, 4, 4, 512)         1401858   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block4_conv2 (ConvLoRALaye  (None, 4, 4, 512)         2655234   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block4_conv3 (ConvLoRALaye  (None, 4, 4, 512)         2655234   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      " block5_conv1 (ConvLoRALaye  (None, 2, 2, 512)         2655234   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block5_conv2 (ConvLoRALaye  (None, 2, 2, 512)         2655234   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block5_conv3 (ConvLoRALaye  (None, 2, 2, 512)         2655234   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 1, 1, 512)         0         \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_6 (LoraLayer)         (None, 4096)              2252802   \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_7 (LoraLayer)         (None, 512)               2245634   \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21508936 (82.05 MB)\n",
      "Trainable params: 2590176 (9.88 MB)\n",
      "Non-trainable params: 18918760 (72.17 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "exp21_lora_vgg16.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "155io3wREiB2",
    "outputId": "1a1abe8e-8ae7-4f7b-a46a-add76eb92b22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: input_3\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block1_conv1\n",
      "  Trainable parameters: 19360\n",
      "  Non-trainable parameters: 1794\n",
      "Layer: block1_conv2\n",
      "  Trainable parameters: 36928\n",
      "  Non-trainable parameters: 36930\n",
      "Layer: block1_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block2_conv1\n",
      "  Trainable parameters: 55424\n",
      "  Non-trainable parameters: 73858\n",
      "Layer: block2_conv2\n",
      "  Trainable parameters: 73856\n",
      "  Non-trainable parameters: 147586\n",
      "Layer: block2_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block3_conv1\n",
      "  Trainable parameters: 110848\n",
      "  Non-trainable parameters: 295170\n",
      "Layer: block3_conv2\n",
      "  Trainable parameters: 147712\n",
      "  Non-trainable parameters: 590082\n",
      "Layer: block3_conv3\n",
      "  Trainable parameters: 147712\n",
      "  Non-trainable parameters: 590082\n",
      "Layer: block3_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block4_conv1\n",
      "  Trainable parameters: 221696\n",
      "  Non-trainable parameters: 1180162\n",
      "Layer: block4_conv2\n",
      "  Trainable parameters: 295424\n",
      "  Non-trainable parameters: 2359810\n",
      "Layer: block4_conv3\n",
      "  Trainable parameters: 295424\n",
      "  Non-trainable parameters: 2359810\n",
      "Layer: block4_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block5_conv1\n",
      "  Trainable parameters: 295424\n",
      "  Non-trainable parameters: 2359810\n",
      "Layer: block5_conv2\n",
      "  Trainable parameters: 295424\n",
      "  Non-trainable parameters: 2359810\n",
      "Layer: block5_conv3\n",
      "  Trainable parameters: 295424\n",
      "  Non-trainable parameters: 2359810\n",
      "Layer: block5_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: flatten_2\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: dense_6\n",
      "  Trainable parameters: 151552\n",
      "  Non-trainable parameters: 2101250\n",
      "Layer: dropout_4\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: dense_7\n",
      "  Trainable parameters: 147968\n",
      "  Non-trainable parameters: 2097666\n",
      "Layer: dropout_5\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: dense_8\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 5130\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#모델의 각 레이어를 순회하며 파라미터 수를 계산\n",
    "for layer in exp21_lora_vgg16.layers:\n",
    "    trainable_count = np.sum([tf.size(w).numpy() for w in layer.trainable_weights])\n",
    "    non_trainable_count = np.sum([tf.size(w).numpy() for w in layer.non_trainable_weights])\n",
    "\n",
    "    print(f\"Layer: {layer.name}\")\n",
    "    print(f\"  Trainable parameters: {trainable_count}\")\n",
    "    print(f\"  Non-trainable parameters: {non_trainable_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "6eHa70iQEo_N"
   },
   "outputs": [],
   "source": [
    "lora_layers = []  # lora 레이어들을 저장할 리스트\n",
    "\n",
    "for layer in exp21_lora_vgg16.layers:\n",
    "    if isinstance(layer, LoraLayer) or isinstance(layer, ConvLoRALayer00_cdn2):\n",
    "        lora_layers.append(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "4vQVbHIuEsVk"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "class PrintCurrentStepCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, lora_layers):\n",
    "        super().__init__()\n",
    "        self.lora_layers = lora_layers\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print()\n",
    "        for i, lora_layer in enumerate(self.lora_layers):\n",
    "            current_step = lora_layer.current_step.value()\n",
    "            decay_factor = lora_layer.decay_factor.value()\n",
    "            print(f\"End of epoch {epoch + 1}, LoraLayer {i}: {current_step} Step\")\n",
    "            print(f\"End of epoch {epoch + 1}, LoraLayer {i}: Decay factor: {decay_factor}\")\n",
    "\n",
    "class TestCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, test_data):\n",
    "        self.test_data = test_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        x, y = self.test_data\n",
    "        loss, acc = self.model.evaluate(x, y, verbose=0)\n",
    "        print('\\nTesting loss: {}, acc: {}\\n'.format(loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "sWlqdMvwFBdM"
   },
   "outputs": [],
   "source": [
    "# 콜백 생성 시 lora_layers 딕셔너리의 값만 사용\n",
    "print_step_callback = PrintCurrentStepCallback(lora_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "Nv9o4F_-FEAb"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# 모델 컴파일\n",
    "exp21_lora_vgg16.compile(optimizer=Adam(learning_rate=0.00005),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2OU9-UYR-4U1"
   },
   "source": [
    "### 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VH7DGSABFKfM",
    "outputId": "4c8bc6cb-79de-4456-d641-301e2d97ded9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.0797 - accuracy: 0.9755\n",
      "End of epoch 1, LoraLayer 0: 1667 Step\n",
      "End of epoch 1, LoraLayer 0: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 1: 1667 Step\n",
      "End of epoch 1, LoraLayer 1: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 2: 1667 Step\n",
      "End of epoch 1, LoraLayer 2: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 3: 1667 Step\n",
      "End of epoch 1, LoraLayer 3: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 4: 1667 Step\n",
      "End of epoch 1, LoraLayer 4: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 5: 1667 Step\n",
      "End of epoch 1, LoraLayer 5: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 6: 1667 Step\n",
      "End of epoch 1, LoraLayer 6: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 7: 1667 Step\n",
      "End of epoch 1, LoraLayer 7: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 8: 1667 Step\n",
      "End of epoch 1, LoraLayer 8: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 9: 1667 Step\n",
      "End of epoch 1, LoraLayer 9: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 10: 1667 Step\n",
      "End of epoch 1, LoraLayer 10: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 11: 1667 Step\n",
      "End of epoch 1, LoraLayer 11: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 12: 1667 Step\n",
      "End of epoch 1, LoraLayer 12: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 13: 1667 Step\n",
      "End of epoch 1, LoraLayer 13: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 14: 1667 Step\n",
      "End of epoch 1, LoraLayer 14: Decay factor: 1.0\n",
      "\n",
      "Testing loss: 2.302628517150879, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 62s 32ms/step - loss: 0.0797 - accuracy: 0.9755 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 2/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.0611 - accuracy: 0.9807\n",
      "End of epoch 2, LoraLayer 0: 3334 Step\n",
      "End of epoch 2, LoraLayer 0: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 1: 3334 Step\n",
      "End of epoch 2, LoraLayer 1: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 2: 3334 Step\n",
      "End of epoch 2, LoraLayer 2: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 3: 3334 Step\n",
      "End of epoch 2, LoraLayer 3: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 4: 3334 Step\n",
      "End of epoch 2, LoraLayer 4: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 5: 3334 Step\n",
      "End of epoch 2, LoraLayer 5: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 6: 3334 Step\n",
      "End of epoch 2, LoraLayer 6: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 7: 3334 Step\n",
      "End of epoch 2, LoraLayer 7: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 8: 3334 Step\n",
      "End of epoch 2, LoraLayer 8: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 9: 3334 Step\n",
      "End of epoch 2, LoraLayer 9: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 10: 3334 Step\n",
      "End of epoch 2, LoraLayer 10: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 11: 3334 Step\n",
      "End of epoch 2, LoraLayer 11: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 12: 3334 Step\n",
      "End of epoch 2, LoraLayer 12: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 13: 3334 Step\n",
      "End of epoch 2, LoraLayer 13: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 14: 3334 Step\n",
      "End of epoch 2, LoraLayer 14: Decay factor: 0.9999625086784363\n",
      "\n",
      "Testing loss: 2.3027145862579346, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 50s 30ms/step - loss: 0.0611 - accuracy: 0.9807 - val_loss: 2.3027 - val_accuracy: 0.1000\n",
      "Epoch 3/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.0462 - accuracy: 0.9855\n",
      "End of epoch 3, LoraLayer 0: 5001 Step\n",
      "End of epoch 3, LoraLayer 0: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 1: 5001 Step\n",
      "End of epoch 3, LoraLayer 1: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 2: 5001 Step\n",
      "End of epoch 3, LoraLayer 2: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 3: 5001 Step\n",
      "End of epoch 3, LoraLayer 3: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 4: 5001 Step\n",
      "End of epoch 3, LoraLayer 4: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 5: 5001 Step\n",
      "End of epoch 3, LoraLayer 5: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 6: 5001 Step\n",
      "End of epoch 3, LoraLayer 6: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 7: 5001 Step\n",
      "End of epoch 3, LoraLayer 7: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 8: 5001 Step\n",
      "End of epoch 3, LoraLayer 8: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 9: 5001 Step\n",
      "End of epoch 3, LoraLayer 9: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 10: 5001 Step\n",
      "End of epoch 3, LoraLayer 10: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 11: 5001 Step\n",
      "End of epoch 3, LoraLayer 11: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 12: 5001 Step\n",
      "End of epoch 3, LoraLayer 12: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 13: 5001 Step\n",
      "End of epoch 3, LoraLayer 13: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 14: 5001 Step\n",
      "End of epoch 3, LoraLayer 14: Decay factor: 0.9374484419822693\n",
      "\n",
      "Testing loss: 2.302722215652466, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 51s 31ms/step - loss: 0.0462 - accuracy: 0.9855 - val_loss: 2.3027 - val_accuracy: 0.1000\n",
      "Epoch 4/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.0473 - accuracy: 0.9854\n",
      "End of epoch 4, LoraLayer 0: 6668 Step\n",
      "End of epoch 4, LoraLayer 0: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 1: 6668 Step\n",
      "End of epoch 4, LoraLayer 1: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 2: 6668 Step\n",
      "End of epoch 4, LoraLayer 2: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 3: 6668 Step\n",
      "End of epoch 4, LoraLayer 3: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 4: 6668 Step\n",
      "End of epoch 4, LoraLayer 4: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 5: 6668 Step\n",
      "End of epoch 4, LoraLayer 5: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 6: 6668 Step\n",
      "End of epoch 4, LoraLayer 6: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 7: 6668 Step\n",
      "End of epoch 4, LoraLayer 7: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 8: 6668 Step\n",
      "End of epoch 4, LoraLayer 8: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 9: 6668 Step\n",
      "End of epoch 4, LoraLayer 9: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 10: 6668 Step\n",
      "End of epoch 4, LoraLayer 10: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 11: 6668 Step\n",
      "End of epoch 4, LoraLayer 11: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 12: 6668 Step\n",
      "End of epoch 4, LoraLayer 12: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 13: 6668 Step\n",
      "End of epoch 4, LoraLayer 13: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 14: 6668 Step\n",
      "End of epoch 4, LoraLayer 14: Decay factor: 0.8749343752861023\n",
      "\n",
      "Testing loss: 2.3029701709747314, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 50s 30ms/step - loss: 0.0473 - accuracy: 0.9854 - val_loss: 2.3030 - val_accuracy: 0.1000\n",
      "Epoch 5/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.0489 - accuracy: 0.9841\n",
      "End of epoch 5, LoraLayer 0: 8335 Step\n",
      "End of epoch 5, LoraLayer 0: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 1: 8335 Step\n",
      "End of epoch 5, LoraLayer 1: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 2: 8335 Step\n",
      "End of epoch 5, LoraLayer 2: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 3: 8335 Step\n",
      "End of epoch 5, LoraLayer 3: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 4: 8335 Step\n",
      "End of epoch 5, LoraLayer 4: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 5: 8335 Step\n",
      "End of epoch 5, LoraLayer 5: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 6: 8335 Step\n",
      "End of epoch 5, LoraLayer 6: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 7: 8335 Step\n",
      "End of epoch 5, LoraLayer 7: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 8: 8335 Step\n",
      "End of epoch 5, LoraLayer 8: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 9: 8335 Step\n",
      "End of epoch 5, LoraLayer 9: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 10: 8335 Step\n",
      "End of epoch 5, LoraLayer 10: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 11: 8335 Step\n",
      "End of epoch 5, LoraLayer 11: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 12: 8335 Step\n",
      "End of epoch 5, LoraLayer 12: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 13: 8335 Step\n",
      "End of epoch 5, LoraLayer 13: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 14: 8335 Step\n",
      "End of epoch 5, LoraLayer 14: Decay factor: 0.8124203085899353\n",
      "\n",
      "Testing loss: 2.303858518600464, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 53s 32ms/step - loss: 0.0489 - accuracy: 0.9841 - val_loss: 2.3039 - val_accuracy: 0.1000\n",
      "Epoch 6/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.0556 - accuracy: 0.9816\n",
      "End of epoch 6, LoraLayer 0: 10002 Step\n",
      "End of epoch 6, LoraLayer 0: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 1: 10002 Step\n",
      "End of epoch 6, LoraLayer 1: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 2: 10002 Step\n",
      "End of epoch 6, LoraLayer 2: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 3: 10002 Step\n",
      "End of epoch 6, LoraLayer 3: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 4: 10002 Step\n",
      "End of epoch 6, LoraLayer 4: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 5: 10002 Step\n",
      "End of epoch 6, LoraLayer 5: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 6: 10002 Step\n",
      "End of epoch 6, LoraLayer 6: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 7: 10002 Step\n",
      "End of epoch 6, LoraLayer 7: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 8: 10002 Step\n",
      "End of epoch 6, LoraLayer 8: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 9: 10002 Step\n",
      "End of epoch 6, LoraLayer 9: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 10: 10002 Step\n",
      "End of epoch 6, LoraLayer 10: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 11: 10002 Step\n",
      "End of epoch 6, LoraLayer 11: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 12: 10002 Step\n",
      "End of epoch 6, LoraLayer 12: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 13: 10002 Step\n",
      "End of epoch 6, LoraLayer 13: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 14: 10002 Step\n",
      "End of epoch 6, LoraLayer 14: Decay factor: 0.7499062418937683\n",
      "\n",
      "Testing loss: 2.3050599098205566, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 53s 32ms/step - loss: 0.0556 - accuracy: 0.9816 - val_loss: 2.3051 - val_accuracy: 0.1000\n",
      "Epoch 7/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.0617 - accuracy: 0.9793\n",
      "End of epoch 7, LoraLayer 0: 11669 Step\n",
      "End of epoch 7, LoraLayer 0: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 1: 11669 Step\n",
      "End of epoch 7, LoraLayer 1: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 2: 11669 Step\n",
      "End of epoch 7, LoraLayer 2: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 3: 11669 Step\n",
      "End of epoch 7, LoraLayer 3: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 4: 11669 Step\n",
      "End of epoch 7, LoraLayer 4: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 5: 11669 Step\n",
      "End of epoch 7, LoraLayer 5: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 6: 11669 Step\n",
      "End of epoch 7, LoraLayer 6: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 7: 11669 Step\n",
      "End of epoch 7, LoraLayer 7: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 8: 11669 Step\n",
      "End of epoch 7, LoraLayer 8: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 9: 11669 Step\n",
      "End of epoch 7, LoraLayer 9: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 10: 11669 Step\n",
      "End of epoch 7, LoraLayer 10: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 11: 11669 Step\n",
      "End of epoch 7, LoraLayer 11: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 12: 11669 Step\n",
      "End of epoch 7, LoraLayer 12: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 13: 11669 Step\n",
      "End of epoch 7, LoraLayer 13: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 14: 11669 Step\n",
      "End of epoch 7, LoraLayer 14: Decay factor: 0.6873922348022461\n",
      "\n",
      "Testing loss: 2.3091020584106445, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 53s 32ms/step - loss: 0.0616 - accuracy: 0.9793 - val_loss: 2.3091 - val_accuracy: 0.1000\n",
      "Epoch 8/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.0749 - accuracy: 0.9749\n",
      "End of epoch 8, LoraLayer 0: 13336 Step\n",
      "End of epoch 8, LoraLayer 0: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 1: 13336 Step\n",
      "End of epoch 8, LoraLayer 1: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 2: 13336 Step\n",
      "End of epoch 8, LoraLayer 2: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 3: 13336 Step\n",
      "End of epoch 8, LoraLayer 3: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 4: 13336 Step\n",
      "End of epoch 8, LoraLayer 4: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 5: 13336 Step\n",
      "End of epoch 8, LoraLayer 5: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 6: 13336 Step\n",
      "End of epoch 8, LoraLayer 6: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 7: 13336 Step\n",
      "End of epoch 8, LoraLayer 7: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 8: 13336 Step\n",
      "End of epoch 8, LoraLayer 8: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 9: 13336 Step\n",
      "End of epoch 8, LoraLayer 9: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 10: 13336 Step\n",
      "End of epoch 8, LoraLayer 10: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 11: 13336 Step\n",
      "End of epoch 8, LoraLayer 11: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 12: 13336 Step\n",
      "End of epoch 8, LoraLayer 12: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 13: 13336 Step\n",
      "End of epoch 8, LoraLayer 13: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 14: 13336 Step\n",
      "End of epoch 8, LoraLayer 14: Decay factor: 0.6248781681060791\n",
      "\n",
      "Testing loss: 2.314950466156006, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 55s 33ms/step - loss: 0.0749 - accuracy: 0.9749 - val_loss: 2.3150 - val_accuracy: 0.1000\n",
      "Epoch 9/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.0884 - accuracy: 0.9697\n",
      "End of epoch 9, LoraLayer 0: 15003 Step\n",
      "End of epoch 9, LoraLayer 0: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 1: 15003 Step\n",
      "End of epoch 9, LoraLayer 1: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 2: 15003 Step\n",
      "End of epoch 9, LoraLayer 2: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 3: 15003 Step\n",
      "End of epoch 9, LoraLayer 3: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 4: 15003 Step\n",
      "End of epoch 9, LoraLayer 4: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 5: 15003 Step\n",
      "End of epoch 9, LoraLayer 5: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 6: 15003 Step\n",
      "End of epoch 9, LoraLayer 6: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 7: 15003 Step\n",
      "End of epoch 9, LoraLayer 7: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 8: 15003 Step\n",
      "End of epoch 9, LoraLayer 8: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 9: 15003 Step\n",
      "End of epoch 9, LoraLayer 9: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 10: 15003 Step\n",
      "End of epoch 9, LoraLayer 10: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 11: 15003 Step\n",
      "End of epoch 9, LoraLayer 11: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 12: 15003 Step\n",
      "End of epoch 9, LoraLayer 12: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 13: 15003 Step\n",
      "End of epoch 9, LoraLayer 13: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 14: 15003 Step\n",
      "End of epoch 9, LoraLayer 14: Decay factor: 0.5623641014099121\n",
      "\n",
      "Testing loss: 2.344534158706665, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 56s 34ms/step - loss: 0.0884 - accuracy: 0.9697 - val_loss: 2.3445 - val_accuracy: 0.1000\n",
      "Epoch 10/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.1143 - accuracy: 0.9598\n",
      "End of epoch 10, LoraLayer 0: 16670 Step\n",
      "End of epoch 10, LoraLayer 0: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 1: 16670 Step\n",
      "End of epoch 10, LoraLayer 1: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 2: 16670 Step\n",
      "End of epoch 10, LoraLayer 2: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 3: 16670 Step\n",
      "End of epoch 10, LoraLayer 3: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 4: 16670 Step\n",
      "End of epoch 10, LoraLayer 4: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 5: 16670 Step\n",
      "End of epoch 10, LoraLayer 5: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 6: 16670 Step\n",
      "End of epoch 10, LoraLayer 6: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 7: 16670 Step\n",
      "End of epoch 10, LoraLayer 7: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 8: 16670 Step\n",
      "End of epoch 10, LoraLayer 8: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 9: 16670 Step\n",
      "End of epoch 10, LoraLayer 9: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 10: 16670 Step\n",
      "End of epoch 10, LoraLayer 10: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 11: 16670 Step\n",
      "End of epoch 10, LoraLayer 11: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 12: 16670 Step\n",
      "End of epoch 10, LoraLayer 12: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 13: 16670 Step\n",
      "End of epoch 10, LoraLayer 13: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 14: 16670 Step\n",
      "End of epoch 10, LoraLayer 14: Decay factor: 0.4998500347137451\n",
      "\n",
      "Testing loss: 2.383890390396118, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 51s 30ms/step - loss: 0.1143 - accuracy: 0.9598 - val_loss: 2.3839 - val_accuracy: 0.1000\n",
      "Epoch 11/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.1371 - accuracy: 0.9522\n",
      "End of epoch 11, LoraLayer 0: 18337 Step\n",
      "End of epoch 11, LoraLayer 0: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 1: 18337 Step\n",
      "End of epoch 11, LoraLayer 1: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 2: 18337 Step\n",
      "End of epoch 11, LoraLayer 2: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 3: 18337 Step\n",
      "End of epoch 11, LoraLayer 3: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 4: 18337 Step\n",
      "End of epoch 11, LoraLayer 4: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 5: 18337 Step\n",
      "End of epoch 11, LoraLayer 5: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 6: 18337 Step\n",
      "End of epoch 11, LoraLayer 6: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 7: 18337 Step\n",
      "End of epoch 11, LoraLayer 7: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 8: 18337 Step\n",
      "End of epoch 11, LoraLayer 8: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 9: 18337 Step\n",
      "End of epoch 11, LoraLayer 9: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 10: 18337 Step\n",
      "End of epoch 11, LoraLayer 10: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 11: 18337 Step\n",
      "End of epoch 11, LoraLayer 11: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 12: 18337 Step\n",
      "End of epoch 11, LoraLayer 12: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 13: 18337 Step\n",
      "End of epoch 11, LoraLayer 13: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 14: 18337 Step\n",
      "End of epoch 11, LoraLayer 14: Decay factor: 0.4373359680175781\n",
      "\n",
      "Testing loss: 2.490032434463501, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 57s 34ms/step - loss: 0.1371 - accuracy: 0.9522 - val_loss: 2.4900 - val_accuracy: 0.1000\n",
      "Epoch 12/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.1716 - accuracy: 0.9396\n",
      "End of epoch 12, LoraLayer 0: 20004 Step\n",
      "End of epoch 12, LoraLayer 0: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 1: 20004 Step\n",
      "End of epoch 12, LoraLayer 1: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 2: 20004 Step\n",
      "End of epoch 12, LoraLayer 2: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 3: 20004 Step\n",
      "End of epoch 12, LoraLayer 3: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 4: 20004 Step\n",
      "End of epoch 12, LoraLayer 4: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 5: 20004 Step\n",
      "End of epoch 12, LoraLayer 5: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 6: 20004 Step\n",
      "End of epoch 12, LoraLayer 6: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 7: 20004 Step\n",
      "End of epoch 12, LoraLayer 7: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 8: 20004 Step\n",
      "End of epoch 12, LoraLayer 8: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 9: 20004 Step\n",
      "End of epoch 12, LoraLayer 9: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 10: 20004 Step\n",
      "End of epoch 12, LoraLayer 10: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 11: 20004 Step\n",
      "End of epoch 12, LoraLayer 11: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 12: 20004 Step\n",
      "End of epoch 12, LoraLayer 12: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 13: 20004 Step\n",
      "End of epoch 12, LoraLayer 13: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 14: 20004 Step\n",
      "End of epoch 12, LoraLayer 14: Decay factor: 0.37482190132141113\n",
      "\n",
      "Testing loss: 2.753675699234009, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 56s 33ms/step - loss: 0.1715 - accuracy: 0.9396 - val_loss: 2.7537 - val_accuracy: 0.1000\n",
      "Epoch 13/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.2047 - accuracy: 0.9274\n",
      "End of epoch 13, LoraLayer 0: 21671 Step\n",
      "End of epoch 13, LoraLayer 0: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 1: 21671 Step\n",
      "End of epoch 13, LoraLayer 1: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 2: 21671 Step\n",
      "End of epoch 13, LoraLayer 2: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 3: 21671 Step\n",
      "End of epoch 13, LoraLayer 3: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 4: 21671 Step\n",
      "End of epoch 13, LoraLayer 4: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 5: 21671 Step\n",
      "End of epoch 13, LoraLayer 5: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 6: 21671 Step\n",
      "End of epoch 13, LoraLayer 6: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 7: 21671 Step\n",
      "End of epoch 13, LoraLayer 7: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 8: 21671 Step\n",
      "End of epoch 13, LoraLayer 8: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 9: 21671 Step\n",
      "End of epoch 13, LoraLayer 9: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 10: 21671 Step\n",
      "End of epoch 13, LoraLayer 10: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 11: 21671 Step\n",
      "End of epoch 13, LoraLayer 11: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 12: 21671 Step\n",
      "End of epoch 13, LoraLayer 12: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 13: 21671 Step\n",
      "End of epoch 13, LoraLayer 13: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 14: 21671 Step\n",
      "End of epoch 13, LoraLayer 14: Decay factor: 0.31230783462524414\n",
      "\n",
      "Testing loss: 3.6042394638061523, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 51s 31ms/step - loss: 0.2047 - accuracy: 0.9274 - val_loss: 3.6042 - val_accuracy: 0.1000\n",
      "Epoch 14/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.2507 - accuracy: 0.9125\n",
      "End of epoch 14, LoraLayer 0: 23338 Step\n",
      "End of epoch 14, LoraLayer 0: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 1: 23338 Step\n",
      "End of epoch 14, LoraLayer 1: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 2: 23338 Step\n",
      "End of epoch 14, LoraLayer 2: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 3: 23338 Step\n",
      "End of epoch 14, LoraLayer 3: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 4: 23338 Step\n",
      "End of epoch 14, LoraLayer 4: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 5: 23338 Step\n",
      "End of epoch 14, LoraLayer 5: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 6: 23338 Step\n",
      "End of epoch 14, LoraLayer 6: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 7: 23338 Step\n",
      "End of epoch 14, LoraLayer 7: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 8: 23338 Step\n",
      "End of epoch 14, LoraLayer 8: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 9: 23338 Step\n",
      "End of epoch 14, LoraLayer 9: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 10: 23338 Step\n",
      "End of epoch 14, LoraLayer 10: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 11: 23338 Step\n",
      "End of epoch 14, LoraLayer 11: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 12: 23338 Step\n",
      "End of epoch 14, LoraLayer 12: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 13: 23338 Step\n",
      "End of epoch 14, LoraLayer 13: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 14: 23338 Step\n",
      "End of epoch 14, LoraLayer 14: Decay factor: 0.24979376792907715\n",
      "\n",
      "Testing loss: 3.8891985416412354, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 56s 34ms/step - loss: 0.2507 - accuracy: 0.9125 - val_loss: 3.8892 - val_accuracy: 0.1000\n",
      "Epoch 15/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.3074 - accuracy: 0.8919\n",
      "End of epoch 15, LoraLayer 0: 25005 Step\n",
      "End of epoch 15, LoraLayer 0: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 1: 25005 Step\n",
      "End of epoch 15, LoraLayer 1: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 2: 25005 Step\n",
      "End of epoch 15, LoraLayer 2: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 3: 25005 Step\n",
      "End of epoch 15, LoraLayer 3: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 4: 25005 Step\n",
      "End of epoch 15, LoraLayer 4: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 5: 25005 Step\n",
      "End of epoch 15, LoraLayer 5: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 6: 25005 Step\n",
      "End of epoch 15, LoraLayer 6: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 7: 25005 Step\n",
      "End of epoch 15, LoraLayer 7: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 8: 25005 Step\n",
      "End of epoch 15, LoraLayer 8: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 9: 25005 Step\n",
      "End of epoch 15, LoraLayer 9: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 10: 25005 Step\n",
      "End of epoch 15, LoraLayer 10: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 11: 25005 Step\n",
      "End of epoch 15, LoraLayer 11: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 12: 25005 Step\n",
      "End of epoch 15, LoraLayer 12: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 13: 25005 Step\n",
      "End of epoch 15, LoraLayer 13: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 14: 25005 Step\n",
      "End of epoch 15, LoraLayer 14: Decay factor: 0.18727970123291016\n",
      "\n",
      "Testing loss: 4.68809175491333, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 51s 30ms/step - loss: 0.3074 - accuracy: 0.8919 - val_loss: 4.6881 - val_accuracy: 0.1000\n",
      "Epoch 16/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.3880 - accuracy: 0.8652\n",
      "End of epoch 16, LoraLayer 0: 26672 Step\n",
      "End of epoch 16, LoraLayer 0: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 1: 26672 Step\n",
      "End of epoch 16, LoraLayer 1: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 2: 26672 Step\n",
      "End of epoch 16, LoraLayer 2: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 3: 26672 Step\n",
      "End of epoch 16, LoraLayer 3: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 4: 26672 Step\n",
      "End of epoch 16, LoraLayer 4: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 5: 26672 Step\n",
      "End of epoch 16, LoraLayer 5: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 6: 26672 Step\n",
      "End of epoch 16, LoraLayer 6: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 7: 26672 Step\n",
      "End of epoch 16, LoraLayer 7: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 8: 26672 Step\n",
      "End of epoch 16, LoraLayer 8: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 9: 26672 Step\n",
      "End of epoch 16, LoraLayer 9: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 10: 26672 Step\n",
      "End of epoch 16, LoraLayer 10: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 11: 26672 Step\n",
      "End of epoch 16, LoraLayer 11: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 12: 26672 Step\n",
      "End of epoch 16, LoraLayer 12: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 13: 26672 Step\n",
      "End of epoch 16, LoraLayer 13: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 14: 26672 Step\n",
      "End of epoch 16, LoraLayer 14: Decay factor: 0.12476563453674316\n",
      "\n",
      "Testing loss: 4.126160144805908, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 58s 35ms/step - loss: 0.3879 - accuracy: 0.8652 - val_loss: 4.1262 - val_accuracy: 0.1000\n",
      "Epoch 17/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.4918 - accuracy: 0.8316\n",
      "End of epoch 17, LoraLayer 0: 28339 Step\n",
      "End of epoch 17, LoraLayer 0: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 1: 28339 Step\n",
      "End of epoch 17, LoraLayer 1: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 2: 28339 Step\n",
      "End of epoch 17, LoraLayer 2: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 3: 28339 Step\n",
      "End of epoch 17, LoraLayer 3: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 4: 28339 Step\n",
      "End of epoch 17, LoraLayer 4: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 5: 28339 Step\n",
      "End of epoch 17, LoraLayer 5: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 6: 28339 Step\n",
      "End of epoch 17, LoraLayer 6: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 7: 28339 Step\n",
      "End of epoch 17, LoraLayer 7: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 8: 28339 Step\n",
      "End of epoch 17, LoraLayer 8: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 9: 28339 Step\n",
      "End of epoch 17, LoraLayer 9: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 10: 28339 Step\n",
      "End of epoch 17, LoraLayer 10: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 11: 28339 Step\n",
      "End of epoch 17, LoraLayer 11: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 12: 28339 Step\n",
      "End of epoch 17, LoraLayer 12: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 13: 28339 Step\n",
      "End of epoch 17, LoraLayer 13: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 14: 28339 Step\n",
      "End of epoch 17, LoraLayer 14: Decay factor: 0.06225156784057617\n",
      "\n",
      "Testing loss: 2.918261766433716, acc: 0.10159999877214432\n",
      "\n",
      "1667/1667 [==============================] - 58s 35ms/step - loss: 0.4918 - accuracy: 0.8316 - val_loss: 2.9183 - val_accuracy: 0.1016\n",
      "Epoch 18/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.6175 - accuracy: 0.7898\n",
      "End of epoch 18, LoraLayer 0: 30006 Step\n",
      "End of epoch 18, LoraLayer 0: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 1: 30006 Step\n",
      "End of epoch 18, LoraLayer 1: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 2: 30006 Step\n",
      "End of epoch 18, LoraLayer 2: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 3: 30006 Step\n",
      "End of epoch 18, LoraLayer 3: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 4: 30006 Step\n",
      "End of epoch 18, LoraLayer 4: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 5: 30006 Step\n",
      "End of epoch 18, LoraLayer 5: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 6: 30006 Step\n",
      "End of epoch 18, LoraLayer 6: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 7: 30006 Step\n",
      "End of epoch 18, LoraLayer 7: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 8: 30006 Step\n",
      "End of epoch 18, LoraLayer 8: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 9: 30006 Step\n",
      "End of epoch 18, LoraLayer 9: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 10: 30006 Step\n",
      "End of epoch 18, LoraLayer 10: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 11: 30006 Step\n",
      "End of epoch 18, LoraLayer 11: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 12: 30006 Step\n",
      "End of epoch 18, LoraLayer 12: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 13: 30006 Step\n",
      "End of epoch 18, LoraLayer 13: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 14: 30006 Step\n",
      "End of epoch 18, LoraLayer 14: Decay factor: 0.0\n",
      "\n",
      "Testing loss: 0.8109445571899414, acc: 0.7297999858856201\n",
      "\n",
      "1667/1667 [==============================] - 50s 30ms/step - loss: 0.6175 - accuracy: 0.7898 - val_loss: 0.8109 - val_accuracy: 0.7298\n",
      "Epoch 19/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.5842 - accuracy: 0.8013\n",
      "End of epoch 19, LoraLayer 0: 31673 Step\n",
      "End of epoch 19, LoraLayer 0: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 1: 31673 Step\n",
      "End of epoch 19, LoraLayer 1: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 2: 31673 Step\n",
      "End of epoch 19, LoraLayer 2: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 3: 31673 Step\n",
      "End of epoch 19, LoraLayer 3: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 4: 31673 Step\n",
      "End of epoch 19, LoraLayer 4: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 5: 31673 Step\n",
      "End of epoch 19, LoraLayer 5: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 6: 31673 Step\n",
      "End of epoch 19, LoraLayer 6: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 7: 31673 Step\n",
      "End of epoch 19, LoraLayer 7: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 8: 31673 Step\n",
      "End of epoch 19, LoraLayer 8: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 9: 31673 Step\n",
      "End of epoch 19, LoraLayer 9: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 10: 31673 Step\n",
      "End of epoch 19, LoraLayer 10: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 11: 31673 Step\n",
      "End of epoch 19, LoraLayer 11: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 12: 31673 Step\n",
      "End of epoch 19, LoraLayer 12: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 13: 31673 Step\n",
      "End of epoch 19, LoraLayer 13: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 14: 31673 Step\n",
      "End of epoch 19, LoraLayer 14: Decay factor: 0.0\n",
      "\n",
      "Testing loss: 0.7328405976295471, acc: 0.7487999796867371\n",
      "\n",
      "1667/1667 [==============================] - 58s 35ms/step - loss: 0.5842 - accuracy: 0.8013 - val_loss: 0.7328 - val_accuracy: 0.7488\n",
      "Epoch 20/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.4890 - accuracy: 0.8329\n",
      "End of epoch 20, LoraLayer 0: 33340 Step\n",
      "End of epoch 20, LoraLayer 0: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 1: 33340 Step\n",
      "End of epoch 20, LoraLayer 1: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 2: 33340 Step\n",
      "End of epoch 20, LoraLayer 2: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 3: 33340 Step\n",
      "End of epoch 20, LoraLayer 3: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 4: 33340 Step\n",
      "End of epoch 20, LoraLayer 4: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 5: 33340 Step\n",
      "End of epoch 20, LoraLayer 5: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 6: 33340 Step\n",
      "End of epoch 20, LoraLayer 6: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 7: 33340 Step\n",
      "End of epoch 20, LoraLayer 7: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 8: 33340 Step\n",
      "End of epoch 20, LoraLayer 8: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 9: 33340 Step\n",
      "End of epoch 20, LoraLayer 9: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 10: 33340 Step\n",
      "End of epoch 20, LoraLayer 10: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 11: 33340 Step\n",
      "End of epoch 20, LoraLayer 11: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 12: 33340 Step\n",
      "End of epoch 20, LoraLayer 12: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 13: 33340 Step\n",
      "End of epoch 20, LoraLayer 13: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 14: 33340 Step\n",
      "End of epoch 20, LoraLayer 14: Decay factor: 0.0\n",
      "\n",
      "Testing loss: 0.7337435483932495, acc: 0.7638999819755554\n",
      "\n",
      "1667/1667 [==============================] - 53s 32ms/step - loss: 0.4890 - accuracy: 0.8329 - val_loss: 0.7337 - val_accuracy: 0.7642\n"
     ]
    }
   ],
   "source": [
    "history_exp21 = exp21_lora_vgg16.fit(x_train, y_train, batch_size=30, epochs=20, validation_data=(x_test, y_test), callbacks=[print_step_callback, TestCallback((x_test, y_test))] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HW-llVuqfwUi",
    "outputId": "7913fd46-3d5e-43c3-a21b-37e64438053b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 3s 10ms/step - loss: 0.7337 - accuracy: 0.7639\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7337435483932495, 0.7638999819755554]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 평가\n",
    "exp21_lora_vgg16.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "id": "cQNbKdwVPyWo",
    "outputId": "42b5f2b2-e3fe-4e09-81b9-84460b7ed130"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 학습 및 검증 정확도 시각화\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# 정확도 그래프\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_exp21.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history_exp21.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# 손실 그래프\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_exp21.history['loss'], label='Training Loss')\n",
    "plt.plot(history_exp21.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NZajLvMZefqO"
   },
   "source": [
    "## 2-2. (32, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "EiCGNbU7eQCm"
   },
   "outputs": [],
   "source": [
    "dataset_size = x_train.shape[0]\n",
    "batch_size = 30\n",
    "epochs = 20\n",
    "\n",
    "total_iteration= int(epochs * (dataset_size / batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "z_Ywjw2xfCHR"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout, InputLayer\n",
    "import tensorflow as tf\n",
    "\n",
    "# 새로운 레이어를 추가하기 위한 레이어 리스트 초기화\n",
    "new_layers = []\n",
    "\n",
    "# Dense 레이어 카운터\n",
    "dense_layer_count = 0\n",
    "\n",
    "# VGG 모델의 각 레이어 순회\n",
    "for layer in best_vgg16.layers:\n",
    "    if isinstance(layer, InputLayer):\n",
    "        continue\n",
    "    if isinstance(layer, Conv2D):\n",
    "        # Conv2D 레이어를 ConvLoRALayer로 대체\n",
    "        conv_lora_layer = ConvLoRALayer00_cdn2(original_conv_layer=layer, rank=32, alpha=32, total_iteration = total_iteration, trainable=True)\n",
    "        new_layers.append(conv_lora_layer)\n",
    "    elif isinstance(layer, Dense):\n",
    "        dense_layer_count += 1\n",
    "        # 첫 번째와 두 번째 Dense 레이어에만 LoraLayer 적용\n",
    "        if dense_layer_count in [1, 2]:\n",
    "            dense_lora_layer = LoraLayer(original_layer=layer, rank=16, alpha=32, total_iteration=total_iteration, trainable=True)\n",
    "            new_layers.append(dense_lora_layer)\n",
    "        else:\n",
    "            new_layers.append(layer)  # 그대로 유지함\n",
    "    else:\n",
    "        # 다른 유형의 레이어는 그대로 유지\n",
    "        new_layers.append(layer)\n",
    "\n",
    "# 새로운 입력 텐서 생성\n",
    "input_tensor = best_vgg16.input\n",
    "\n",
    "# 새로운 모델 구성\n",
    "x = input_tensor\n",
    "for layer in new_layers:\n",
    "    x = layer(x)\n",
    "\n",
    "# 새로운 모델 생성\n",
    "exp22_lora_vgg16 = Model(inputs=input_tensor, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "7FBFKA9pfDgm"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D\n",
    "\n",
    "# 새로 생성된 모델에서 Dense 레이어 중 LoRA가 적용되지 않은 레이어의 trainable을 False로 설정\n",
    "for layer in exp22_lora_vgg16.layers:\n",
    "    if isinstance(layer, Dense) and not isinstance(layer, LoraLayer):\n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "05V-KQUSfEoE",
    "outputId": "a1c2c52a-a08c-48b3-b0f0-3f497d2e8cb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " block1_conv1 (ConvLoRALaye  (None, 32, 32, 64)        21154     \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block1_conv2 (ConvLoRALaye  (None, 32, 32, 64)        73858     \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " block2_conv1 (ConvLoRALaye  (None, 16, 16, 128)       129282    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block2_conv2 (ConvLoRALaye  (None, 16, 16, 128)       221442    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " block3_conv1 (ConvLoRALaye  (None, 8, 8, 256)         406018    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block3_conv2 (ConvLoRALaye  (None, 8, 8, 256)         737794    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block3_conv3 (ConvLoRALaye  (None, 8, 8, 256)         737794    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 4, 4, 256)         0         \n",
      "                                                                 \n",
      " block4_conv1 (ConvLoRALaye  (None, 4, 4, 512)         1401858   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block4_conv2 (ConvLoRALaye  (None, 4, 4, 512)         2655234   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block4_conv3 (ConvLoRALaye  (None, 4, 4, 512)         2655234   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      " block5_conv1 (ConvLoRALaye  (None, 2, 2, 512)         2655234   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block5_conv2 (ConvLoRALaye  (None, 2, 2, 512)         2655234   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block5_conv3 (ConvLoRALaye  (None, 2, 2, 512)         2655234   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 1, 1, 512)         0         \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_6 (LoraLayer)         (None, 4096)              2179074   \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_7 (LoraLayer)         (None, 512)               2171906   \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21361480 (81.49 MB)\n",
      "Trainable params: 2442720 (9.32 MB)\n",
      "Non-trainable params: 18918760 (72.17 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "exp22_lora_vgg16.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mtt2vlBufF-G",
    "outputId": "a8e87d2b-fe6e-43ac-bc90-6e1d0281c3c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: input_3\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block1_conv1\n",
      "  Trainable parameters: 19360\n",
      "  Non-trainable parameters: 1794\n",
      "Layer: block1_conv2\n",
      "  Trainable parameters: 36928\n",
      "  Non-trainable parameters: 36930\n",
      "Layer: block1_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block2_conv1\n",
      "  Trainable parameters: 55424\n",
      "  Non-trainable parameters: 73858\n",
      "Layer: block2_conv2\n",
      "  Trainable parameters: 73856\n",
      "  Non-trainable parameters: 147586\n",
      "Layer: block2_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block3_conv1\n",
      "  Trainable parameters: 110848\n",
      "  Non-trainable parameters: 295170\n",
      "Layer: block3_conv2\n",
      "  Trainable parameters: 147712\n",
      "  Non-trainable parameters: 590082\n",
      "Layer: block3_conv3\n",
      "  Trainable parameters: 147712\n",
      "  Non-trainable parameters: 590082\n",
      "Layer: block3_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block4_conv1\n",
      "  Trainable parameters: 221696\n",
      "  Non-trainable parameters: 1180162\n",
      "Layer: block4_conv2\n",
      "  Trainable parameters: 295424\n",
      "  Non-trainable parameters: 2359810\n",
      "Layer: block4_conv3\n",
      "  Trainable parameters: 295424\n",
      "  Non-trainable parameters: 2359810\n",
      "Layer: block4_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block5_conv1\n",
      "  Trainable parameters: 295424\n",
      "  Non-trainable parameters: 2359810\n",
      "Layer: block5_conv2\n",
      "  Trainable parameters: 295424\n",
      "  Non-trainable parameters: 2359810\n",
      "Layer: block5_conv3\n",
      "  Trainable parameters: 295424\n",
      "  Non-trainable parameters: 2359810\n",
      "Layer: block5_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: flatten_2\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: dense_6\n",
      "  Trainable parameters: 77824\n",
      "  Non-trainable parameters: 2101250\n",
      "Layer: dropout_4\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: dense_7\n",
      "  Trainable parameters: 74240\n",
      "  Non-trainable parameters: 2097666\n",
      "Layer: dropout_5\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: dense_8\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 5130\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#모델의 각 레이어를 순회하며 파라미터 수를 계산\n",
    "for layer in exp22_lora_vgg16.layers:\n",
    "    trainable_count = np.sum([tf.size(w).numpy() for w in layer.trainable_weights])\n",
    "    non_trainable_count = np.sum([tf.size(w).numpy() for w in layer.non_trainable_weights])\n",
    "\n",
    "    print(f\"Layer: {layer.name}\")\n",
    "    print(f\"  Trainable parameters: {trainable_count}\")\n",
    "    print(f\"  Non-trainable parameters: {non_trainable_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "p7JVgT5zfG8x"
   },
   "outputs": [],
   "source": [
    "lora_layers = []  # lora 레이어들을 저장할 리스트\n",
    "\n",
    "for layer in exp22_lora_vgg16.layers:\n",
    "    if isinstance(layer, LoraLayer) or isinstance(layer, ConvLoRALayer00_cdn2):\n",
    "        lora_layers.append(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "ywJfZm9PfIlc"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "class PrintCurrentStepCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, lora_layers):\n",
    "        super().__init__()\n",
    "        self.lora_layers = lora_layers\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print()\n",
    "        for i, lora_layer in enumerate(self.lora_layers):\n",
    "            current_step = lora_layer.current_step.value()\n",
    "            decay_factor = lora_layer.decay_factor.value()\n",
    "            print(f\"End of epoch {epoch + 1}, LoraLayer {i}: {current_step} Step\")\n",
    "            print(f\"End of epoch {epoch + 1}, LoraLayer {i}: Decay factor: {decay_factor}\")\n",
    "\n",
    "class TestCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, test_data):\n",
    "        self.test_data = test_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        x, y = self.test_data\n",
    "        loss, acc = self.model.evaluate(x, y, verbose=0)\n",
    "        print('\\nTesting loss: {}, acc: {}\\n'.format(loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "yc4NMzPJfJf6"
   },
   "outputs": [],
   "source": [
    "# 콜백 생성 시 lora_layers 딕셔너리의 값만 사용\n",
    "print_step_callback = PrintCurrentStepCallback(lora_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "3uc9kMrGfKXi"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# 모델 컴파일\n",
    "exp22_lora_vgg16.compile(optimizer=Adam(learning_rate=0.00005),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vo9O94YY--zH"
   },
   "source": [
    "### 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kSIOk0uqfMog",
    "outputId": "cd37895e-6326-49ba-f63c-b00cbb229672"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.0809 - accuracy: 0.9745\n",
      "End of epoch 1, LoraLayer 0: 1667 Step\n",
      "End of epoch 1, LoraLayer 0: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 1: 1667 Step\n",
      "End of epoch 1, LoraLayer 1: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 2: 1667 Step\n",
      "End of epoch 1, LoraLayer 2: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 3: 1667 Step\n",
      "End of epoch 1, LoraLayer 3: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 4: 1667 Step\n",
      "End of epoch 1, LoraLayer 4: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 5: 1667 Step\n",
      "End of epoch 1, LoraLayer 5: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 6: 1667 Step\n",
      "End of epoch 1, LoraLayer 6: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 7: 1667 Step\n",
      "End of epoch 1, LoraLayer 7: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 8: 1667 Step\n",
      "End of epoch 1, LoraLayer 8: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 9: 1667 Step\n",
      "End of epoch 1, LoraLayer 9: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 10: 1667 Step\n",
      "End of epoch 1, LoraLayer 10: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 11: 1667 Step\n",
      "End of epoch 1, LoraLayer 11: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 12: 1667 Step\n",
      "End of epoch 1, LoraLayer 12: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 13: 1667 Step\n",
      "End of epoch 1, LoraLayer 13: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 14: 1667 Step\n",
      "End of epoch 1, LoraLayer 14: Decay factor: 1.0\n",
      "\n",
      "Testing loss: 2.302625894546509, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 66s 34ms/step - loss: 0.0809 - accuracy: 0.9745 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 2/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.0575 - accuracy: 0.9821\n",
      "End of epoch 2, LoraLayer 0: 3334 Step\n",
      "End of epoch 2, LoraLayer 0: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 1: 3334 Step\n",
      "End of epoch 2, LoraLayer 1: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 2: 3334 Step\n",
      "End of epoch 2, LoraLayer 2: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 3: 3334 Step\n",
      "End of epoch 2, LoraLayer 3: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 4: 3334 Step\n",
      "End of epoch 2, LoraLayer 4: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 5: 3334 Step\n",
      "End of epoch 2, LoraLayer 5: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 6: 3334 Step\n",
      "End of epoch 2, LoraLayer 6: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 7: 3334 Step\n",
      "End of epoch 2, LoraLayer 7: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 8: 3334 Step\n",
      "End of epoch 2, LoraLayer 8: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 9: 3334 Step\n",
      "End of epoch 2, LoraLayer 9: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 10: 3334 Step\n",
      "End of epoch 2, LoraLayer 10: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 11: 3334 Step\n",
      "End of epoch 2, LoraLayer 11: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 12: 3334 Step\n",
      "End of epoch 2, LoraLayer 12: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 13: 3334 Step\n",
      "End of epoch 2, LoraLayer 13: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 14: 3334 Step\n",
      "End of epoch 2, LoraLayer 14: Decay factor: 0.9999625086784363\n",
      "\n",
      "Testing loss: 2.302708864212036, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 53s 32ms/step - loss: 0.0576 - accuracy: 0.9820 - val_loss: 2.3027 - val_accuracy: 0.1000\n",
      "Epoch 3/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.0496 - accuracy: 0.9837\n",
      "End of epoch 3, LoraLayer 0: 5001 Step\n",
      "End of epoch 3, LoraLayer 0: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 1: 5001 Step\n",
      "End of epoch 3, LoraLayer 1: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 2: 5001 Step\n",
      "End of epoch 3, LoraLayer 2: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 3: 5001 Step\n",
      "End of epoch 3, LoraLayer 3: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 4: 5001 Step\n",
      "End of epoch 3, LoraLayer 4: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 5: 5001 Step\n",
      "End of epoch 3, LoraLayer 5: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 6: 5001 Step\n",
      "End of epoch 3, LoraLayer 6: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 7: 5001 Step\n",
      "End of epoch 3, LoraLayer 7: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 8: 5001 Step\n",
      "End of epoch 3, LoraLayer 8: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 9: 5001 Step\n",
      "End of epoch 3, LoraLayer 9: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 10: 5001 Step\n",
      "End of epoch 3, LoraLayer 10: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 11: 5001 Step\n",
      "End of epoch 3, LoraLayer 11: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 12: 5001 Step\n",
      "End of epoch 3, LoraLayer 12: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 13: 5001 Step\n",
      "End of epoch 3, LoraLayer 13: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 14: 5001 Step\n",
      "End of epoch 3, LoraLayer 14: Decay factor: 0.9374484419822693\n",
      "\n",
      "Testing loss: 2.302964210510254, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 53s 32ms/step - loss: 0.0496 - accuracy: 0.9837 - val_loss: 2.3030 - val_accuracy: 0.1000\n",
      "Epoch 4/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.0491 - accuracy: 0.9844\n",
      "End of epoch 4, LoraLayer 0: 6668 Step\n",
      "End of epoch 4, LoraLayer 0: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 1: 6668 Step\n",
      "End of epoch 4, LoraLayer 1: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 2: 6668 Step\n",
      "End of epoch 4, LoraLayer 2: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 3: 6668 Step\n",
      "End of epoch 4, LoraLayer 3: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 4: 6668 Step\n",
      "End of epoch 4, LoraLayer 4: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 5: 6668 Step\n",
      "End of epoch 4, LoraLayer 5: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 6: 6668 Step\n",
      "End of epoch 4, LoraLayer 6: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 7: 6668 Step\n",
      "End of epoch 4, LoraLayer 7: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 8: 6668 Step\n",
      "End of epoch 4, LoraLayer 8: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 9: 6668 Step\n",
      "End of epoch 4, LoraLayer 9: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 10: 6668 Step\n",
      "End of epoch 4, LoraLayer 10: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 11: 6668 Step\n",
      "End of epoch 4, LoraLayer 11: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 12: 6668 Step\n",
      "End of epoch 4, LoraLayer 12: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 13: 6668 Step\n",
      "End of epoch 4, LoraLayer 13: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 14: 6668 Step\n",
      "End of epoch 4, LoraLayer 14: Decay factor: 0.8749343752861023\n",
      "\n",
      "Testing loss: 2.303173542022705, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 55s 33ms/step - loss: 0.0491 - accuracy: 0.9844 - val_loss: 2.3032 - val_accuracy: 0.1000\n",
      "Epoch 5/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.0529 - accuracy: 0.9832\n",
      "End of epoch 5, LoraLayer 0: 8335 Step\n",
      "End of epoch 5, LoraLayer 0: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 1: 8335 Step\n",
      "End of epoch 5, LoraLayer 1: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 2: 8335 Step\n",
      "End of epoch 5, LoraLayer 2: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 3: 8335 Step\n",
      "End of epoch 5, LoraLayer 3: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 4: 8335 Step\n",
      "End of epoch 5, LoraLayer 4: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 5: 8335 Step\n",
      "End of epoch 5, LoraLayer 5: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 6: 8335 Step\n",
      "End of epoch 5, LoraLayer 6: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 7: 8335 Step\n",
      "End of epoch 5, LoraLayer 7: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 8: 8335 Step\n",
      "End of epoch 5, LoraLayer 8: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 9: 8335 Step\n",
      "End of epoch 5, LoraLayer 9: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 10: 8335 Step\n",
      "End of epoch 5, LoraLayer 10: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 11: 8335 Step\n",
      "End of epoch 5, LoraLayer 11: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 12: 8335 Step\n",
      "End of epoch 5, LoraLayer 12: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 13: 8335 Step\n",
      "End of epoch 5, LoraLayer 13: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 14: 8335 Step\n",
      "End of epoch 5, LoraLayer 14: Decay factor: 0.8124203085899353\n",
      "\n",
      "Testing loss: 2.305567741394043, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 53s 32ms/step - loss: 0.0529 - accuracy: 0.9832 - val_loss: 2.3056 - val_accuracy: 0.1000\n",
      "Epoch 6/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.0588 - accuracy: 0.9810\n",
      "End of epoch 6, LoraLayer 0: 10002 Step\n",
      "End of epoch 6, LoraLayer 0: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 1: 10002 Step\n",
      "End of epoch 6, LoraLayer 1: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 2: 10002 Step\n",
      "End of epoch 6, LoraLayer 2: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 3: 10002 Step\n",
      "End of epoch 6, LoraLayer 3: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 4: 10002 Step\n",
      "End of epoch 6, LoraLayer 4: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 5: 10002 Step\n",
      "End of epoch 6, LoraLayer 5: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 6: 10002 Step\n",
      "End of epoch 6, LoraLayer 6: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 7: 10002 Step\n",
      "End of epoch 6, LoraLayer 7: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 8: 10002 Step\n",
      "End of epoch 6, LoraLayer 8: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 9: 10002 Step\n",
      "End of epoch 6, LoraLayer 9: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 10: 10002 Step\n",
      "End of epoch 6, LoraLayer 10: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 11: 10002 Step\n",
      "End of epoch 6, LoraLayer 11: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 12: 10002 Step\n",
      "End of epoch 6, LoraLayer 12: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 13: 10002 Step\n",
      "End of epoch 6, LoraLayer 13: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 14: 10002 Step\n",
      "End of epoch 6, LoraLayer 14: Decay factor: 0.7499062418937683\n",
      "\n",
      "Testing loss: 2.310473680496216, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 52s 31ms/step - loss: 0.0588 - accuracy: 0.9810 - val_loss: 2.3105 - val_accuracy: 0.1000\n",
      "Epoch 7/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.0673 - accuracy: 0.9778\n",
      "End of epoch 7, LoraLayer 0: 11669 Step\n",
      "End of epoch 7, LoraLayer 0: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 1: 11669 Step\n",
      "End of epoch 7, LoraLayer 1: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 2: 11669 Step\n",
      "End of epoch 7, LoraLayer 2: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 3: 11669 Step\n",
      "End of epoch 7, LoraLayer 3: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 4: 11669 Step\n",
      "End of epoch 7, LoraLayer 4: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 5: 11669 Step\n",
      "End of epoch 7, LoraLayer 5: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 6: 11669 Step\n",
      "End of epoch 7, LoraLayer 6: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 7: 11669 Step\n",
      "End of epoch 7, LoraLayer 7: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 8: 11669 Step\n",
      "End of epoch 7, LoraLayer 8: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 9: 11669 Step\n",
      "End of epoch 7, LoraLayer 9: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 10: 11669 Step\n",
      "End of epoch 7, LoraLayer 10: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 11: 11669 Step\n",
      "End of epoch 7, LoraLayer 11: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 12: 11669 Step\n",
      "End of epoch 7, LoraLayer 12: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 13: 11669 Step\n",
      "End of epoch 7, LoraLayer 13: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 14: 11669 Step\n",
      "End of epoch 7, LoraLayer 14: Decay factor: 0.6873922348022461\n",
      "\n",
      "Testing loss: 2.330169200897217, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 56s 33ms/step - loss: 0.0673 - accuracy: 0.9777 - val_loss: 2.3302 - val_accuracy: 0.1000\n",
      "Epoch 8/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.0815 - accuracy: 0.9727\n",
      "End of epoch 8, LoraLayer 0: 13336 Step\n",
      "End of epoch 8, LoraLayer 0: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 1: 13336 Step\n",
      "End of epoch 8, LoraLayer 1: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 2: 13336 Step\n",
      "End of epoch 8, LoraLayer 2: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 3: 13336 Step\n",
      "End of epoch 8, LoraLayer 3: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 4: 13336 Step\n",
      "End of epoch 8, LoraLayer 4: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 5: 13336 Step\n",
      "End of epoch 8, LoraLayer 5: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 6: 13336 Step\n",
      "End of epoch 8, LoraLayer 6: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 7: 13336 Step\n",
      "End of epoch 8, LoraLayer 7: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 8: 13336 Step\n",
      "End of epoch 8, LoraLayer 8: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 9: 13336 Step\n",
      "End of epoch 8, LoraLayer 9: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 10: 13336 Step\n",
      "End of epoch 8, LoraLayer 10: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 11: 13336 Step\n",
      "End of epoch 8, LoraLayer 11: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 12: 13336 Step\n",
      "End of epoch 8, LoraLayer 12: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 13: 13336 Step\n",
      "End of epoch 8, LoraLayer 13: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 14: 13336 Step\n",
      "End of epoch 8, LoraLayer 14: Decay factor: 0.6248781681060791\n",
      "\n",
      "Testing loss: 2.354937791824341, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 51s 31ms/step - loss: 0.0815 - accuracy: 0.9727 - val_loss: 2.3549 - val_accuracy: 0.1000\n",
      "Epoch 9/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.0952 - accuracy: 0.9679\n",
      "End of epoch 9, LoraLayer 0: 15003 Step\n",
      "End of epoch 9, LoraLayer 0: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 1: 15003 Step\n",
      "End of epoch 9, LoraLayer 1: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 2: 15003 Step\n",
      "End of epoch 9, LoraLayer 2: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 3: 15003 Step\n",
      "End of epoch 9, LoraLayer 3: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 4: 15003 Step\n",
      "End of epoch 9, LoraLayer 4: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 5: 15003 Step\n",
      "End of epoch 9, LoraLayer 5: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 6: 15003 Step\n",
      "End of epoch 9, LoraLayer 6: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 7: 15003 Step\n",
      "End of epoch 9, LoraLayer 7: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 8: 15003 Step\n",
      "End of epoch 9, LoraLayer 8: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 9: 15003 Step\n",
      "End of epoch 9, LoraLayer 9: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 10: 15003 Step\n",
      "End of epoch 9, LoraLayer 10: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 11: 15003 Step\n",
      "End of epoch 9, LoraLayer 11: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 12: 15003 Step\n",
      "End of epoch 9, LoraLayer 12: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 13: 15003 Step\n",
      "End of epoch 9, LoraLayer 13: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 14: 15003 Step\n",
      "End of epoch 9, LoraLayer 14: Decay factor: 0.5623641014099121\n",
      "\n",
      "Testing loss: 2.435476064682007, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 51s 31ms/step - loss: 0.0951 - accuracy: 0.9679 - val_loss: 2.4355 - val_accuracy: 0.1000\n",
      "Epoch 10/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.1172 - accuracy: 0.9603\n",
      "End of epoch 10, LoraLayer 0: 16670 Step\n",
      "End of epoch 10, LoraLayer 0: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 1: 16670 Step\n",
      "End of epoch 10, LoraLayer 1: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 2: 16670 Step\n",
      "End of epoch 10, LoraLayer 2: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 3: 16670 Step\n",
      "End of epoch 10, LoraLayer 3: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 4: 16670 Step\n",
      "End of epoch 10, LoraLayer 4: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 5: 16670 Step\n",
      "End of epoch 10, LoraLayer 5: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 6: 16670 Step\n",
      "End of epoch 10, LoraLayer 6: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 7: 16670 Step\n",
      "End of epoch 10, LoraLayer 7: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 8: 16670 Step\n",
      "End of epoch 10, LoraLayer 8: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 9: 16670 Step\n",
      "End of epoch 10, LoraLayer 9: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 10: 16670 Step\n",
      "End of epoch 10, LoraLayer 10: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 11: 16670 Step\n",
      "End of epoch 10, LoraLayer 11: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 12: 16670 Step\n",
      "End of epoch 10, LoraLayer 12: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 13: 16670 Step\n",
      "End of epoch 10, LoraLayer 13: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 14: 16670 Step\n",
      "End of epoch 10, LoraLayer 14: Decay factor: 0.4998500347137451\n",
      "\n",
      "Testing loss: 2.5238847732543945, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 56s 33ms/step - loss: 0.1173 - accuracy: 0.9603 - val_loss: 2.5239 - val_accuracy: 0.1000\n",
      "Epoch 11/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.1511 - accuracy: 0.9472\n",
      "End of epoch 11, LoraLayer 0: 18337 Step\n",
      "End of epoch 11, LoraLayer 0: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 1: 18337 Step\n",
      "End of epoch 11, LoraLayer 1: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 2: 18337 Step\n",
      "End of epoch 11, LoraLayer 2: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 3: 18337 Step\n",
      "End of epoch 11, LoraLayer 3: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 4: 18337 Step\n",
      "End of epoch 11, LoraLayer 4: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 5: 18337 Step\n",
      "End of epoch 11, LoraLayer 5: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 6: 18337 Step\n",
      "End of epoch 11, LoraLayer 6: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 7: 18337 Step\n",
      "End of epoch 11, LoraLayer 7: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 8: 18337 Step\n",
      "End of epoch 11, LoraLayer 8: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 9: 18337 Step\n",
      "End of epoch 11, LoraLayer 9: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 10: 18337 Step\n",
      "End of epoch 11, LoraLayer 10: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 11: 18337 Step\n",
      "End of epoch 11, LoraLayer 11: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 12: 18337 Step\n",
      "End of epoch 11, LoraLayer 12: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 13: 18337 Step\n",
      "End of epoch 11, LoraLayer 13: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 14: 18337 Step\n",
      "End of epoch 11, LoraLayer 14: Decay factor: 0.4373359680175781\n",
      "\n",
      "Testing loss: 2.5554497241973877, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 52s 31ms/step - loss: 0.1511 - accuracy: 0.9472 - val_loss: 2.5555 - val_accuracy: 0.1000\n",
      "Epoch 12/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.1776 - accuracy: 0.9388\n",
      "End of epoch 12, LoraLayer 0: 20004 Step\n",
      "End of epoch 12, LoraLayer 0: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 1: 20004 Step\n",
      "End of epoch 12, LoraLayer 1: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 2: 20004 Step\n",
      "End of epoch 12, LoraLayer 2: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 3: 20004 Step\n",
      "End of epoch 12, LoraLayer 3: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 4: 20004 Step\n",
      "End of epoch 12, LoraLayer 4: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 5: 20004 Step\n",
      "End of epoch 12, LoraLayer 5: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 6: 20004 Step\n",
      "End of epoch 12, LoraLayer 6: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 7: 20004 Step\n",
      "End of epoch 12, LoraLayer 7: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 8: 20004 Step\n",
      "End of epoch 12, LoraLayer 8: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 9: 20004 Step\n",
      "End of epoch 12, LoraLayer 9: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 10: 20004 Step\n",
      "End of epoch 12, LoraLayer 10: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 11: 20004 Step\n",
      "End of epoch 12, LoraLayer 11: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 12: 20004 Step\n",
      "End of epoch 12, LoraLayer 12: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 13: 20004 Step\n",
      "End of epoch 12, LoraLayer 13: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 14: 20004 Step\n",
      "End of epoch 12, LoraLayer 14: Decay factor: 0.37482190132141113\n",
      "\n",
      "Testing loss: 2.804800271987915, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 56s 33ms/step - loss: 0.1776 - accuracy: 0.9388 - val_loss: 2.8048 - val_accuracy: 0.1000\n",
      "Epoch 13/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.2185 - accuracy: 0.9241\n",
      "End of epoch 13, LoraLayer 0: 21671 Step\n",
      "End of epoch 13, LoraLayer 0: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 1: 21671 Step\n",
      "End of epoch 13, LoraLayer 1: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 2: 21671 Step\n",
      "End of epoch 13, LoraLayer 2: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 3: 21671 Step\n",
      "End of epoch 13, LoraLayer 3: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 4: 21671 Step\n",
      "End of epoch 13, LoraLayer 4: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 5: 21671 Step\n",
      "End of epoch 13, LoraLayer 5: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 6: 21671 Step\n",
      "End of epoch 13, LoraLayer 6: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 7: 21671 Step\n",
      "End of epoch 13, LoraLayer 7: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 8: 21671 Step\n",
      "End of epoch 13, LoraLayer 8: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 9: 21671 Step\n",
      "End of epoch 13, LoraLayer 9: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 10: 21671 Step\n",
      "End of epoch 13, LoraLayer 10: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 11: 21671 Step\n",
      "End of epoch 13, LoraLayer 11: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 12: 21671 Step\n",
      "End of epoch 13, LoraLayer 12: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 13: 21671 Step\n",
      "End of epoch 13, LoraLayer 13: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 14: 21671 Step\n",
      "End of epoch 13, LoraLayer 14: Decay factor: 0.31230783462524414\n",
      "\n",
      "Testing loss: 2.941934823989868, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 54s 32ms/step - loss: 0.2185 - accuracy: 0.9241 - val_loss: 2.9419 - val_accuracy: 0.1000\n",
      "Epoch 14/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.2594 - accuracy: 0.9098\n",
      "End of epoch 14, LoraLayer 0: 23338 Step\n",
      "End of epoch 14, LoraLayer 0: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 1: 23338 Step\n",
      "End of epoch 14, LoraLayer 1: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 2: 23338 Step\n",
      "End of epoch 14, LoraLayer 2: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 3: 23338 Step\n",
      "End of epoch 14, LoraLayer 3: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 4: 23338 Step\n",
      "End of epoch 14, LoraLayer 4: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 5: 23338 Step\n",
      "End of epoch 14, LoraLayer 5: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 6: 23338 Step\n",
      "End of epoch 14, LoraLayer 6: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 7: 23338 Step\n",
      "End of epoch 14, LoraLayer 7: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 8: 23338 Step\n",
      "End of epoch 14, LoraLayer 8: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 9: 23338 Step\n",
      "End of epoch 14, LoraLayer 9: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 10: 23338 Step\n",
      "End of epoch 14, LoraLayer 10: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 11: 23338 Step\n",
      "End of epoch 14, LoraLayer 11: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 12: 23338 Step\n",
      "End of epoch 14, LoraLayer 12: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 13: 23338 Step\n",
      "End of epoch 14, LoraLayer 13: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 14: 23338 Step\n",
      "End of epoch 14, LoraLayer 14: Decay factor: 0.24979376792907715\n",
      "\n",
      "Testing loss: 3.0685160160064697, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 56s 33ms/step - loss: 0.2594 - accuracy: 0.9098 - val_loss: 3.0685 - val_accuracy: 0.1000\n",
      "Epoch 15/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.3175 - accuracy: 0.8903\n",
      "End of epoch 15, LoraLayer 0: 25005 Step\n",
      "End of epoch 15, LoraLayer 0: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 1: 25005 Step\n",
      "End of epoch 15, LoraLayer 1: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 2: 25005 Step\n",
      "End of epoch 15, LoraLayer 2: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 3: 25005 Step\n",
      "End of epoch 15, LoraLayer 3: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 4: 25005 Step\n",
      "End of epoch 15, LoraLayer 4: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 5: 25005 Step\n",
      "End of epoch 15, LoraLayer 5: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 6: 25005 Step\n",
      "End of epoch 15, LoraLayer 6: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 7: 25005 Step\n",
      "End of epoch 15, LoraLayer 7: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 8: 25005 Step\n",
      "End of epoch 15, LoraLayer 8: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 9: 25005 Step\n",
      "End of epoch 15, LoraLayer 9: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 10: 25005 Step\n",
      "End of epoch 15, LoraLayer 10: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 11: 25005 Step\n",
      "End of epoch 15, LoraLayer 11: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 12: 25005 Step\n",
      "End of epoch 15, LoraLayer 12: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 13: 25005 Step\n",
      "End of epoch 15, LoraLayer 13: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 14: 25005 Step\n",
      "End of epoch 15, LoraLayer 14: Decay factor: 0.18727970123291016\n",
      "\n",
      "Testing loss: 4.13332986831665, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 54s 32ms/step - loss: 0.3175 - accuracy: 0.8903 - val_loss: 4.1333 - val_accuracy: 0.1000\n",
      "Epoch 16/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.3939 - accuracy: 0.8652\n",
      "End of epoch 16, LoraLayer 0: 26672 Step\n",
      "End of epoch 16, LoraLayer 0: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 1: 26672 Step\n",
      "End of epoch 16, LoraLayer 1: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 2: 26672 Step\n",
      "End of epoch 16, LoraLayer 2: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 3: 26672 Step\n",
      "End of epoch 16, LoraLayer 3: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 4: 26672 Step\n",
      "End of epoch 16, LoraLayer 4: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 5: 26672 Step\n",
      "End of epoch 16, LoraLayer 5: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 6: 26672 Step\n",
      "End of epoch 16, LoraLayer 6: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 7: 26672 Step\n",
      "End of epoch 16, LoraLayer 7: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 8: 26672 Step\n",
      "End of epoch 16, LoraLayer 8: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 9: 26672 Step\n",
      "End of epoch 16, LoraLayer 9: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 10: 26672 Step\n",
      "End of epoch 16, LoraLayer 10: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 11: 26672 Step\n",
      "End of epoch 16, LoraLayer 11: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 12: 26672 Step\n",
      "End of epoch 16, LoraLayer 12: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 13: 26672 Step\n",
      "End of epoch 16, LoraLayer 13: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 14: 26672 Step\n",
      "End of epoch 16, LoraLayer 14: Decay factor: 0.12476563453674316\n",
      "\n",
      "Testing loss: 4.3215837478637695, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 52s 31ms/step - loss: 0.3939 - accuracy: 0.8652 - val_loss: 4.3216 - val_accuracy: 0.1000\n",
      "Epoch 17/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.5027 - accuracy: 0.8270\n",
      "End of epoch 17, LoraLayer 0: 28339 Step\n",
      "End of epoch 17, LoraLayer 0: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 1: 28339 Step\n",
      "End of epoch 17, LoraLayer 1: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 2: 28339 Step\n",
      "End of epoch 17, LoraLayer 2: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 3: 28339 Step\n",
      "End of epoch 17, LoraLayer 3: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 4: 28339 Step\n",
      "End of epoch 17, LoraLayer 4: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 5: 28339 Step\n",
      "End of epoch 17, LoraLayer 5: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 6: 28339 Step\n",
      "End of epoch 17, LoraLayer 6: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 7: 28339 Step\n",
      "End of epoch 17, LoraLayer 7: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 8: 28339 Step\n",
      "End of epoch 17, LoraLayer 8: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 9: 28339 Step\n",
      "End of epoch 17, LoraLayer 9: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 10: 28339 Step\n",
      "End of epoch 17, LoraLayer 10: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 11: 28339 Step\n",
      "End of epoch 17, LoraLayer 11: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 12: 28339 Step\n",
      "End of epoch 17, LoraLayer 12: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 13: 28339 Step\n",
      "End of epoch 17, LoraLayer 13: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 14: 28339 Step\n",
      "End of epoch 17, LoraLayer 14: Decay factor: 0.06225156784057617\n",
      "\n",
      "Testing loss: 2.7417359352111816, acc: 0.11270000040531158\n",
      "\n",
      "1667/1667 [==============================] - 58s 35ms/step - loss: 0.5027 - accuracy: 0.8270 - val_loss: 2.7418 - val_accuracy: 0.1128\n",
      "Epoch 18/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.6254 - accuracy: 0.7863\n",
      "End of epoch 18, LoraLayer 0: 30006 Step\n",
      "End of epoch 18, LoraLayer 0: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 1: 30006 Step\n",
      "End of epoch 18, LoraLayer 1: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 2: 30006 Step\n",
      "End of epoch 18, LoraLayer 2: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 3: 30006 Step\n",
      "End of epoch 18, LoraLayer 3: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 4: 30006 Step\n",
      "End of epoch 18, LoraLayer 4: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 5: 30006 Step\n",
      "End of epoch 18, LoraLayer 5: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 6: 30006 Step\n",
      "End of epoch 18, LoraLayer 6: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 7: 30006 Step\n",
      "End of epoch 18, LoraLayer 7: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 8: 30006 Step\n",
      "End of epoch 18, LoraLayer 8: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 9: 30006 Step\n",
      "End of epoch 18, LoraLayer 9: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 10: 30006 Step\n",
      "End of epoch 18, LoraLayer 10: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 11: 30006 Step\n",
      "End of epoch 18, LoraLayer 11: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 12: 30006 Step\n",
      "End of epoch 18, LoraLayer 12: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 13: 30006 Step\n",
      "End of epoch 18, LoraLayer 13: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 14: 30006 Step\n",
      "End of epoch 18, LoraLayer 14: Decay factor: 0.0\n",
      "\n",
      "Testing loss: 0.8609888553619385, acc: 0.7135000228881836\n",
      "\n",
      "1667/1667 [==============================] - 51s 31ms/step - loss: 0.6255 - accuracy: 0.7863 - val_loss: 0.8610 - val_accuracy: 0.7135\n",
      "Epoch 19/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.5920 - accuracy: 0.7979\n",
      "End of epoch 19, LoraLayer 0: 31673 Step\n",
      "End of epoch 19, LoraLayer 0: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 1: 31673 Step\n",
      "End of epoch 19, LoraLayer 1: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 2: 31673 Step\n",
      "End of epoch 19, LoraLayer 2: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 3: 31673 Step\n",
      "End of epoch 19, LoraLayer 3: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 4: 31673 Step\n",
      "End of epoch 19, LoraLayer 4: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 5: 31673 Step\n",
      "End of epoch 19, LoraLayer 5: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 6: 31673 Step\n",
      "End of epoch 19, LoraLayer 6: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 7: 31673 Step\n",
      "End of epoch 19, LoraLayer 7: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 8: 31673 Step\n",
      "End of epoch 19, LoraLayer 8: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 9: 31673 Step\n",
      "End of epoch 19, LoraLayer 9: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 10: 31673 Step\n",
      "End of epoch 19, LoraLayer 10: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 11: 31673 Step\n",
      "End of epoch 19, LoraLayer 11: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 12: 31673 Step\n",
      "End of epoch 19, LoraLayer 12: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 13: 31673 Step\n",
      "End of epoch 19, LoraLayer 13: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 14: 31673 Step\n",
      "End of epoch 19, LoraLayer 14: Decay factor: 0.0\n",
      "\n",
      "Testing loss: 0.694469153881073, acc: 0.7656999826431274\n",
      "\n",
      "1667/1667 [==============================] - 58s 35ms/step - loss: 0.5920 - accuracy: 0.7979 - val_loss: 0.6945 - val_accuracy: 0.7657\n",
      "Epoch 20/20\n",
      "1665/1667 [============================>.] - ETA: 0s - loss: 0.4993 - accuracy: 0.8285\n",
      "End of epoch 20, LoraLayer 0: 33340 Step\n",
      "End of epoch 20, LoraLayer 0: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 1: 33340 Step\n",
      "End of epoch 20, LoraLayer 1: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 2: 33340 Step\n",
      "End of epoch 20, LoraLayer 2: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 3: 33340 Step\n",
      "End of epoch 20, LoraLayer 3: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 4: 33340 Step\n",
      "End of epoch 20, LoraLayer 4: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 5: 33340 Step\n",
      "End of epoch 20, LoraLayer 5: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 6: 33340 Step\n",
      "End of epoch 20, LoraLayer 6: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 7: 33340 Step\n",
      "End of epoch 20, LoraLayer 7: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 8: 33340 Step\n",
      "End of epoch 20, LoraLayer 8: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 9: 33340 Step\n",
      "End of epoch 20, LoraLayer 9: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 10: 33340 Step\n",
      "End of epoch 20, LoraLayer 10: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 11: 33340 Step\n",
      "End of epoch 20, LoraLayer 11: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 12: 33340 Step\n",
      "End of epoch 20, LoraLayer 12: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 13: 33340 Step\n",
      "End of epoch 20, LoraLayer 13: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 14: 33340 Step\n",
      "End of epoch 20, LoraLayer 14: Decay factor: 0.0\n",
      "\n",
      "Testing loss: 0.7479420900344849, acc: 0.7608000040054321\n",
      "\n",
      "1667/1667 [==============================] - 50s 30ms/step - loss: 0.4993 - accuracy: 0.8285 - val_loss: 0.7479 - val_accuracy: 0.7609\n"
     ]
    }
   ],
   "source": [
    "history_exp22 = exp22_lora_vgg16.fit(x_train, y_train, batch_size=30, epochs=20, validation_data=(x_test, y_test), callbacks=[print_step_callback, TestCallback((x_test, y_test))] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3tq5Lt1mf8vz",
    "outputId": "8e66bab1-efb5-452c-fa46-30f411411960"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 3s 9ms/step - loss: 0.7479 - accuracy: 0.7608\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7479420900344849, 0.7608000040054321]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 평가\n",
    "exp22_lora_vgg16.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "id": "NYKZmdk1fPCA",
    "outputId": "51e2bd34-3a2c-4ec9-9272-e19b5769da30"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 학습 및 검증 정확도 시각화\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# 정확도 그래프\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_exp22.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history_exp22.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# 손실 그래프\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_exp22.history['loss'], label='Training Loss')\n",
    "plt.plot(history_exp22.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n5MbMZ4hehMi"
   },
   "source": [
    "## 2-3. (32, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "8WLRVDyWej1q"
   },
   "outputs": [],
   "source": [
    "dataset_size = x_train.shape[0]\n",
    "batch_size = 30\n",
    "epochs = 20\n",
    "\n",
    "total_iteration= int(epochs * (dataset_size / batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "1fICgSqQqwIW"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout, InputLayer\n",
    "import tensorflow as tf\n",
    "\n",
    "# 새로운 레이어를 추가하기 위한 레이어 리스트 초기화\n",
    "new_layers = []\n",
    "\n",
    "# Dense 레이어 카운터\n",
    "dense_layer_count = 0\n",
    "\n",
    "# VGG 모델의 각 레이어 순회\n",
    "for layer in best_vgg16.layers:\n",
    "    if isinstance(layer, InputLayer):\n",
    "        continue\n",
    "    if isinstance(layer, Conv2D):\n",
    "        # Conv2D 레이어를 ConvLoRALayer로 대체\n",
    "        conv_lora_layer = ConvLoRALayer00_cdn2(original_conv_layer=layer, rank=32, alpha=32, total_iteration = total_iteration, trainable=True)\n",
    "        new_layers.append(conv_lora_layer)\n",
    "    elif isinstance(layer, Dense):\n",
    "        dense_layer_count += 1\n",
    "        # 첫 번째와 두 번째 Dense 레이어에만 LoraLayer 적용\n",
    "        if dense_layer_count in [1, 2]:\n",
    "            dense_lora_layer = LoraLayer(original_layer=layer, rank=64, alpha=32, total_iteration=total_iteration, trainable=True)\n",
    "            new_layers.append(dense_lora_layer)\n",
    "        else:\n",
    "            new_layers.append(layer)  # 그대로 유지함\n",
    "    else:\n",
    "        # 다른 유형의 레이어는 그대로 유지\n",
    "        new_layers.append(layer)\n",
    "\n",
    "# 새로운 입력 텐서 생성\n",
    "input_tensor = best_vgg16.input\n",
    "\n",
    "# 새로운 모델 구성\n",
    "x = input_tensor\n",
    "for layer in new_layers:\n",
    "    x = layer(x)\n",
    "\n",
    "# 새로운 모델 생성\n",
    "exp23_lora_vgg16 = Model(inputs=input_tensor, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "rDp5dkpgqx7G"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D\n",
    "\n",
    "# 새로 생성된 모델에서 Dense 레이어 중 LoRA가 적용되지 않은 레이어의 trainable을 False로 설정\n",
    "for layer in exp23_lora_vgg16.layers:\n",
    "    if isinstance(layer, Dense) and not isinstance(layer, LoraLayer):\n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DrXZDLTRq4LV",
    "outputId": "65255648-33de-4b3f-ac2a-744afa3bfe9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " block1_conv1 (ConvLoRALaye  (None, 32, 32, 64)        21154     \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block1_conv2 (ConvLoRALaye  (None, 32, 32, 64)        73858     \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " block2_conv1 (ConvLoRALaye  (None, 16, 16, 128)       129282    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block2_conv2 (ConvLoRALaye  (None, 16, 16, 128)       221442    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " block3_conv1 (ConvLoRALaye  (None, 8, 8, 256)         406018    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block3_conv2 (ConvLoRALaye  (None, 8, 8, 256)         737794    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block3_conv3 (ConvLoRALaye  (None, 8, 8, 256)         737794    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 4, 4, 256)         0         \n",
      "                                                                 \n",
      " block4_conv1 (ConvLoRALaye  (None, 4, 4, 512)         1401858   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block4_conv2 (ConvLoRALaye  (None, 4, 4, 512)         2655234   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block4_conv3 (ConvLoRALaye  (None, 4, 4, 512)         2655234   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      " block5_conv1 (ConvLoRALaye  (None, 2, 2, 512)         2655234   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block5_conv2 (ConvLoRALaye  (None, 2, 2, 512)         2655234   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block5_conv3 (ConvLoRALaye  (None, 2, 2, 512)         2655234   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 1, 1, 512)         0         \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_6 (LoraLayer)         (None, 4096)              2400258   \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_7 (LoraLayer)         (None, 512)               2393090   \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21803848 (83.18 MB)\n",
      "Trainable params: 2885088 (11.01 MB)\n",
      "Non-trainable params: 18918760 (72.17 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "exp23_lora_vgg16.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7BISzZtRq14v",
    "outputId": "33c7d8e4-b132-4cd0-8d0e-66e9495e7579"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: input_3\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block1_conv1\n",
      "  Trainable parameters: 19360\n",
      "  Non-trainable parameters: 1794\n",
      "Layer: block1_conv2\n",
      "  Trainable parameters: 36928\n",
      "  Non-trainable parameters: 36930\n",
      "Layer: block1_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block2_conv1\n",
      "  Trainable parameters: 55424\n",
      "  Non-trainable parameters: 73858\n",
      "Layer: block2_conv2\n",
      "  Trainable parameters: 73856\n",
      "  Non-trainable parameters: 147586\n",
      "Layer: block2_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block3_conv1\n",
      "  Trainable parameters: 110848\n",
      "  Non-trainable parameters: 295170\n",
      "Layer: block3_conv2\n",
      "  Trainable parameters: 147712\n",
      "  Non-trainable parameters: 590082\n",
      "Layer: block3_conv3\n",
      "  Trainable parameters: 147712\n",
      "  Non-trainable parameters: 590082\n",
      "Layer: block3_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block4_conv1\n",
      "  Trainable parameters: 221696\n",
      "  Non-trainable parameters: 1180162\n",
      "Layer: block4_conv2\n",
      "  Trainable parameters: 295424\n",
      "  Non-trainable parameters: 2359810\n",
      "Layer: block4_conv3\n",
      "  Trainable parameters: 295424\n",
      "  Non-trainable parameters: 2359810\n",
      "Layer: block4_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block5_conv1\n",
      "  Trainable parameters: 295424\n",
      "  Non-trainable parameters: 2359810\n",
      "Layer: block5_conv2\n",
      "  Trainable parameters: 295424\n",
      "  Non-trainable parameters: 2359810\n",
      "Layer: block5_conv3\n",
      "  Trainable parameters: 295424\n",
      "  Non-trainable parameters: 2359810\n",
      "Layer: block5_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: flatten_2\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: dense_6\n",
      "  Trainable parameters: 299008\n",
      "  Non-trainable parameters: 2101250\n",
      "Layer: dropout_4\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: dense_7\n",
      "  Trainable parameters: 295424\n",
      "  Non-trainable parameters: 2097666\n",
      "Layer: dropout_5\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: dense_8\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 5130\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#모델의 각 레이어를 순회하며 파라미터 수를 계산\n",
    "for layer in exp23_lora_vgg16.layers:\n",
    "    trainable_count = np.sum([tf.size(w).numpy() for w in layer.trainable_weights])\n",
    "    non_trainable_count = np.sum([tf.size(w).numpy() for w in layer.non_trainable_weights])\n",
    "\n",
    "    print(f\"Layer: {layer.name}\")\n",
    "    print(f\"  Trainable parameters: {trainable_count}\")\n",
    "    print(f\"  Non-trainable parameters: {non_trainable_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "_jH8Ng1iq8s9"
   },
   "outputs": [],
   "source": [
    "lora_layers = []  # lora 레이어들을 저장할 리스트\n",
    "\n",
    "for layer in exp23_lora_vgg16.layers:\n",
    "    if isinstance(layer, LoraLayer) or isinstance(layer, ConvLoRALayer00_cdn2):\n",
    "        lora_layers.append(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "6M9Ijmuhq_ql"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "class PrintCurrentStepCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, lora_layers):\n",
    "        super().__init__()\n",
    "        self.lora_layers = lora_layers\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print()\n",
    "        for i, lora_layer in enumerate(self.lora_layers):\n",
    "            current_step = lora_layer.current_step.value()\n",
    "            decay_factor = lora_layer.decay_factor.value()\n",
    "            print(f\"End of epoch {epoch + 1}, LoraLayer {i}: {current_step} Step\")\n",
    "            print(f\"End of epoch {epoch + 1}, LoraLayer {i}: Decay factor: {decay_factor}\")\n",
    "\n",
    "class TestCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, test_data):\n",
    "        self.test_data = test_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        x, y = self.test_data\n",
    "        loss, acc = self.model.evaluate(x, y, verbose=0)\n",
    "        print('\\nTesting loss: {}, acc: {}\\n'.format(loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "CdXjUGAUrCRa"
   },
   "outputs": [],
   "source": [
    "# 콜백 생성 시 lora_layers 딕셔너리의 값만 사용\n",
    "print_step_callback = PrintCurrentStepCallback(lora_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "ZMc-zQIQrEyM"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# 모델 컴파일\n",
    "exp23_lora_vgg16.compile(optimizer=Adam(learning_rate=0.00005),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dkn_4j2Y_B46"
   },
   "source": [
    "### 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "5Xzwh-2nrHaz",
    "outputId": "e31b4a46-a8c9-49d8-de53-23945d1097ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.0811 - accuracy: 0.9741\n",
      "End of epoch 1, LoraLayer 0: 1667 Step\n",
      "End of epoch 1, LoraLayer 0: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 1: 1667 Step\n",
      "End of epoch 1, LoraLayer 1: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 2: 1667 Step\n",
      "End of epoch 1, LoraLayer 2: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 3: 1667 Step\n",
      "End of epoch 1, LoraLayer 3: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 4: 1667 Step\n",
      "End of epoch 1, LoraLayer 4: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 5: 1667 Step\n",
      "End of epoch 1, LoraLayer 5: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 6: 1667 Step\n",
      "End of epoch 1, LoraLayer 6: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 7: 1667 Step\n",
      "End of epoch 1, LoraLayer 7: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 8: 1667 Step\n",
      "End of epoch 1, LoraLayer 8: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 9: 1667 Step\n",
      "End of epoch 1, LoraLayer 9: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 10: 1667 Step\n",
      "End of epoch 1, LoraLayer 10: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 11: 1667 Step\n",
      "End of epoch 1, LoraLayer 11: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 12: 1667 Step\n",
      "End of epoch 1, LoraLayer 12: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 13: 1667 Step\n",
      "End of epoch 1, LoraLayer 13: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 14: 1667 Step\n",
      "End of epoch 1, LoraLayer 14: Decay factor: 1.0\n",
      "\n",
      "Testing loss: 2.302624225616455, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 62s 31ms/step - loss: 0.0811 - accuracy: 0.9741 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 2/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.0553 - accuracy: 0.9824\n",
      "End of epoch 2, LoraLayer 0: 3334 Step\n",
      "End of epoch 2, LoraLayer 0: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 1: 3334 Step\n",
      "End of epoch 2, LoraLayer 1: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 2: 3334 Step\n",
      "End of epoch 2, LoraLayer 2: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 3: 3334 Step\n",
      "End of epoch 2, LoraLayer 3: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 4: 3334 Step\n",
      "End of epoch 2, LoraLayer 4: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 5: 3334 Step\n",
      "End of epoch 2, LoraLayer 5: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 6: 3334 Step\n",
      "End of epoch 2, LoraLayer 6: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 7: 3334 Step\n",
      "End of epoch 2, LoraLayer 7: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 8: 3334 Step\n",
      "End of epoch 2, LoraLayer 8: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 9: 3334 Step\n",
      "End of epoch 2, LoraLayer 9: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 10: 3334 Step\n",
      "End of epoch 2, LoraLayer 10: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 11: 3334 Step\n",
      "End of epoch 2, LoraLayer 11: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 12: 3334 Step\n",
      "End of epoch 2, LoraLayer 12: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 13: 3334 Step\n",
      "End of epoch 2, LoraLayer 13: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 14: 3334 Step\n",
      "End of epoch 2, LoraLayer 14: Decay factor: 0.9999625086784363\n",
      "\n",
      "Testing loss: 2.302638292312622, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 51s 31ms/step - loss: 0.0553 - accuracy: 0.9824 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 3/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.0496 - accuracy: 0.9839\n",
      "End of epoch 3, LoraLayer 0: 5001 Step\n",
      "End of epoch 3, LoraLayer 0: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 1: 5001 Step\n",
      "End of epoch 3, LoraLayer 1: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 2: 5001 Step\n",
      "End of epoch 3, LoraLayer 2: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 3: 5001 Step\n",
      "End of epoch 3, LoraLayer 3: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 4: 5001 Step\n",
      "End of epoch 3, LoraLayer 4: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 5: 5001 Step\n",
      "End of epoch 3, LoraLayer 5: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 6: 5001 Step\n",
      "End of epoch 3, LoraLayer 6: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 7: 5001 Step\n",
      "End of epoch 3, LoraLayer 7: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 8: 5001 Step\n",
      "End of epoch 3, LoraLayer 8: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 9: 5001 Step\n",
      "End of epoch 3, LoraLayer 9: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 10: 5001 Step\n",
      "End of epoch 3, LoraLayer 10: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 11: 5001 Step\n",
      "End of epoch 3, LoraLayer 11: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 12: 5001 Step\n",
      "End of epoch 3, LoraLayer 12: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 13: 5001 Step\n",
      "End of epoch 3, LoraLayer 13: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 14: 5001 Step\n",
      "End of epoch 3, LoraLayer 14: Decay factor: 0.9374484419822693\n",
      "\n",
      "Testing loss: 2.3026914596557617, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 51s 30ms/step - loss: 0.0496 - accuracy: 0.9839 - val_loss: 2.3027 - val_accuracy: 0.1000\n",
      "Epoch 4/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.0471 - accuracy: 0.9849\n",
      "End of epoch 4, LoraLayer 0: 6668 Step\n",
      "End of epoch 4, LoraLayer 0: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 1: 6668 Step\n",
      "End of epoch 4, LoraLayer 1: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 2: 6668 Step\n",
      "End of epoch 4, LoraLayer 2: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 3: 6668 Step\n",
      "End of epoch 4, LoraLayer 3: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 4: 6668 Step\n",
      "End of epoch 4, LoraLayer 4: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 5: 6668 Step\n",
      "End of epoch 4, LoraLayer 5: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 6: 6668 Step\n",
      "End of epoch 4, LoraLayer 6: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 7: 6668 Step\n",
      "End of epoch 4, LoraLayer 7: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 8: 6668 Step\n",
      "End of epoch 4, LoraLayer 8: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 9: 6668 Step\n",
      "End of epoch 4, LoraLayer 9: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 10: 6668 Step\n",
      "End of epoch 4, LoraLayer 10: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 11: 6668 Step\n",
      "End of epoch 4, LoraLayer 11: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 12: 6668 Step\n",
      "End of epoch 4, LoraLayer 12: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 13: 6668 Step\n",
      "End of epoch 4, LoraLayer 13: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 14: 6668 Step\n",
      "End of epoch 4, LoraLayer 14: Decay factor: 0.8749343752861023\n",
      "\n",
      "Testing loss: 2.302703857421875, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 57s 34ms/step - loss: 0.0471 - accuracy: 0.9849 - val_loss: 2.3027 - val_accuracy: 0.1000\n",
      "Epoch 5/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.0464 - accuracy: 0.9843\n",
      "End of epoch 5, LoraLayer 0: 8335 Step\n",
      "End of epoch 5, LoraLayer 0: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 1: 8335 Step\n",
      "End of epoch 5, LoraLayer 1: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 2: 8335 Step\n",
      "End of epoch 5, LoraLayer 2: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 3: 8335 Step\n",
      "End of epoch 5, LoraLayer 3: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 4: 8335 Step\n",
      "End of epoch 5, LoraLayer 4: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 5: 8335 Step\n",
      "End of epoch 5, LoraLayer 5: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 6: 8335 Step\n",
      "End of epoch 5, LoraLayer 6: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 7: 8335 Step\n",
      "End of epoch 5, LoraLayer 7: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 8: 8335 Step\n",
      "End of epoch 5, LoraLayer 8: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 9: 8335 Step\n",
      "End of epoch 5, LoraLayer 9: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 10: 8335 Step\n",
      "End of epoch 5, LoraLayer 10: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 11: 8335 Step\n",
      "End of epoch 5, LoraLayer 11: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 12: 8335 Step\n",
      "End of epoch 5, LoraLayer 12: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 13: 8335 Step\n",
      "End of epoch 5, LoraLayer 13: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 14: 8335 Step\n",
      "End of epoch 5, LoraLayer 14: Decay factor: 0.8124203085899353\n",
      "\n",
      "Testing loss: 2.3030574321746826, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 51s 30ms/step - loss: 0.0464 - accuracy: 0.9843 - val_loss: 2.3031 - val_accuracy: 0.1000\n",
      "Epoch 6/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.0535 - accuracy: 0.9821\n",
      "End of epoch 6, LoraLayer 0: 10002 Step\n",
      "End of epoch 6, LoraLayer 0: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 1: 10002 Step\n",
      "End of epoch 6, LoraLayer 1: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 2: 10002 Step\n",
      "End of epoch 6, LoraLayer 2: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 3: 10002 Step\n",
      "End of epoch 6, LoraLayer 3: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 4: 10002 Step\n",
      "End of epoch 6, LoraLayer 4: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 5: 10002 Step\n",
      "End of epoch 6, LoraLayer 5: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 6: 10002 Step\n",
      "End of epoch 6, LoraLayer 6: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 7: 10002 Step\n",
      "End of epoch 6, LoraLayer 7: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 8: 10002 Step\n",
      "End of epoch 6, LoraLayer 8: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 9: 10002 Step\n",
      "End of epoch 6, LoraLayer 9: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 10: 10002 Step\n",
      "End of epoch 6, LoraLayer 10: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 11: 10002 Step\n",
      "End of epoch 6, LoraLayer 11: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 12: 10002 Step\n",
      "End of epoch 6, LoraLayer 12: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 13: 10002 Step\n",
      "End of epoch 6, LoraLayer 13: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 14: 10002 Step\n",
      "End of epoch 6, LoraLayer 14: Decay factor: 0.7499062418937683\n",
      "\n",
      "Testing loss: 2.303922176361084, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 52s 31ms/step - loss: 0.0535 - accuracy: 0.9821 - val_loss: 2.3039 - val_accuracy: 0.1000\n",
      "Epoch 7/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.0602 - accuracy: 0.9797\n",
      "End of epoch 7, LoraLayer 0: 11669 Step\n",
      "End of epoch 7, LoraLayer 0: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 1: 11669 Step\n",
      "End of epoch 7, LoraLayer 1: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 2: 11669 Step\n",
      "End of epoch 7, LoraLayer 2: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 3: 11669 Step\n",
      "End of epoch 7, LoraLayer 3: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 4: 11669 Step\n",
      "End of epoch 7, LoraLayer 4: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 5: 11669 Step\n",
      "End of epoch 7, LoraLayer 5: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 6: 11669 Step\n",
      "End of epoch 7, LoraLayer 6: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 7: 11669 Step\n",
      "End of epoch 7, LoraLayer 7: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 8: 11669 Step\n",
      "End of epoch 7, LoraLayer 8: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 9: 11669 Step\n",
      "End of epoch 7, LoraLayer 9: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 10: 11669 Step\n",
      "End of epoch 7, LoraLayer 10: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 11: 11669 Step\n",
      "End of epoch 7, LoraLayer 11: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 12: 11669 Step\n",
      "End of epoch 7, LoraLayer 12: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 13: 11669 Step\n",
      "End of epoch 7, LoraLayer 13: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 14: 11669 Step\n",
      "End of epoch 7, LoraLayer 14: Decay factor: 0.6873922348022461\n",
      "\n",
      "Testing loss: 2.307950973510742, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 56s 33ms/step - loss: 0.0602 - accuracy: 0.9797 - val_loss: 2.3080 - val_accuracy: 0.1000\n",
      "Epoch 8/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.0687 - accuracy: 0.9768\n",
      "End of epoch 8, LoraLayer 0: 13336 Step\n",
      "End of epoch 8, LoraLayer 0: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 1: 13336 Step\n",
      "End of epoch 8, LoraLayer 1: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 2: 13336 Step\n",
      "End of epoch 8, LoraLayer 2: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 3: 13336 Step\n",
      "End of epoch 8, LoraLayer 3: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 4: 13336 Step\n",
      "End of epoch 8, LoraLayer 4: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 5: 13336 Step\n",
      "End of epoch 8, LoraLayer 5: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 6: 13336 Step\n",
      "End of epoch 8, LoraLayer 6: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 7: 13336 Step\n",
      "End of epoch 8, LoraLayer 7: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 8: 13336 Step\n",
      "End of epoch 8, LoraLayer 8: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 9: 13336 Step\n",
      "End of epoch 8, LoraLayer 9: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 10: 13336 Step\n",
      "End of epoch 8, LoraLayer 10: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 11: 13336 Step\n",
      "End of epoch 8, LoraLayer 11: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 12: 13336 Step\n",
      "End of epoch 8, LoraLayer 12: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 13: 13336 Step\n",
      "End of epoch 8, LoraLayer 13: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 14: 13336 Step\n",
      "End of epoch 8, LoraLayer 14: Decay factor: 0.6248781681060791\n",
      "\n",
      "Testing loss: 2.3161075115203857, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 52s 31ms/step - loss: 0.0688 - accuracy: 0.9768 - val_loss: 2.3161 - val_accuracy: 0.1000\n",
      "Epoch 9/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.0832 - accuracy: 0.9719\n",
      "End of epoch 9, LoraLayer 0: 15003 Step\n",
      "End of epoch 9, LoraLayer 0: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 1: 15003 Step\n",
      "End of epoch 9, LoraLayer 1: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 2: 15003 Step\n",
      "End of epoch 9, LoraLayer 2: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 3: 15003 Step\n",
      "End of epoch 9, LoraLayer 3: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 4: 15003 Step\n",
      "End of epoch 9, LoraLayer 4: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 5: 15003 Step\n",
      "End of epoch 9, LoraLayer 5: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 6: 15003 Step\n",
      "End of epoch 9, LoraLayer 6: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 7: 15003 Step\n",
      "End of epoch 9, LoraLayer 7: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 8: 15003 Step\n",
      "End of epoch 9, LoraLayer 8: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 9: 15003 Step\n",
      "End of epoch 9, LoraLayer 9: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 10: 15003 Step\n",
      "End of epoch 9, LoraLayer 10: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 11: 15003 Step\n",
      "End of epoch 9, LoraLayer 11: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 12: 15003 Step\n",
      "End of epoch 9, LoraLayer 12: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 13: 15003 Step\n",
      "End of epoch 9, LoraLayer 13: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 14: 15003 Step\n",
      "End of epoch 9, LoraLayer 14: Decay factor: 0.5623641014099121\n",
      "\n",
      "Testing loss: 2.3261663913726807, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 50s 30ms/step - loss: 0.0832 - accuracy: 0.9719 - val_loss: 2.3262 - val_accuracy: 0.1000\n",
      "Epoch 10/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.1055 - accuracy: 0.9635\n",
      "End of epoch 10, LoraLayer 0: 16670 Step\n",
      "End of epoch 10, LoraLayer 0: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 1: 16670 Step\n",
      "End of epoch 10, LoraLayer 1: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 2: 16670 Step\n",
      "End of epoch 10, LoraLayer 2: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 3: 16670 Step\n",
      "End of epoch 10, LoraLayer 3: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 4: 16670 Step\n",
      "End of epoch 10, LoraLayer 4: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 5: 16670 Step\n",
      "End of epoch 10, LoraLayer 5: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 6: 16670 Step\n",
      "End of epoch 10, LoraLayer 6: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 7: 16670 Step\n",
      "End of epoch 10, LoraLayer 7: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 8: 16670 Step\n",
      "End of epoch 10, LoraLayer 8: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 9: 16670 Step\n",
      "End of epoch 10, LoraLayer 9: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 10: 16670 Step\n",
      "End of epoch 10, LoraLayer 10: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 11: 16670 Step\n",
      "End of epoch 10, LoraLayer 11: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 12: 16670 Step\n",
      "End of epoch 10, LoraLayer 12: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 13: 16670 Step\n",
      "End of epoch 10, LoraLayer 13: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 14: 16670 Step\n",
      "End of epoch 10, LoraLayer 14: Decay factor: 0.4998500347137451\n",
      "\n",
      "Testing loss: 2.361013174057007, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 50s 30ms/step - loss: 0.1056 - accuracy: 0.9635 - val_loss: 2.3610 - val_accuracy: 0.1000\n",
      "Epoch 11/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.1314 - accuracy: 0.9529\n",
      "End of epoch 11, LoraLayer 0: 18337 Step\n",
      "End of epoch 11, LoraLayer 0: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 1: 18337 Step\n",
      "End of epoch 11, LoraLayer 1: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 2: 18337 Step\n",
      "End of epoch 11, LoraLayer 2: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 3: 18337 Step\n",
      "End of epoch 11, LoraLayer 3: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 4: 18337 Step\n",
      "End of epoch 11, LoraLayer 4: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 5: 18337 Step\n",
      "End of epoch 11, LoraLayer 5: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 6: 18337 Step\n",
      "End of epoch 11, LoraLayer 6: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 7: 18337 Step\n",
      "End of epoch 11, LoraLayer 7: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 8: 18337 Step\n",
      "End of epoch 11, LoraLayer 8: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 9: 18337 Step\n",
      "End of epoch 11, LoraLayer 9: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 10: 18337 Step\n",
      "End of epoch 11, LoraLayer 10: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 11: 18337 Step\n",
      "End of epoch 11, LoraLayer 11: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 12: 18337 Step\n",
      "End of epoch 11, LoraLayer 12: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 13: 18337 Step\n",
      "End of epoch 11, LoraLayer 13: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 14: 18337 Step\n",
      "End of epoch 11, LoraLayer 14: Decay factor: 0.4373359680175781\n",
      "\n",
      "Testing loss: 2.429898738861084, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 50s 30ms/step - loss: 0.1314 - accuracy: 0.9529 - val_loss: 2.4299 - val_accuracy: 0.1000\n",
      "Epoch 12/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.1630 - accuracy: 0.9424\n",
      "End of epoch 12, LoraLayer 0: 20004 Step\n",
      "End of epoch 12, LoraLayer 0: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 1: 20004 Step\n",
      "End of epoch 12, LoraLayer 1: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 2: 20004 Step\n",
      "End of epoch 12, LoraLayer 2: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 3: 20004 Step\n",
      "End of epoch 12, LoraLayer 3: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 4: 20004 Step\n",
      "End of epoch 12, LoraLayer 4: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 5: 20004 Step\n",
      "End of epoch 12, LoraLayer 5: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 6: 20004 Step\n",
      "End of epoch 12, LoraLayer 6: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 7: 20004 Step\n",
      "End of epoch 12, LoraLayer 7: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 8: 20004 Step\n",
      "End of epoch 12, LoraLayer 8: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 9: 20004 Step\n",
      "End of epoch 12, LoraLayer 9: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 10: 20004 Step\n",
      "End of epoch 12, LoraLayer 10: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 11: 20004 Step\n",
      "End of epoch 12, LoraLayer 11: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 12: 20004 Step\n",
      "End of epoch 12, LoraLayer 12: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 13: 20004 Step\n",
      "End of epoch 12, LoraLayer 13: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 14: 20004 Step\n",
      "End of epoch 12, LoraLayer 14: Decay factor: 0.37482190132141113\n",
      "\n",
      "Testing loss: 2.512956142425537, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 50s 30ms/step - loss: 0.1630 - accuracy: 0.9424 - val_loss: 2.5130 - val_accuracy: 0.1000\n",
      "Epoch 13/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.1947 - accuracy: 0.9323\n",
      "End of epoch 13, LoraLayer 0: 21671 Step\n",
      "End of epoch 13, LoraLayer 0: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 1: 21671 Step\n",
      "End of epoch 13, LoraLayer 1: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 2: 21671 Step\n",
      "End of epoch 13, LoraLayer 2: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 3: 21671 Step\n",
      "End of epoch 13, LoraLayer 3: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 4: 21671 Step\n",
      "End of epoch 13, LoraLayer 4: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 5: 21671 Step\n",
      "End of epoch 13, LoraLayer 5: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 6: 21671 Step\n",
      "End of epoch 13, LoraLayer 6: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 7: 21671 Step\n",
      "End of epoch 13, LoraLayer 7: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 8: 21671 Step\n",
      "End of epoch 13, LoraLayer 8: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 9: 21671 Step\n",
      "End of epoch 13, LoraLayer 9: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 10: 21671 Step\n",
      "End of epoch 13, LoraLayer 10: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 11: 21671 Step\n",
      "End of epoch 13, LoraLayer 11: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 12: 21671 Step\n",
      "End of epoch 13, LoraLayer 12: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 13: 21671 Step\n",
      "End of epoch 13, LoraLayer 13: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 14: 21671 Step\n",
      "End of epoch 13, LoraLayer 14: Decay factor: 0.31230783462524414\n",
      "\n",
      "Testing loss: 2.7391209602355957, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 49s 29ms/step - loss: 0.1947 - accuracy: 0.9323 - val_loss: 2.7391 - val_accuracy: 0.1000\n",
      "Epoch 14/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.2365 - accuracy: 0.9174\n",
      "End of epoch 14, LoraLayer 0: 23338 Step\n",
      "End of epoch 14, LoraLayer 0: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 1: 23338 Step\n",
      "End of epoch 14, LoraLayer 1: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 2: 23338 Step\n",
      "End of epoch 14, LoraLayer 2: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 3: 23338 Step\n",
      "End of epoch 14, LoraLayer 3: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 4: 23338 Step\n",
      "End of epoch 14, LoraLayer 4: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 5: 23338 Step\n",
      "End of epoch 14, LoraLayer 5: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 6: 23338 Step\n",
      "End of epoch 14, LoraLayer 6: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 7: 23338 Step\n",
      "End of epoch 14, LoraLayer 7: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 8: 23338 Step\n",
      "End of epoch 14, LoraLayer 8: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 9: 23338 Step\n",
      "End of epoch 14, LoraLayer 9: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 10: 23338 Step\n",
      "End of epoch 14, LoraLayer 10: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 11: 23338 Step\n",
      "End of epoch 14, LoraLayer 11: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 12: 23338 Step\n",
      "End of epoch 14, LoraLayer 12: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 13: 23338 Step\n",
      "End of epoch 14, LoraLayer 13: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 14: 23338 Step\n",
      "End of epoch 14, LoraLayer 14: Decay factor: 0.24979376792907715\n",
      "\n",
      "Testing loss: 3.2032084465026855, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 50s 30ms/step - loss: 0.2365 - accuracy: 0.9174 - val_loss: 3.2032 - val_accuracy: 0.1000\n",
      "Epoch 15/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.3017 - accuracy: 0.8940\n",
      "End of epoch 15, LoraLayer 0: 25005 Step\n",
      "End of epoch 15, LoraLayer 0: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 1: 25005 Step\n",
      "End of epoch 15, LoraLayer 1: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 2: 25005 Step\n",
      "End of epoch 15, LoraLayer 2: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 3: 25005 Step\n",
      "End of epoch 15, LoraLayer 3: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 4: 25005 Step\n",
      "End of epoch 15, LoraLayer 4: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 5: 25005 Step\n",
      "End of epoch 15, LoraLayer 5: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 6: 25005 Step\n",
      "End of epoch 15, LoraLayer 6: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 7: 25005 Step\n",
      "End of epoch 15, LoraLayer 7: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 8: 25005 Step\n",
      "End of epoch 15, LoraLayer 8: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 9: 25005 Step\n",
      "End of epoch 15, LoraLayer 9: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 10: 25005 Step\n",
      "End of epoch 15, LoraLayer 10: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 11: 25005 Step\n",
      "End of epoch 15, LoraLayer 11: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 12: 25005 Step\n",
      "End of epoch 15, LoraLayer 12: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 13: 25005 Step\n",
      "End of epoch 15, LoraLayer 13: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 14: 25005 Step\n",
      "End of epoch 15, LoraLayer 14: Decay factor: 0.18727970123291016\n",
      "\n",
      "Testing loss: 3.563525676727295, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 50s 30ms/step - loss: 0.3017 - accuracy: 0.8940 - val_loss: 3.5635 - val_accuracy: 0.1000\n",
      "Epoch 16/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.3738 - accuracy: 0.8690\n",
      "End of epoch 16, LoraLayer 0: 26672 Step\n",
      "End of epoch 16, LoraLayer 0: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 1: 26672 Step\n",
      "End of epoch 16, LoraLayer 1: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 2: 26672 Step\n",
      "End of epoch 16, LoraLayer 2: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 3: 26672 Step\n",
      "End of epoch 16, LoraLayer 3: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 4: 26672 Step\n",
      "End of epoch 16, LoraLayer 4: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 5: 26672 Step\n",
      "End of epoch 16, LoraLayer 5: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 6: 26672 Step\n",
      "End of epoch 16, LoraLayer 6: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 7: 26672 Step\n",
      "End of epoch 16, LoraLayer 7: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 8: 26672 Step\n",
      "End of epoch 16, LoraLayer 8: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 9: 26672 Step\n",
      "End of epoch 16, LoraLayer 9: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 10: 26672 Step\n",
      "End of epoch 16, LoraLayer 10: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 11: 26672 Step\n",
      "End of epoch 16, LoraLayer 11: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 12: 26672 Step\n",
      "End of epoch 16, LoraLayer 12: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 13: 26672 Step\n",
      "End of epoch 16, LoraLayer 13: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 14: 26672 Step\n",
      "End of epoch 16, LoraLayer 14: Decay factor: 0.12476563453674316\n",
      "\n",
      "Testing loss: 3.5453507900238037, acc: 0.10189999639987946\n",
      "\n",
      "1667/1667 [==============================] - 50s 30ms/step - loss: 0.3738 - accuracy: 0.8690 - val_loss: 3.5453 - val_accuracy: 0.1018\n",
      "Epoch 17/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.4758 - accuracy: 0.8351\n",
      "End of epoch 17, LoraLayer 0: 28339 Step\n",
      "End of epoch 17, LoraLayer 0: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 1: 28339 Step\n",
      "End of epoch 17, LoraLayer 1: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 2: 28339 Step\n",
      "End of epoch 17, LoraLayer 2: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 3: 28339 Step\n",
      "End of epoch 17, LoraLayer 3: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 4: 28339 Step\n",
      "End of epoch 17, LoraLayer 4: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 5: 28339 Step\n",
      "End of epoch 17, LoraLayer 5: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 6: 28339 Step\n",
      "End of epoch 17, LoraLayer 6: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 7: 28339 Step\n",
      "End of epoch 17, LoraLayer 7: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 8: 28339 Step\n",
      "End of epoch 17, LoraLayer 8: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 9: 28339 Step\n",
      "End of epoch 17, LoraLayer 9: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 10: 28339 Step\n",
      "End of epoch 17, LoraLayer 10: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 11: 28339 Step\n",
      "End of epoch 17, LoraLayer 11: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 12: 28339 Step\n",
      "End of epoch 17, LoraLayer 12: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 13: 28339 Step\n",
      "End of epoch 17, LoraLayer 13: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 14: 28339 Step\n",
      "End of epoch 17, LoraLayer 14: Decay factor: 0.06225156784057617\n",
      "\n",
      "Testing loss: 2.7948544025421143, acc: 0.12610000371932983\n",
      "\n",
      "1667/1667 [==============================] - 50s 30ms/step - loss: 0.4757 - accuracy: 0.8350 - val_loss: 2.7949 - val_accuracy: 0.1260\n",
      "Epoch 18/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.6058 - accuracy: 0.7932\n",
      "End of epoch 18, LoraLayer 0: 30006 Step\n",
      "End of epoch 18, LoraLayer 0: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 1: 30006 Step\n",
      "End of epoch 18, LoraLayer 1: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 2: 30006 Step\n",
      "End of epoch 18, LoraLayer 2: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 3: 30006 Step\n",
      "End of epoch 18, LoraLayer 3: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 4: 30006 Step\n",
      "End of epoch 18, LoraLayer 4: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 5: 30006 Step\n",
      "End of epoch 18, LoraLayer 5: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 6: 30006 Step\n",
      "End of epoch 18, LoraLayer 6: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 7: 30006 Step\n",
      "End of epoch 18, LoraLayer 7: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 8: 30006 Step\n",
      "End of epoch 18, LoraLayer 8: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 9: 30006 Step\n",
      "End of epoch 18, LoraLayer 9: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 10: 30006 Step\n",
      "End of epoch 18, LoraLayer 10: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 11: 30006 Step\n",
      "End of epoch 18, LoraLayer 11: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 12: 30006 Step\n",
      "End of epoch 18, LoraLayer 12: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 13: 30006 Step\n",
      "End of epoch 18, LoraLayer 13: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 14: 30006 Step\n",
      "End of epoch 18, LoraLayer 14: Decay factor: 0.0\n",
      "\n",
      "Testing loss: 0.7442606687545776, acc: 0.746999979019165\n",
      "\n",
      "1667/1667 [==============================] - 50s 30ms/step - loss: 0.6058 - accuracy: 0.7932 - val_loss: 0.7443 - val_accuracy: 0.7468\n",
      "Epoch 19/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.5709 - accuracy: 0.8047\n",
      "End of epoch 19, LoraLayer 0: 31673 Step\n",
      "End of epoch 19, LoraLayer 0: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 1: 31673 Step\n",
      "End of epoch 19, LoraLayer 1: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 2: 31673 Step\n",
      "End of epoch 19, LoraLayer 2: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 3: 31673 Step\n",
      "End of epoch 19, LoraLayer 3: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 4: 31673 Step\n",
      "End of epoch 19, LoraLayer 4: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 5: 31673 Step\n",
      "End of epoch 19, LoraLayer 5: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 6: 31673 Step\n",
      "End of epoch 19, LoraLayer 6: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 7: 31673 Step\n",
      "End of epoch 19, LoraLayer 7: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 8: 31673 Step\n",
      "End of epoch 19, LoraLayer 8: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 9: 31673 Step\n",
      "End of epoch 19, LoraLayer 9: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 10: 31673 Step\n",
      "End of epoch 19, LoraLayer 10: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 11: 31673 Step\n",
      "End of epoch 19, LoraLayer 11: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 12: 31673 Step\n",
      "End of epoch 19, LoraLayer 12: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 13: 31673 Step\n",
      "End of epoch 19, LoraLayer 13: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 14: 31673 Step\n",
      "End of epoch 19, LoraLayer 14: Decay factor: 0.0\n",
      "\n",
      "Testing loss: 0.7031186819076538, acc: 0.7645000219345093\n",
      "\n",
      "1667/1667 [==============================] - 51s 31ms/step - loss: 0.5708 - accuracy: 0.8048 - val_loss: 0.7031 - val_accuracy: 0.7644\n",
      "Epoch 20/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.4690 - accuracy: 0.8395\n",
      "End of epoch 20, LoraLayer 0: 33340 Step\n",
      "End of epoch 20, LoraLayer 0: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 1: 33340 Step\n",
      "End of epoch 20, LoraLayer 1: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 2: 33340 Step\n",
      "End of epoch 20, LoraLayer 2: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 3: 33340 Step\n",
      "End of epoch 20, LoraLayer 3: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 4: 33340 Step\n",
      "End of epoch 20, LoraLayer 4: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 5: 33340 Step\n",
      "End of epoch 20, LoraLayer 5: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 6: 33340 Step\n",
      "End of epoch 20, LoraLayer 6: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 7: 33340 Step\n",
      "End of epoch 20, LoraLayer 7: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 8: 33340 Step\n",
      "End of epoch 20, LoraLayer 8: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 9: 33340 Step\n",
      "End of epoch 20, LoraLayer 9: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 10: 33340 Step\n",
      "End of epoch 20, LoraLayer 10: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 11: 33340 Step\n",
      "End of epoch 20, LoraLayer 11: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 12: 33340 Step\n",
      "End of epoch 20, LoraLayer 12: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 13: 33340 Step\n",
      "End of epoch 20, LoraLayer 13: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 14: 33340 Step\n",
      "End of epoch 20, LoraLayer 14: Decay factor: 0.0\n",
      "\n",
      "Testing loss: 0.6960694789886475, acc: 0.7763000130653381\n",
      "\n",
      "1667/1667 [==============================] - 53s 32ms/step - loss: 0.4690 - accuracy: 0.8395 - val_loss: 0.6961 - val_accuracy: 0.7763\n"
     ]
    }
   ],
   "source": [
    "history_exp23 = exp23_lora_vgg16.fit(x_train, y_train, batch_size=30, epochs=20, validation_data=(x_test, y_test), callbacks=[print_step_callback, TestCallback((x_test, y_test))] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "ndAMXGdSgAmG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 4s 12ms/step - loss: 0.6961 - accuracy: 0.7763\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6960694789886475, 0.7763000130653381]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 평가\n",
    "exp23_lora_vgg16.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "dEnYK6elrKvV"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA+kAAAHWCAYAAAALjsguAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAACzeUlEQVR4nOzdd3hT5fvH8Xe6d1ltKVBWKVA2lCEoU5AlslREpgwXoIgo4kBABRVUBPyBgyEKgiCgflFZAsqQIYJs2bNQZid0pOf3R2igtJQW2iZtP6/rypWTJ885504ayLnzLJNhGAYiIiIiIiIiYnMOtg5ARERERERERCyUpIuIiIiIiIjYCSXpIiIiIiIiInZCSbqIiIiIiIiInVCSLiIiIiIiImInlKSLiIiIiIiI2Akl6SIiIiIiIiJ2Qkm6iIiIiIiIiJ1Qki4iIiIiIiJiJ5Ski13p27cvZcuWvat9R48ejclkyt6A7MyxY8cwmUzMnj07189tMpkYPXq09fHs2bMxmUwcO3bsjvuWLVuWvn37Zms89/JZERGR/EHXDRnTdcMNum6QvERJumSKyWTK1G3t2rW2DrXAe+GFFzCZTBw6dOi2dd544w1MJhP//vtvLkaWdWfOnGH06NHs2LHD1qGka9++fZhMJtzc3Lhy5YqtwxERsRu6bsg7dN2Qs1J+KJk4caKtQ5E8xMnWAUje8M0336R6PGfOHFauXJmmPDQ09J7O8+WXX5KcnHxX+7755pu89tpr93T+/KBHjx5MmTKFefPmMWrUqHTrfPfdd1SvXp0aNWrc9Xl69erFE088gaur610f407OnDnDmDFjKFu2LLVq1Ur13L18VrLLt99+S/Hixbl8+TKLFi1iwIABNo1HRMRe6Loh79B1g4j9UZIumdKzZ89Uj//66y9WrlyZpvxWcXFxeHh4ZPo8zs7OdxUfgJOTE05O+kg3aNCAChUq8N1336X7Zbtp0yaOHj3K+++/f0/ncXR0xNHR8Z6OcS/u5bOSHQzDYN68eTz55JMcPXqUuXPn2m2SHhsbi6enp63DEJECRNcNeYeuG0Tsj7q7S7Zp1qwZ1apV4++//6ZJkyZ4eHjw+uuvA/Djjz/Svn17SpQogaurK8HBwbzzzjuYzeZUx7h1vNDNXYS++OILgoODcXV1pV69emzdujXVvumNLTOZTAwePJilS5dSrVo1XF1dqVq1Kr/99lua+NeuXUvdunVxc3MjODiYzz//PNPj1f78808ee+wxSpcujaurK0FBQbz00ktcvXo1zevz8vLi9OnTdOrUCS8vL/z8/Bg+fHia9+LKlSv07dsXX19fChUqRJ8+fTLdpbpHjx7s37+f7du3p3lu3rx5mEwmunfvTkJCAqNGjSIsLAxfX188PT1p3Lgxa9asueM50htbZhgG7777LqVKlcLDw4PmzZuzZ8+eNPteunSJ4cOHU716dby8vPDx8aFt27bs3LnTWmft2rXUq1cPgKeeesraNTJlXF16Y8tiY2N5+eWXCQoKwtXVlUqVKjFx4kQMw0hVLyufi9vZsGEDx44d44knnuCJJ57gjz/+4NSpU2nqJScn8+mnn1K9enXc3Nzw8/OjTZs2bNu2LVW9b7/9lvr16+Ph4UHhwoVp0qQJK1asSBXzzWP7Utw6bi/l77Ju3Tqef/55/P39KVWqFADHjx/n+eefp1KlSri7u1O0aFEee+yxdMcHXrlyhZdeeomyZcvi6upKqVKl6N27NxcuXCAmJgZPT09efPHFNPudOnUKR0dHxo8fn8l3UkQKKl036LqhIF033ElERAT9+/cnICAANzc3atasyddff52m3vz58wkLC8Pb2xsfHx+qV6/Op59+an0+MTGRMWPGEBISgpubG0WLFuWBBx5g5cqV2Rar5Dz9fCjZ6uLFi7Rt25YnnniCnj17EhAQAFj+Y/by8mLYsGF4eXnx+++/M2rUKKKiopgwYcIdjztv3jyio6N55plnMJlMfPjhh3Tp0oUjR47c8ZfR9evXs3jxYp5//nm8vb2ZPHkyXbt25cSJExQtWhSAf/75hzZt2hAYGMiYMWMwm82MHTsWPz+/TL3uhQsXEhcXx3PPPUfRokXZsmULU6ZM4dSpUyxcuDBVXbPZTOvWrWnQoAETJ05k1apVfPTRRwQHB/Pcc88Bli+tjh07sn79ep599llCQ0NZsmQJffr0yVQ8PXr0YMyYMcybN486deqkOvf3339P48aNKV26NBcuXOCrr76ie/fuDBw4kOjoaGbMmEHr1q3ZsmVLmq5idzJq1Cjeffdd2rVrR7t27di+fTsPPfQQCQkJqeodOXKEpUuX8thjj1GuXDnOnTvH559/TtOmTdm7dy8lSpQgNDSUsWPHMmrUKJ5++mkaN24MQKNGjdI9t2EYPPLII6xZs4b+/ftTq1Ytli9fziuvvMLp06f55JNPUtXPzOciI3PnziU4OJh69epRrVo1PDw8+O6773jllVdS1evfvz+zZ8+mbdu2DBgwgKSkJP7880/++usv6tatC8CYMWMYPXo0jRo1YuzYsbi4uLB582Z+//13HnrooUy//zd7/vnn8fPzY9SoUcTGxgKwdetWNm7cyBNPPEGpUqU4duwY06ZNo1mzZuzdu9faehUTE0Pjxo3Zt28f/fr1o06dOly4cIGffvqJU6dOUatWLTp37syCBQv4+OOPU7WMfPfddxiGQY8ePe4qbhEpWHTdoOuGgnLdkJGrV6/SrFkzDh06xODBgylXrhwLFy6kb9++XLlyxfqj+MqVK+nevTsPPvggH3zwAWCZH2fDhg3WOqNHj2b8+PEMGDCA+vXrExUVxbZt29i+fTutWrW6pzglFxkid2HQoEHGrR+fpk2bGoAxffr0NPXj4uLSlD3zzDOGh4eHce3aNWtZnz59jDJlylgfHz161ACMokWLGpcuXbKW//jjjwZg/Pzzz9ayt99+O01MgOHi4mIcOnTIWrZz504DMKZMmWIt69Chg+Hh4WGcPn3aWnbw4EHDyckpzTHTk97rGz9+vGEymYzjx4+nen2AMXbs2FR1a9eubYSFhVkfL1261ACMDz/80FqWlJRkNG7c2ACMWbNm3TGmevXqGaVKlTLMZrO17LfffjMA4/PPP7ceMz4+PtV+ly9fNgICAox+/fqlKgeMt99+2/p41qxZBmAcPXrUMAzDiIiIMFxcXIz27dsbycnJ1nqvv/66ARh9+vSxll27di1VXIZh+Vu7urqmem+2bt1629d762cl5T179913U9V79NFHDZPJlOozkNnPxe0kJCQYRYsWNd544w1r2ZNPPmnUrFkzVb3ff//dAIwXXnghzTFS3qODBw8aDg4ORufOndO8Jze/j7e+/ynKlCmT6r1N+bs88MADRlJSUqq66X1ON23aZADGnDlzrGWjRo0yAGPx4sW3jXv58uUGYPz666+pnq9Ro4bRtGnTNPuJSMGm64Y7vz5dN1jkt+uGlM/khAkTbltn0qRJBmB8++231rKEhASjYcOGhpeXlxEVFWUYhmG8+OKLho+PT5rv95vVrFnTaN++fYYxif1Td3fJVq6urjz11FNpyt3d3a3b0dHRXLhwgcaNGxMXF8f+/fvveNxu3bpRuHBh6+OUX0ePHDlyx31btmxJcHCw9XGNGjXw8fGx7ms2m1m1ahWdOnWiRIkS1noVKlSgbdu2dzw+pH59sbGxXLhwgUaNGmEYBv/880+a+s8++2yqx40bN071Wn755RecnJysv5CDZSzXkCFDMhUPWMYDnjp1ij/++MNaNm/ePFxcXHjsscesx3RxcQEs3bIvXbpEUlISdevWTbfLW0ZWrVpFQkICQ4YMSdXVb+jQoWnqurq64uBg+e/HbDZz8eJFvLy8qFSpUpbPm+KXX37B0dGRF154IVX5yy+/jGEY/Prrr6nK7/S5yMivv/7KxYsX6d69u7Wse/fu7Ny5M1U3vR9++AGTycTbb7+d5hgp79HSpUtJTk5m1KhR1vfk1jp3Y+DAgWnG/t38OU1MTOTixYtUqFCBQoUKpXrff/jhB2rWrEnnzp1vG3fLli0pUaIEc+fOtT63e/du/v333zuOORURSaHrBl03FITrhszEUrx48VTXFc7OzrzwwgvExMSwbt06AAoVKkRsbGyGXdcLFSrEnj17OHjw4D3HJbajJF2yVcmSJa3/ed9sz549dO7cGV9fX3x8fPDz87NeyEdGRt7xuKVLl071OOWL9/Lly1neN2X/lH0jIiK4evUqFSpUSFMvvbL0nDhxgr59+1KkSBHreLGmTZsCaV9fyrjk28UDlrHDgYGBeHl5papXqVKlTMUD8MQTT+Do6Mi8efMAuHbtGkuWLKFt27apLly+/vpratSoYR235Ofnx7JlyzL1d7nZ8ePHAQgJCUlV7ufnl+p8YPli/+STTwgJCcHV1ZVixYrh5+fHv//+m+Xz3nz+EiVK4O3tnao8ZebglPhS3OlzkZFvv/2WcuXK4erqyqFDhzh06BDBwcF4eHikSloPHz5MiRIlKFKkyG2PdfjwYRwcHKhSpcodz5sV5cqVS1N29epVRo0aZR17l/K+X7lyJdX7fvjwYapVq5bh8R0cHOjRowdLly4lLi4OsAwBcHNzs17MiYjcia4bdN1QEK4bMhNLSEhImh/rb43l+eefp2LFirRt25ZSpUrRr1+/NOPix44dy5UrV6hYsSLVq1fnlVdesful8yQtJemSrW7+ZTjFlStXaNq0KTt37mTs2LH8/PPPrFy50jqWJjPLYdxuNlDjlok9snvfzDCbzbRq1Yply5YxYsQIli5dysqVK60Tldz6+nJrZlN/f39atWrFDz/8QGJiIj///DPR0dGpxgp/++239O3bl+DgYGbMmMFvv/3GypUradGiRY4uUzJu3DiGDRtGkyZN+Pbbb1m+fDkrV66katWqubY8yt1+LqKiovj55585evQoISEh1luVKlWIi4tj3rx52fbZyoxbJw5Kkd6/xSFDhvDee+/x+OOP8/3337NixQpWrlxJ0aJF7+p97927NzExMSxdutQ62/3DDz+Mr69vlo8lIgWTrht03ZAZefm6ITv5+/uzY8cOfvrpJ+t4+rZt26aae6BJkyYcPnyYmTNnUq1aNb766ivq1KnDV199lWtxyr3TxHGS49auXcvFixdZvHgxTZo0sZYfPXrUhlHd4O/vj5ubG4cOHUrzXHplt9q1axf//fcfX3/9Nb1797aW38ssmmXKlGH16tXExMSk+lX8wIEDWTpOjx49+O233/j111+ZN28ePj4+dOjQwfr8okWLKF++PIsXL07V1Sy97tmZiRng4MGDlC9f3lp+/vz5NL8yL1q0iObNmzNjxoxU5VeuXKFYsWLWx1np7l2mTBlWrVpFdHR0ql/FU7pFpsR3rxYvXsy1a9eYNm1aqljB8vd588032bBhAw888ADBwcEsX76cS5cu3bY1PTg4mOTkZPbu3ZvhhDuFCxdOM0tvQkIC4eHhmY590aJF9OnTh48++shadu3atTTHDQ4OZvfu3Xc8XrVq1ahduzZz586lVKlSnDhxgilTpmQ6HhGR9Oi6Iet03WBhj9cNmY3l33//JTk5OVVrenqxuLi40KFDBzp06EBycjLPP/88n3/+OW+99Za1J0eRIkV46qmneOqpp4iJiaFJkyaMHj3abpeKlbTUki45LuWXx5t/aUxISOD//u//bBVSKo6OjrRs2ZKlS5dy5swZa/mhQ4fSjEe63f6Q+vUZhpFqOYysateuHUlJSUybNs1aZjabs5wAderUCQ8PD/7v//6PX3/9lS5duuDm5pZh7Js3b2bTpk1Zjrlly5Y4OzszZcqUVMebNGlSmrqOjo5pfnleuHAhp0+fTlWWsrZ3ZpaQadeuHWazmalTp6Yq/+STTzCZTJkeJ3gn3377LeXLl+fZZ5/l0UcfTXUbPnw4Xl5e1i7vXbt2xTAMxowZk+Y4Ka+/U6dOODg4MHbs2DStATe/R8HBwanGCQJ88cUXt21JT0967/uUKVPSHKNr167s3LmTJUuW3DbuFL169WLFihVMmjSJokWLZtv7LCIFl64bsk7XDRb2eN2QGe3atePs2bMsWLDAWpaUlMSUKVPw8vKyDoW4ePFiqv0cHByoUaMGAPHx8enW8fLyokKFCtbnJW9QS7rkuEaNGlG4cGH69OnDCy+8gMlk4ptvvsnV7kF3Mnr0aFasWMH999/Pc889Z/1Pu1q1auzYsSPDfStXrkxwcDDDhw/n9OnT+Pj48MMPP9zTGKUOHTpw//3389prr3Hs2DGqVKnC4sWLszzuysvLi06dOlnHl926LNbDDz/M4sWL6dy5M+3bt+fo0aNMnz6dKlWqEBMTk6VzpazbOn78eB5++GHatWvHP//8w6+//pqmxfnhhx9m7NixPPXUUzRq1Ihdu3Yxd+7cVL+kgyUxLVSoENOnT8fb2xtPT08aNGiQ7njrDh060Lx5c9544w2OHTtGzZo1WbFiBT/++CNDhw5NNdnL3Tpz5gxr1qxJM8lMCldXV1q3bs3ChQuZPHkyzZs3p1evXkyePJmDBw/Spk0bkpOT+fPPP2nevDmDBw+mQoUKvPHGG7zzzjs0btyYLl264OrqytatWylRooR1vfEBAwbw7LPP0rVrV1q1asXOnTtZvnx5mvc2Iw8//DDffPMNvr6+VKlShU2bNrFq1ao0S8e88sorLFq0iMcee4x+/foRFhbGpUuX+Omnn5g+fTo1a9a01n3yySd59dVXWbJkCc8999wdlzYSEbkTXTdkna4bLOztuuFmq1ev5tq1a2nKO3XqxNNPP83nn39O3759+fvvvylbtiyLFi1iw4YNTJo0ydrSP2DAAC5dukSLFi0oVaoUx48fZ8qUKdSqVcs6fr1KlSo0a9aMsLAwihQpwrZt21i0aBGDBw/O1tcjOSwXZpCXfOh2S6lUrVo13fobNmww7rvvPsPd3d0oUaKE8eqrr1qXcFqzZo213u2WUklv2QpuWdrjdkupDBo0KM2+ty5bZRiGsXr1aqN27dqGi4uLERwcbHz11VfGyy+/bLi5ud3mXbhh7969RsuWLQ0vLy+jWLFixsCBA61Lc9y8DEifPn0MT0/PNPunF/vFixeNXr16GT4+Poavr6/Rq1cv459//sn0Uiopli1bZgBGYGBgukt8jRs3zihTpozh6upq1K5d2/jf//6X5u9gGHdeSsUwDMNsNhtjxowxAgMDDXd3d6NZs2bG7t2707zf165dM15++WVrvfvvv9/YtGmT0bRp0zTLd/34449GlSpVrMvapLz29GKMjo42XnrpJaNEiRKGs7OzERISYkyYMCHV0i4pryWzn4ubffTRRwZgrF69+rZ1Zs+ebQDGjz/+aBiGZbmaCRMmGJUrVzZcXFwMPz8/o23btsbff/+dar+ZM2catWvXNlxdXY3ChQsbTZs2NVauXGl93mw2GyNGjDCKFStmeHh4GK1btzYOHTp02yXYtm7dmia2y5cvG0899ZRRrFgxw8vLy2jdurWxf//+dF/3xYsXjcGDBxslS5Y0XFxcjFKlShl9+vQxLly4kOa47dq1MwBj48aNt31fRKRg03VDarpusMjv1w2GceMzebvbN998YxiGYZw7d876He3i4mJUr149zd9t0aJFxkMPPWT4+/sbLi4uRunSpY1nnnnGCA8Pt9Z59913jfr16xuFChUy3N3djcqVKxvvvfeekZCQkGGcYl9MhmFHP0uK2JlOnTppGQuRO+jcuTO7du3K1FhMEZH8TNcNIpIdNCZd5LqrV6+menzw4EF++eUXmjVrZpuARPKA8PBwli1bRq9evWwdiohIrtJ1g4jkFLWki1wXGBhI3759KV++PMePH2fatGnEx8fzzz//pFnDU6SgO3r0KBs2bOCrr75i69atHD58mOLFi9s6LBGRXKPrBhHJKZo4TuS6Nm3a8N1333H27FlcXV1p2LAh48aN0xetSDrWrVvHU089RenSpfn666+VoItIgaPrBhHJKWpJFxEREREREbETGpMuIiIiIiIiYieUpIuIiIiIiIjYiQI3Jj05OZkzZ87g7e2NyWSydTgiIiIYhkF0dDQlSpTAwUG/n2cHfd+LiIg9ycp3fYFL0s+cOUNQUJCtwxAREUnj5MmTlCpVytZh5Av6vhcREXuUme/6Apeke3t7A5Y3x8fHx8bRiIiIQFRUFEFBQdbvKLl3+r4XERF7kpXv+gKXpKd0efPx8dGXtoiI2BV1y84++r4XERF7lJnveg18ExEREREREbETStJFRERERERE7ISSdBERERERERE7UeDGpIuIiIgYhkFSUhJms9nWoUg+4+joiJOTk+aYEJG7ZtMk/Y8//mDChAn8/fffhIeHs2TJEjp16pThPmvXrmXYsGHs2bOHoKAg3nzzTfr27Zsr8YqIiEjel5CQQHh4OHFxcbYORfIpDw8PAgMDcXFxsXUoIpIH2TRJj42NpWbNmvTr148uXbrcsf7Ro0dp3749zz77LHPnzmX16tUMGDCAwMBAWrdunQsRi4iISF6WnJzM0aNHcXR0pESJEri4uKjFU7KNYRgkJCRw/vx5jh49SkhICA4OGl0qIllj0yS9bdu2tG3bNtP1p0+fTrly5fjoo48ACA0NZf369XzyySdK0kVEROSOEhISSE5OJigoCA8PD1uHI/mQu7s7zs7OHD9+nISEBNzc3GwdkojkMXnqp71NmzbRsmXLVGWtW7dm06ZNt90nPj6eqKioVDcREREp2NS6KTlJny8RuRd56n+Qs2fPEhAQkKosICCAqKgorl69mu4+48ePx9fX13oLCgrKjVBFREREREREsixPJel3Y+TIkURGRlpvJ0+etHVIIiIiIiIiIunKU0l68eLFOXfuXKqyc+fO4ePjg7u7e7r7uLq64uPjk+omIiIiIlC2bFkmTZqU6fpr167FZDJx5cqVHItJRKSgy1NJesOGDVm9enWqspUrV9KwYUMbRSQiIiKS80wmU4a30aNH39Vxt27dytNPP53p+o0aNSI8PBxfX9+7Ol9m6ccAESnIbDq7e0xMDIcOHbI+Pnr0KDt27KBIkSKULl2akSNHcvr0aebMmQPAs88+y9SpU3n11Vfp168fv//+O99//z3Lli2z1UsQERERyXHh4eHW7QULFjBq1CgOHDhgLfPy8rJuG4aB2WzGyenOl3l+fn5ZisPFxYXixYtnaR8REckamybp27Zto3nz5tbHw4YNA6BPnz7Mnj2b8PBwTpw4YX2+XLlyLFu2jJdeeolPP/2UUqVK8dVXX+X75dcMwyDRbJBgTiY+0UyCOZmEJMst/votISk5zfMp5YnmZBLNBknmZBKTLfdJyQZJZoOk5BvPJSUbJJqTU5ened7AMAwcTCYcHUw4OJhwNIGjg+WXfMd0ylPXNVnLHK4/7+Rowt3ZEXcXJ9ydHfFwccTdxXLv4eKIm7MjHi5OlnLnlHIn3JwdtLatiIjcM8MwuJpotsm53Z0dM/VddnNi7Ovri8lkspatXbuW5s2b88svv/Dmm2+ya9cuVqxYQVBQEMOGDeOvv/4iNjaW0NBQxo8fn2qlnLJlyzJ06FCGDh0KWFrsv/zyS5YtW8by5cspWbIkH330EY888kiqc12+fJlChQoxe/Zshg4dyoIFCxg6dCgnT57kgQceYNasWQQGBgKQlJTEsGHDmDNnDo6OjgwYMICzZ88SGRnJ0qVL7+p9u3z5Mi+++CI///wz8fHxNG3alMmTJxMSEgLA8ePHGTx4MOvXrychIYGyZcsyYcIE2rVrx+XLlxk8eDArVqwgJiaGUqVK8frrr/PUU0/dVSwi2cKcCD8OgktHwNMfvPyu3/uDZ7Gbtv3AzRd0DZyv2TRJb9asGYZh3Pb52bNnp7vPP//8k4NR5Z5riWaOnI/lYEQ0hyNiOBgRw5HzsURfS7Qk3Dcl2pK+W5N6S7LviJerEx4uTni6Ol6/d8LTxREPVye8UsquP+/pavkBIGUfF6c8NQpERETu0dVEM1VGLbfJufeObY2HS/Zcjr322mtMnDiR8uXLU7hwYU6ePEm7du147733cHV1Zc6cOXTo0IEDBw5QunTp2x5nzJgxfPjhh0yYMIEpU6bQo0cPjh8/TpEiRdKtHxcXx8SJE/nmm29wcHCgZ8+eDB8+nLlz5wLwwQcfMHfuXGbNmkVoaCiffvopS5cuTdVQk1V9+/bl4MGD/PTTT/j4+DBixAjatWvH3r17cXZ2ZtCgQSQkJPDHH3/g6enJ3r17rb0N3nrrLfbu3cuvv/5KsWLFOHTo0G1XCRLJNdtmwr8LMlfX0dWSrFsTeT/L45sTeS9/y2P3wqAlAfMcmybpBUVMfJI1CT8UEcOhiGgORsRw8lIcybf/jeK2nB1NuDg64OJkubk6OVq2HR1wdXawPud6/XkXRwecHB1wut5q7eTggLOjCSdHB5wdLPdOjiacHRyuP2+y1nd2dEizj4MJzMkGyYZBcjKYDYPkZAOzYVjLzcmQnLKd8nyygdkgVd2UXgLXEs3EJVhuVxOTLPcJZq5eL7+aYCYuwVIef9OPFlcTLXWIzb6/l7OjCQ8Xp+tJuyWx93a1PPZ2c8LLzQlvN2e8rz/2dnO+XuZ0vczy2NMlc60jIiIi2WHs2LG0atXK+rhIkSLUrFnT+vidd95hyZIl/PTTTwwePPi2x+nbty/du3cHYNy4cUyePJktW7bQpk2bdOsnJiYyffp0goODARg8eDBjx461Pj9lyhRGjhxJ586dAZg6dSq//PLLXb/OlOR8w4YNNGrUCIC5c+cSFBTE0qVLeeyxxzhx4gRdu3alevXqAJQvX966/4kTJ6hduzZ169YFLL0JRGzq6mVYO96y3XAwFCkHMech9jzERlzfvn6fEA3meIg6ZbndSaHS8PQ68Ej/RzaxT0rSs9GVuAQOXU/GD56L4dD5GA6di+ZM5LXb7uPr7kyIvxchAV5U8Pcm2M+Twh4uqZPs64m46/WE28GhYCd+yclG6uT9elJ/7XqSH5uQxNUEM7EJZmLjk4hNSCIu/qbtBDMx8dfLbnqc0mMh0WwQeTWRyKuJ9xSnyQRerk74uDmnSfALuTvj5+2Kv7fr9Xs3/LxdKerlgrOjfu0UEclN7s6O7B1rm6Fz7s6O2XaslKQzRUxMDKNHj2bZsmWEh4eTlJTE1atXUw0lTE+NGjWs256envj4+BAREXHb+h4eHtYEHSAwMNBaPzIyknPnzlG/fn3r846OjoSFhZGcfHc9Bfft24eTkxMNGjSwlhUtWpRKlSqxb98+AF544QWee+45VqxYQcuWLenatav1dT333HN07dqV7du389BDD9GpUydrsi9iE39MtCTqfqHQcgw4ZpCiJV6FmAhLAp9yf2siHxthee7aFbhyAg6ugJpP5NrLkXunJP0erNp7jnX/nbcm5hdi4m9b18/blRB/Lyr4exHi70Wwvxch/t4U83JRa2sWOTiYLN3XXbP345toTrYk+fFJxCUkEXs9sY+5ntzHXEsi6prlcfS1RKKvWcqiryURfb3M8lzS9V4CWJ67lpTpGEwmKOLhgt/15P3mBP5GQm+593J10mdHROzetGnTmDZtGseOHQOgatWqjBo1irZt26Zbf/bs2WnGBru6unLt2u1/8L5XJpMp27qc25Knp2eqx8OHD2flypVMnDiRChUq4O7uzqOPPkpCQkKGx3F2dk712GQyZZhQp1c/o+GMuWHAgAG0bt2aZcuWsWLFCsaPH89HH33EkCFDaNu2LcePH+eXX35h5cqVPPjggwwaNIiJEyfaNGYpoC4ehs2fW7Zbv5txgg7g7A6Fy1hud7JqDKz/GA7/riQ9j8n730g2tOHwBb7563iqspKF3KlwUzIeEuBFBT9vfD2cb3MUsRfOjg74ujvg635vfyvDMLiWmGxJ5K8n7ZZk/sbjK3EJnI+OJyI6/vr9NS7EJGBONrgYm8DF2AT2n43O8Dzuzo7WRL64rxvBfjc+c+WKeeLqlH2tMyIid6tUqVK8//77hISEYBgGX3/9NR07duSff/6hatWq6e7j4+OTauZy/SB5dzZs2EDfvn2t3cxjYmKsP5bkFl9fXwICAti6dStNmjQBwGw2s337dmrVqnVXxwwNDSUpKYnNmzdbW8AvXrzIgQMHqFKlirVeUFAQzz77LM8++ywjR47kyy+/ZMiQIYBlVvs+ffrQp08fGjduzCuvvKIkXWxj5ShIToQKLS237BTc3JKkH1kLhqHJ5vIQJen3oHklf1ydHK2JUbCfV7a37kreYzKZcL8+mZ1/FvZLTja4dEvynpLAp5RduH4fE5/E1UQzJy7FceJSXJpjOTqYKFPEgwr+XlQM8L4+nMLyGXXLxq6VIiJ30qFDh1SP33vvPaZNm8Zff/112yT95pnL5e6FhISwePFiOnTogMlk4q233rrrLub3YsiQIYwfP54KFSpQuXJlpkyZwuXLlzP148uuXbvw9va2PjaZTNSsWZOOHTsycOBAPv/8c7y9vXnttdcoWbIkHTt2BGDo0KG0bduWihUrcvnyZdasWUNoaCgAo0aNIiwsjKpVqxIfH8///vc/63Miueron7D/f2ByhIfey/7jBzUAZw+IOQcReyEg/f9zxf4oo7wHTSr60aRi1tYXFbkdBwcTxbxcKeblSmhgxnXjEpKsSfz56HhOXb56fdiFZVLC6GtJHLkQy5ELsazYe866n8kEpYt4XB964a0fmEQkV5nNZhYuXEhsbCwNGza8bb2YmBjKlClDcnIyderUYdy4cbdN6FPEx8cTH39j2FlUVFS2xZ1Xffzxx/Tr149GjRpRrFgxRowYYZP3ZcSIEZw9e5bevXvj6OjI008/TevWrXF0vPOPximt7ykcHR1JSkpi1qxZvPjiizz88MMkJCTQpEkTfvnlF2vXe7PZzKBBgzh16hQ+Pj60adOGTz75BLCs9T5y5EiOHTuGu7s7jRs3Zv78+dn/wkUykpwMy1+3bIf1Bf/K2X8OJ1cocz8cWmnp8q4kPc8wGbYeNJTLoqKi8PX1JTIyEh8fH1uHI5LtDMMgIjqeg+duJO2HzsXwX0Q0V+JuPxleyULuhARc7zLv703Vkj5UDPDWRHYiuSC/fzft2rWLhg0bcu3aNby8vJg3bx7t2rVLt+6mTZs4ePAgNWrUIDIykokTJ/LHH3+wZ88eSpUqddtzjB49mjFjxqQpv/U9vXbtGkePHqVcuXK4ubnd+4uTLEtOTiY0NJTHH3+cd955x9bh5Ah9zuSO/pkLPz4Prj7wwj+WtdBzwqbPLD8GBD8IvRbnzDkkU7LyXa8kXaSAMAzLePeD524sA2hJ5G8/6aGrkwNVS/hQo1Qhagb5UqNUIcoV9SzwKwyIZLf8/t2UkJDAiRMniIyMZNGiRXz11VesW7cu1fjh20lMTCQ0NJTu3btnmNCl15IeFBSkJN0OHD9+nBUrVtC0aVPi4+OZOnUqs2bNYufOnfm2m7k+Z5KhhFiYXAdizkKrsXD/izl3rnN7YVpDcHKHEcfAWZ9HW8nKd736t4oUECbTje70DYOLpnrucmwCh87HWFvfD5yNZtfpSKKvJbH9xBW2n7hirevt6kS1kr7UCPKlZqlC1CjlS8lC7prYSURuy8XFhQoVKgAQFhbG1q1b+fTTT/n888/vuK+zszO1a9fm0KFDGdZzdXXF1dU1W+KV7OXg4MDs2bMZPnw4hmFQrVo1Vq1alW8TdJE72jDZkqAXKgMNns3Zc/mHgldxy/lO/gXlm+Xs+SRbKEkXEQp7ulDPswj1yhaxliUnGxy7GMu/pyLZeeoK/56KZM+ZSKLjk9h05CKbjly01i3q6UKNUr5UL1WImqUsLe5+3rpYFpH0JScnp2r1zojZbGbXrl237R4v9i8oKIgNGzbYOgwR+xB5GjZ8atluNdYybjwnmUyWWd53fgeH1yhJzyOUpItIuhwcTJT386K8nxedapcEIMmczH/nYth1+go7T0Xy76kr7A+P5mJsAmsOnGfNgfPW/Uv4ulGjVCFri3u1kr73vLydiOQ9I0eOpG3btpQuXZro6GjmzZvH2rVrWb58OQC9e/emZMmSjB8/HoCxY8dy3333UaFCBa5cucKECRM4fvw4AwYMsOXLEBHJHr+/A0lXoXRDqNIxd85Z/nqSfmQNkHbuDrE/StJFJNOcHB2oUsKHKiV86FbPUnYt0cy+8Chri/uuU5EcOh/DmchrnIk8y297zlr3L1fMkxrXW9prlvKlSgkfPFz035BIfhYREUHv3r0JDw/H19eXGjVqsHz5clq1agXAiRMncHC4MUHl5cuXGThwIGfPnqVw4cKEhYWxcePGTI1fFxGxa6e3W5JlgNbv5d665Smt5+E7IfZCzk1SJ9lGE8eJSLaLiU9i92lLS3tKi/vJS1fT1HMwQcUAb2viXqOUL5WL++DipBnlpWDRd1P2u917qgm9JDfocyZpGAbMagcnNkKNbtDli9w9/7T74dxu6DoDqj+au+cWQBPHiYiNebk6cV/5otxX/sYEdZdjE/j3dCT/nryRuEdEx7P/bDT7z0bz/bZTALg4OhAa6E2NUoWoXsrSVb6CvxeOmlFeRERE8qp9P1sSdCd3eHBU7p+/fDNLkn5kjZL0PEBJuojkisKeLjSt6EfTin7WsrOR1/j3+qR0KZPTRV5NZOepSHaeirTW83BxpFoJX6qX8qXG9cS9TFEPzSgvIiIi9i8pHla+ZdluNAR8S+V+DMHNYdNUy+RxhpF7Xe3lrihJFxGbKe7rRnHf4jxUtThgWcv9xKU4/j11o6v87tORxCWY2XLsEluOXbLu6+NmWQou5Va9pC9linhoDXcRERGxL1u+gMvHLEuh5eSa6Bkp3QgcXSHqNFw4CH4VbROHZIqSdBGxGyaTiTJFPSlT1JMONUsAYE42OHI+xtpFfuepSPadiSLqWhIbD19k4+EbS8F5uzpRpYQP1W9K3ssX81TiLiJyXbNmzahVqxaTJk0CoGzZsgwdOpShQ4fedh+TycSSJUvo1KnTPZ07u44jkqfEXoB1EyzbD74Frl62icPFA0rfB0fXWbq8K0m3a0rSRcSuOTqYCAnwJiTAm0fDLN3DEpKS+e9cNHvORLLrdCS7TkexLzyK6PgkNh+9xOajN1rcPV0cqVrCl6olLcl79ZK+lPfTGHcRyVs6dOhAYmIiv/32W5rn/vzzT5o0acLOnTupUaNGlo67detWPD09sytMAEaPHs3SpUvZsWNHqvLw8HAKFy6cree61ezZsxk6dChXrlzJ0fOIZNra9yE+EopXh5rdbRtLcHNLkn74d2jwjG1jkQwpSReRPMfFycHaUp6yFFyiOZlDETHsPm3pIr/rdCR7w6OITaervLuzo7XFvWoJH6qX8qWCnxdOjppVXkTsU//+/enatSunTp2iVKnU41lnzZpF3bp1s5ygA/j5+d25UjYpXrx4rp1LxC5E7IdtMy3brceBg6Nt4wluAatGw7H1YE4ER2fbxiO3pStSEckXnB0dCA304bG6QYzpWI3Fz9/PnjFtWPFSEz56rCZP3V+WemUL4+HiyNVEM38fv8zsjcd4ZdG/tJn0JzXHrODpOduYv+UE56Ku2frliEhuMgxIiLXNLZMr4T788MP4+fkxe/bsVOUxMTEsXLiQ/v37c/HiRbp3707JkiXx8PCgevXqfPfddxket2zZstau7wAHDx6kSZMmuLm5UaVKFVauXJlmnxEjRlCxYkU8PDwoX748b731FomJiYClJXvMmDHs3LkTk8mEyWSyxmwymVi6dKn1OLt27aJFixa4u7tTtGhRnn76aWJiYqzP9+3bl06dOjFx4kQCAwMpWrQogwYNsp7rbpw4cYKOHTvi5eWFj48Pjz/+OOfOnbM+v3PnTpo3b463tzc+Pj6EhYWxbds2AI4fP06HDh0oXLgwnp6eVK1alV9++eWuY5ECYOVbYJihUnso18TW0UBAdfAoBgkxcGqrraORDKglXUTyLUcHExUDvKkY4E3X613lzckGRy/Epmpx33Mmipj4JFbsPceKvZaLtSqBPrSo7E/zyn7UCiqs7vEi+VliHIwrYZtzv34GXO7c3dzJyYnevXsze/Zs3njjDevqFgsXLsRsNtO9e3diYmIICwtjxIgR+Pj4sGzZMnr16kVwcDD169e/4zmSk5Pp0qULAQEBbN68mcjIyHTHqnt7ezN79mxKlCjBrl27GDhwIN7e3rz66qt069aN3bt389tvv7Fq1SoAfH190xwjNjaW1q1b07BhQ7Zu3UpERAQDBgxg8ODBqX6IWLNmDYGBgaxZs4ZDhw7RrVs3atWqxcCBA+/4etJ7fSkJ+rp160hKSmLQoEF069aNtWvXAtCjRw9q167NtGnTcHR0ZMeOHTg7W1obBw0aREJCAn/88Qeenp7s3bsXLy8bjS8W+3doNRxcAQ5O0GqsraOxcHCA8k1h9w+WLu9lGtk6IrkNJekiUqA4Opio4O9FBX8vOtUuCUByssHe8Ch+3x/BmgMR7Dh5hb3hUewNj2LqmkMU9nCmaUU/mlf2p2lFPwp5uNj4VYhIQdSvXz8mTJjAunXraNasGWDp6t61a1d8fX3x9fVl+PDh1vpDhgxh+fLlfP/995lK0letWsX+/ftZvnw5JUpYfrQYN24cbdu2TVXvzTfftG6XLVuW4cOHM3/+fF599VXc3d3x8vLCyckpw+7t8+bN49q1a8yZM8c6Jn7q1Kl06NCBDz74gICAAAAKFy7M1KlTcXR0pHLlyrRv357Vq1ffVZK+evVqdu3axdGjRwkKCgJgzpw5VK1ala1bt1KvXj1OnDjBK6+8QuXKlQEICQmx7n/ixAm6du1K9erVAShfvnyWY5ACwpwEK67/O6n/NBSrYNt4bhbc4nqSvgZavHnn+mITStJFpMBzcDBZx7i/8GAIF2PiWfffedYcOM+6AxFcjktk6Y4zLN1xBgcT1CldmOaV/WleyZ/QQG+t1y6S1zl7WFq0bXXuTKpcuTKNGjVi5syZNGvWjEOHDvHnn38ydqyllc5sNjNu3Di+//57Tp8+TUJCAvHx8Xh4ZO4c+/btIygoyJqgAzRs2DBNvQULFjB58mQOHz5MTEwMSUlJ+Pj4ZPp1pJyrZs2aqSatu//++0lOTubAgQPWJL1q1ao4Ot4YxxsYGMiuXbuydK6bzxkUFGRN0AGqVKlCoUKF2LdvH/Xq1WPYsGEMGDCAb775hpYtW/LYY48RHBwMwAsvvMBzzz3HihUraNmyJV27dr2reQCkAPjnG4jYC26FoMkrto4mtfLNLfdntsPVy+Ces5M5yt3RmHQRkVsU9XKlS51STOlem+1vtWLhsw15rlkwlYt7k2zAtuOXmbD8AO0m/0nD8b8zcvG/LN9zltj4JFuHLiJ3w2SydDm3xS2LP/L179+fH374gejoaGbNmkVwcDBNmzYFYMKECXz66aeMGDGCNWvWsGPHDlq3bk1CQkK2vVWbNm2iR48etGvXjv/973/8888/vPHGG9l6jpuldDVPYTKZSE5OzpFzgWVm+j179tC+fXt+//13qlSpwpIlSwAYMGAAR44coVevXuzatYu6desyZcqUHItF8qhrUbDmPct2s9fAo4ht47mVb0koVhGMZDj6h62jkdtQki4ikgEnRwfqlS3CiDaV+W1oEza+1oL3OlejZWgA7s6OnI26xndbTvLMN39Te+xKen61mRnrj3L0QqytQxeRfOjxxx/HwcGBefPmMWfOHPr162ftzbNhwwY6duxIz549qVmzJuXLl+e///7L9LFDQ0M5efIk4eHh1rK//vorVZ2NGzdSpkwZ3njjDerWrUtISAjHjx9PVcfFxQWz2XzHc+3cuZPY2Bv/V27YsAEHBwcqVaqU6ZizIuX1nTx50lq2d+9erly5QpUqVaxlFStW5KWXXmLFihV06dKFWbNmWZ8LCgri2WefZfHixbz88st8+eWXORKr5GHrP4bY81C0AtQbYOto0hfcwnJ/eI1t45DbUnd3EZEsKFHInR4NytCjQRmuJZrZcvSSdSz78YtxrD90gfWHLvDO//bSsHxRnm5anmYV/dQlXkSyhZeXF926dWPkyJFERUXRt29f63MhISEsWrSIjRs3UrhwYT7++GPOnTuXKgHNSMuWLalYsSJ9+vRhwoQJREVF8cYbb6SqExISwokTJ5g/fz716tVj2bJl1pbmFGXLluXo0aPs2LGDUqVK4e3tjaura6o6PXr04O2336ZPnz6MHj2a8+fPM2TIEHr16mXt6n63zGZzmjXaXV1dadmyJdWrV6dHjx5MmjSJpKQknn/+eZo2bUrdunW5evUqr7zyCo8++ijlypXj1KlTbN26la5duwIwdOhQ2rZtS8WKFbl8+TJr1qwhNDT0nmKVfObycdj0f5btVu/Y7xJn5ZvD5ulwREm6vVJLuojIXXJzdqRJRT9GP1KVtcOb8fvLTXmzfSgPVCiGo4OJTUcu8tSsrbSZ9CeL/j5FQlLOddEUkYKjf//+XL58mdatW6caP/7mm29Sp04dWrduTbNmzShevDidOnXK9HEdHBxYsmQJV69epX79+gwYMID33nsvVZ1HHnmEl156icGDB1OrVi02btzIW2+9lapO165dadOmDc2bN8fPzy/dZeA8PDxYvnw5ly5dol69ejz66KM8+OCDTJ06NWtvRjpiYmKoXbt2qluHDh0wmUz8+OOPFC5cmCZNmtCyZUvKly/PggULAHB0dOTixYv07t2bihUr8vjjj9O2bVvGjBkDWJL/QYMGERoaSps2bahYsSL/93//d8/xSj6yajSY4y3LrVVqe8fqNlP2fsus85ePwaUjto5G0mEyjEwu0JlPREVF4evrS2RkZJYnORERyazTV64ya/1RvttygtgES7fP4j5uPHV/Wbo3KI2Pm53+ui42oe+m7He79/TatWscPXqUcuXK4ebmZsMIJT/T56wAOrkFZrQCTPDMHxBo55MKzmoHxzdA+4+hXn9bR1MgZOW7Xi3pIiI5oGQhd958uAobRz7IiDaV8fd25WzUNcb/up9G439n3C/7CI+8auswRURE5F4lJ8NvIy3btXvYf4ION2Z5V5d3u6QkXUQkB/m6O/Ncs2D+HNGcDx+tQYi/FzHxSXzxxxEaf7CGYd/vYP/ZKFuHKSIiIndrz2I4vQ2cPaHFW3eubw+CU5L0PyzruotdUZIuIpILXJ0cebxuEMuHNmFm37o0KFeEpGSDxdtP02bSn/SZuYWNhy5QwEYgiYiI5G2JVy1j0QEeeAm8i9s0nEwrURvcfCE+Es78Y+to5BZK0kVEcpGDg4kWlQNY8ExDfhx0P+2rB+JggnX/nefJrzbTYep6ftp5hiSzJpkTERGxe5s+g8iT4FMKGg22dTSZ5+AI5ZpattXl3e4oSRcRsZGaQYX4rEcd1gxvRu+GZXBzdmD36She+O4fmk1cy6wNR4mNVxc0kZygXiuSk/T5KiCiz8H6TyzbLd8GZ3fbxpNVKV3eD/9u2zgkDSXpIiI2VqaoJ2M7VmPjaw/yUsuKFPV04dTlq4z5eS+N3v+dCcv3ExF9zdZhiuQLzs6WlRXi4uJsHInkZymfr5TPm+RTa96FhBgoUQeqPWrraLIuuIXl/tRWiI+2bSySipOtAxAREYsini682DKEZ5qWZ9Hfp/jqzyMcuxjHZ2sO8+WfR3k0rBTPNQ0mqIiHrUMVybMcHR0pVKgQERERgGW9bpPJZOOoJL8wDIO4uDgiIiIoVKgQjo6Otg5Jcsq5vbD9G8t2m/HgkAfbPguXhcLl4PJROLbevtd2L2CUpIuI2Bk3Z0d63leG7vVLs3LvWT7/4wj/nLjCvM0n+H7rSbrUKcnzzSpQtpinrUMVyZOKF7dM7JSSqItkt0KFClk/Z5JPbZoKGFD5YSh9n62juXvBzWHbUUuXdyXpdkNJuoiInXJ0MNGmWiBtqgWy+chFpvx+iPWHLvD9tlMs+vsUHWuVZFDzClTw97J1qCJ5islkIjAwEH9/fxITE20djuQzzs7OakHP72IiYNdCy/b9Q20ayj0LbgHbZsJhTR5nT5Ski4jkAQ3KF6VB+aL8ffwyU38/yJoD51nyz2mW7jhN++qBDGkRQqXi3rYOUyRPcXR0VDIlIlm3bSaYE6BkXQiqZ+to7k3ZxmBygIsH4cpJKBRk64gETRwnIpKnhJUpzKyn6vPT4PtpVSUAw4D//RtO60l/8Mw329h9OtLWIYqIiORfSfGw9SvL9n3P2TaW7OBeCEqGWba1FJvdUJIuIpIH1ShViC971+XXFxvTvnogJhMs33OOh6esp9/srfxz4rKtQxQREcl/dv8AsefBuwRU6WjraLJHyizv6vJuN5Ski4jkYaGBPnzWow4rhjahY60SOJjg9/0RdP6/jfSasZmtxy7ZOkQREZH8wTDgr/+zbNcfCI75ZIm98tfXSz+yFpKTbRqKWChJFxHJB0ICvPn0idqsfrkZj4aVwtHBxJ8HL/DY9E088cUmNh66gGEYtg5TREQk7zq+Ac7uAid3COtr62iyT6m64OINVy/B2Z22jkZQki4ikq+UK+bJxMdqsnZ4M7rXL42zo4m/jlziya828+j0Taw9EKFkXURE5G78Nc1yX/MJ8Chi21iyk6MzlGts2VaXd7ugJF1EJB8KKuLB+C7VWfdKc3o3LIOLkwN/H79M31lb6fTZBlbtPadkXUREJLMuHYX9yyzb+WHCuFtZu7wrSbcHStJFRPKxEoXcGduxGn++2pz+D5TDzdmBnaciGTBnG12mbeRQRLStQxQREbF/mz8HDKjQEvwq2Tqa7Bd8PUk/8RckxNk2FlGSLiJSEAT4uPHWw1VYP6IFzzYNxsPFkX9OXKHd5PVMW3uYJLMmihEREUnXtSj451vLdn5sRQcoWgF8gyzrvx/faOtoCjwl6SIiBUgxL1dea1uZ1S83pXklPxKSkvngt/10nbaRg+fUqi4iIpLGP99CQjQUqwTBD9o6mpxhMkH5ZpZtdXm3OSXpIiIFUKCvOzP71mPiYzXxdnNi56lI2k9ez2drDqlVXUREJEWyGTZPt2zf96wlmc2vUrq8H/7dtnGIknQRkYLKZDLxaFgpVr7UlBaV/UkwJzNh+QG6TNvIgbNqVRcREeHAr3DlOLgVghpP2DqanFWuGWCCiL0QfdbGwRRsStJFRAq44r5uzOhTl48fr4mPmxP/nork4Sl/MvX3gySqVV1ERAqylGXX6j4FLh62jSWneRaFwJqW7SNrbRpKQackXUREMJlMdKlTipXDmtIy1J9Es8HEFf/R+f82sC88ytbhiYiI5L7wnXB8PTg4Qb2Bto4md6jLu11Qki4iIlYBPm582bsuk7rVwtfdmd2no3hk6nomr1arutydadOmUaNGDXx8fPDx8aFhw4b8+uuvGe6zcOFCKleujJubG9WrV+eXX37JpWhFRG7y1/Wx6FU6gW9Jm4aSa6zrpa8Fw7BpKAWZknQREUnFZDLRqXZJVr7UhFZVAkg0G3y88j86fbaBvWfUqi5ZU6pUKd5//33+/vtvtm3bRosWLejYsSN79uxJt/7GjRvp3r07/fv3559//qFTp0506tSJ3bt353LkIlKgRZ+D3Yss2/c9b9tYclPp+8DJHWLOWcami02YDKNg/UQSFRWFr68vkZGR+Pj42DocERG7ZhgGP+08w9s/7eFKXCJODiYGt6jA880q4OKk33mzS0H7bipSpAgTJkygf//+aZ7r1q0bsbGx/O9//7OW3XfffdSqVYvp06dn+hwF7T0VkWy2Zjysex9K1YcBK20dTe76tiscWgUPvQuNhtg6mnwjK99LusISEZHbMplMdKxVkhUvNaF11QCSkg0mrTpIx882sOdMpK3DkzzGbDYzf/58YmNjadiwYbp1Nm3aRMuWLVOVtW7dmk2bNmV47Pj4eKKiolLdRETuSuI12PqVZfu+52wbiy2kdHk/rPXSbUVJuoiI3JG/txvTe4YxpXttCns4sy88io5TN/Dxyv9ISCp4Y9UNw2Dx9lNsOXrJ1qHkCbt27cLLywtXV1eeffZZlixZQpUqVdKte/bsWQICAlKVBQQEcPZsxssBjR8/Hl9fX+stKCgo2+IXkQJm9yKIuwA+pSD0EVtHk/uCW1juj2+0/GAhuU5JuoiIZIrJZKJDzRKseKkpbasVJynZYPLqgzwydT27TxecVvXz0fE8883fDPt+Jy8v3EFMfJKtQ7J7lSpVYseOHWzevJnnnnuOPn36sHdv9o51HDlyJJGRkdbbyZMns/X4IlJAGMaNZdfqDwRHJ9vGYwv+oeBVHJKuwsm/bB1NgaQkXUREssTP25VpPcP47Mk6FPF0Yf/ZaDp+toGPVxzAnJy/pzn5ZVc4rSf9wYq953B2NPFEvdK4aWz+Hbm4uFChQgXCwsIYP348NWvW5NNPP023bvHixTl37lyqsnPnzlG8ePEMz+Hq6mqdQT7lJiKSZcf+hHO7wdkD6vS2dTS2YTJB+WaWbXV5twldWYiIyF1pXyOQlS81oX2NQMzJBpN/P0S/2VuJvJpo69Cy3ZW4BF6c/w/Pz93OpdgEQgN9+HHQAwxqXgEnR32VZlVycjLx8fHpPtewYUNWr16dqmzlypW3HcMuIpKtUlrRa3YHjyK2jcWWUrq8H1GSbgsFsP+GiIhkl6Jernz2ZB0eqnKaET/8y7r/ztP5sw182acuwX5etg4vW6zZH8GIH/4lIjoeBxM836wCLzwYotntM2nkyJG0bduW0qVLEx0dzbx581i7di3Lly8HoHfv3pQsWZLx48cD8OKLL9K0aVM++ugj2rdvz/z589m2bRtffPGFLV+GiBQEFw/DgV8t2wVxwribpbSkh++E2AvgWcym4RQ0usIQEZF71rFWSRY924gSvm4cuRBLp882sOZAhK3DuifR1xJ57Yd/eWr2ViKi4ynv58kPzzVieOtKStCzICIigt69e1OpUiUefPBBtm7dyvLly2nVqhUAJ06cIDw83Fq/UaNGzJs3jy+++IKaNWuyaNEili5dSrVq1Wz1EkSkoNjyBWBAyENQLMTW0diWdwD4V7VsH1lr01AKIq2TLiIi2eZCTDzPfvM3245fxsEEr7WtzMDG5TGZTLYOLUs2Hr7AKwv/5fSVq5hM0O/+crzSuhJuzo45cj59N2U/vacikiXXIuHjKpAQA72W3OjuXZAtfwM2TYXaPaHjZ7aOJs/TOukiImITxbxcmTfwPp6oF0SyAeN+2c+w73dyLdFs69Ay5WqCmdE/7eHJLzdz+spVgoq4893A+3jr4So5lqCLiIgd+OdbS4LuV/nGOuEFXfBN66UXrHZdm9OYdBERyVYuTg6M71Kd0EAfxv5vL0v+Oc2RC7F80SuMAB83W4d3W38fv8zwhTs5eiEWgCcblOb1dqF4ueqrUkQkX0s2w+bplu37nrPMbi5QuhE4ukDUabhwEPwq2jqiAkMt6SIiku1MJhN9GpXlm371KeThzM6TV+gwZT07Tl6xdWhpxCeZ+eC3/Tw2fSNHL8RS3MeNr/vVZ1zn6krQRUQKgv3L4MoJcC8CNbrZOhr74eIBpa+vrKFZ3nOVknQREckxjSoU46dBD1AxwIuI6Hge/3wTi7efsnVYVrtPR9Jx6gamrT1MsgFdapdk+dAmNK3oZ+vQREQkt6Qsu1b3KXB2t20s9sba5f1328ZRwChJFxGRHFW6qAeLn7+flqEBJCQlM+z7nYz7ZR/mZNuNb0syJzN59UE6fbaB/WejKerpwvSeYXzcrRa+Hs42i0tERHLZmX/gxEZwcIJ6A2wdjf1JGZ9/bD2YE20bSwGiJF1ERHKcl6sTX/QKY0iLCgB88ccR+s3eSuTV3P/CPxQRTddpG/l45X8kJRu0qVqcFS81oU214rkei4iI2Nhf18eiV+0CPiVsG4s9Kl4DPIpaJtU7tdXW0RQYNk/SP/vsM8qWLYubmxsNGjRgy5YtGdafNGkSlSpVwt3dnaCgIF566SWuXbuWS9GKiMjdcnAw8fJDlZjSvTZuzg6s++88nT/bwOHzMblyfnOywZd/HKHd5PXsPBWJj5sTnz5Ri2k961DUyzVXYhARETsSfRZ2/2DZvu8528ZirxwcoHwzy7a6vOcamybpCxYsYNiwYbz99tts376dmjVr0rp1ayIiItKtP2/ePF577TXefvtt9u3bx4wZM1iwYAGvv/56LkcuIiJ3q0PNEix6thElfN04ciGWTp9tYO2B9P/fv1eGYXDkfAwLt53kiS828d4v+0hISqZZJT9WDmtKx1ol89wa7iIikk22zoDkRAi6D0rWsXU09qv8TUuxSa4wGYbtFr1r0KAB9erVY+rUqQAkJycTFBTEkCFDeO2119LUHzx4MPv27WP16tXWspdffpnNmzezfv36TJ0zK4vIi4hIzjkfHc9z3/7NtuOXcTDByLahDGhc7p6S5qsJZv49dYW/T1xm+/HL/H38MpfjbnSp93Rx5K2Hq9CtXpBdJef6bsp+ek9FJEOJ1+CTKhB3ER77Gqp2snVE9ivyFHxSFUwO8OoRcC9s64jypKx8L9lsbZmEhAT+/vtvRo4caS1zcHCgZcuWbNq0Kd19GjVqxLfffsuWLVuoX78+R44c4ZdffqFXr163PU98fDzx8fHWx1FRUdn3IkRE5K75ebsyd2ADRi3dw4JtJ3nvl33sC49iXJfquDk7ZuoY4ZFX+ft6Mr79+GX2nIki6ZYJ6VycHKhZypewMkXo0aA0QUU8cuLliIhIXrJroSVB9w2Cyg/bOhr75lsKilWEC//B0T+gSkdbR5Tv2SxJv3DhAmazmYCAgFTlAQEB7N+/P919nnzySS5cuMADDzyAYRgkJSXx7LPPZtjdffz48YwZMyZbYxcRkezh6uTI+12rExrozTvL9rH4n9McvhDLF73CCPBxS1U30ZzMvvCoVEn5mci0c5L4e7tSt2xh6pQuTFiZwlQt4YuLk82nYBEREXthGDeWXav/NDjaLCXKO8o3tyTph9coSc8FeeoTuXbtWsaNG8f//d//0aBBAw4dOsSLL77IO++8w1tvvZXuPiNHjmTYsGHWx1FRUQQFBeVWyCIicgcmk4m+95cjJMCb5+duZ+fJK3SYsp5J3WpxLcnMtmOWpHznqStcS0xOta+jg4nQQG/CShemThlLUl6ykLtddWUXERE7c/QPiNgDzp5Q5/Y9cuUmwS1gy+dwROPSc4PNkvRixYrh6OjIuXPnUpWfO3eO4sXTXwbnrbfeolevXgwYYFnDsHr16sTGxvL000/zxhtv4OCQtqXE1dUVV1fN2isiYu/ur1CMnwbfz4Cvt3EwIoYnv9qcpo6vuzN1ShcirIwlKa9ZqhCernnq92YREbG1v/7Pcl/rSY2vzqyy91vWkr98DC4dgSLlbR1RvmazKxsXFxfCwsJYvXo1nTp1AiwTx61evZrBgwenu09cXFyaRNzR0TJu0Ybz34mISDYpU9STxc834tVF/7Ji7znKFvUg7HoLeViZwpQv5oWDg1rJRUTkLl08DP/9ZtnWsmuZ5+oNperDiY1wbL2S9Bxm0+aHYcOG0adPH+rWrUv9+vWZNGkSsbGxPPXUUwD07t2bkiVLMn78eAA6dOjAxx9/TO3ata3d3d966y06dOhgTdZFRCRv83ZzZlrPMJLMyTg5aiy5iIhko83TLfcV20DRYNvGktf4V7Yk6VdO2jqSfM+mSXq3bt04f/48o0aN4uzZs9SqVYvffvvNOpnciRMnUrWcv/nmm5hMJt58801Onz6Nn58fHTp04L333rPVSxARkRyiBF1ERLLV1Svwz1zLtlrRs8470HIfHW7bOAoAm66TbgtaN1VEROyNvpuyn95TEUlj4xRY8Sb4V4HnNoImGc2av7+Gn1+AkIegx0JbR5PnZOV7Sc0UIiIiIiKSv5mTYPMXlu37nlOCfje8r0/urZb0HKcpcUVERLLq6mU4uRWSk6ByO1tHIyIid7J3KUSeAI+iUP0xW0eTN1mT9HMZ15N7piRdREQkI4YBV07Aib/g5F+W+4h9gAHFqytJFxGxd+E74ecXLdv1BoCzu23jyau8rifpsectPRMclUrmFL2zIiIiNzMnwbndcHIznNgEJzZD9Jm09YoEQ2AtSxKvbpMiIvbp0lH49lFIiIGyjaHxy7aOKO/yLAYmRzDMEBsBPiVsHVG+pSRdREQKtvgYOL3N0kJ+4i84tdVyMXczBycIrAmlG0Lp+yCoAXj52yZeERHJnJjz8G0XS0IZUB2emAtOrraOKu9ycLR890WHQ/RZJek5SEm6iIgULFHh17utX28pP7vL0ipwM1cfCKp/PSG/D0qGgYuHbeIVEZGsi4+BeY/DpSNQqDT0XARuvraOKu/zCrAk6TEal56TlKSLiEj+lngNDq2E/cvg+Ea4cjxtHd+gGy3kpRuCf6ilxUBERPIecyIs7ANntlsmiuu55MakZ3JvvAMhfIdmeM9hStJFRCT/MSfCkXWwe5ElOY+PuvGcyQECqlqS8aAGluTct5TtYhURkexjGPDTEDi0Cpw94MnvoVgFW0eVf3gHWO41w3uOUpIuIiL5Q7LZ0lK++wfY+yNcvXTjOZ+SULUzBLeAUvXAzcd2cYqISM5ZPQZ2fmeZ4Oyxr6FUXVtHlL+kzPAec9a2ceRzStJFRCTvMgw4tc2SmO9ZkvqiwdMPqnSCal0tLeYODjYLU0REcsFf02H9J5btR6ZAxYdsG09+ZF0rXUl6TlKSLiIieYthWJZI27UI9iy2rGGews0XQh+xJOZlG2sNVxGRgmL3YvjtNcv2g6Ogdg/bxpNfKUnPFbp6ERGRvOHCQUuL+e4f4MJ/N8qdPaFye0tiHtwCnFxsF6OIiOS+o3/AkmcAA+o/DQ8Ms3VE+ZeS9FyhJF1EROzX5eOW1vLdP1iWSkvh6GrpxlitK4S01vJoIiIFVfi/8N2TYE6AKh2hzftgMtk6qvwrZUx6bIRlLhithJIjlKSLiIj9ObkFlr8Op7beKHNwsrSUV+sKldpp8jcRkYLu8nGY+ygkREOZB6DzF0oac5qnH2ACIxliL9yY7V2ylZJ0ERGxP39MvJ6gm6BcY0tiHvoIeBSxdWQiImIPYi/Ct10g5hz4V4Un5oKzm62jyv8cncDL3/K+R4crSc8hStJFRMT+pMzS/vjXlu6LIiIiKRJiYd7jcPEQ+AZBz0XgXsjWURUcXgGWJD1Ga6XnFK1HIyIi9if2ouXet5Rt4xAREftiToSFfeH0NnAvDD1/AJ8Sto6qYNHkcTlOSbqIiNgXw4C4C5Ztj2K2jUVEROyHYcDPQ+HgCnByhye/B79Kto6q4FGSnuOUpIuIiH1JiIGka5ZtTyXpIiJy3e/vwo5vweQAj82CoPq2jqhgSpnhPUZJek5Rki4iIvYl9norupM7uHjaNhYREbEPW76EPydath+eBJXa2jScAk0t6TlOSbqIiNiXuOvj0dWKLiIiAHuWwi+vWLabvwFhfWwaToGnJD3HKUkXERH7ktKS7lHUtnGIiIjtHVsPiwcCBtTtB01esXVEYu3urtndc4qSdBERsS8pk8apJT1fGD9+PPXq1cPb2xt/f386derEgQMHMtxn9uzZmEymVDc3N61/LFLgnN0N33UHcwJUfhjaTQSTydZRifdNSXpysm1jyaeUpIuIiH2J1czu+cm6desYNGgQf/31FytXriQxMZGHHnqI2NjYDPfz8fEhPDzcejt+/HguRSwiduHyMZj7KMRHQelG0PUrcHC0dVQC4OUPmCA56cYQNclWTrYOQEREJBW1pOcrv/32W6rHs2fPxt/fn7///psmTZrcdj+TyUTx4sVzOjwRsSeGASc2wbaZsPdHSwu6Xyh0nwfO7raOTlI4OluGpMVdsMzw7uVn64jyHSXpIiJiX2Kv/yqvMen5UmRkJABFihTJsF5MTAxlypQhOTmZOnXqMG7cOKpWrXrb+vHx8cTHx1sfR0VFZU/AIpLzrkXBvwssyXnE3hvlperDY7PBvbDNQpPb8A60JOnRZ6F4dVtHk+8oSRcREfuilvR8Kzk5maFDh3L//fdTrVq129arVKkSM2fOpEaNGkRGRjJx4kQaNWrEnj17KFWqVLr7jB8/njFjxuRU6CKSE8J3wtYZsGsRJF4fAuPsAdUftUwSV6K2beOT2/MOgHO7NMN7DlGSLiIi9iVlTLqnus/lN4MGDWL37t2sX78+w3oNGzakYcOG1seNGjUiNDSUzz//nHfeeSfdfUaOHMmwYcOsj6OioggKCsqewEUk+yRehd2LLa3mp7fdKPerbEnMa3QD90I2C08yScuw5Sgl6SIiYl/iNHFcfjR48GD+97//8ccff9y2Nfx2nJ2dqV27NocOHbptHVdXV1xdXe81TBHJKRcOWhLzHXPhmmXYCw7OUKWjJTkv00gzt+cl1mXYlKTnBCXpIiJiX6wt6RqTnh8YhsGQIUNYsmQJa9eupVy5clk+htlsZteuXbRr1y4HIhSRHGNOhP3/syTnR/+4UV6oNIQ9BbV7adKxvEot6TlKSbqIiNiPhDhIjLNsqyU9Xxg0aBDz5s3jxx9/xNvbm7NnLRd0vr6+uLtbZmvu3bs3JUuWZPz48QCMHTuW++67jwoVKnDlyhUmTJjA8ePHGTBggM1eh4hkwZWTsP1r2D7HspY2gMkBKraxtJoHPwgOWgk6T1OSnqOUpIuIiP1I6eru6AKu3raNRbLFtGnTAGjWrFmq8lmzZtG3b18ATpw4gcNNF+yXL19m4MCBnD17lsKFCxMWFsbGjRupUqVKboUtIlmVbIZDqy2t5geXg5FsKfcKgDp9oE5vKKR5IvINa3f3c7aNI59Ski4iIvYj9qbx6BqbmC8YhnHHOmvXrk31+JNPPuGTTz7JoYhEJNsYBpzeDnsWw54lEHX6xnPlmlpazSu3t6yrLfmLd4DlPvqs5XOg7+xspSRdRETsR9z1NdI1Hl1ExD4ZhmXptJTE/MqJG8+5FYLaPSGsLxQLsVWEkhu8rifpyYkQd0nf29lMSbqIiNiPWM3sLiJidwwDzu25kZhfOnLjOWdPqNQWqnWxjDV3drNdnJJ7nFzBvQhcvWSZ4V1JerZSki4iIvYjZUy6p5J0ERGbi9h/IzG/8N+Ncid3qNjakphXaAUuHraLUWzHO9CSpEeHQ0BVW0eTryhJFxER+6GWdBER27pwyJKU71kMEXtvlDu6QkgrqNrZMku7q5ftYhT74B0AEXsgWpPHZTcl6SIiYj/itEa6iEiuu3T0RmJ+dteNcgdnqPAgVO1i6dLu5mO7GMX+WGd41zJs2U1JuoiI2I/Y6xPHqSVdRCTnGAZcPgr7/mdJzM/8c+M5Byco38zSYl65PbgXtlmYYue0VnqOUZIuIiL2w9qS7mfbOERE8gPDgNjzlm7rEftuut8PCdE36pkcoGxjyxjz0EfAo4jtYpa8Q0l6jlGSLiIi9iNWE8eJiNyVq5ctyff5fdcT8etJecrSlrdycIagBlCtsyUx9/LP3Xgl70tZhi1GY9Kzm5J0ERGxH5o4TkQkYwmxcP7ALS3j+yD6zG12MEGR8uAfCv5VbtwXDQZH51wNXfIZ70DLfXS4bePIh5Ski4iIfUiKv9H9UhPHiUh+ZRiQGAfxMZCQcou9/jj6pu3rt5Ttq1csy6BdPgYY6R/bN+h6Eh4Kfin3lcDZPRdfoBQY3tdb0qPPWT7XJpNt48lHlKSLiIh9SGlFd3ACt0I2DUVECrhksyWRTrxqSZoTr16/pWxn5rm460n29cT75oT7dkl2Znn6pW0Z96sEbr7Z8vJFMiVldndzPFy7okkGs5GSdBERsQ8pk8Z5FNWv8WI/os/CsfW2juLOjHtM+tIe8Jbj3nT8W8syXce4zf31543kDOrccm8kW27mREhOvH6fdNPjpOv3CTdt31rnlsdJCTcSbHN89ryNGTKBi5dlvXEXz+vb3jdte1nuU7ZdvS3d1v1CwUuTa4odcHaz/Kh+7Yrl/0ol6dlGSbqIiNgHjUcXe3RuN/zQ39ZRiM2YwNnD0l085d7F45aym7Zdbq2bknhfT8Rv3nb2AAcHW79AkXvjXfxGku4fauto8g0l6SIiYh9SZiDWeHSxJ+6FoVyTHDhwHugtYu3RYrrlcXplmaxjMt24v3k7zXMOt38Ok+XQDk6WGcodnS3bjs7XH99cfutjJ3B0Sf85R2dw9ryeXF+/d3JTzx6RjHgXh/P7tQxbNlOSLiIi9kEt6WKPSoZBn59tHYWIiH1KGZceoyQ9O6mPjYiI2Ic4rZEuIiKSp3hfT9KjtVZ6dlKSLiIi9kEt6SIiInmLNUnXWunZSUm6iIjYB+uYdCXpIiIieYLX9bXSY9SSnp2UpIuIiH2IVXd3ERGRPMU70HKvieOylZJ0ERGxD7HnLffq7i4iIpI3eF9vSY8+C4Zh21jyESXpIiJiHzRxnIiISN6SMrt70lWIj7JtLPmIknQREbE9cyJci7RsqyVdREQkb3DxAFdfy7a6vGcbJekiImJ7KZPGmRzAvbBtYxEREZHMu7nLu2QLJekiImJ7KZPGuRcBB301iYiI5Bkpy7BphvdsoyshERGxPY1HFxERyZu8tFZ6dlOSLiIitpfSkq7x6CIiInmLtbu7WtKzi5J0ERGxvZQx6Z5FbRuHiIiIZE3KWukxGpOeXZSki4iI7aklXUREJG/y0sRx2U1JuoiI2J51TLqfbeMQERGRrEmZOE5JerZRki4iIrYXq4njRERE8qSU7u5K0rONknQREbE9a3d3jUkXERHJU1K6uyfGQny0bWPJJ5Ski4iI7WkJNhERkbzJ1QtcvC3bmuE9WyhJFxER29PEcSIiInmXdRk2rZWeHZSki4iIbSWb4eply7Za0kVERPIer+uTx8WoJT07ZDlJL1u2LGPHjuXEiRM5EY+IiBQ0cZcAw7LtXsSmoYiIiMhd0Azv2SrLSfrQoUNZvHgx5cuXp1WrVsyfP5/4+PiciE1ERAqClPHo7oXB0cm2sYiIiEjWWZN0dXfPDneVpO/YsYMtW7YQGhrKkCFDCAwMZPDgwWzfvj3LAXz22WeULVsWNzc3GjRowJYtWzKsf+XKFQYNGkRgYCCurq5UrFiRX375JcvnFRERO6Hx6CIiInlbygzv6u6eLe56THqdOnWYPHkyZ86c4e233+arr76iXr161KpVi5kzZ2IYxh2PsWDBAoYNG8bbb7/N9u3bqVmzJq1btyYiIiLd+gkJCbRq1Ypjx46xaNEiDhw4wJdffknJkiXv9mWIiIitaWZ3ERGRvE1rpWeru+5XmJiYyJIlS5g1axYrV67kvvvuo3///pw6dYrXX3+dVatWMW/evAyP8fHHHzNw4ECeeuopAKZPn86yZcuYOXMmr732Wpr6M2fO5NKlS2zcuBFnZ2fAMkY+I/Hx8am640dFRWXq9ZnNZhITEzNVVyQvcXZ2xtHR0dZhiNygNdJFRETyNuvs7krSs0OWk/Tt27cza9YsvvvuOxwcHOjduzeffPIJlStXttbp3Lkz9erVy/A4CQkJ/P3334wcOdJa5uDgQMuWLdm0aVO6+/z00080bNiQQYMG8eOPP+Ln58eTTz7JiBEjbpt0jB8/njFjxmT69RmGwdmzZ7ly5Uqm9xHJawoVKkTx4sUxmUy2DkUE4i5a7tWSni+NHz+exYsXs3//ftzd3WnUqBEffPABlSpVynC/hQsX8tZbb3Hs2DFCQkL44IMPaNeuXS5FLSIiWZLSkq7u7tkiy0l6vXr1aNWqFdOmTaNTp07WFu2blStXjieeeCLD41y4cAGz2UxAQECq8oCAAPbv35/uPkeOHOH333+nR48e/PLLLxw6dIjnn3+exMRE3n777XT3GTlyJMOGDbM+joqKIigo6LZxpSTo/v7+eHh4KImRfMUwDOLi4qxDSgIDA20ckQg3WtI9/Wwbh+SIdevWMWjQIOrVq0dSUhKvv/46Dz30EHv37sXT0zPdfTZu3Ej37t0ZP348Dz/8MPPmzaNTp05s376datWq5fIrEBGRO0oZkx4fBQmx4JL+/++SOVlO0o8cOUKZMmUyrOPp6cmsWbPuOqjbSU5Oxt/fny+++AJHR0fCwsI4ffo0EyZMuG2S7urqiqura6aObzabrQl60aLqdin5k7u7OwARERH4+/ur67vYXux5y70mjsuXfvvtt1SPZ8+ejb+/P3///TdNmjRJd59PP/2UNm3a8MorrwDwzjvvsHLlSqZOncr06dNzPGYREckiV29w9oDEOEuX96LBto4oT8vyxHERERFs3rw5TfnmzZvZtm1bpo9TrFgxHB0dOXcudZeIc+fOUbx48XT3CQwMpGLFiqmSitDQUM6ePUtCQkKmz307KWPQPTw87vlYIvYs5TOueRfELqi7e4ESGRkJQJEiRW5bZ9OmTbRs2TJVWevWrW87HA4sc9BERUWluomISC4xmW4sw6Yu7/csy0n6oEGDOHnyZJry06dPM2jQoEwfx8XFhbCwMFavXm0tS05OZvXq1TRs2DDdfe6//34OHTpEcnKytey///4jMDAQFxeXLLyKjKmLu+R3+oyLXdHEcQVGcnIyQ4cO5f7778+w2/rZs2fTHQ539uztJyQaP348vr6+1ltGQ9tERCQHeGmt9OyS5SR979691KlTJ0157dq12bt3b5aONWzYML788ku+/vpr9u3bx3PPPUdsbKx1tvfevXunmljuueee49KlS7z44ov8999/LFu2jHHjxmXpxwEREbEzWoKtwBg0aBC7d+9m/vz52X7skSNHEhkZab2l16AgIiI5yDrDu1rS71WWk3RXV9c0XdQBwsPDcXLK2hD3bt26MXHiREaNGkWtWrXYsWMHv/32m/XX8xMnThAefuOXmKCgIJYvX87WrVupUaMGL7zwAi+++GK6y7XJvStbtiyTJk3KdP21a9diMpk0M76IZF5yMsRdsmxrTHq+NnjwYP73v/+xZs0aSpUqlWHd4sWLZ2k4HFiuT3x8fFLdREQkF1lneNcybPcqyxPHPfTQQ4wcOZIff/wRX19fAK5cucLrr79Oq1atshzA4MGDGTx4cLrPrV27Nk1Zw4YN+euvv7J8nvzsTl2X3377bUaPHp3l427duvW2M++mp1GjRoSHh1s/F7mhcuXKHD16lOPHj2d48SYiduraFTDMlm11d8+XDMNgyJAhLFmyhLVr11KuXLk77tOwYUNWr17N0KFDrWUrV6687XA4ERGxA15aKz27ZDlJnzhxIk2aNKFMmTLUrl0bgB07dhAQEMA333yT7QHKnd3c22DBggWMGjWKAwcOWMu8vLys24ZhYDabM9Xrwc8va8shubi45GqivH79eq5evcqjjz7K119/zYgRI3Lt3OlJTExMd0lCEclAynh0V19wyr65RcR+DBo0iHnz5vHjjz/i7e1tHVfu6+trXW2id+/elCxZkvHjxwPw4osv0rRpUz766CPat2/P/Pnz2bZtG1988YXNXoeIiNxBSku6kvR7luXu7iVLluTff//lww8/pEqVKoSFhfHpp5+ya9cuTdJiI8WLF7fefH19MZlM1sf79+/H29ubX3/9lbCwMFxdXVm/fj2HDx+mY8eOBAQE4OXlRb169Vi1alWq497a3d1kMvHVV1/RuXNnPDw8CAkJ4aeffrI+f2t399mzZ1OoUCGWL19OaGgoXl5etGnTJtWPCklJSbzwwgsUKlSIokWLMmLECPr06UOnTp3u+LpnzJjBk08+Sa9evZg5c2aa50+dOkX37t0pUqQInp6e1K1bN9XKBD///DP16tXDzc2NYsWK0blz51SvdenSpamOV6hQIWbPng3AsWPHMJlMLFiwgKZNm+Lm5sbcuXO5ePEi3bt3p2TJknh4eFC9enW+++67VMdJTk7mww8/pEKFCri6ulK6dGnee+89AFq0aJGmZ8n58+dxcXFJNcmiSL5hHY+uVvT8atq0aURGRtKsWTMCAwOttwULFljr3Dq8rVGjRsybN48vvviCmjVrsmjRIpYuXao10kVE7Jm3WtKzS5Zb0sGyDvrTTz+d3bHYJcMwuJpotsm53Z0ds20W7tdee42JEydSvnx5ChcuzMmTJ2nXrh3vvfcerq6uzJkzhw4dOnDgwAFKly592+OMGTOGDz/8kAkTJjBlyhR69OjB8ePHb7uUTlxcHBMnTuSbb77BwcGBnj17Mnz4cObOnQvABx98wNy5c5k1axahoaF8+umnLF26lObNm2f4eqKjo1m4cCGbN2+mcuXKREZG8ueff9K4cWMAYmJiaNq0KSVLluSnn36iePHibN++3boywLJly+jcuTNvvPEGc+bMISEhgV9++eWu3tePPvqI2rVr4+bmxrVr1wgLC2PEiBH4+PiwbNkyevXqRXBwMPXr1wcskxt9+eWXfPLJJzzwwAOEh4ezf/9+AAYMGMDgwYP56KOPcHV1BeDbb7+lZMmStGjRIsvxidg968zuGo+eXxmGccc66Q1ve+yxx3jsscdyICIREckRKbO7a0z6PburJB0ss7yfOHEizfrkjzzyyD0HZU+uJpqpMmq5Tc69d2xrPFzu+k+UytixY1PNGVCkSBFq1qxpffzOO++wZMkSfvrpp9vOEQDQt29funfvDsC4ceOYPHkyW7ZsoU2bNunWT0xMZPr06QQHBwOWOQjGjh1rfX7KlCmMHDnS2oo9derUTCXL8+fPJyQkhKpVqwLwxBNPMGPGDGuSPm/ePM6fP8/WrVutPyBUqFDBuv97773HE088wZgxY6xlN78fmTV06FC6dOmSqmz48OHW7SFDhrB8+XK+//576tevT3R0NJ9++ilTp06lT58+AAQHB/PAAw8A0KVLFwYPHsyPP/7I448/Dlh6JPTt21fLpkn+pJndRURE8oeUddKvRULiVXB2t208eViWM8AjR47QuXNndu3ahclksv5CnpJAmM22aXWWjNWtWzfV45iYGEaPHs2yZcsIDw8nKSmJq1evcuLEiQyPU6NGDeu2p6cnPj4+RERE3La+h4eHNUEHCAwMtNaPjIzk3Llz1hZmAEdHR8LCwqwt3rczc+ZMevbsaX3cs2dPmjZtypQpU/D29mbHjh3Url37ti38O3bsYODAgRmeIzNufV/NZjPjxo3j+++/5/Tp0yQkJBAfH4+HhwcA+/btIz4+ngcffDDd47m5uVm77z/++ONs376d3bt3pxpWIJKvxF603CtJt0snT57EZDJZZ2PfsmUL8+bNo0qVKgWmR52IiGSSmy84uUHSNUuX9yJ3nihU0pflJP3FF1+kXLlyrF69mnLlyrFlyxYuXrzIyy+/zMSJE3MiRptyd3Zk79jWNjt3drl1lvbhw4ezcuVKJk6cSIUKFXB3d+fRRx9N0zPiVrdOjGYymTJMqNOrn5mujxnZu3cvf/31F1u2bEk1WZzZbGb+/PkMHDjQOhnR7dzp+fTiTExMTFPv1vd1woQJfPrpp0yaNInq1avj6enJ0KFDre/rnc4Lli7vtWrV4tSpU8yaNYsWLVpQpkyZO+4nkifFnrfcq7u7XXryySd5+umn6dWrF2fPnqVVq1ZUrVqVuXPncvbsWUaNGmXrEEVExF6YTJYZ3q8ch5hzStLvQZYnjtu0aRNjx46lWLFiODg44ODgwAMPPMD48eN54YUXciJGmzKZTHi4ONnklpPdmzds2EDfvn3p3Lkz1atXp3jx4hw7dizHzpceX19fAgIC2Lp1q7XMbDazffv2DPebMWMGTZo0YefOnezYscN6GzZsGDNmzAAsLf47duzg0qVL6R6jRo0aGU7E5ufnl2oSo4MHDxIXF3fH17RhwwY6duxIz549qVmzJuXLl+e///6zPh8SEoK7u3uG565evTp169blyy+/ZN68efTr1++O5xXJs9Td3a7t3r3b2tvp+++/p1q1amzcuJG5c+daJ9IUERGx0gzv2SLLSbrZbMbb2xuAYsWKcebMGQDKlCmTatkvsW8hISEsXryYHTt2sHPnTp588sk7djHPCUOGDGH8+PH8+OOPHDhwgBdffJHLly/f9geKxMREvvnmG7p37061atVS3QYMGMDmzZvZs2cP3bt3p3jx4nTq1IkNGzZw5MgRfvjhBzZt2gRY1o7/7rvvePvtt9m3bx+7du3igw8+sJ6nRYsWTJ06lX/++Ydt27bx7LPPZmp5tZCQEFauXMnGjRvZt28fzzzzDOfOnbM+7+bmxogRI3j11VeZM2cOhw8f5q+//rL+uJBiwIABvP/++xiGkWrWeZF8RxPH2bXExETrJJarVq2yzjtTuXLlVD9kioiIAJrhPZtkOUmvVq0aO3fuBKBBgwZ8+OGHbNiwgbFjx1K+fPlsD1Byxscff0zhwoVp1KgRHTp0oHXr1tSpUyfX4xgxYgTdu3end+/eNGzYEC8vL1q3bo2bm1u69X/66ScuXryYbuIaGhpKaGgoM2bMwMXFhRUrVuDv70+7du2oXr0677//Po6OliEEzZo1Y+HChfz000/UqlWLFi1asGXLFuuxPvroI4KCgmjcuDFPPvkkw4cPt44rz8ibb75JnTp1aN26Nc2aNbP+UHCzt956i5dffplRo0YRGhpKt27d0ozr7969O05OTnTv3v2274VIvhCXMiZdS7DZo6pVqzJ9+nT+/PNPVq5caZ0k9MyZMxQtqr+ZiIjcIqUlXTO83xOTkcUBwsuXLyc2NpYuXbpw6NAhHn74Yf777z+KFi3KggUL7H6ZqKioKHx9fYmMjMTHxyfVc9euXePo0aOUK1dOiZGNJCcnExoayuOPP84777xj63Bs5tixYwQHB7N169Yc+fFEn3WxGxMrWb7In14HJWrZOhqbyei7yZbWrl1L586diYqKok+fPsycOROA119/nf3797N48WIbR3h79vqeiojka39+DKvHQM3u0Hm6raOxK1n5XsryxHGtW9+YRK1ChQrs37+fS5cuUbhwYS0RJVl2/PhxVqxYQdOmTYmPj2fq1KkcPXqUJ5980tah2URiYiIXL17kzTff5L777rNJ7waRXGMYN7Wkq7u7PWrWrBkXLlwgKiqKwoULW8uffvrpTPUuEhGRAiZlGTZ1d78nWerunpiYiJOTE7t3705VXqRIESXoclccHByYPXs29erV4/7772fXrl2sWrWK0NBQW4dmExs2bCAwMJCtW7cyfbp+fZR87lokJF9fNUFj0u3S1atXiY+Ptybox48fZ9KkSRw4cAB/f38bRyciInYnJUmPOZdxPclQllrSnZ2dKV26tNZCl2wTFBTEhg0bbB2G3WjWrNk9L1EnkmektKK7eIGzhl3Yo44dO9KlSxeeffZZrly5QoMGDXB2dubChQt8/PHHPPfcc7YOUURE7IlXSku6Jhe9F1meOO6NN97g9ddfv+3SViIiIplindldE5DZq+3bt9O4cWMAFi1aREBAAMePH2fOnDlMnjzZxtGJiIjdSWlJv3oZkuJtG0seluUx6VOnTuXQoUOUKFGCMmXK4Onpmer5O61xLSIiAmiN9DwgLi7OuuzqihUr6NKlCw4ODtx3330cP37cxtGJiIjdcS8Mji5gTrB0eS9U2tYR5UlZTtJvXU5KRETkrqS0pHv62TYOua0KFSqwdOlSOnfuzPLly3nppZcAiIiI0IzpIiKSlslk6fIeecIyeZyS9LuS5ST97bffzok4RESkoIk9b7nXpHF2a9SoUTz55JO89NJLtGjRgoYNGwKWVvXatWvbODoREbFL3jcl6XJXspyki4iIZAvr8msak26vHn30UR544AHCw8OpWbOmtfzBBx+kc+fONoxMRETslneA5V5J+l3LcpLu4OCQ4XJrmvldREQyxTpxnFrS7Vnx4sUpXrw4p06dAqBUqVLUr1/fxlGJiIjdSpnhPUZJ+t3K8uzuS5YsYfHixdbbggULeO211wgMDOSLL77IiRgllzRr1oyhQ4daH5ctW5ZJkyZluI/JZGLp0qX3fO7sOo6I5CGaOM7uJScnM3bsWHx9fSlTpgxlypShUKFCvPPOOyQnJ9s6PBERsUcpM7xHa630u5XllvSOHTumKXv00UepWrUqCxYsoH///tkSmGRehw4dSExM5Lfffkvz3J9//kmTJk3YuXMnNWrUyNJxt27dmmb2/ns1evRoli5dyo4dO1KVh4eHU7hw4Ww91+1cvXqVkiVL4uDgwOnTp3F1dc2V84rILdSSbvfeeOMNZsyYwfvvv8/9998PwPr16xk9ejTXrl3jvffes3GEIiJid7y1Vvq9yrYx6ffddx9PP/10dh1OsqB///507dqVU6dOUapUqVTPzZo1i7p162Y5QQfw88u9GZeLFy+ea+f64YcfqFq1KoZhsHTpUrp165Zr576VYRiYzWacnDQ9hBRAGpNu977++mu++uorHnnkEWtZjRo1KFmyJM8//7ySdBERScva3V0t6Xcry93d03P16lUmT55MyZIls+NwkkUPP/wwfn5+zJ49O1V5TEwMCxcupH///ly8eJHu3btTsmRJPDw8qF69Ot99912Gx721u/vBgwdp0qQJbm5uVKlShZUrV6bZZ8SIEVSsWBEPDw/Kly/PW2+9RWJiIgCzZ89mzJgx7Ny5E5PJhMlkssZ8a3f3Xbt20aJFC9zd3SlatChPP/00MTEx1uf79u1Lp06dmDhxIoGBgRQtWpRBgwZZz5WRGTNm0LNnT3r27MmMGTPSPL9nzx4efvhhfHx88Pb2pnHjxhw+fNj6/MyZM6latSqurq4EBgYyePBgAI4dO4bJZErVS+DKlSuYTCbWrl0LwNq1azGZTPz666+EhYXh6urK+vXrOXz4MB07diQgIAAvLy/q1avHqlWrUsUVHx/PiBEjCAoKwtXVlQoVKjBjxgwMw6BChQpMnDgxVf0dO3ZgMpk4dOjQHd8TkVxnGGpJzwMuXbpE5cqV05RXrlyZS5cu2SAiERGxe9aWdI1Jv1tZbr4rXLhwqonjDMMgOjoaDw8Pvv3222wNzi4YBiTG2ebczh6WtQbvwMnJid69ezN79mzeeOMN699n4cKFmM1munfvTkxMDGFhYYwYMQIfHx+WLVtGr169CA4OztQEQMnJyXTp0oWAgAA2b95MZGRkqvHrKby9vZk9ezYlSpRg165dDBw4EG9vb1599VW6devG7t27+e2336wJqK+vb5pjxMbG0rp1axo2bMjWrVuJiIhgwIABDB48ONUPEWvWrCEwMJA1a9Zw6NAhunXrRq1atRg4cOBtX8fhw4fZtGkTixcvxjAMXnrpJY4fP06ZMmUAOH36NE2aNKFZs2b8/vvv+Pj4sGHDBpKSkgCYNm0aw4YN4/3336dt27ZERkayYcOGO75/t3rttdeYOHEi5cuXp3Dhwpw8eZJ27drx3nvv4erqypw5c+jQoQMHDhygdGnL+pK9e/dm06ZNTJ48mZo1a3L06FEuXLiAyWSiX79+zJo1i+HDh1vPMWvWLJo0aUKFChWyHJ9IjkuIAXO8ZVtj0u1WzZo1mTp1KpMnT05VPnXq1LvqoSUiIgVASpIedwGSEsDJxbbx5EFZTtI/+eSTVEm6g4MDfn5+NGjQINfGFOeqxDgYV8I25379DLhkbkx4v379mDBhAuvWraNZs2aAJUnr2rUrvr6++Pr6pkrghgwZwvLly/n+++8zlaSvWrWK/fv3s3z5ckqUsLwf48aNo23btqnqvfnmm9btsmXLMnz4cObPn8+rr76Ku7s7Xl5eODk5Zdi9fd68eVy7do05c+ZYx8RPnTqVDh068MEHHxAQYFnWoXDhwkydOhVHR0cqV65M+/btWb16dYZJ+syZM2nbtq31s9q6dWtmzZrF6NGjAfjss8/w9fVl/vz5ODs7A1CxYkXr/u+++y4vv/wyL774orWsXr16d3z/bjV27FhatWplfVykSJFUyxu98847LFmyhJ9++onBgwfz33//8f3337Ny5UpatmwJQPny5a31+/bty6hRo9iyZQv169cnMTGRefPmpWldF7EbKa3oTu6Z/n9Oct+HH35I+/btWbVqlXWN9E2bNnHy5El++eUXG0cnIiJ2yb0IODhDciLERoBvqTvvI6lkubt737596dOnj/XWq1cv2rRpkz8T9DykcuXKNGrUiJkzZwJw6NAh/vzzT+tEfmazmXfeeYfq1atTpEgRvLy8WL58OSdOnMjU8fft20dQUJA1QQesF2w3W7BgAffffz/FixfHy8uLN998M9PnuPlcNWvWTDVp3f33309ycjIHDhywllWtWhVHR0fr48DAQCIiIm57XLPZzNdff03Pnj2tZT179mT27NnWWYp37NhB48aNrQn6zSIiIjhz5gwPPvhgll5PeurWrZvqcUxMDMOHDyc0NJRChQrh5eXFvn37rO/djh07cHR0pGnTpuker0SJErRv39769//555+Jj4/nscceu+dYRXKEdTy6WtHtWdOmTfnvv//o3LkzV65c4cqVK3Tp0oU9e/bwzTff2Do8ERGxRw4O4KW10u9FllvSZ82ahZeXV5qL/4ULFxIXF0efPn2yLTi74OxhadG21bmzoH///gwZMoTPPvuMWbNmERwcbE3qJkyYwKeffsqkSZOoXr06np6eDB06lISEhGwLd9OmTfTo0YMxY8bQunVra4v0Rx99lG3nuNmtibTJZMpwSaDly5dz+vTpNBPFmc1mVq9eTatWrXB3d7/t/hk9B5ZeJWAZApLidmPkb501f/jw4axcuZKJEydSoUIF3N3defTRR61/nzudG2DAgAH06tWLTz75hFmzZtGtWzc8PLL2GRLJNdbx6Jo0zt6VKFEizQRxO3fuZMaMGVp6VURE0ucdAFGnlKTfpSy3pI8fP55ixdK2fPj7+zNu3LhsCcqumEyWrpi2uGViPPrNHn/8cRwcHJg3bx5z5syhX79+1qEJGzZsoGPHjvTs2ZOaNWtSvnx5/vvvv0wfOzQ0lJMnTxIefmMphb/++itVnY0bN1KmTBneeOMN6tatS0hICMePH09Vx8XFBbPZfMdz7dy5k9jYWGvZhg0bcHBwoFKlSpmO+VYzZszgiSeeYMeOHaluTzzxhHUCuRo1avDnn3+mm1x7e3tTtmxZVq9ene7xU2bDv/k9unWpudvZsGEDffv2pXPnzlSvXp3ixYtz7Ngx6/PVq1cnOTmZdevW3fYY7dq1w9PTk2nTpvHbb7/Rr1+/TJ1bxCZiz1vuPXNvFQkRERHJJd6BlvsYJel3I8tJ+okTJyhXrlya8jJlymS5W7NkLy8vL7p168bIkSMJDw+nb9++1udCQkJYuXIlGzduZN++fTzzzDOcO5f5ZRFatmxJxYoV6dOnDzt37uTPP//kjTfeSFUnJCSEEydOMH/+fA4fPszkyZNZsmRJqjply5bl6NGj7NixgwsXLhAfH5/mXD169MDNzY0+ffqwe/du1qxZw5AhQ+jVq5d1PHpWnT9/np9//pk+ffpQrVq1VLfevXuzdOlSLl26xODBg4mKiuKJJ55g27ZtHDx4kG+++cbazX706NF89NFHTJ48mYMHD7J9+3amTJkCWFq777vvPt5//3327dvHunXrUo3Rz0hISAiLFy9mx44d7Ny5kyeffDJVr4CyZcvSp08f+vXrx9KlSzl69Chr167l+++/t9ZxdHSkb9++jBw5kpCQkHSHI4jYjbjrLenq7i4iIpL/qLv7Pclyku7v78+///6bpnznzp0ULapui7bWv39/Ll++TOvWrVONH3/zzTepU6cOrVu3plmzZhQvXpxOnTpl+rgODg4sWbKEq1evUr9+fQYMGJCm++MjjzzCSy+9xODBg6lVqxYbN27krbfeSlWna9eutGnThubNm+Pn55fuMnAeHh4sX76cS5cuUa9ePR599FEefPBBpk6dmrU34yYpk9ClN578wQcfxN3dnW+//ZaiRYvy+++/ExMTQ9OmTQkLC+PLL7+0dq3v06cPkyZN4v/+7/+oWrUqDz/8MAcPHrQea+bMmSQlJREWFsbQoUN59913MxXfxx9/TOHChWnUqBEdOnSgdevW1KlTJ1WdadOm8eijj/L8889TuXJlBg4cmKq3AVj+/gkJCTz11FNZfYtEcpe6u4uIiORfWobtnpiMmwfQZsKIESNYsGCBdXkngHXr1tGvXz8effRRu59NOioqCl9fXyIjI/Hx8Un13LVr1zh69CjlypXDzc3NRhGK3L0///yTBx98kJMnT2bY60CfdbG5Jc/Czu+g5Wh44CVbR2NzGX032UKXLl0yfP7KlSusW7fujsOXbMne3lMRkQJl+xz4aQiEPAQ9Fto6GruQle+lLE8c984773Ds2DEefPBBnJwsuycnJ9O7d+/8OSZdJA+Ij4/n/PnzjB49mscee+yuhwWI5BprS7q6u9sjX1/fOz7fu3fvXIpGRETyHK+UlvTwjOtJurKcpLu4uLBgwQLeffddduzYgbu7O9WrV6dMmTI5EZ+IZMJ3331H//79qVWrFnPmzLF1OCJ3pjHpdm3WrFm2DkFERPIya3f3zM+BJTdkOUlPERISQkhISHbGIiJ3qW/fvqkmChSxe7HX10lXS7qIiEj+k5Kkx54HcxI43nXaWSBleeK4rl278sEHH6Qp//DDD9OsnS4iIpIua0u6Jo4TERHJdzyKgckRMCA2wtbR5DlZTtL/+OMP2rVrl6a8bdu2/PHHH9kSlK1lcS49kTxHn3GxqYQ4SIyzbKslXUREJP9xcNAybPcgy0l6TEwMLi4uacqdnZ2JiorKlqBsJWWZrbi4OBtHIpKzUj7jKZ95kVyV0oru6AKu3raNRURERHKGt5L0u5XlwQHVq1dnwYIFjBo1KlX5/PnzqVKlSrYFZguOjo4UKlSIiAhLlwwPDw9MJpONoxLJPoZhEBcXR0REBIUKFcLR0dHWIUlBdPPM7vo/VkREJH9KmeE9Rkl6VmU5SX/rrbfo0qULhw8fpkWLFgCsXr2aefPmsWjRomwPMLcVL275MKUk6iL5UaFChayfdZFcF3d90jjN7C4iIpJ/aYb3u5blJL1Dhw4sXbqUcePGsWjRItzd3alZsya///47RYoUyYkYc5XJZCIwMBB/f38SExNtHY5ItnN2dlYLuthW7HnLvZJ0ERGR/Mtba6XfrbuaC799+/a0b98egKioKL777juGDx/O33//jdlsztYAbcXR0VGJjIhITri5u7uIiIjkTylJeoxa0rMqyxPHpfjjjz/o06cPJUqU4KOPPqJFixb89ddf2RmbiIjkR9bl15Ski4iI5FspY9I1cVyWZSlJP3v2LO+//z4hISE89thj+Pj4EB8fz9KlS3n//fepV69eTsUpIiL5Rez1MekeWiO9IPjjjz/o0KEDJUqUwGQysXTp0gzrr127FpPJlOZ29qwu8kRE8hTN7n7XMp2kd+jQgUqVKvHvv/8yadIkzpw5w5QpU3IyNhERyY/Ukl6gxMbGUrNmTT777LMs7XfgwAHCw8OtN39//xyKUEREcoR3oOU+NgKS88eQ6NyS6THpv/76Ky+88ALPPfccISEhORmTiIjkZxqTXqC0bduWtm3bZnk/f39/ChUqlP0BiYhI7vD0A5MDGMmWSWO9tbJQZmW6JX39+vVER0cTFhZGgwYNmDp1KhcuXMjJ2EREJD9SS7pkQq1atQgMDKRVq1Zs2LDhjvXj4+OJiopKdRMRERtycLQk6qAu71mU6ST9vvvu48svvyQ8PJxnnnmG+fPnU6JECZKTk1m5ciXR0dE5GaeIiOQX1jHpStIlrcDAQKZPn84PP/zADz/8QFBQEM2aNWP79u0Z7jd+/Hh8fX2tt6CgoFyKWEREbkszvN+VLM/u7unpSb9+/Vi/fj27du3i5Zdf5v3338ff359HHnkkJ2IUEZH8IikeEq7/qOupieMkrUqVKvHMM88QFhZGo0aNmDlzJo0aNeKTTz7JcL+RI0cSGRlpvZ08eTKXIhYRkdvy0lrpd+Oul2ADyxfphx9+yKlTp/juu++yKyYREcmvUsajOziBWyGbhiJ5R/369Tl06FCGdVxdXfHx8Ul1ExERG0tpSY9WS3pW3FOSnsLR0ZFOnTrx008/ZcfhREQkv4q7adI4k8m2sUiesWPHDgIDA20dhoiIZJW1u7vGpGdFpmd3FxERuWex5y33mjSuwIiJiUnVCn706FF27NhBkSJFKF26NCNHjuT06dPMmTMHgEmTJlGuXDmqVq3KtWvX+Oqrr/j9999ZsWKFrV6CiIjcLS+tlX43lKSLiEjusU4ap/HoBcW2bdto3ry59fGwYcMA6NOnD7NnzyY8PJwTJ05Yn09ISODll1/m9OnTeHh4UKNGDVatWpXqGCIikkekrJWuJD1LlKSLiEju0fJrBU6zZs0wDOO2z8+ePTvV41dffZVXX301h6MSEZFc4a2W9LuRLWPSRUREMiX2pjHpIiIikr+lzO4eGwHJybaNJQ9Rki4iIrlHLekiIiIFh5c/YILkJIi7aOto8gwl6SIikns0Jl1ERKTgcHS+8cO81krPNCXpIiKSe9SSLiIiUrBYl2HTWumZpSRdRERyj8aki4iIFCwp49I1eVymKUkXEZHco5Z0ERGRgkUzvGeZknQREckd5kS4FmnZVku6iIhIwZCyVnqMkvTMUpIuIiK5I2VWV5MDuBe2bSwiIiKSO7zUkp5VStJFRCR3xJ633HsUBQd9/YiIiBQI3hqTnlW6ShIRkdyhSeNEREQKHmt3d83unllK0kVEJHekdHfXpHEiIiIFx83d3Q3DtrHkEUrSRUQkd1hb0ovaNg4RERHJPSlJenIixF2ybSx5hJJ0ERHJHVp+TUREpOBxcrnxA71meM8UJekiIpI7NCZdRESkYPJKmTwu3LZx5BFK0kVEJHeoJV1ERKRgss7wrsnjMsMukvTPPvuMsmXL4ubmRoMGDdiyZUum9ps/fz4mk4lOnTrlbIAiInLvYq9PHKcx6SIiIgVLSpKu7u6ZYvMkfcGCBQwbNoy3336b7du3U7NmTVq3bk1ERESG+x07dozhw4fTuHHjXIpURETuiVrSRURECqabZ3iXO7J5kv7xxx8zcOBAnnrqKapUqcL06dPx8PBg5syZt93HbDbTo0cPxowZQ/ny5XMxWhERuWsaky4iIlIwpayVriQ9U2yapCckJPD333/TsmVLa5mDgwMtW7Zk06ZNt91v7Nix+Pv7079//zueIz4+nqioqFQ3ERHJZeYkuHrZsq2WdBERkYLFWy3pWWHTJP3ChQuYzWYCAgJSlQcEBHD2bPp/wPXr1zNjxgy+/PLLTJ1j/Pjx+Pr6Wm9BQUH3HLeIiGTR1UuAAZjAvYitoxEREZHclNKSrjHpmWLz7u5ZER0dTa9evfjyyy8pVixzLTEjR44kMjLSejt58mQORykiImmkdHV3LwyOTraNRURERHKXdUz6OTAM28aSB9j0SqlYsWI4Ojpy7lzqqfjPnTtH8eLF09Q/fPgwx44do0OHDtay5ORkAJycnDhw4ADBwcGp9nF1dcXV1TUHohcRkUzTpHEiIiIFV0qSbo63DH/zUK+6jNi0Jd3FxYWwsDBWr15tLUtOTmb16tU0bNgwTf3KlSuza9cuduzYYb098sgjNG/enB07dqgru4iIvdKkcSIiIgWXs5ulNx1AjNZKvxOb9zkcNmwYffr0oW7dutSvX59JkyYRGxvLU089BUDv3r0pWbIk48ePx83NjWrVqqXav1ChQgBpykVExI7EXV8j3VNrpIuIiBRIXsUtrejRZ8E/1NbR2DWbJ+ndunXj/PnzjBo1irNnz1KrVi1+++0362RyJ06cwMEhTw2dFxGRW6klXUREpGDzDoDz+zTDeybYPEkHGDx4MIMHD073ubVr12a47+zZs7M/IBERyV4aky4iIlKwaYb3TFMTtYiI5Dy1pIuIiBRsXlorPbOUpIuISM6zjklXki4iIlIgpbSkK0m/IyXpIiKS86wt6Zo4TkREpEDyvt6Srtnd70hJuoiI5DyNSRcRESnYvIpb7qPDbRtHHqAkXUREclZy8k3d3f1sG4uIiIjYhndKkn4ODMO2sdg5JekiIpKzrl4GI9myre7uIiIiBVNKkp50FeKjbBuLnVOSLiIiOSulq7ubLzg62zYWERERsQ1nd3D1tWxr8rgMKUkXEZGcpeXXREREBG7q8q4kPSNK0kVEJGdp0jgRERGBGzO8K0nPkJJ0ERHJWWpJFxEREbixVnqMkvSMKEkXEZGcZZ3ZXZPGiYiIFGheKS3pWis9I0rSRUQkZ6klXUREROCmMelaKz0jStJFRCRnaUx6gfbHH3/QoUMHSpQogclkYunSpXfcZ+3atdSpUwdXV1cqVKjA7NmzczxOkf9v787joyoP/Y9/Z7JMFrJByAaRXWQNFDGNS1XMNaCt0GpdXl5Br0u14E/L7atKbxW999cfWqtyqxRsK9K+rGtvRa+28AIKqBBF2aUQEZFFmBCE7Hvm+f0xCxmSCQnMZE4mn/er53VmznnO8Dw5kz5+85zzHADdwBvSqxlJ7wghHQAQWoyk92o1NTXKy8vTokWLOlV+//79uvbaa3XllVdq27ZtevDBB3XXXXdp5cqVIa4pACDk+jC7e2dEh7sCAIAIxz3pvdq0adM0bdq0TpdfsmSJhgwZoqefflqSNGrUKH344Yd69tlnVVRUFKpqAgC6A49g6xRG0gEAoVVT5l4zko5OKC4uVmFhod+2oqIiFRcXd3hcQ0ODKisr/RYAgMV4J45rqpEaqsJbFwsjpAMAQseYViPp/cNbF/QITqdTmZmZftsyMzNVWVmpurq6gMctWLBAKSkpviU3NzfUVQUAdJWjjxSb5H7NDO8BEdIBAKFTXy65mt2vmTgOITRv3jxVVFT4lkOHDoW7SgCA9jDD+xlxTzoAIHRqPKPosUlStCO8dUGPkJWVpdJS/9GV0tJSJScnKz4+PuBxDodDDgffMQCwvKQs6Zu9zPDeAUbSAQCh43v8GpPGoXMKCgq0Zs0av22rVq1SQUFBmGoEAAgq733pjKQHREgHAIQOj1/r9aqrq7Vt2zZt27ZNkvsRa9u2bdPBgwcluS9Tnzlzpq/8vffeqy+//FI/+9nPtGfPHv32t7/VG2+8oZ/85CfhqD4AINiY4f2MCOkAgNDxjaQT0nurTz/9VBMnTtTEiRMlSXPnztXEiRP16KOPSpKOHj3qC+ySNGTIEL333ntatWqV8vLy9PTTT+sPf/gDj18DgEjhDelc7h4Q96QDAEKHkfRe74orrpAxJuD+ZcuWtXvM1q1bQ1grAEDY9GEk/UwYSQcAhI7v8Wvckw4AAMTl7p1ASAcAhA4j6QAAoDUudz8jQjoAIHS4Jx0AALTmDekNlVJjTXjrYlGEdABA6NSUudeMpAMAAElyJEkxie7XXPLeLkI6ACB0arz3pBPSAQCAR5L3WemE9PYQ0gEAoWEMl7sDAIC2krLd62pCensI6QCA0Giokloa3a+53B0AAHj18Y6kM3lcewjpAIDQ8I6ixyRIsQnhrQsAALAO32PYjoa3HhZFSAcAhIb3fnRG0QEAQGs8hq1DhHQAQGj47kfvF956AAAAa+njHUnnnvT2ENIBAKFR4wnpjKQDAIDWkgjpHSGkAwBCg5ndAQBAe3yXuxPS20NIBwCEhm8kncvdAQBAK97Z3esrpKa68NbFggjpAIDQqPVMHMdIOgAAaC0uRYqOd7/mkvc2COkAgNDgnnQAANAem01K8oymM8N7G4R0AEBo1JS514n9w1sPAABgPX14VnoghHQAQGhwuTsAAAjEN8M7I+mnI6QDAEKDieMAAEAg3pBecUgyJrx1sZjocFcAABCBGmukZs9srYykAwCA03lDevHz0s43pewJUs7EU4v3nvVeiJAOAAg+7yh6lEOK7RPeugAAAOsZPV3659vS0R3uyeP2rnQvXkk5Us6EU6E9e4LUp3fMc0NIBwAEX60npCemu2dwBQAAaK3vUOmedVJjrVT6mXRk66mlrESqOiKVHJFK/nbqmJRcKTvPf8Q9oW/YmhAqhHQAQPDVeCaN4350AADQkdgEKfci9+LVUC05d54K7Ue3Scf3uu9frzgk7Xn3VNnU8/xDe/IAKTpOiok/tbZHdXuzzgUhHQAQfK1H0gEAALrC0UcaVOBevOorJecOT3Df5l6f2CeVH3Qv/3w78OfZYzyBPU6KjvesTwvybdaOU2Un3S7FpYS61T6EdABA8PlmdiekAwCAIIhLlgZf6l686sqlo9vdI+3e8F53Qmqql1oaTpVzNUmNTVJj1dn92+N+SEgHAPRwjKQDAIBQi0+Vhl7uXk7ncknN9e6lqa79dXO9O9A313W8diR1a7MI6QCA4OOedAAAEE52u/t+99iEcNeky+zhrgAAIALVlLnXib3jUSkAAADBQkgHAAQfl7sDAACcFUI6ACD4mDgOAADgrBDSAQDBV+u5J52RdAAAgC4hpAMAgqupXmqsdr9m4jgAAIAuYXZ3AEBwee9Ht8d06zNFAQBAz1FR26R3dx5RH0e0RmYlaWh6H8VGM4YsEdIBAMHmux+9n2SzhbcuAADAUowx+ttOp+a/s0vHqxt826PtNg1JT9T5WUkamZmkkZ51bt8ERdl7139PENIBAMHFzO4AAKAdRyvq9MjyXVq9u1SSNCQ9Uf0SY1VSWqWq+mbtPVatvceq9Z6O+o6Ji7FrREaSzs9M0sisPp51krKS42SL0MEAQjoAILhqPJPGcT86AACQ5HIZ/fnjA3pyRYmqG5oVE2XTj68Yrh9fOUyO6CgZY+SsrFeJs8q9lFbp89Iq7S2tVn2TSzu/rtDOryv8PjMpLvrUiHuWJ8RnJiktMTZMrQweQjoAILgYSQcAAB57S6s076879emBk5Kkieel6snrx+v8zCRfGZvNpuyUeGWnxOuKkRm+7S0uo4MnalXidIf2klJ3iN9/vEZV9c369MBJ3+d65eWm6sGrRuiKkf177Eg7IR0AEFw8Ix0AgF6voblFi9ft02/X7lNji0uJsVH62dQL9K/fHtTpe8yjPPepD0lP1NSxWX6f/WVZjTu4twrwh07Uafuhct2x7BPlDUzRg4Xn98iwTkgHAARXTZl7zUg6AAC90uYDJ/Xw/+zQ3mPuR7JOuSBD/3fGWOWkxgfl8x3RURqVnaxR2cl+28uqGvSHD77Un4oPaPvhCndYz03Vg4UjdMX5PSesE9IBAMFV67knnZAOAECvUt3QrKdW7NGfPjogY6R+ibF67Lox+u747G4JyP2THJp3zSjd/Z2h+v37nrB+qFx3vNSzwjoPogMABBeXu6MdixYt0uDBgxUXF6f8/Hxt2rQpYNlly5bJZrP5LXFxcd1YWwBAV63ZXap/eWa9/ljsDug3TBqo1XMv1/fycro9FKf3cYf1Dx66Uvd8Z6jiYuy+sD7jtxu1tuSYjDHdWqeuYCQdABBcTByH07z++uuaO3eulixZovz8fC1cuFBFRUUqKSlRRkZGu8ckJyerpKTE997qox4A0FuVVTXo8f/dpXd3uB+bdl7fBP2/74/TpSPC/98B6X0c+vk1o3T3ZUP1+w++1J+Kv/KF9QmekfXLLTiyzkg6ACC4fI9gC3/nDGt45plndPfdd+uOO+7Q6NGjtWTJEiUkJGjp0qUBj7HZbMrKyvItmZmZ3VhjAMCZGGP0xqeHVPjMer2746jsNulH3xmqlQ9+xxIBvbX+Se6w/sHPpujuy4YoLsaubYfKdftLn+j7v92odRYbWSekAwCCp7lRavA8x5SRdEhqbGzU5s2bVVhY6Ntmt9tVWFio4uLigMdVV1dr0KBBys3N1fTp07Vr164O/52GhgZVVlb6LQCA0DjwTY3+9cWP9bO/7FBFXZPG5CTrnTmXat41oxQfGxXu6gXUP8mh/7h2dLth/QeLN2r952WWCOuEdABA8HgnjbNFSXGpYa0KrOH48eNqaWlpMxKemZkpp9PZ7jEjR47U0qVL9fbbb+vll1+Wy+XSxRdfrMOHDwf8dxYsWKCUlBTfkpubG9R2AACk5haXXli/T0UL39eGL76RI9quedMu0NuzL9HYASnhrl6necP6+z+7Undd6g7rWw+Wa9bSTZYI64R0AEDweO9HT+gr2elicHYKCgo0c+ZMTZgwQZdffrn++te/qn///nrhhRcCHjNv3jxVVFT4lkOHDnVjjQEg8n361QlNX7RBC/6+R/VNLl08rJ9WPvgd/ejyYYqO6pl9fkZSnH7xXeuFdUv8NLsy4+vvf/97XXbZZUpLS1NaWpoKCws7LA8A6EbM7I7TpKenKyoqSqWlpX7bS0tLlZWV1anPiImJ0cSJE/XFF18ELONwOJScnOy3AADO3eYDJ3Tbix/rhiXF2nWkUinxMfrVDeP157vyNTg9MdzVC4rTw7oj+lRYv37xRn1eWtWt9Ql7SPfO+Dp//nxt2bJFeXl5Kioq0rFjx9otv27dOt1yyy1au3atiouLlZubq6uvvlpff/11N9ccANAGz0jHaWJjYzVp0iStWbPGt83lcmnNmjUqKCjo1Ge0tLRo586dys7ODlU1AQCn2XzgpG578WNdv7hYH+w9rmi7TTdPztWqud/RjRfmWm5G9GDwhvUPHrpSd3rC+q4jlUqNj+nWethMmO+Mz8/P1+TJk/X8889Lcnfcubm5uv/++/Xwww+f8fiWlhalpaXp+eef18yZM89YvrKyUikpKaqoqOCv7AAQbB8tkVY8JI2eId34x3DXpseI9L7p9ddf16xZs/TCCy/ooosu0sKFC/XGG29oz549yszM1MyZMzVgwAAtWLBAkvSf//mf+va3v63hw4ervLxcTz31lJYvX67Nmzdr9OjRnfo3I/1nCgChsvnASS1c/bk+2Ou+Oi7abtMNkwZq9pXDlds3Icy1617Hquq19WC5isZ07sqvjnSlXwrrc9K9M77OmzfPt60zM762Vltbq6amJvXt27fd/Q0NDWpoaPC9Z7ZXAAihmjL3mpF0tHLTTTeprKxMjz76qJxOpyZMmKAVK1b4JpM7ePCg7K3mMDh58qTuvvtuOZ1OpaWladKkSdq4cWOnAzoAoOu2HDyphav36v3P3X15lN2mG77lDufn9etd4dwrIykuKAG9q8Ia0jua8XXPnj2d+oyHHnpIOTk5fo92aW3BggV6/PHHz7muAIBO8E4cl9g/vPWA5cyZM0dz5sxpd9+6dev83j/77LN69tlnu6FWAICtnnC+nnBuGWEN6efqiSee0GuvvaZ169YpLi6u3TLz5s3T3Llzfe8rKyt5LAsAhIpv4rh+4a0HAADo0LZD5Vq4+nOtKzkVzq//1gDNuXIE4TzMwhrSz2XG11//+td64okntHr1ao0fPz5gOYfDIYfDEZT6AgDOgInjAACwtG2HyvXfqz/X2lbh/AcTB2jOlOEa1C8yZmvv6cIa0lvP+DpjxgxJp2Z8DXRJnCT96le/0i9/+UutXLlSF154YTfVFgBwRjyCDQAAS9ruGTlvHc6/P3GA7iecW07YL3efO3euZs2apQsvvNA342tNTY3uuOMOSWoz4+uTTz6pRx99VK+88ooGDx4sp9MpSerTp4/69OkTtnYAANTqnnRCOgAAVrD9ULn+e81e/WOP+xHX3nA+58rhEfOc80gT9pDe1RlfFy9erMbGRt1www1+nzN//nw99thj3Vl1AEBrLc1S3Un3a0bSAQAIG5fL6NMDJ/XC+n1a4wnndpv0/YkDdf8UwrnVhT2kS12b8fWrr74KfYUAAF1Xd8LzwiYltP9YTAAAEDolziot3/a13tl2RF+X10lyh/MZEwfo/ikjNIRw3iNYIqQDACKA9370+DTJHhXeugAA0Et8XV6nd7Yd0dvbvtYeZ5Vvex9HtK4dl617rxhGOO9hCOkAgODgfnQAALpFeW2j/rbTqeXbvtam/Sd822OibLpiZIZmTBigq0ZlKC6GP5r3RIR0AEBwMLM7AAAhU9/UotW7S7V86xGt//yYmlqMb1/+kL6aMXGApo3NUmpCbBhriWAgpAMAgsMb0hP7hbceAABEiOYWlzbu+0Zvbzuilbucqm5o9u27ICtJMyYO0HV5OcpJjQ9jLRFshHQAQHD4LnfvH956AADQgxljtONwhZZv+1r/u/2ojlc3+PYNSI3X9Ak5mj5hgEZmJYWxlgglQjoAIDi43B0AgLP2ZVm13t52RO9sP6L9x2t821MTYnTtuGzNmDhAk85Lk91uC2Mt0R0I6QCA4GDiOAAAOs0Yo91Hq7Ril1MrPjuqz0urffviYuz6l9FZmjEhR5eN6K/YaHsYa4ruRkgHAARHzTfudQL3pAMA0B6Xy2jroXKt3OXUis+cOnii1rcv2m7TxcPTNWNCjq4ek6U+DqJab8WZBwAEByPpAAC00dzi0qb9J7Ril1MrdzlVWnnqHnNHtF2Xn99fU8dm6aoLMpWSEBPGmsIqCOkAgODgnnQAACS5H5e24YvjWvGZU6t2l6q8tsm3r48jWlMuyNDUsVm6YmR/JcQSyeCPbwQA4Ny5XFLdCfdrRtIBAL1QTUOz1pWUacUup9buOeb3uLS0hBhdPTpLU8dm6eLh/eSIjgpjTWF1hHQAwLmrOykZl/s196QDAHqJ8tpGrd59TCs+c+r9vWVqbHb59mUlx2nq2CwVjcnS5MFpio5i8jd0DiEdAHDuvPejx6VIUdxPBwCITM6Kem0/XK6dhyu0+cBJbfrqhFpcxrd/UL8ETR2bpaljspQ3MJXHpeGsENIBAOeO+9EBABHmRE2jdhwu147DFb71saqGNuUuyEpyB/OxWRqZmSSbjWCOc0NIBwCcu5oy95r70QEAPVBVfZN2fl2hnYcrtONwhbYfLtfhk3Vtytlt0vmZSRo/MEXjBqbqsuHpGpyeGIYaI5IR0gEA566WkXQAQM9Q39SiXUcqtcNz2fr2w+X68niNjGlbdmh6osYNTNH4ganKG5ii0TnJzMaOkOMbBgA4dzXfuNeMpAMALMLlMvq6vE77yqq1r6xGe0urtP1whT4vrfK7j9xrQGq8xnsC+fiBKRo7IEUp8cyzgu5HSAcAnDvvSDohHQDQzeoaW/TlcXcQ33es2hfKvyyrVkOr2dZbS+8T6wvjeQNTNW5gitL7OLq55kD7COkAgHPHxHEAgBAyxqisqkFflJ0K418ed6+/Lm9777hXbJRdg9MTNKx/Hw3r30djByRr/MBUZafEMcEbLIuQDgA4d4ykAwCCoLHZpYMnavTFsRrPiLhnVPxYtaoamgMel5YQ4wviwzISfa8HpsXzfHL0OIR0AMC5896TntAvvPUAAPQIJ2sata+sWl+W+Yfxgydq271fXHLPrH5eX8+oeEYfDevvDuND+/dR38TYbm4BEDqEdADA2Wuokj5dKp3Y537PSDoAwKO5xaXDJ+t8IfxUIK/RiZrGgMclxkZpWEYfDU1P1PCMPr5QPqhfghzRUd3YAiA8COkAgK6r+Ub6eLG06XdSfYV7W7/hUvr54a0XAKBbGWNUVt2gwyfr9NVxTwj3XKp+4JtaNba0P3Gb5J5NfWh/76Xpib4wnpHk4H5x9GqEdABA51UcljY+L235o9RU697Wb4R06YPSuBulaC43BIBIYozRydomHT5Zq0Mn6tzrk7U6fLJOh06414FmUJckR7RdQ/u3vjT91JrnjQPt4zcDAHBmx/dKHy6UdrwuuZrc27InSJfNlS74rmTn8kMA6Kkq65t06MSpEH74ZJ1fKK9pbOnweLtNykqO03n9ElpN3uYO5jkp8bLbGRUHuoKQDgAI7MhW6YNnpN3/K8kzkc/gy6RLfyINmyJxOSIAWFpDc4tKKxp0tKJOzsp6Ha2ol7OiXkfK6/R1uXs0vLI+8KzpXhlJDuX2TdDAtHjlpnnWnvfZKfGKjWYGdSBYCOkAAH/GSF99KH34jLTvH6e2j7xGunSulDs5fHUDAPjUN7XIWeEO3kcr6nwB/GhFvZyVdXJW1Ot4deAJ2lrrlxirgWnxGthOEB+QGq+4GK6YAroLIR0A4OZySZ+vcIfzw5+4t9mipLHXu0fOM0eHt35AGFTUNqn4y+MamJag3LQEpSTEhLtKiGDGGFU3NKu8tkkVdU0qr23SydpGldc1qbym0TcS7g7jdTpZ29Spz3VE25WdEqeslDhlp8R71nEakHoqhCc6iAWAVfDbCAC9XUuz9Nn/SB8+K5Xtdm+LckgT/1W65P9IaYPDWj0gnHYdrdC9L2/xvU+Ki/YE9nj/S3/7utcEHUhSi8uoprFZFbXuoF1e1+he13rWngBe7g3gtY2+UN4c4BnhgcTF2JXjCd5ZKXG+161DeVpCDLOlAz0IPQkA9FZNddLWl6WNv5HKD7q3xSZJk++Uvv1jKSkzvPUDLMAmmybkpurwyVodr25UVX2zdh+t1O6jle2WT0uIUW7fBN+lwqdfOswlw+Hnchk1NLvU0Nyi+qZT6/qmFtU2tqiuqVm1je7X3m21jS2qa2z27G9RnW+b+31tY7N7m6d8YweznXdGbLRdaQkxSo2PVUpCjFLjY5SaEKOs5DhlpcT7AnhOSryS46MJ4ECEIaQDQE9ljNTcIDXXSU31Hazr3YG89bq+QtrxhlRzzP1ZCf3cwXzyXVJ8alibBVhJwbB+Wj77EklSXWOLb+br1o+g8r52X5rcpJO1FdpxuKLdz+uf5FBuWrz6JjrkiLbLEW1XrHeJcq8d0VGntkXb5fBt76BslF12uxRltynKZnOv7TbZW72322yK9myzAmOMGltcvqDc4AnK9U0u1Te3+F63DtH1TS1qaHb5rz3lG5raBu9Ta5caPMd09NzuYIuLsSstIVYpnpCdGh+r1IQYT/COdQfxhBileLZ7y8TH8sccoDcjpJ+LDb+Rtr8W7loA6BWMJ2SfFr7Vtcsi20ge6L6kfeJtUmxCUGoKRKr42CiNyEzSiMykdvdX1Tf5PTv69GdJVzc0q6yqQWVVDd1c87a8Yd5ul1+o94b5KLtNNslvhNZm8yyynXrvKeMr1c4270c0tRhf0PYGcXOO/xd2rqLtNjmi7YqLiVJcTJTiY6MU71kneJb4mGjFx9qVEBut+JhW21u9b10+LiZKCbHRvtcA0FWE9HNRXSod2xXuWgCAZLNL0fFSTFw76zgpJr7tOmeiNOYHUnRsuGsPRISkuBiNyo7RqOzkNvuMMaqoa9KhE+7wXlHXpMZml3tp8Yzytrh82xqaXX77Gz2XZ/v2tfiXa2pxqcVl5DLGs+64ri0uoxYZqePHX3cbm02nwnJ0lOJi3K8dMVGK84Vo95UD3n1xnn2OmCj3VQmeddwZ1q1fR0fx2DAA1kNIPxeT7pCGF4a7FgB6i2iHJ2S3E8ajYnhmOSxt0aJFeuqpp+R0OpWXl6fnnntOF110UcDyb775ph555BF99dVXGjFihJ588kldc8013Vjj4LLZbEpNiFVqQqzGDUwJ+b9nPGG9xRi5XFKL973L+IX50197y7lckpHxjXQbz2e61+4t/vtO3+8+3vM/GeO+z9oXsD1h2+EJ37FRdu6rBgAPQvq5SB/uXgAAQECvv/665s6dqyVLlig/P18LFy5UUVGRSkpKlJGR0ab8xo0bdcstt2jBggX67ne/q1deeUUzZszQli1bNHbs2DC0oOex2WyKjrLxH3oA0APZjAn33UDdq7KyUikpKaqoqFByctvL0QAA6G6R3jfl5+dr8uTJev755yVJLpdLubm5uv/++/Xwww+3KX/TTTeppqZG7777rm/bt7/9bU2YMEFLlizp1L8Z6T9TAEDP0pV+iRtxAABAyDQ2Nmrz5s0qLDx1e5jdbldhYaGKi4vbPaa4uNivvCQVFRUFLC9JDQ0Nqqys9FsAAOiJCOkAACBkjh8/rpaWFmVmZvptz8zMlNPpbPcYp9PZpfKStGDBAqWkpPiW3Nzcc688AABhQEgHAAA93rx581RRUeFbDh06FO4qAQBwVphPBAAAhEx6erqioqJUWlrqt720tFRZWVntHpOVldWl8pLkcDjkcDjOvcIAAIQZI+kAACBkYmNjNWnSJK1Zs8a3zeVyac2aNSooKGj3mIKCAr/ykrRq1aqA5QEAiCSMpAMAgJCaO3euZs2apQsvvFAXXXSRFi5cqJqaGt1xxx2SpJkzZ2rAgAFasGCBJOmBBx7Q5ZdfrqefflrXXnutXnvtNX366af63e9+F85mAADQLQjpAAAgpG666SaVlZXp0UcfldPp1IQJE7RixQrf5HAHDx6U3X7q4r6LL75Yr7zyin7xi1/o5z//uUaMGKHly5fzjHQAQK/Ac9IBAAgz+qbg42cKALASnpMOAAAAAEAPREgHAAAAAMAiCOkAAAAAAFgEIR0AAAAAAIsgpAMAAAAAYBG97hFs3snsKysrw1wTAADcvH1SL3vgSkjR3wMArKQrfX2vC+lVVVWSpNzc3DDXBAAAf1VVVUpJSQl3NSIC/T0AwIo609f3uueku1wuHTlyRElJSbLZbOf0WZWVlcrNzdWhQ4d6/DNYaYv1REo7pMhpS6S0Q4qctkRKO4wxqqqqUk5Ojux27kQLBvr7tiKlHVLktCVS2iHRFiuKlHZIkdGWrvT1vW4k3W63a+DAgUH9zOTk5B77ZTkdbbGeSGmHFDltiZR2SJHTlkhoByPowUV/H1iktEOKnLZESjsk2mJFkdIOqee3pbN9PX+uBwAAAADAIgjpAAAAAABYBCH9HDgcDs2fP18OhyPcVTlntMV6IqUdUuS0JVLaIUVOWyKlHbC2SPmeRUo7pMhpS6S0Q6ItVhQp7ZAiqy2d0esmjgMAAAAAwKoYSQcAAAAAwCII6QAAAAAAWAQhHQAAAAAAiyCkAwAAAABgEYT0M1i0aJEGDx6suLg45efna9OmTR2Wf/PNN3XBBRcoLi5O48aN09/+9rduqmlgCxYs0OTJk5WUlKSMjAzNmDFDJSUlHR6zbNky2Ww2vyUuLq6bahzYY4891qZeF1xwQYfHWPGcDB48uE07bDabZs+e3W55K52P999/X9/73veUk5Mjm82m5cuX++03xujRRx9Vdna24uPjVVhYqL17957xc7v6uxYMHbWlqalJDz30kMaNG6fExETl5ORo5syZOnLkSIefeTbf0VC2Q5Juv/32NnWaOnXqGT/XaudEUru/NzabTU899VTAzwzHOUHP09P7e/p6a50Pr57a39PX09eHEn39mRHSO/D6669r7ty5mj9/vrZs2aK8vDwVFRXp2LFj7ZbfuHGjbrnlFt15553aunWrZsyYoRkzZuizzz7r5pr7W79+vWbPnq2PPvpIq1atUlNTk66++mrV1NR0eFxycrKOHj3qWw4cONBNNe7YmDFj/Or14YcfBixr1XPyySef+LVh1apVkqQf/vCHAY+xyvmoqalRXl6eFi1a1O7+X/3qV/rNb36jJUuW6OOPP1ZiYqKKiopUX18f8DO7+rsWLB21pba2Vlu2bNEjjzyiLVu26K9//atKSkp03XXXnfFzu/IdDYYznRNJmjp1ql+dXn311Q4/04rnRJJfG44ePaqlS5fKZrPp+uuv7/Bzu/ucoGeJhP6evt5a58Orp/b39PX09aFEX98JBgFddNFFZvbs2b73LS0tJicnxyxYsKDd8jfeeKO59tpr/bbl5+ebH/3oRyGtZ1cdO3bMSDLr168PWOall14yKSkp3VepTpo/f77Jy8vrdPmeck4eeOABM2zYMONyudrdb9XzIcm89dZbvvcul8tkZWWZp556yretvLzcOBwO8+qrrwb8nK7+roXC6W1pz6ZNm4wkc+DAgYBluvodDbb22jFr1iwzffr0Ln1OTzkn06dPN1OmTOmwTLjPCawvEvt7+nprnQ+vntjf09e3Fe5+hb6+rXCfk2BjJD2AxsZGbd68WYWFhb5tdrtdhYWFKi4ubveY4uJiv/KSVFRUFLB8uFRUVEiS+vbt22G56upqDRo0SLm5uZo+fbp27drVHdU7o7179yonJ0dDhw7VrbfeqoMHDwYs2xPOSWNjo15++WX927/9m2w2W8ByVj0fre3fv19Op9PvZ56SkqL8/PyAP/Oz+V0Ll4qKCtlsNqWmpnZYrivf0e6ybt06ZWRkaOTIkbrvvvv0zTffBCzbU85JaWmp3nvvPd15551nLGvFcwJriNT+nr7eWudDipz+nr7ezYr9Cn299c7J2SKkB3D8+HG1tLQoMzPTb3tmZqacTme7xzidzi6VDweXy6UHH3xQl1xyicaOHRuw3MiRI7V06VK9/fbbevnll+VyuXTxxRfr8OHD3VjbtvLz87Vs2TKtWLFCixcv1v79+3XZZZepqqqq3fI94ZwsX75c5eXluv322wOWser5OJ3359qVn/nZ/K6FQ319vR566CHdcsstSk5ODliuq9/R7jB16lT96U9/0po1a/Tkk09q/fr1mjZtmlpaWtot31POyR//+EclJSXpBz/4QYflrHhOYB2R2N/T11vrfHhFSn9PX2/NfoW+3nrn5FxEh7sC6F6zZ8/WZ599dsZ7NAoKClRQUOB7f/HFF2vUqFF64YUX9F//9V+hrmZA06ZN870eP3688vPzNWjQIL3xxhud+gubFb344ouaNm2acnJyApax6vnoLZqamnTjjTfKGKPFixd3WNaK39Gbb77Z93rcuHEaP368hg0bpnXr1umqq64KS52CYenSpbr11lvPOKmSFc8JEEr09dZEf29t9PXW1Fv7ekbSA0hPT1dUVJRKS0v9tpeWliorK6vdY7KysrpUvrvNmTNH7777rtauXauBAwd26diYmBhNnDhRX3zxRYhqd3ZSU1N1/vnnB6yX1c/JgQMHtHr1at11111dOs6q58P7c+3Kz/xsfte6k7fTPnDggFatWtXhX9bbc6bvaDgMHTpU6enpAetk9XMiSR988IFKSkq6/LsjWfOcIHwirb+nr3ezyvnwiqT+nr6+LSv2K/T11jsnXUFIDyA2NlaTJk3SmjVrfNtcLpfWrFnj9xfO1goKCvzKS9KqVasClu8uxhjNmTNHb731lv7xj39oyJAhXf6MlpYW7dy5U9nZ2SGo4dmrrq7Wvn37AtbLqufE66WXXlJGRoauvfbaLh1n1fMxZMgQZWVl+f3MKysr9fHHHwf8mZ/N71p38Xbae/fu1erVq9WvX78uf8aZvqPhcPjwYX3zzTcB62Tlc+L14osvatKkScrLy+vysVY8JwifSOnv6eutdT5OF0n9PX19W1bsV+jrrXdOuiS889ZZ22uvvWYcDodZtmyZ+ec//2nuuecek5qaapxOpzHGmNtuu808/PDDvvIbNmww0dHR5te//rXZvXu3mT9/vomJiTE7d+4MVxOMMcbcd999JiUlxaxbt84cPXrUt9TW1vrKnN6Wxx9/3KxcudLs27fPbN682dx8880mLi7O7Nq1KxxN8Pn3f/93s27dOrN//36zYcMGU1hYaNLT082xY8eMMT3nnBjjnkHzvPPOMw899FCbfVY+H1VVVWbr1q1m69atRpJ55plnzNatW32zoD7xxBMmNTXVvP3222bHjh1m+vTpZsiQIaaurs73GVOmTDHPPfec7/2ZftfC0ZbGxkZz3XXXmYEDB5pt27b5/e40NDQEbMuZvqPd3Y6qqirz05/+1BQXF5v9+/eb1atXm29961tmxIgRpr6+PmA7rHhOvCoqKkxCQoJZvHhxu59hhXOCniUS+nv6emudj9Z6Yn9PX09fH0r09WdGSD+D5557zpx33nkmNjbWXHTRReajjz7y7bv88svNrFmz/Mq/8cYb5vzzzzexsbFmzJgx5r333uvmGrclqd3lpZde8pU5vS0PPvigr92ZmZnmmmuuMVu2bOn+yp/mpptuMtnZ2SY2NtYMGDDA3HTTTeaLL77w7e8p58QYY1auXGkkmZKSkjb7rHw+1q5d2+73yVtfl8tlHnnkEZOZmWkcDoe56qqr2rRx0KBBZv78+X7bOvpdC0db9u/fH/B3Z+3atQHbcqbvaHe3o7a21lx99dWmf//+JiYmxgwaNMjcfffdbTrgnnBOvF544QUTHx9vysvL2/0MK5wT9Dw9vb+nr7fW+WitJ/b39PX09eFqi1dv7+ttxhhztqPwAAAAAAAgeLgnHQAAAAAAiyCkAwAAAABgEYR0AAAAAAAsgpAOAAAAAIBFENIBAAAAALAIQjoAAAAAABZBSAcAAAAAwCII6QAAAAAAWAQhHUC3s9lsWr58ebirAQAAQoS+Hjh7hHSgl7n99ttls9naLFOnTg131QAAQBDQ1wM9W3S4KwCg+02dOlUvvfSS3zaHwxGm2gAAgGCjrwd6LkbSgV7I4XAoKyvLb0lLS5Pkvjxt8eLFmjZtmuLj4zV06FD95S9/8Tt+586dmjJliuLj49WvXz/dc889qq6u9iuzdOlSjRkzRg6HQ9nZ2ZozZ47f/uPHj+v73/++EhISNGLECL3zzjuhbTQAAL0IfT3QcxHSAbTxyCOP6Prrr9f27dt166236uabb9bu3bslSTU1NSoqKlJaWpo++eQTvfnmm1q9erVfx7x48WLNnj1b99xzj3bu3Kl33nlHw4cP9/s3Hn/8cd14443asWOHrrnmGt166606ceJEt7YTAIDeir4esDADoFeZNWuWiYqKMomJiX7LL3/5S2OMMZLMvffe63dMfn6+ue+++4wxxvzud78zaWlpprq62rf/vffeM3a73TidTmOMMTk5OeY//uM/AtZBkvnFL37he19dXW0kmb///e9BaycAAL0VfT3Qs3FPOtALXXnllVq8eLHftr59+/peFxQU+O0rKCjQtm3bJEm7d+9WXl6eEhMTffsvueQSuVwulZSUyGaz6ciRI7rqqqs6rMP48eN9rxMTE5WcnKxjx46dbZMAAEAr9PVAz0VIB3qhxMTENpekBUt8fHynysXExPi9t9lscrlcoagSAAC9Dn090HNxTzqANj766KM270eNGiVJGjVqlLZv366amhrf/g0bNshut2vkyJFKSkrS4MGDtWbNmm6tMwAA6Dz6esC6GEkHeqGGhgY5nU6/bdHR0UpPT5ckvfnmm7rwwgt16aWX6s9//rM2bdqkF198UZJ06623av78+Zo1a5Yee+wxlZWV6f7779dtt92mzMxMSdJjjz2me++9VxkZGZo2bZqqqqq0YcMG3X///d3bUAAAein6eqDnIqQDvdCKFSuUnZ3tt23kyJHas2ePJPdsrK+99pp+/OMfKzs7W6+++qpGjx4tSUpISNDKlSv1wAMPaPLkyUpISND111+vZ555xvdZs2bNUn19vZ599ln99Kc/VXp6um644YbuayAAAL0cfT3Qc9mMMSbclQBgHTabTW+99ZZmzJgR7qoAAIAQoK8HrI170gEAAAAAsAhCOgAAAAAAFsHl7gAAAAAAWAQj6QAAAAAAWAQhHQAAAAAAiyCkAwAAAABgEYR0AAAAAAAsgpAOAAAAAIBFENIBAAAAALAIQjoAAAAAABZBSAcAAAAAwCL+PxORgIWFYeIpAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 학습 및 검증 정확도 시각화\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# 정확도 그래프\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_exp23.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history_exp23.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# 손실 그래프\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_exp23.history['loss'], label='Training Loss')\n",
    "plt.plot(history_exp23.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3lR_5oviqMrk"
   },
   "source": [
    "## 2-4. (16, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "xshgj9pkqMrt"
   },
   "outputs": [],
   "source": [
    "dataset_size = x_train.shape[0]\n",
    "batch_size = 30\n",
    "epochs = 20\n",
    "\n",
    "total_iteration= int(epochs * (dataset_size / batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "xFEe2jFMqMrt"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout, InputLayer\n",
    "import tensorflow as tf\n",
    "\n",
    "# 새로운 레이어를 추가하기 위한 레이어 리스트 초기화\n",
    "new_layers = []\n",
    "\n",
    "# Dense 레이어 카운터\n",
    "dense_layer_count = 0\n",
    "\n",
    "# VGG 모델의 각 레이어 순회\n",
    "for layer in best_vgg16.layers:\n",
    "    if isinstance(layer, InputLayer):\n",
    "        continue\n",
    "    if isinstance(layer, Conv2D):\n",
    "        # Conv2D 레이어를 ConvLoRALayer로 대체\n",
    "        conv_lora_layer = ConvLoRALayer00_cdn2(original_conv_layer=layer, rank=16, alpha=32, total_iteration = total_iteration, trainable=True)\n",
    "        new_layers.append(conv_lora_layer)\n",
    "    elif isinstance(layer, Dense):\n",
    "        dense_layer_count += 1\n",
    "        # 첫 번째와 두 번째 Dense 레이어에만 LoraLayer 적용\n",
    "        if dense_layer_count in [1, 2]:\n",
    "            dense_lora_layer = LoraLayer(original_layer=layer, rank=16, alpha=32, total_iteration=total_iteration, trainable=True)\n",
    "            new_layers.append(dense_lora_layer)\n",
    "        else:\n",
    "            new_layers.append(layer)  # 그대로 유지함\n",
    "    else:\n",
    "        # 다른 유형의 레이어는 그대로 유지\n",
    "        new_layers.append(layer)\n",
    "\n",
    "# 새로운 입력 텐서 생성\n",
    "input_tensor = best_vgg16.input\n",
    "\n",
    "# 새로운 모델 구성\n",
    "x = input_tensor\n",
    "for layer in new_layers:\n",
    "    x = layer(x)\n",
    "\n",
    "# 새로운 모델 생성\n",
    "exp24_lora_vgg16 = Model(inputs=input_tensor, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "ff6yIKoKqMrt"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D\n",
    "\n",
    "# 새로 생성된 모델에서 Dense 레이어 중 LoRA가 적용되지 않은 레이어의 trainable을 False로 설정\n",
    "for layer in exp24_lora_vgg16.layers:\n",
    "    if isinstance(layer, Dense) and not isinstance(layer, LoraLayer):\n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZnmdfI4IqMrt",
    "outputId": "23d02fd4-4e40-45f0-b281-0e9eebcce05f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " block1_conv1 (ConvLoRALaye  (None, 32, 32, 64)        11506     \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block1_conv2 (ConvLoRALaye  (None, 32, 32, 64)        55426     \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " block2_conv1 (ConvLoRALaye  (None, 16, 16, 128)       101634    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block2_conv2 (ConvLoRALaye  (None, 16, 16, 128)       184578    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " block3_conv1 (ConvLoRALaye  (None, 8, 8, 256)         350722    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block3_conv2 (ConvLoRALaye  (None, 8, 8, 256)         664066    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block3_conv3 (ConvLoRALaye  (None, 8, 8, 256)         664066    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 4, 4, 256)         0         \n",
      "                                                                 \n",
      " block4_conv1 (ConvLoRALaye  (None, 4, 4, 512)         1291266   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block4_conv2 (ConvLoRALaye  (None, 4, 4, 512)         2507778   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block4_conv3 (ConvLoRALaye  (None, 4, 4, 512)         2507778   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      " block5_conv1 (ConvLoRALaye  (None, 2, 2, 512)         2507778   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block5_conv2 (ConvLoRALaye  (None, 2, 2, 512)         2507778   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block5_conv3 (ConvLoRALaye  (None, 2, 2, 512)         2507778   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 1, 1, 512)         0         \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_6 (LoraLayer)         (None, 4096)              2179074   \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_7 (LoraLayer)         (None, 512)               2171906   \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20218264 (77.13 MB)\n",
      "Trainable params: 1299504 (4.96 MB)\n",
      "Non-trainable params: 18918760 (72.17 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "exp24_lora_vgg16.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F00Ij82-qMrt",
    "outputId": "30affc13-5eb2-4c04-e7d2-7922b86656c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: input_3\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block1_conv1\n",
      "  Trainable parameters: 9712\n",
      "  Non-trainable parameters: 1794\n",
      "Layer: block1_conv2\n",
      "  Trainable parameters: 18496\n",
      "  Non-trainable parameters: 36930\n",
      "Layer: block1_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block2_conv1\n",
      "  Trainable parameters: 27776\n",
      "  Non-trainable parameters: 73858\n",
      "Layer: block2_conv2\n",
      "  Trainable parameters: 36992\n",
      "  Non-trainable parameters: 147586\n",
      "Layer: block2_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block3_conv1\n",
      "  Trainable parameters: 55552\n",
      "  Non-trainable parameters: 295170\n",
      "Layer: block3_conv2\n",
      "  Trainable parameters: 73984\n",
      "  Non-trainable parameters: 590082\n",
      "Layer: block3_conv3\n",
      "  Trainable parameters: 73984\n",
      "  Non-trainable parameters: 590082\n",
      "Layer: block3_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block4_conv1\n",
      "  Trainable parameters: 111104\n",
      "  Non-trainable parameters: 1180162\n",
      "Layer: block4_conv2\n",
      "  Trainable parameters: 147968\n",
      "  Non-trainable parameters: 2359810\n",
      "Layer: block4_conv3\n",
      "  Trainable parameters: 147968\n",
      "  Non-trainable parameters: 2359810\n",
      "Layer: block4_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block5_conv1\n",
      "  Trainable parameters: 147968\n",
      "  Non-trainable parameters: 2359810\n",
      "Layer: block5_conv2\n",
      "  Trainable parameters: 147968\n",
      "  Non-trainable parameters: 2359810\n",
      "Layer: block5_conv3\n",
      "  Trainable parameters: 147968\n",
      "  Non-trainable parameters: 2359810\n",
      "Layer: block5_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: flatten_2\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: dense_6\n",
      "  Trainable parameters: 77824\n",
      "  Non-trainable parameters: 2101250\n",
      "Layer: dropout_4\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: dense_7\n",
      "  Trainable parameters: 74240\n",
      "  Non-trainable parameters: 2097666\n",
      "Layer: dropout_5\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: dense_8\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 5130\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#모델의 각 레이어를 순회하며 파라미터 수를 계산\n",
    "for layer in exp24_lora_vgg16.layers:\n",
    "    trainable_count = np.sum([tf.size(w).numpy() for w in layer.trainable_weights])\n",
    "    non_trainable_count = np.sum([tf.size(w).numpy() for w in layer.non_trainable_weights])\n",
    "\n",
    "    print(f\"Layer: {layer.name}\")\n",
    "    print(f\"  Trainable parameters: {trainable_count}\")\n",
    "    print(f\"  Non-trainable parameters: {non_trainable_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "iXZC49h7qMru"
   },
   "outputs": [],
   "source": [
    "lora_layers = []  # lora 레이어들을 저장할 리스트\n",
    "\n",
    "for layer in exp24_lora_vgg16.layers:\n",
    "    if isinstance(layer, LoraLayer) or isinstance(layer, ConvLoRALayer00_cdn2):\n",
    "        lora_layers.append(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "HNE35A0EqMru"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "class PrintCurrentStepCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, lora_layers):\n",
    "        super().__init__()\n",
    "        self.lora_layers = lora_layers\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print()\n",
    "        for i, lora_layer in enumerate(self.lora_layers):\n",
    "            current_step = lora_layer.current_step.value()\n",
    "            decay_factor = lora_layer.decay_factor.value()\n",
    "            print(f\"End of epoch {epoch + 1}, LoraLayer {i}: {current_step} Step\")\n",
    "            print(f\"End of epoch {epoch + 1}, LoraLayer {i}: Decay factor: {decay_factor}\")\n",
    "\n",
    "class TestCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, test_data):\n",
    "        self.test_data = test_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        x, y = self.test_data\n",
    "        loss, acc = self.model.evaluate(x, y, verbose=0)\n",
    "        print('\\nTesting loss: {}, acc: {}\\n'.format(loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "zeLLoQGzqMru"
   },
   "outputs": [],
   "source": [
    "# 콜백 생성 시 lora_layers 딕셔너리의 값만 사용\n",
    "print_step_callback = PrintCurrentStepCallback(lora_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "e6Dj91ILqMru"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# 모델 컴파일\n",
    "exp24_lora_vgg16.compile(optimizer=Adam(learning_rate=0.00005),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3AQTPMJ_GOG"
   },
   "source": [
    "### 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gp_zFeAtqMru",
    "outputId": "ce31c6e2-6916-462a-f2f5-1a2bbcf534b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.1163 - accuracy: 0.9631\n",
      "End of epoch 1, LoraLayer 0: 1667 Step\n",
      "End of epoch 1, LoraLayer 0: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 1: 1667 Step\n",
      "End of epoch 1, LoraLayer 1: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 2: 1667 Step\n",
      "End of epoch 1, LoraLayer 2: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 3: 1667 Step\n",
      "End of epoch 1, LoraLayer 3: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 4: 1667 Step\n",
      "End of epoch 1, LoraLayer 4: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 5: 1667 Step\n",
      "End of epoch 1, LoraLayer 5: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 6: 1667 Step\n",
      "End of epoch 1, LoraLayer 6: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 7: 1667 Step\n",
      "End of epoch 1, LoraLayer 7: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 8: 1667 Step\n",
      "End of epoch 1, LoraLayer 8: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 9: 1667 Step\n",
      "End of epoch 1, LoraLayer 9: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 10: 1667 Step\n",
      "End of epoch 1, LoraLayer 10: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 11: 1667 Step\n",
      "End of epoch 1, LoraLayer 11: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 12: 1667 Step\n",
      "End of epoch 1, LoraLayer 12: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 13: 1667 Step\n",
      "End of epoch 1, LoraLayer 13: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 14: 1667 Step\n",
      "End of epoch 1, LoraLayer 14: Decay factor: 1.0\n",
      "\n",
      "Testing loss: 2.302642345428467, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 67s 35ms/step - loss: 0.1163 - accuracy: 0.9631 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 2/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.0931 - accuracy: 0.9700\n",
      "End of epoch 2, LoraLayer 0: 3334 Step\n",
      "End of epoch 2, LoraLayer 0: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 1: 3334 Step\n",
      "End of epoch 2, LoraLayer 1: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 2: 3334 Step\n",
      "End of epoch 2, LoraLayer 2: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 3: 3334 Step\n",
      "End of epoch 2, LoraLayer 3: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 4: 3334 Step\n",
      "End of epoch 2, LoraLayer 4: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 5: 3334 Step\n",
      "End of epoch 2, LoraLayer 5: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 6: 3334 Step\n",
      "End of epoch 2, LoraLayer 6: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 7: 3334 Step\n",
      "End of epoch 2, LoraLayer 7: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 8: 3334 Step\n",
      "End of epoch 2, LoraLayer 8: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 9: 3334 Step\n",
      "End of epoch 2, LoraLayer 9: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 10: 3334 Step\n",
      "End of epoch 2, LoraLayer 10: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 11: 3334 Step\n",
      "End of epoch 2, LoraLayer 11: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 12: 3334 Step\n",
      "End of epoch 2, LoraLayer 12: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 13: 3334 Step\n",
      "End of epoch 2, LoraLayer 13: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 14: 3334 Step\n",
      "End of epoch 2, LoraLayer 14: Decay factor: 0.9999625086784363\n",
      "\n",
      "Testing loss: 2.3027310371398926, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 49s 30ms/step - loss: 0.0931 - accuracy: 0.9700 - val_loss: 2.3027 - val_accuracy: 0.1000\n",
      "Epoch 3/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.0824 - accuracy: 0.9728\n",
      "End of epoch 3, LoraLayer 0: 5001 Step\n",
      "End of epoch 3, LoraLayer 0: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 1: 5001 Step\n",
      "End of epoch 3, LoraLayer 1: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 2: 5001 Step\n",
      "End of epoch 3, LoraLayer 2: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 3: 5001 Step\n",
      "End of epoch 3, LoraLayer 3: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 4: 5001 Step\n",
      "End of epoch 3, LoraLayer 4: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 5: 5001 Step\n",
      "End of epoch 3, LoraLayer 5: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 6: 5001 Step\n",
      "End of epoch 3, LoraLayer 6: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 7: 5001 Step\n",
      "End of epoch 3, LoraLayer 7: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 8: 5001 Step\n",
      "End of epoch 3, LoraLayer 8: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 9: 5001 Step\n",
      "End of epoch 3, LoraLayer 9: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 10: 5001 Step\n",
      "End of epoch 3, LoraLayer 10: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 11: 5001 Step\n",
      "End of epoch 3, LoraLayer 11: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 12: 5001 Step\n",
      "End of epoch 3, LoraLayer 12: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 13: 5001 Step\n",
      "End of epoch 3, LoraLayer 13: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 14: 5001 Step\n",
      "End of epoch 3, LoraLayer 14: Decay factor: 0.9374484419822693\n",
      "\n",
      "Testing loss: 2.3026695251464844, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 50s 30ms/step - loss: 0.0826 - accuracy: 0.9728 - val_loss: 2.3027 - val_accuracy: 0.1000\n",
      "Epoch 4/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.0755 - accuracy: 0.9759\n",
      "End of epoch 4, LoraLayer 0: 6668 Step\n",
      "End of epoch 4, LoraLayer 0: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 1: 6668 Step\n",
      "End of epoch 4, LoraLayer 1: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 2: 6668 Step\n",
      "End of epoch 4, LoraLayer 2: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 3: 6668 Step\n",
      "End of epoch 4, LoraLayer 3: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 4: 6668 Step\n",
      "End of epoch 4, LoraLayer 4: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 5: 6668 Step\n",
      "End of epoch 4, LoraLayer 5: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 6: 6668 Step\n",
      "End of epoch 4, LoraLayer 6: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 7: 6668 Step\n",
      "End of epoch 4, LoraLayer 7: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 8: 6668 Step\n",
      "End of epoch 4, LoraLayer 8: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 9: 6668 Step\n",
      "End of epoch 4, LoraLayer 9: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 10: 6668 Step\n",
      "End of epoch 4, LoraLayer 10: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 11: 6668 Step\n",
      "End of epoch 4, LoraLayer 11: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 12: 6668 Step\n",
      "End of epoch 4, LoraLayer 12: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 13: 6668 Step\n",
      "End of epoch 4, LoraLayer 13: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 14: 6668 Step\n",
      "End of epoch 4, LoraLayer 14: Decay factor: 0.8749343752861023\n",
      "\n",
      "Testing loss: 2.302854061126709, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 55s 33ms/step - loss: 0.0754 - accuracy: 0.9759 - val_loss: 2.3029 - val_accuracy: 0.1000\n",
      "Epoch 5/20\n",
      "1665/1667 [============================>.] - ETA: 0s - loss: 0.0882 - accuracy: 0.9709\n",
      "End of epoch 5, LoraLayer 0: 8335 Step\n",
      "End of epoch 5, LoraLayer 0: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 1: 8335 Step\n",
      "End of epoch 5, LoraLayer 1: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 2: 8335 Step\n",
      "End of epoch 5, LoraLayer 2: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 3: 8335 Step\n",
      "End of epoch 5, LoraLayer 3: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 4: 8335 Step\n",
      "End of epoch 5, LoraLayer 4: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 5: 8335 Step\n",
      "End of epoch 5, LoraLayer 5: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 6: 8335 Step\n",
      "End of epoch 5, LoraLayer 6: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 7: 8335 Step\n",
      "End of epoch 5, LoraLayer 7: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 8: 8335 Step\n",
      "End of epoch 5, LoraLayer 8: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 9: 8335 Step\n",
      "End of epoch 5, LoraLayer 9: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 10: 8335 Step\n",
      "End of epoch 5, LoraLayer 10: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 11: 8335 Step\n",
      "End of epoch 5, LoraLayer 11: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 12: 8335 Step\n",
      "End of epoch 5, LoraLayer 12: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 13: 8335 Step\n",
      "End of epoch 5, LoraLayer 13: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 14: 8335 Step\n",
      "End of epoch 5, LoraLayer 14: Decay factor: 0.8124203085899353\n",
      "\n",
      "Testing loss: 2.3028740882873535, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 55s 33ms/step - loss: 0.0884 - accuracy: 0.9708 - val_loss: 2.3029 - val_accuracy: 0.1000\n",
      "Epoch 6/20\n",
      "1665/1667 [============================>.] - ETA: 0s - loss: 0.0927 - accuracy: 0.9688\n",
      "End of epoch 6, LoraLayer 0: 10002 Step\n",
      "End of epoch 6, LoraLayer 0: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 1: 10002 Step\n",
      "End of epoch 6, LoraLayer 1: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 2: 10002 Step\n",
      "End of epoch 6, LoraLayer 2: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 3: 10002 Step\n",
      "End of epoch 6, LoraLayer 3: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 4: 10002 Step\n",
      "End of epoch 6, LoraLayer 4: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 5: 10002 Step\n",
      "End of epoch 6, LoraLayer 5: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 6: 10002 Step\n",
      "End of epoch 6, LoraLayer 6: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 7: 10002 Step\n",
      "End of epoch 6, LoraLayer 7: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 8: 10002 Step\n",
      "End of epoch 6, LoraLayer 8: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 9: 10002 Step\n",
      "End of epoch 6, LoraLayer 9: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 10: 10002 Step\n",
      "End of epoch 6, LoraLayer 10: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 11: 10002 Step\n",
      "End of epoch 6, LoraLayer 11: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 12: 10002 Step\n",
      "End of epoch 6, LoraLayer 12: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 13: 10002 Step\n",
      "End of epoch 6, LoraLayer 13: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 14: 10002 Step\n",
      "End of epoch 6, LoraLayer 14: Decay factor: 0.7499062418937683\n",
      "\n",
      "Testing loss: 2.3030083179473877, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 51s 30ms/step - loss: 0.0927 - accuracy: 0.9688 - val_loss: 2.3030 - val_accuracy: 0.1000\n",
      "Epoch 7/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.1038 - accuracy: 0.9655\n",
      "End of epoch 7, LoraLayer 0: 11669 Step\n",
      "End of epoch 7, LoraLayer 0: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 1: 11669 Step\n",
      "End of epoch 7, LoraLayer 1: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 2: 11669 Step\n",
      "End of epoch 7, LoraLayer 2: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 3: 11669 Step\n",
      "End of epoch 7, LoraLayer 3: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 4: 11669 Step\n",
      "End of epoch 7, LoraLayer 4: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 5: 11669 Step\n",
      "End of epoch 7, LoraLayer 5: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 6: 11669 Step\n",
      "End of epoch 7, LoraLayer 6: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 7: 11669 Step\n",
      "End of epoch 7, LoraLayer 7: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 8: 11669 Step\n",
      "End of epoch 7, LoraLayer 8: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 9: 11669 Step\n",
      "End of epoch 7, LoraLayer 9: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 10: 11669 Step\n",
      "End of epoch 7, LoraLayer 10: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 11: 11669 Step\n",
      "End of epoch 7, LoraLayer 11: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 12: 11669 Step\n",
      "End of epoch 7, LoraLayer 12: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 13: 11669 Step\n",
      "End of epoch 7, LoraLayer 13: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 14: 11669 Step\n",
      "End of epoch 7, LoraLayer 14: Decay factor: 0.6873922348022461\n",
      "\n",
      "Testing loss: 2.3043789863586426, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 55s 33ms/step - loss: 0.1038 - accuracy: 0.9655 - val_loss: 2.3044 - val_accuracy: 0.1000\n",
      "Epoch 8/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.1179 - accuracy: 0.9600\n",
      "End of epoch 8, LoraLayer 0: 13336 Step\n",
      "End of epoch 8, LoraLayer 0: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 1: 13336 Step\n",
      "End of epoch 8, LoraLayer 1: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 2: 13336 Step\n",
      "End of epoch 8, LoraLayer 2: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 3: 13336 Step\n",
      "End of epoch 8, LoraLayer 3: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 4: 13336 Step\n",
      "End of epoch 8, LoraLayer 4: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 5: 13336 Step\n",
      "End of epoch 8, LoraLayer 5: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 6: 13336 Step\n",
      "End of epoch 8, LoraLayer 6: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 7: 13336 Step\n",
      "End of epoch 8, LoraLayer 7: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 8: 13336 Step\n",
      "End of epoch 8, LoraLayer 8: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 9: 13336 Step\n",
      "End of epoch 8, LoraLayer 9: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 10: 13336 Step\n",
      "End of epoch 8, LoraLayer 10: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 11: 13336 Step\n",
      "End of epoch 8, LoraLayer 11: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 12: 13336 Step\n",
      "End of epoch 8, LoraLayer 12: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 13: 13336 Step\n",
      "End of epoch 8, LoraLayer 13: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 14: 13336 Step\n",
      "End of epoch 8, LoraLayer 14: Decay factor: 0.6248781681060791\n",
      "\n",
      "Testing loss: 2.308436393737793, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 54s 32ms/step - loss: 0.1179 - accuracy: 0.9600 - val_loss: 2.3084 - val_accuracy: 0.1000\n",
      "Epoch 9/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.1410 - accuracy: 0.9528\n",
      "End of epoch 9, LoraLayer 0: 15003 Step\n",
      "End of epoch 9, LoraLayer 0: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 1: 15003 Step\n",
      "End of epoch 9, LoraLayer 1: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 2: 15003 Step\n",
      "End of epoch 9, LoraLayer 2: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 3: 15003 Step\n",
      "End of epoch 9, LoraLayer 3: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 4: 15003 Step\n",
      "End of epoch 9, LoraLayer 4: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 5: 15003 Step\n",
      "End of epoch 9, LoraLayer 5: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 6: 15003 Step\n",
      "End of epoch 9, LoraLayer 6: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 7: 15003 Step\n",
      "End of epoch 9, LoraLayer 7: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 8: 15003 Step\n",
      "End of epoch 9, LoraLayer 8: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 9: 15003 Step\n",
      "End of epoch 9, LoraLayer 9: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 10: 15003 Step\n",
      "End of epoch 9, LoraLayer 10: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 11: 15003 Step\n",
      "End of epoch 9, LoraLayer 11: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 12: 15003 Step\n",
      "End of epoch 9, LoraLayer 12: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 13: 15003 Step\n",
      "End of epoch 9, LoraLayer 13: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 14: 15003 Step\n",
      "End of epoch 9, LoraLayer 14: Decay factor: 0.5623641014099121\n",
      "\n",
      "Testing loss: 2.3134331703186035, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 55s 33ms/step - loss: 0.1410 - accuracy: 0.9527 - val_loss: 2.3134 - val_accuracy: 0.1000\n",
      "Epoch 10/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.1679 - accuracy: 0.9417\n",
      "End of epoch 10, LoraLayer 0: 16670 Step\n",
      "End of epoch 10, LoraLayer 0: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 1: 16670 Step\n",
      "End of epoch 10, LoraLayer 1: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 2: 16670 Step\n",
      "End of epoch 10, LoraLayer 2: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 3: 16670 Step\n",
      "End of epoch 10, LoraLayer 3: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 4: 16670 Step\n",
      "End of epoch 10, LoraLayer 4: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 5: 16670 Step\n",
      "End of epoch 10, LoraLayer 5: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 6: 16670 Step\n",
      "End of epoch 10, LoraLayer 6: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 7: 16670 Step\n",
      "End of epoch 10, LoraLayer 7: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 8: 16670 Step\n",
      "End of epoch 10, LoraLayer 8: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 9: 16670 Step\n",
      "End of epoch 10, LoraLayer 9: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 10: 16670 Step\n",
      "End of epoch 10, LoraLayer 10: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 11: 16670 Step\n",
      "End of epoch 10, LoraLayer 11: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 12: 16670 Step\n",
      "End of epoch 10, LoraLayer 12: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 13: 16670 Step\n",
      "End of epoch 10, LoraLayer 13: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 14: 16670 Step\n",
      "End of epoch 10, LoraLayer 14: Decay factor: 0.4998500347137451\n",
      "\n",
      "Testing loss: 2.3490827083587646, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 52s 31ms/step - loss: 0.1679 - accuracy: 0.9417 - val_loss: 2.3491 - val_accuracy: 0.1000\n",
      "Epoch 11/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.2086 - accuracy: 0.9273\n",
      "End of epoch 11, LoraLayer 0: 18337 Step\n",
      "End of epoch 11, LoraLayer 0: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 1: 18337 Step\n",
      "End of epoch 11, LoraLayer 1: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 2: 18337 Step\n",
      "End of epoch 11, LoraLayer 2: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 3: 18337 Step\n",
      "End of epoch 11, LoraLayer 3: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 4: 18337 Step\n",
      "End of epoch 11, LoraLayer 4: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 5: 18337 Step\n",
      "End of epoch 11, LoraLayer 5: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 6: 18337 Step\n",
      "End of epoch 11, LoraLayer 6: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 7: 18337 Step\n",
      "End of epoch 11, LoraLayer 7: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 8: 18337 Step\n",
      "End of epoch 11, LoraLayer 8: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 9: 18337 Step\n",
      "End of epoch 11, LoraLayer 9: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 10: 18337 Step\n",
      "End of epoch 11, LoraLayer 10: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 11: 18337 Step\n",
      "End of epoch 11, LoraLayer 11: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 12: 18337 Step\n",
      "End of epoch 11, LoraLayer 12: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 13: 18337 Step\n",
      "End of epoch 11, LoraLayer 13: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 14: 18337 Step\n",
      "End of epoch 11, LoraLayer 14: Decay factor: 0.4373359680175781\n",
      "\n",
      "Testing loss: 2.442025661468506, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 54s 32ms/step - loss: 0.2086 - accuracy: 0.9273 - val_loss: 2.4420 - val_accuracy: 0.1000\n",
      "Epoch 12/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.2636 - accuracy: 0.9081\n",
      "End of epoch 12, LoraLayer 0: 20004 Step\n",
      "End of epoch 12, LoraLayer 0: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 1: 20004 Step\n",
      "End of epoch 12, LoraLayer 1: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 2: 20004 Step\n",
      "End of epoch 12, LoraLayer 2: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 3: 20004 Step\n",
      "End of epoch 12, LoraLayer 3: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 4: 20004 Step\n",
      "End of epoch 12, LoraLayer 4: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 5: 20004 Step\n",
      "End of epoch 12, LoraLayer 5: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 6: 20004 Step\n",
      "End of epoch 12, LoraLayer 6: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 7: 20004 Step\n",
      "End of epoch 12, LoraLayer 7: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 8: 20004 Step\n",
      "End of epoch 12, LoraLayer 8: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 9: 20004 Step\n",
      "End of epoch 12, LoraLayer 9: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 10: 20004 Step\n",
      "End of epoch 12, LoraLayer 10: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 11: 20004 Step\n",
      "End of epoch 12, LoraLayer 11: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 12: 20004 Step\n",
      "End of epoch 12, LoraLayer 12: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 13: 20004 Step\n",
      "End of epoch 12, LoraLayer 13: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 14: 20004 Step\n",
      "End of epoch 12, LoraLayer 14: Decay factor: 0.37482190132141113\n",
      "\n",
      "Testing loss: 2.5288398265838623, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 53s 32ms/step - loss: 0.2636 - accuracy: 0.9081 - val_loss: 2.5288 - val_accuracy: 0.1000\n",
      "Epoch 13/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.3206 - accuracy: 0.8895\n",
      "End of epoch 13, LoraLayer 0: 21671 Step\n",
      "End of epoch 13, LoraLayer 0: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 1: 21671 Step\n",
      "End of epoch 13, LoraLayer 1: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 2: 21671 Step\n",
      "End of epoch 13, LoraLayer 2: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 3: 21671 Step\n",
      "End of epoch 13, LoraLayer 3: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 4: 21671 Step\n",
      "End of epoch 13, LoraLayer 4: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 5: 21671 Step\n",
      "End of epoch 13, LoraLayer 5: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 6: 21671 Step\n",
      "End of epoch 13, LoraLayer 6: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 7: 21671 Step\n",
      "End of epoch 13, LoraLayer 7: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 8: 21671 Step\n",
      "End of epoch 13, LoraLayer 8: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 9: 21671 Step\n",
      "End of epoch 13, LoraLayer 9: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 10: 21671 Step\n",
      "End of epoch 13, LoraLayer 10: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 11: 21671 Step\n",
      "End of epoch 13, LoraLayer 11: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 12: 21671 Step\n",
      "End of epoch 13, LoraLayer 12: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 13: 21671 Step\n",
      "End of epoch 13, LoraLayer 13: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 14: 21671 Step\n",
      "End of epoch 13, LoraLayer 14: Decay factor: 0.31230783462524414\n",
      "\n",
      "Testing loss: 2.7728323936462402, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 53s 32ms/step - loss: 0.3206 - accuracy: 0.8895 - val_loss: 2.7728 - val_accuracy: 0.1000\n",
      "Epoch 14/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.3864 - accuracy: 0.8677\n",
      "End of epoch 14, LoraLayer 0: 23338 Step\n",
      "End of epoch 14, LoraLayer 0: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 1: 23338 Step\n",
      "End of epoch 14, LoraLayer 1: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 2: 23338 Step\n",
      "End of epoch 14, LoraLayer 2: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 3: 23338 Step\n",
      "End of epoch 14, LoraLayer 3: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 4: 23338 Step\n",
      "End of epoch 14, LoraLayer 4: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 5: 23338 Step\n",
      "End of epoch 14, LoraLayer 5: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 6: 23338 Step\n",
      "End of epoch 14, LoraLayer 6: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 7: 23338 Step\n",
      "End of epoch 14, LoraLayer 7: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 8: 23338 Step\n",
      "End of epoch 14, LoraLayer 8: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 9: 23338 Step\n",
      "End of epoch 14, LoraLayer 9: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 10: 23338 Step\n",
      "End of epoch 14, LoraLayer 10: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 11: 23338 Step\n",
      "End of epoch 14, LoraLayer 11: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 12: 23338 Step\n",
      "End of epoch 14, LoraLayer 12: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 13: 23338 Step\n",
      "End of epoch 14, LoraLayer 13: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 14: 23338 Step\n",
      "End of epoch 14, LoraLayer 14: Decay factor: 0.24979376792907715\n",
      "\n",
      "Testing loss: 2.9710869789123535, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 54s 32ms/step - loss: 0.3864 - accuracy: 0.8676 - val_loss: 2.9711 - val_accuracy: 0.1000\n",
      "Epoch 15/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.4710 - accuracy: 0.8389\n",
      "End of epoch 15, LoraLayer 0: 25005 Step\n",
      "End of epoch 15, LoraLayer 0: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 1: 25005 Step\n",
      "End of epoch 15, LoraLayer 1: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 2: 25005 Step\n",
      "End of epoch 15, LoraLayer 2: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 3: 25005 Step\n",
      "End of epoch 15, LoraLayer 3: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 4: 25005 Step\n",
      "End of epoch 15, LoraLayer 4: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 5: 25005 Step\n",
      "End of epoch 15, LoraLayer 5: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 6: 25005 Step\n",
      "End of epoch 15, LoraLayer 6: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 7: 25005 Step\n",
      "End of epoch 15, LoraLayer 7: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 8: 25005 Step\n",
      "End of epoch 15, LoraLayer 8: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 9: 25005 Step\n",
      "End of epoch 15, LoraLayer 9: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 10: 25005 Step\n",
      "End of epoch 15, LoraLayer 10: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 11: 25005 Step\n",
      "End of epoch 15, LoraLayer 11: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 12: 25005 Step\n",
      "End of epoch 15, LoraLayer 12: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 13: 25005 Step\n",
      "End of epoch 15, LoraLayer 13: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 14: 25005 Step\n",
      "End of epoch 15, LoraLayer 14: Decay factor: 0.18727970123291016\n",
      "\n",
      "Testing loss: 2.644351005554199, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 51s 30ms/step - loss: 0.4710 - accuracy: 0.8389 - val_loss: 2.6443 - val_accuracy: 0.1000\n",
      "Epoch 16/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.5514 - accuracy: 0.8132\n",
      "End of epoch 16, LoraLayer 0: 26672 Step\n",
      "End of epoch 16, LoraLayer 0: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 1: 26672 Step\n",
      "End of epoch 16, LoraLayer 1: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 2: 26672 Step\n",
      "End of epoch 16, LoraLayer 2: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 3: 26672 Step\n",
      "End of epoch 16, LoraLayer 3: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 4: 26672 Step\n",
      "End of epoch 16, LoraLayer 4: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 5: 26672 Step\n",
      "End of epoch 16, LoraLayer 5: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 6: 26672 Step\n",
      "End of epoch 16, LoraLayer 6: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 7: 26672 Step\n",
      "End of epoch 16, LoraLayer 7: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 8: 26672 Step\n",
      "End of epoch 16, LoraLayer 8: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 9: 26672 Step\n",
      "End of epoch 16, LoraLayer 9: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 10: 26672 Step\n",
      "End of epoch 16, LoraLayer 10: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 11: 26672 Step\n",
      "End of epoch 16, LoraLayer 11: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 12: 26672 Step\n",
      "End of epoch 16, LoraLayer 12: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 13: 26672 Step\n",
      "End of epoch 16, LoraLayer 13: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 14: 26672 Step\n",
      "End of epoch 16, LoraLayer 14: Decay factor: 0.12476563453674316\n",
      "\n",
      "Testing loss: 2.6343750953674316, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 55s 33ms/step - loss: 0.5514 - accuracy: 0.8132 - val_loss: 2.6344 - val_accuracy: 0.1000\n",
      "Epoch 17/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.6448 - accuracy: 0.7815\n",
      "End of epoch 17, LoraLayer 0: 28339 Step\n",
      "End of epoch 17, LoraLayer 0: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 1: 28339 Step\n",
      "End of epoch 17, LoraLayer 1: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 2: 28339 Step\n",
      "End of epoch 17, LoraLayer 2: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 3: 28339 Step\n",
      "End of epoch 17, LoraLayer 3: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 4: 28339 Step\n",
      "End of epoch 17, LoraLayer 4: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 5: 28339 Step\n",
      "End of epoch 17, LoraLayer 5: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 6: 28339 Step\n",
      "End of epoch 17, LoraLayer 6: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 7: 28339 Step\n",
      "End of epoch 17, LoraLayer 7: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 8: 28339 Step\n",
      "End of epoch 17, LoraLayer 8: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 9: 28339 Step\n",
      "End of epoch 17, LoraLayer 9: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 10: 28339 Step\n",
      "End of epoch 17, LoraLayer 10: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 11: 28339 Step\n",
      "End of epoch 17, LoraLayer 11: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 12: 28339 Step\n",
      "End of epoch 17, LoraLayer 12: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 13: 28339 Step\n",
      "End of epoch 17, LoraLayer 13: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 14: 28339 Step\n",
      "End of epoch 17, LoraLayer 14: Decay factor: 0.06225156784057617\n",
      "\n",
      "Testing loss: 2.3469457626342773, acc: 0.25209999084472656\n",
      "\n",
      "1667/1667 [==============================] - 53s 32ms/step - loss: 0.6448 - accuracy: 0.7815 - val_loss: 2.3470 - val_accuracy: 0.2520\n",
      "Epoch 18/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.7204 - accuracy: 0.7559\n",
      "End of epoch 18, LoraLayer 0: 30006 Step\n",
      "End of epoch 18, LoraLayer 0: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 1: 30006 Step\n",
      "End of epoch 18, LoraLayer 1: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 2: 30006 Step\n",
      "End of epoch 18, LoraLayer 2: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 3: 30006 Step\n",
      "End of epoch 18, LoraLayer 3: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 4: 30006 Step\n",
      "End of epoch 18, LoraLayer 4: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 5: 30006 Step\n",
      "End of epoch 18, LoraLayer 5: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 6: 30006 Step\n",
      "End of epoch 18, LoraLayer 6: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 7: 30006 Step\n",
      "End of epoch 18, LoraLayer 7: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 8: 30006 Step\n",
      "End of epoch 18, LoraLayer 8: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 9: 30006 Step\n",
      "End of epoch 18, LoraLayer 9: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 10: 30006 Step\n",
      "End of epoch 18, LoraLayer 10: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 11: 30006 Step\n",
      "End of epoch 18, LoraLayer 11: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 12: 30006 Step\n",
      "End of epoch 18, LoraLayer 12: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 13: 30006 Step\n",
      "End of epoch 18, LoraLayer 13: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 14: 30006 Step\n",
      "End of epoch 18, LoraLayer 14: Decay factor: 0.0\n",
      "\n",
      "Testing loss: 0.8291952013969421, acc: 0.722000002861023\n",
      "\n",
      "1667/1667 [==============================] - 54s 33ms/step - loss: 0.7205 - accuracy: 0.7558 - val_loss: 0.8291 - val_accuracy: 0.7223\n",
      "Epoch 19/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.6862 - accuracy: 0.7678\n",
      "End of epoch 19, LoraLayer 0: 31673 Step\n",
      "End of epoch 19, LoraLayer 0: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 1: 31673 Step\n",
      "End of epoch 19, LoraLayer 1: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 2: 31673 Step\n",
      "End of epoch 19, LoraLayer 2: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 3: 31673 Step\n",
      "End of epoch 19, LoraLayer 3: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 4: 31673 Step\n",
      "End of epoch 19, LoraLayer 4: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 5: 31673 Step\n",
      "End of epoch 19, LoraLayer 5: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 6: 31673 Step\n",
      "End of epoch 19, LoraLayer 6: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 7: 31673 Step\n",
      "End of epoch 19, LoraLayer 7: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 8: 31673 Step\n",
      "End of epoch 19, LoraLayer 8: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 9: 31673 Step\n",
      "End of epoch 19, LoraLayer 9: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 10: 31673 Step\n",
      "End of epoch 19, LoraLayer 10: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 11: 31673 Step\n",
      "End of epoch 19, LoraLayer 11: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 12: 31673 Step\n",
      "End of epoch 19, LoraLayer 12: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 13: 31673 Step\n",
      "End of epoch 19, LoraLayer 13: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 14: 31673 Step\n",
      "End of epoch 19, LoraLayer 14: Decay factor: 0.0\n",
      "\n",
      "Testing loss: 0.7780873775482178, acc: 0.7491999864578247\n",
      "\n",
      "1667/1667 [==============================] - 52s 31ms/step - loss: 0.6862 - accuracy: 0.7678 - val_loss: 0.7781 - val_accuracy: 0.7493\n",
      "Epoch 20/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.6102 - accuracy: 0.7968\n",
      "End of epoch 20, LoraLayer 0: 33340 Step\n",
      "End of epoch 20, LoraLayer 0: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 1: 33340 Step\n",
      "End of epoch 20, LoraLayer 1: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 2: 33340 Step\n",
      "End of epoch 20, LoraLayer 2: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 3: 33340 Step\n",
      "End of epoch 20, LoraLayer 3: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 4: 33340 Step\n",
      "End of epoch 20, LoraLayer 4: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 5: 33340 Step\n",
      "End of epoch 20, LoraLayer 5: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 6: 33340 Step\n",
      "End of epoch 20, LoraLayer 6: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 7: 33340 Step\n",
      "End of epoch 20, LoraLayer 7: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 8: 33340 Step\n",
      "End of epoch 20, LoraLayer 8: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 9: 33340 Step\n",
      "End of epoch 20, LoraLayer 9: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 10: 33340 Step\n",
      "End of epoch 20, LoraLayer 10: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 11: 33340 Step\n",
      "End of epoch 20, LoraLayer 11: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 12: 33340 Step\n",
      "End of epoch 20, LoraLayer 12: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 13: 33340 Step\n",
      "End of epoch 20, LoraLayer 13: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 14: 33340 Step\n",
      "End of epoch 20, LoraLayer 14: Decay factor: 0.0\n",
      "\n",
      "Testing loss: 0.7477818131446838, acc: 0.7599999904632568\n",
      "\n",
      "1667/1667 [==============================] - 52s 31ms/step - loss: 0.6102 - accuracy: 0.7968 - val_loss: 0.7478 - val_accuracy: 0.7601\n"
     ]
    }
   ],
   "source": [
    "history_exp24 = exp24_lora_vgg16.fit(x_train, y_train, batch_size=30, epochs=20, validation_data=(x_test, y_test), callbacks=[print_step_callback, TestCallback((x_test, y_test))] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "tNJCmN8QgDiO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 3s 9ms/step - loss: 0.7478 - accuracy: 0.7600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7477818131446838, 0.7599999904632568]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 평가\n",
    "exp24_lora_vgg16.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "IrLS0MAbqMru"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 학습 및 검증 정확도 시각화\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# 정확도 그래프\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_exp24.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history_exp24.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# 손실 그래프\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_exp24.history['loss'], label='Training Loss')\n",
    "plt.plot(history_exp24.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "05hTVBWz_a7Q"
   },
   "source": [
    "## 2-5. (16, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "yAMaW8tq_a7a"
   },
   "outputs": [],
   "source": [
    "dataset_size = x_train.shape[0]\n",
    "batch_size = 30\n",
    "epochs = 20\n",
    "\n",
    "total_iteration= int(epochs * (dataset_size / batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "yQEGKcv1_a7a"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout, InputLayer\n",
    "import tensorflow as tf\n",
    "\n",
    "# 새로운 레이어를 추가하기 위한 레이어 리스트 초기화\n",
    "new_layers = []\n",
    "\n",
    "# Dense 레이어 카운터\n",
    "dense_layer_count = 0\n",
    "\n",
    "# VGG 모델의 각 레이어 순회\n",
    "for layer in best_vgg16.layers:\n",
    "    if isinstance(layer, InputLayer):\n",
    "        continue\n",
    "    if isinstance(layer, Conv2D):\n",
    "        # Conv2D 레이어를 ConvLoRALayer로 대체\n",
    "        conv_lora_layer = ConvLoRALayer00_cdn2(original_conv_layer=layer, rank=16, alpha=32, total_iteration = total_iteration, trainable=True)\n",
    "        new_layers.append(conv_lora_layer)\n",
    "    elif isinstance(layer, Dense):\n",
    "        dense_layer_count += 1\n",
    "        # 첫 번째와 두 번째 Dense 레이어에만 LoraLayer 적용\n",
    "        if dense_layer_count in [1, 2]:\n",
    "            dense_lora_layer = LoraLayer(original_layer=layer, rank=32, alpha=32, total_iteration=total_iteration, trainable=True)\n",
    "            new_layers.append(dense_lora_layer)\n",
    "        else:\n",
    "            new_layers.append(layer)  # 그대로 유지함\n",
    "    else:\n",
    "        # 다른 유형의 레이어는 그대로 유지\n",
    "        new_layers.append(layer)\n",
    "\n",
    "# 새로운 입력 텐서 생성\n",
    "input_tensor = best_vgg16.input\n",
    "\n",
    "# 새로운 모델 구성\n",
    "x = input_tensor\n",
    "for layer in new_layers:\n",
    "    x = layer(x)\n",
    "\n",
    "# 새로운 모델 생성\n",
    "exp25_lora_vgg16 = Model(inputs=input_tensor, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "_Nx7V_RG_a7a"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D\n",
    "\n",
    "# 새로 생성된 모델에서 Dense 레이어 중 LoRA가 적용되지 않은 레이어의 trainable을 False로 설정\n",
    "for layer in exp25_lora_vgg16.layers:\n",
    "    if isinstance(layer, Dense) and not isinstance(layer, LoraLayer):\n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QISgWsIB_a7b",
    "outputId": "63671162-e007-44da-f92a-49b173cff6c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " block1_conv1 (ConvLoRALaye  (None, 32, 32, 64)        11506     \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block1_conv2 (ConvLoRALaye  (None, 32, 32, 64)        55426     \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " block2_conv1 (ConvLoRALaye  (None, 16, 16, 128)       101634    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block2_conv2 (ConvLoRALaye  (None, 16, 16, 128)       184578    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " block3_conv1 (ConvLoRALaye  (None, 8, 8, 256)         350722    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block3_conv2 (ConvLoRALaye  (None, 8, 8, 256)         664066    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block3_conv3 (ConvLoRALaye  (None, 8, 8, 256)         664066    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 4, 4, 256)         0         \n",
      "                                                                 \n",
      " block4_conv1 (ConvLoRALaye  (None, 4, 4, 512)         1291266   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block4_conv2 (ConvLoRALaye  (None, 4, 4, 512)         2507778   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block4_conv3 (ConvLoRALaye  (None, 4, 4, 512)         2507778   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      " block5_conv1 (ConvLoRALaye  (None, 2, 2, 512)         2507778   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block5_conv2 (ConvLoRALaye  (None, 2, 2, 512)         2507778   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block5_conv3 (ConvLoRALaye  (None, 2, 2, 512)         2507778   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 1, 1, 512)         0         \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_6 (LoraLayer)         (None, 4096)              2252802   \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_7 (LoraLayer)         (None, 512)               2245634   \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20365720 (77.69 MB)\n",
      "Trainable params: 1446960 (5.52 MB)\n",
      "Non-trainable params: 18918760 (72.17 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "exp25_lora_vgg16.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3E9kf2WU_a7b",
    "outputId": "1a1abe8e-8ae7-4f7b-a46a-add76eb92b22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: input_3\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block1_conv1\n",
      "  Trainable parameters: 9712\n",
      "  Non-trainable parameters: 1794\n",
      "Layer: block1_conv2\n",
      "  Trainable parameters: 18496\n",
      "  Non-trainable parameters: 36930\n",
      "Layer: block1_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block2_conv1\n",
      "  Trainable parameters: 27776\n",
      "  Non-trainable parameters: 73858\n",
      "Layer: block2_conv2\n",
      "  Trainable parameters: 36992\n",
      "  Non-trainable parameters: 147586\n",
      "Layer: block2_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block3_conv1\n",
      "  Trainable parameters: 55552\n",
      "  Non-trainable parameters: 295170\n",
      "Layer: block3_conv2\n",
      "  Trainable parameters: 73984\n",
      "  Non-trainable parameters: 590082\n",
      "Layer: block3_conv3\n",
      "  Trainable parameters: 73984\n",
      "  Non-trainable parameters: 590082\n",
      "Layer: block3_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block4_conv1\n",
      "  Trainable parameters: 111104\n",
      "  Non-trainable parameters: 1180162\n",
      "Layer: block4_conv2\n",
      "  Trainable parameters: 147968\n",
      "  Non-trainable parameters: 2359810\n",
      "Layer: block4_conv3\n",
      "  Trainable parameters: 147968\n",
      "  Non-trainable parameters: 2359810\n",
      "Layer: block4_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block5_conv1\n",
      "  Trainable parameters: 147968\n",
      "  Non-trainable parameters: 2359810\n",
      "Layer: block5_conv2\n",
      "  Trainable parameters: 147968\n",
      "  Non-trainable parameters: 2359810\n",
      "Layer: block5_conv3\n",
      "  Trainable parameters: 147968\n",
      "  Non-trainable parameters: 2359810\n",
      "Layer: block5_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: flatten_2\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: dense_6\n",
      "  Trainable parameters: 151552\n",
      "  Non-trainable parameters: 2101250\n",
      "Layer: dropout_4\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: dense_7\n",
      "  Trainable parameters: 147968\n",
      "  Non-trainable parameters: 2097666\n",
      "Layer: dropout_5\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: dense_8\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 5130\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#모델의 각 레이어를 순회하며 파라미터 수를 계산\n",
    "for layer in exp25_lora_vgg16.layers:\n",
    "    trainable_count = np.sum([tf.size(w).numpy() for w in layer.trainable_weights])\n",
    "    non_trainable_count = np.sum([tf.size(w).numpy() for w in layer.non_trainable_weights])\n",
    "\n",
    "    print(f\"Layer: {layer.name}\")\n",
    "    print(f\"  Trainable parameters: {trainable_count}\")\n",
    "    print(f\"  Non-trainable parameters: {non_trainable_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "kuMPWKdD_a7b"
   },
   "outputs": [],
   "source": [
    "lora_layers = []  # lora 레이어들을 저장할 리스트\n",
    "\n",
    "for layer in exp25_lora_vgg16.layers:\n",
    "    if isinstance(layer, LoraLayer) or isinstance(layer, ConvLoRALayer00_cdn2):\n",
    "        lora_layers.append(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "2er5vDY4_a7b"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "class PrintCurrentStepCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, lora_layers):\n",
    "        super().__init__()\n",
    "        self.lora_layers = lora_layers\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print()\n",
    "        for i, lora_layer in enumerate(self.lora_layers):\n",
    "            current_step = lora_layer.current_step.value()\n",
    "            decay_factor = lora_layer.decay_factor.value()\n",
    "            print(f\"End of epoch {epoch + 1}, LoraLayer {i}: {current_step} Step\")\n",
    "            print(f\"End of epoch {epoch + 1}, LoraLayer {i}: Decay factor: {decay_factor}\")\n",
    "\n",
    "class TestCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, test_data):\n",
    "        self.test_data = test_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        x, y = self.test_data\n",
    "        loss, acc = self.model.evaluate(x, y, verbose=0)\n",
    "        print('\\nTesting loss: {}, acc: {}\\n'.format(loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "Pw6efjC2_a7c"
   },
   "outputs": [],
   "source": [
    "# 콜백 생성 시 lora_layers 딕셔너리의 값만 사용\n",
    "print_step_callback = PrintCurrentStepCallback(lora_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "ENCdYvNn_a7c"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# 모델 컴파일\n",
    "exp25_lora_vgg16.compile(optimizer=Adam(learning_rate=0.00005),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h_LWfZn7_a7c"
   },
   "source": [
    "### 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ubx3l9if_a7c",
    "outputId": "4c8bc6cb-79de-4456-d641-301e2d97ded9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.1169 - accuracy: 0.9631\n",
      "End of epoch 1, LoraLayer 0: 1667 Step\n",
      "End of epoch 1, LoraLayer 0: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 1: 1667 Step\n",
      "End of epoch 1, LoraLayer 1: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 2: 1667 Step\n",
      "End of epoch 1, LoraLayer 2: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 3: 1667 Step\n",
      "End of epoch 1, LoraLayer 3: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 4: 1667 Step\n",
      "End of epoch 1, LoraLayer 4: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 5: 1667 Step\n",
      "End of epoch 1, LoraLayer 5: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 6: 1667 Step\n",
      "End of epoch 1, LoraLayer 6: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 7: 1667 Step\n",
      "End of epoch 1, LoraLayer 7: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 8: 1667 Step\n",
      "End of epoch 1, LoraLayer 8: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 9: 1667 Step\n",
      "End of epoch 1, LoraLayer 9: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 10: 1667 Step\n",
      "End of epoch 1, LoraLayer 10: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 11: 1667 Step\n",
      "End of epoch 1, LoraLayer 11: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 12: 1667 Step\n",
      "End of epoch 1, LoraLayer 12: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 13: 1667 Step\n",
      "End of epoch 1, LoraLayer 13: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 14: 1667 Step\n",
      "End of epoch 1, LoraLayer 14: Decay factor: 1.0\n",
      "\n",
      "Testing loss: 2.3026294708251953, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 66s 34ms/step - loss: 0.1169 - accuracy: 0.9631 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 2/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.0891 - accuracy: 0.9716\n",
      "End of epoch 2, LoraLayer 0: 3334 Step\n",
      "End of epoch 2, LoraLayer 0: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 1: 3334 Step\n",
      "End of epoch 2, LoraLayer 1: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 2: 3334 Step\n",
      "End of epoch 2, LoraLayer 2: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 3: 3334 Step\n",
      "End of epoch 2, LoraLayer 3: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 4: 3334 Step\n",
      "End of epoch 2, LoraLayer 4: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 5: 3334 Step\n",
      "End of epoch 2, LoraLayer 5: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 6: 3334 Step\n",
      "End of epoch 2, LoraLayer 6: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 7: 3334 Step\n",
      "End of epoch 2, LoraLayer 7: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 8: 3334 Step\n",
      "End of epoch 2, LoraLayer 8: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 9: 3334 Step\n",
      "End of epoch 2, LoraLayer 9: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 10: 3334 Step\n",
      "End of epoch 2, LoraLayer 10: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 11: 3334 Step\n",
      "End of epoch 2, LoraLayer 11: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 12: 3334 Step\n",
      "End of epoch 2, LoraLayer 12: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 13: 3334 Step\n",
      "End of epoch 2, LoraLayer 13: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 14: 3334 Step\n",
      "End of epoch 2, LoraLayer 14: Decay factor: 0.9999625086784363\n",
      "\n",
      "Testing loss: 2.3026630878448486, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 52s 31ms/step - loss: 0.0891 - accuracy: 0.9716 - val_loss: 2.3027 - val_accuracy: 0.1000\n",
      "Epoch 3/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.0830 - accuracy: 0.9731\n",
      "End of epoch 3, LoraLayer 0: 5001 Step\n",
      "End of epoch 3, LoraLayer 0: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 1: 5001 Step\n",
      "End of epoch 3, LoraLayer 1: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 2: 5001 Step\n",
      "End of epoch 3, LoraLayer 2: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 3: 5001 Step\n",
      "End of epoch 3, LoraLayer 3: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 4: 5001 Step\n",
      "End of epoch 3, LoraLayer 4: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 5: 5001 Step\n",
      "End of epoch 3, LoraLayer 5: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 6: 5001 Step\n",
      "End of epoch 3, LoraLayer 6: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 7: 5001 Step\n",
      "End of epoch 3, LoraLayer 7: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 8: 5001 Step\n",
      "End of epoch 3, LoraLayer 8: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 9: 5001 Step\n",
      "End of epoch 3, LoraLayer 9: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 10: 5001 Step\n",
      "End of epoch 3, LoraLayer 10: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 11: 5001 Step\n",
      "End of epoch 3, LoraLayer 11: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 12: 5001 Step\n",
      "End of epoch 3, LoraLayer 12: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 13: 5001 Step\n",
      "End of epoch 3, LoraLayer 13: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 14: 5001 Step\n",
      "End of epoch 3, LoraLayer 14: Decay factor: 0.9374484419822693\n",
      "\n",
      "Testing loss: 2.3026840686798096, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 52s 31ms/step - loss: 0.0830 - accuracy: 0.9731 - val_loss: 2.3027 - val_accuracy: 0.1000\n",
      "Epoch 4/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.0785 - accuracy: 0.9748\n",
      "End of epoch 4, LoraLayer 0: 6668 Step\n",
      "End of epoch 4, LoraLayer 0: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 1: 6668 Step\n",
      "End of epoch 4, LoraLayer 1: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 2: 6668 Step\n",
      "End of epoch 4, LoraLayer 2: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 3: 6668 Step\n",
      "End of epoch 4, LoraLayer 3: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 4: 6668 Step\n",
      "End of epoch 4, LoraLayer 4: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 5: 6668 Step\n",
      "End of epoch 4, LoraLayer 5: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 6: 6668 Step\n",
      "End of epoch 4, LoraLayer 6: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 7: 6668 Step\n",
      "End of epoch 4, LoraLayer 7: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 8: 6668 Step\n",
      "End of epoch 4, LoraLayer 8: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 9: 6668 Step\n",
      "End of epoch 4, LoraLayer 9: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 10: 6668 Step\n",
      "End of epoch 4, LoraLayer 10: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 11: 6668 Step\n",
      "End of epoch 4, LoraLayer 11: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 12: 6668 Step\n",
      "End of epoch 4, LoraLayer 12: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 13: 6668 Step\n",
      "End of epoch 4, LoraLayer 13: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 14: 6668 Step\n",
      "End of epoch 4, LoraLayer 14: Decay factor: 0.8749343752861023\n",
      "\n",
      "Testing loss: 2.3027820587158203, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 49s 30ms/step - loss: 0.0785 - accuracy: 0.9749 - val_loss: 2.3028 - val_accuracy: 0.1000\n",
      "Epoch 5/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.0822 - accuracy: 0.9721\n",
      "End of epoch 5, LoraLayer 0: 8335 Step\n",
      "End of epoch 5, LoraLayer 0: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 1: 8335 Step\n",
      "End of epoch 5, LoraLayer 1: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 2: 8335 Step\n",
      "End of epoch 5, LoraLayer 2: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 3: 8335 Step\n",
      "End of epoch 5, LoraLayer 3: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 4: 8335 Step\n",
      "End of epoch 5, LoraLayer 4: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 5: 8335 Step\n",
      "End of epoch 5, LoraLayer 5: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 6: 8335 Step\n",
      "End of epoch 5, LoraLayer 6: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 7: 8335 Step\n",
      "End of epoch 5, LoraLayer 7: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 8: 8335 Step\n",
      "End of epoch 5, LoraLayer 8: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 9: 8335 Step\n",
      "End of epoch 5, LoraLayer 9: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 10: 8335 Step\n",
      "End of epoch 5, LoraLayer 10: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 11: 8335 Step\n",
      "End of epoch 5, LoraLayer 11: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 12: 8335 Step\n",
      "End of epoch 5, LoraLayer 12: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 13: 8335 Step\n",
      "End of epoch 5, LoraLayer 13: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 14: 8335 Step\n",
      "End of epoch 5, LoraLayer 14: Decay factor: 0.8124203085899353\n",
      "\n",
      "Testing loss: 2.3030686378479004, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 55s 33ms/step - loss: 0.0822 - accuracy: 0.9721 - val_loss: 2.3031 - val_accuracy: 0.1000\n",
      "Epoch 6/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.0923 - accuracy: 0.9690\n",
      "End of epoch 6, LoraLayer 0: 10002 Step\n",
      "End of epoch 6, LoraLayer 0: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 1: 10002 Step\n",
      "End of epoch 6, LoraLayer 1: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 2: 10002 Step\n",
      "End of epoch 6, LoraLayer 2: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 3: 10002 Step\n",
      "End of epoch 6, LoraLayer 3: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 4: 10002 Step\n",
      "End of epoch 6, LoraLayer 4: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 5: 10002 Step\n",
      "End of epoch 6, LoraLayer 5: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 6: 10002 Step\n",
      "End of epoch 6, LoraLayer 6: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 7: 10002 Step\n",
      "End of epoch 6, LoraLayer 7: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 8: 10002 Step\n",
      "End of epoch 6, LoraLayer 8: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 9: 10002 Step\n",
      "End of epoch 6, LoraLayer 9: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 10: 10002 Step\n",
      "End of epoch 6, LoraLayer 10: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 11: 10002 Step\n",
      "End of epoch 6, LoraLayer 11: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 12: 10002 Step\n",
      "End of epoch 6, LoraLayer 12: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 13: 10002 Step\n",
      "End of epoch 6, LoraLayer 13: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 14: 10002 Step\n",
      "End of epoch 6, LoraLayer 14: Decay factor: 0.7499062418937683\n",
      "\n",
      "Testing loss: 2.304598569869995, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 51s 31ms/step - loss: 0.0923 - accuracy: 0.9690 - val_loss: 2.3046 - val_accuracy: 0.1000\n",
      "Epoch 7/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.0974 - accuracy: 0.9674\n",
      "End of epoch 7, LoraLayer 0: 11669 Step\n",
      "End of epoch 7, LoraLayer 0: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 1: 11669 Step\n",
      "End of epoch 7, LoraLayer 1: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 2: 11669 Step\n",
      "End of epoch 7, LoraLayer 2: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 3: 11669 Step\n",
      "End of epoch 7, LoraLayer 3: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 4: 11669 Step\n",
      "End of epoch 7, LoraLayer 4: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 5: 11669 Step\n",
      "End of epoch 7, LoraLayer 5: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 6: 11669 Step\n",
      "End of epoch 7, LoraLayer 6: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 7: 11669 Step\n",
      "End of epoch 7, LoraLayer 7: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 8: 11669 Step\n",
      "End of epoch 7, LoraLayer 8: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 9: 11669 Step\n",
      "End of epoch 7, LoraLayer 9: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 10: 11669 Step\n",
      "End of epoch 7, LoraLayer 10: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 11: 11669 Step\n",
      "End of epoch 7, LoraLayer 11: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 12: 11669 Step\n",
      "End of epoch 7, LoraLayer 12: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 13: 11669 Step\n",
      "End of epoch 7, LoraLayer 13: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 14: 11669 Step\n",
      "End of epoch 7, LoraLayer 14: Decay factor: 0.6873922348022461\n",
      "\n",
      "Testing loss: 2.3076839447021484, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 53s 32ms/step - loss: 0.0974 - accuracy: 0.9674 - val_loss: 2.3077 - val_accuracy: 0.1000\n",
      "Epoch 8/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.1190 - accuracy: 0.9583\n",
      "End of epoch 8, LoraLayer 0: 13336 Step\n",
      "End of epoch 8, LoraLayer 0: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 1: 13336 Step\n",
      "End of epoch 8, LoraLayer 1: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 2: 13336 Step\n",
      "End of epoch 8, LoraLayer 2: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 3: 13336 Step\n",
      "End of epoch 8, LoraLayer 3: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 4: 13336 Step\n",
      "End of epoch 8, LoraLayer 4: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 5: 13336 Step\n",
      "End of epoch 8, LoraLayer 5: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 6: 13336 Step\n",
      "End of epoch 8, LoraLayer 6: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 7: 13336 Step\n",
      "End of epoch 8, LoraLayer 7: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 8: 13336 Step\n",
      "End of epoch 8, LoraLayer 8: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 9: 13336 Step\n",
      "End of epoch 8, LoraLayer 9: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 10: 13336 Step\n",
      "End of epoch 8, LoraLayer 10: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 11: 13336 Step\n",
      "End of epoch 8, LoraLayer 11: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 12: 13336 Step\n",
      "End of epoch 8, LoraLayer 12: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 13: 13336 Step\n",
      "End of epoch 8, LoraLayer 13: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 14: 13336 Step\n",
      "End of epoch 8, LoraLayer 14: Decay factor: 0.6248781681060791\n",
      "\n",
      "Testing loss: 2.3119304180145264, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 52s 31ms/step - loss: 0.1190 - accuracy: 0.9583 - val_loss: 2.3119 - val_accuracy: 0.1000\n",
      "Epoch 9/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.1328 - accuracy: 0.9537\n",
      "End of epoch 9, LoraLayer 0: 15003 Step\n",
      "End of epoch 9, LoraLayer 0: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 1: 15003 Step\n",
      "End of epoch 9, LoraLayer 1: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 2: 15003 Step\n",
      "End of epoch 9, LoraLayer 2: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 3: 15003 Step\n",
      "End of epoch 9, LoraLayer 3: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 4: 15003 Step\n",
      "End of epoch 9, LoraLayer 4: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 5: 15003 Step\n",
      "End of epoch 9, LoraLayer 5: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 6: 15003 Step\n",
      "End of epoch 9, LoraLayer 6: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 7: 15003 Step\n",
      "End of epoch 9, LoraLayer 7: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 8: 15003 Step\n",
      "End of epoch 9, LoraLayer 8: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 9: 15003 Step\n",
      "End of epoch 9, LoraLayer 9: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 10: 15003 Step\n",
      "End of epoch 9, LoraLayer 10: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 11: 15003 Step\n",
      "End of epoch 9, LoraLayer 11: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 12: 15003 Step\n",
      "End of epoch 9, LoraLayer 12: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 13: 15003 Step\n",
      "End of epoch 9, LoraLayer 13: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 14: 15003 Step\n",
      "End of epoch 9, LoraLayer 14: Decay factor: 0.5623641014099121\n",
      "\n",
      "Testing loss: 2.3215839862823486, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 55s 33ms/step - loss: 0.1328 - accuracy: 0.9537 - val_loss: 2.3216 - val_accuracy: 0.1000\n",
      "Epoch 10/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.1627 - accuracy: 0.9440\n",
      "End of epoch 10, LoraLayer 0: 16670 Step\n",
      "End of epoch 10, LoraLayer 0: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 1: 16670 Step\n",
      "End of epoch 10, LoraLayer 1: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 2: 16670 Step\n",
      "End of epoch 10, LoraLayer 2: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 3: 16670 Step\n",
      "End of epoch 10, LoraLayer 3: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 4: 16670 Step\n",
      "End of epoch 10, LoraLayer 4: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 5: 16670 Step\n",
      "End of epoch 10, LoraLayer 5: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 6: 16670 Step\n",
      "End of epoch 10, LoraLayer 6: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 7: 16670 Step\n",
      "End of epoch 10, LoraLayer 7: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 8: 16670 Step\n",
      "End of epoch 10, LoraLayer 8: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 9: 16670 Step\n",
      "End of epoch 10, LoraLayer 9: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 10: 16670 Step\n",
      "End of epoch 10, LoraLayer 10: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 11: 16670 Step\n",
      "End of epoch 10, LoraLayer 11: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 12: 16670 Step\n",
      "End of epoch 10, LoraLayer 12: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 13: 16670 Step\n",
      "End of epoch 10, LoraLayer 13: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 14: 16670 Step\n",
      "End of epoch 10, LoraLayer 14: Decay factor: 0.4998500347137451\n",
      "\n",
      "Testing loss: 2.33886981010437, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 56s 34ms/step - loss: 0.1627 - accuracy: 0.9440 - val_loss: 2.3389 - val_accuracy: 0.1000\n",
      "Epoch 11/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.1984 - accuracy: 0.9308\n",
      "End of epoch 11, LoraLayer 0: 18337 Step\n",
      "End of epoch 11, LoraLayer 0: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 1: 18337 Step\n",
      "End of epoch 11, LoraLayer 1: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 2: 18337 Step\n",
      "End of epoch 11, LoraLayer 2: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 3: 18337 Step\n",
      "End of epoch 11, LoraLayer 3: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 4: 18337 Step\n",
      "End of epoch 11, LoraLayer 4: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 5: 18337 Step\n",
      "End of epoch 11, LoraLayer 5: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 6: 18337 Step\n",
      "End of epoch 11, LoraLayer 6: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 7: 18337 Step\n",
      "End of epoch 11, LoraLayer 7: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 8: 18337 Step\n",
      "End of epoch 11, LoraLayer 8: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 9: 18337 Step\n",
      "End of epoch 11, LoraLayer 9: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 10: 18337 Step\n",
      "End of epoch 11, LoraLayer 10: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 11: 18337 Step\n",
      "End of epoch 11, LoraLayer 11: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 12: 18337 Step\n",
      "End of epoch 11, LoraLayer 12: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 13: 18337 Step\n",
      "End of epoch 11, LoraLayer 13: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 14: 18337 Step\n",
      "End of epoch 11, LoraLayer 14: Decay factor: 0.4373359680175781\n",
      "\n",
      "Testing loss: 2.36210036277771, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 56s 34ms/step - loss: 0.1984 - accuracy: 0.9308 - val_loss: 2.3621 - val_accuracy: 0.1000\n",
      "Epoch 12/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.2438 - accuracy: 0.9131\n",
      "End of epoch 12, LoraLayer 0: 20004 Step\n",
      "End of epoch 12, LoraLayer 0: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 1: 20004 Step\n",
      "End of epoch 12, LoraLayer 1: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 2: 20004 Step\n",
      "End of epoch 12, LoraLayer 2: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 3: 20004 Step\n",
      "End of epoch 12, LoraLayer 3: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 4: 20004 Step\n",
      "End of epoch 12, LoraLayer 4: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 5: 20004 Step\n",
      "End of epoch 12, LoraLayer 5: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 6: 20004 Step\n",
      "End of epoch 12, LoraLayer 6: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 7: 20004 Step\n",
      "End of epoch 12, LoraLayer 7: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 8: 20004 Step\n",
      "End of epoch 12, LoraLayer 8: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 9: 20004 Step\n",
      "End of epoch 12, LoraLayer 9: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 10: 20004 Step\n",
      "End of epoch 12, LoraLayer 10: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 11: 20004 Step\n",
      "End of epoch 12, LoraLayer 11: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 12: 20004 Step\n",
      "End of epoch 12, LoraLayer 12: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 13: 20004 Step\n",
      "End of epoch 12, LoraLayer 13: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 14: 20004 Step\n",
      "End of epoch 12, LoraLayer 14: Decay factor: 0.37482190132141113\n",
      "\n",
      "Testing loss: 2.434152603149414, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 55s 33ms/step - loss: 0.2438 - accuracy: 0.9131 - val_loss: 2.4342 - val_accuracy: 0.1000\n",
      "Epoch 13/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.3053 - accuracy: 0.8933\n",
      "End of epoch 13, LoraLayer 0: 21671 Step\n",
      "End of epoch 13, LoraLayer 0: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 1: 21671 Step\n",
      "End of epoch 13, LoraLayer 1: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 2: 21671 Step\n",
      "End of epoch 13, LoraLayer 2: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 3: 21671 Step\n",
      "End of epoch 13, LoraLayer 3: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 4: 21671 Step\n",
      "End of epoch 13, LoraLayer 4: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 5: 21671 Step\n",
      "End of epoch 13, LoraLayer 5: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 6: 21671 Step\n",
      "End of epoch 13, LoraLayer 6: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 7: 21671 Step\n",
      "End of epoch 13, LoraLayer 7: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 8: 21671 Step\n",
      "End of epoch 13, LoraLayer 8: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 9: 21671 Step\n",
      "End of epoch 13, LoraLayer 9: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 10: 21671 Step\n",
      "End of epoch 13, LoraLayer 10: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 11: 21671 Step\n",
      "End of epoch 13, LoraLayer 11: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 12: 21671 Step\n",
      "End of epoch 13, LoraLayer 12: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 13: 21671 Step\n",
      "End of epoch 13, LoraLayer 13: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 14: 21671 Step\n",
      "End of epoch 13, LoraLayer 14: Decay factor: 0.31230783462524414\n",
      "\n",
      "Testing loss: 2.5767533779144287, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 51s 31ms/step - loss: 0.3055 - accuracy: 0.8933 - val_loss: 2.5768 - val_accuracy: 0.1000\n",
      "Epoch 14/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.3740 - accuracy: 0.8705\n",
      "End of epoch 14, LoraLayer 0: 23338 Step\n",
      "End of epoch 14, LoraLayer 0: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 1: 23338 Step\n",
      "End of epoch 14, LoraLayer 1: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 2: 23338 Step\n",
      "End of epoch 14, LoraLayer 2: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 3: 23338 Step\n",
      "End of epoch 14, LoraLayer 3: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 4: 23338 Step\n",
      "End of epoch 14, LoraLayer 4: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 5: 23338 Step\n",
      "End of epoch 14, LoraLayer 5: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 6: 23338 Step\n",
      "End of epoch 14, LoraLayer 6: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 7: 23338 Step\n",
      "End of epoch 14, LoraLayer 7: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 8: 23338 Step\n",
      "End of epoch 14, LoraLayer 8: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 9: 23338 Step\n",
      "End of epoch 14, LoraLayer 9: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 10: 23338 Step\n",
      "End of epoch 14, LoraLayer 10: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 11: 23338 Step\n",
      "End of epoch 14, LoraLayer 11: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 12: 23338 Step\n",
      "End of epoch 14, LoraLayer 12: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 13: 23338 Step\n",
      "End of epoch 14, LoraLayer 13: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 14: 23338 Step\n",
      "End of epoch 14, LoraLayer 14: Decay factor: 0.24979376792907715\n",
      "\n",
      "Testing loss: 2.638339042663574, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 52s 31ms/step - loss: 0.3740 - accuracy: 0.8705 - val_loss: 2.6383 - val_accuracy: 0.1000\n",
      "Epoch 15/20\n",
      "1665/1667 [============================>.] - ETA: 0s - loss: 0.4466 - accuracy: 0.8460\n",
      "End of epoch 15, LoraLayer 0: 25005 Step\n",
      "End of epoch 15, LoraLayer 0: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 1: 25005 Step\n",
      "End of epoch 15, LoraLayer 1: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 2: 25005 Step\n",
      "End of epoch 15, LoraLayer 2: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 3: 25005 Step\n",
      "End of epoch 15, LoraLayer 3: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 4: 25005 Step\n",
      "End of epoch 15, LoraLayer 4: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 5: 25005 Step\n",
      "End of epoch 15, LoraLayer 5: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 6: 25005 Step\n",
      "End of epoch 15, LoraLayer 6: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 7: 25005 Step\n",
      "End of epoch 15, LoraLayer 7: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 8: 25005 Step\n",
      "End of epoch 15, LoraLayer 8: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 9: 25005 Step\n",
      "End of epoch 15, LoraLayer 9: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 10: 25005 Step\n",
      "End of epoch 15, LoraLayer 10: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 11: 25005 Step\n",
      "End of epoch 15, LoraLayer 11: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 12: 25005 Step\n",
      "End of epoch 15, LoraLayer 12: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 13: 25005 Step\n",
      "End of epoch 15, LoraLayer 13: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 14: 25005 Step\n",
      "End of epoch 15, LoraLayer 14: Decay factor: 0.18727970123291016\n",
      "\n",
      "Testing loss: 2.6076300144195557, acc: 0.09960000216960907\n",
      "\n",
      "1667/1667 [==============================] - 50s 30ms/step - loss: 0.4466 - accuracy: 0.8460 - val_loss: 2.6076 - val_accuracy: 0.0996\n",
      "Epoch 16/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.5368 - accuracy: 0.8151\n",
      "End of epoch 16, LoraLayer 0: 26672 Step\n",
      "End of epoch 16, LoraLayer 0: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 1: 26672 Step\n",
      "End of epoch 16, LoraLayer 1: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 2: 26672 Step\n",
      "End of epoch 16, LoraLayer 2: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 3: 26672 Step\n",
      "End of epoch 16, LoraLayer 3: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 4: 26672 Step\n",
      "End of epoch 16, LoraLayer 4: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 5: 26672 Step\n",
      "End of epoch 16, LoraLayer 5: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 6: 26672 Step\n",
      "End of epoch 16, LoraLayer 6: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 7: 26672 Step\n",
      "End of epoch 16, LoraLayer 7: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 8: 26672 Step\n",
      "End of epoch 16, LoraLayer 8: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 9: 26672 Step\n",
      "End of epoch 16, LoraLayer 9: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 10: 26672 Step\n",
      "End of epoch 16, LoraLayer 10: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 11: 26672 Step\n",
      "End of epoch 16, LoraLayer 11: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 12: 26672 Step\n",
      "End of epoch 16, LoraLayer 12: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 13: 26672 Step\n",
      "End of epoch 16, LoraLayer 13: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 14: 26672 Step\n",
      "End of epoch 16, LoraLayer 14: Decay factor: 0.12476563453674316\n",
      "\n",
      "Testing loss: 2.5366950035095215, acc: 0.10339999943971634\n",
      "\n",
      "1667/1667 [==============================] - 50s 30ms/step - loss: 0.5368 - accuracy: 0.8151 - val_loss: 2.5367 - val_accuracy: 0.1032\n",
      "Epoch 17/20\n",
      "1665/1667 [============================>.] - ETA: 0s - loss: 0.6244 - accuracy: 0.7883\n",
      "End of epoch 17, LoraLayer 0: 28339 Step\n",
      "End of epoch 17, LoraLayer 0: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 1: 28339 Step\n",
      "End of epoch 17, LoraLayer 1: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 2: 28339 Step\n",
      "End of epoch 17, LoraLayer 2: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 3: 28339 Step\n",
      "End of epoch 17, LoraLayer 3: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 4: 28339 Step\n",
      "End of epoch 17, LoraLayer 4: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 5: 28339 Step\n",
      "End of epoch 17, LoraLayer 5: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 6: 28339 Step\n",
      "End of epoch 17, LoraLayer 6: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 7: 28339 Step\n",
      "End of epoch 17, LoraLayer 7: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 8: 28339 Step\n",
      "End of epoch 17, LoraLayer 8: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 9: 28339 Step\n",
      "End of epoch 17, LoraLayer 9: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 10: 28339 Step\n",
      "End of epoch 17, LoraLayer 10: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 11: 28339 Step\n",
      "End of epoch 17, LoraLayer 11: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 12: 28339 Step\n",
      "End of epoch 17, LoraLayer 12: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 13: 28339 Step\n",
      "End of epoch 17, LoraLayer 13: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 14: 28339 Step\n",
      "End of epoch 17, LoraLayer 14: Decay factor: 0.06225156784057617\n",
      "\n",
      "Testing loss: 1.8413422107696533, acc: 0.36070001125335693\n",
      "\n",
      "1667/1667 [==============================] - 50s 30ms/step - loss: 0.6246 - accuracy: 0.7883 - val_loss: 1.8413 - val_accuracy: 0.3605\n",
      "Epoch 18/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.6972 - accuracy: 0.7627\n",
      "End of epoch 18, LoraLayer 0: 30006 Step\n",
      "End of epoch 18, LoraLayer 0: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 1: 30006 Step\n",
      "End of epoch 18, LoraLayer 1: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 2: 30006 Step\n",
      "End of epoch 18, LoraLayer 2: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 3: 30006 Step\n",
      "End of epoch 18, LoraLayer 3: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 4: 30006 Step\n",
      "End of epoch 18, LoraLayer 4: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 5: 30006 Step\n",
      "End of epoch 18, LoraLayer 5: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 6: 30006 Step\n",
      "End of epoch 18, LoraLayer 6: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 7: 30006 Step\n",
      "End of epoch 18, LoraLayer 7: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 8: 30006 Step\n",
      "End of epoch 18, LoraLayer 8: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 9: 30006 Step\n",
      "End of epoch 18, LoraLayer 9: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 10: 30006 Step\n",
      "End of epoch 18, LoraLayer 10: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 11: 30006 Step\n",
      "End of epoch 18, LoraLayer 11: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 12: 30006 Step\n",
      "End of epoch 18, LoraLayer 12: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 13: 30006 Step\n",
      "End of epoch 18, LoraLayer 13: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 14: 30006 Step\n",
      "End of epoch 18, LoraLayer 14: Decay factor: 0.0\n",
      "\n",
      "Testing loss: 0.8112574219703674, acc: 0.7243000268936157\n",
      "\n",
      "1667/1667 [==============================] - 49s 29ms/step - loss: 0.6972 - accuracy: 0.7627 - val_loss: 0.8112 - val_accuracy: 0.7245\n",
      "Epoch 19/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.6626 - accuracy: 0.7754\n",
      "End of epoch 19, LoraLayer 0: 31673 Step\n",
      "End of epoch 19, LoraLayer 0: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 1: 31673 Step\n",
      "End of epoch 19, LoraLayer 1: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 2: 31673 Step\n",
      "End of epoch 19, LoraLayer 2: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 3: 31673 Step\n",
      "End of epoch 19, LoraLayer 3: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 4: 31673 Step\n",
      "End of epoch 19, LoraLayer 4: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 5: 31673 Step\n",
      "End of epoch 19, LoraLayer 5: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 6: 31673 Step\n",
      "End of epoch 19, LoraLayer 6: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 7: 31673 Step\n",
      "End of epoch 19, LoraLayer 7: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 8: 31673 Step\n",
      "End of epoch 19, LoraLayer 8: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 9: 31673 Step\n",
      "End of epoch 19, LoraLayer 9: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 10: 31673 Step\n",
      "End of epoch 19, LoraLayer 10: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 11: 31673 Step\n",
      "End of epoch 19, LoraLayer 11: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 12: 31673 Step\n",
      "End of epoch 19, LoraLayer 12: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 13: 31673 Step\n",
      "End of epoch 19, LoraLayer 13: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 14: 31673 Step\n",
      "End of epoch 19, LoraLayer 14: Decay factor: 0.0\n",
      "\n",
      "Testing loss: 0.7786628603935242, acc: 0.7427999973297119\n",
      "\n",
      "1667/1667 [==============================] - 50s 30ms/step - loss: 0.6627 - accuracy: 0.7754 - val_loss: 0.7787 - val_accuracy: 0.7426\n",
      "Epoch 20/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.5864 - accuracy: 0.8025\n",
      "End of epoch 20, LoraLayer 0: 33340 Step\n",
      "End of epoch 20, LoraLayer 0: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 1: 33340 Step\n",
      "End of epoch 20, LoraLayer 1: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 2: 33340 Step\n",
      "End of epoch 20, LoraLayer 2: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 3: 33340 Step\n",
      "End of epoch 20, LoraLayer 3: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 4: 33340 Step\n",
      "End of epoch 20, LoraLayer 4: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 5: 33340 Step\n",
      "End of epoch 20, LoraLayer 5: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 6: 33340 Step\n",
      "End of epoch 20, LoraLayer 6: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 7: 33340 Step\n",
      "End of epoch 20, LoraLayer 7: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 8: 33340 Step\n",
      "End of epoch 20, LoraLayer 8: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 9: 33340 Step\n",
      "End of epoch 20, LoraLayer 9: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 10: 33340 Step\n",
      "End of epoch 20, LoraLayer 10: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 11: 33340 Step\n",
      "End of epoch 20, LoraLayer 11: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 12: 33340 Step\n",
      "End of epoch 20, LoraLayer 12: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 13: 33340 Step\n",
      "End of epoch 20, LoraLayer 13: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 14: 33340 Step\n",
      "End of epoch 20, LoraLayer 14: Decay factor: 0.0\n",
      "\n",
      "Testing loss: 0.7575260996818542, acc: 0.7555000185966492\n",
      "\n",
      "1667/1667 [==============================] - 51s 31ms/step - loss: 0.5864 - accuracy: 0.8025 - val_loss: 0.7575 - val_accuracy: 0.7558\n"
     ]
    }
   ],
   "source": [
    "history_exp25 = exp25_lora_vgg16.fit(x_train, y_train, batch_size=30, epochs=20, validation_data=(x_test, y_test), callbacks=[print_step_callback, TestCallback((x_test, y_test))] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LAh8O2E__a7c",
    "outputId": "7913fd46-3d5e-43c3-a21b-37e64438053b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 4s 11ms/step - loss: 0.7575 - accuracy: 0.7555\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7575260996818542, 0.7555000185966492]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 평가\n",
    "exp25_lora_vgg16.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "id": "GFAWHfeB_a7c",
    "outputId": "42b5f2b2-e3fe-4e09-81b9-84460b7ed130"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 학습 및 검증 정확도 시각화\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# 정확도 그래프\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_exp25.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history_exp25.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# 손실 그래프\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_exp25.history['loss'], label='Training Loss')\n",
    "plt.plot(history_exp25.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9mqm8sMZACK3"
   },
   "source": [
    "## 2-6. (16, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "wj2k2FjkACK_"
   },
   "outputs": [],
   "source": [
    "dataset_size = x_train.shape[0]\n",
    "batch_size = 30\n",
    "epochs = 20\n",
    "\n",
    "total_iteration= int(epochs * (dataset_size / batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "VvfLtCIcACK_"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout, InputLayer\n",
    "import tensorflow as tf\n",
    "\n",
    "# 새로운 레이어를 추가하기 위한 레이어 리스트 초기화\n",
    "new_layers = []\n",
    "\n",
    "# Dense 레이어 카운터\n",
    "dense_layer_count = 0\n",
    "\n",
    "# VGG 모델의 각 레이어 순회\n",
    "for layer in best_vgg16.layers:\n",
    "    if isinstance(layer, InputLayer):\n",
    "        continue\n",
    "    if isinstance(layer, Conv2D):\n",
    "        # Conv2D 레이어를 ConvLoRALayer로 대체\n",
    "        conv_lora_layer = ConvLoRALayer00_cdn2(original_conv_layer=layer, rank=16, alpha=32, total_iteration = total_iteration, trainable=True)\n",
    "        new_layers.append(conv_lora_layer)\n",
    "    elif isinstance(layer, Dense):\n",
    "        dense_layer_count += 1\n",
    "        # 첫 번째와 두 번째 Dense 레이어에만 LoraLayer 적용\n",
    "        if dense_layer_count in [1, 2]:\n",
    "            dense_lora_layer = LoraLayer(original_layer=layer, rank=64, alpha=32, total_iteration=total_iteration, trainable=True)\n",
    "            new_layers.append(dense_lora_layer)\n",
    "        else:\n",
    "            new_layers.append(layer)  # 그대로 유지함\n",
    "    else:\n",
    "        # 다른 유형의 레이어는 그대로 유지\n",
    "        new_layers.append(layer)\n",
    "\n",
    "# 새로운 입력 텐서 생성\n",
    "input_tensor = best_vgg16.input\n",
    "\n",
    "# 새로운 모델 구성\n",
    "x = input_tensor\n",
    "for layer in new_layers:\n",
    "    x = layer(x)\n",
    "\n",
    "# 새로운 모델 생성\n",
    "exp26_lora_vgg16 = Model(inputs=input_tensor, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "yZSvkVUZACK_"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D\n",
    "\n",
    "# 새로 생성된 모델에서 Dense 레이어 중 LoRA가 적용되지 않은 레이어의 trainable을 False로 설정\n",
    "for layer in exp26_lora_vgg16.layers:\n",
    "    if isinstance(layer, Dense) and not isinstance(layer, LoraLayer):\n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "587UuheQACK_",
    "outputId": "63671162-e007-44da-f92a-49b173cff6c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " block1_conv1 (ConvLoRALaye  (None, 32, 32, 64)        11506     \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block1_conv2 (ConvLoRALaye  (None, 32, 32, 64)        55426     \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " block2_conv1 (ConvLoRALaye  (None, 16, 16, 128)       101634    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block2_conv2 (ConvLoRALaye  (None, 16, 16, 128)       184578    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " block3_conv1 (ConvLoRALaye  (None, 8, 8, 256)         350722    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block3_conv2 (ConvLoRALaye  (None, 8, 8, 256)         664066    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block3_conv3 (ConvLoRALaye  (None, 8, 8, 256)         664066    \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 4, 4, 256)         0         \n",
      "                                                                 \n",
      " block4_conv1 (ConvLoRALaye  (None, 4, 4, 512)         1291266   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block4_conv2 (ConvLoRALaye  (None, 4, 4, 512)         2507778   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block4_conv3 (ConvLoRALaye  (None, 4, 4, 512)         2507778   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      " block5_conv1 (ConvLoRALaye  (None, 2, 2, 512)         2507778   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block5_conv2 (ConvLoRALaye  (None, 2, 2, 512)         2507778   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block5_conv3 (ConvLoRALaye  (None, 2, 2, 512)         2507778   \n",
      " r00_cdn2)                                                       \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 1, 1, 512)         0         \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_6 (LoraLayer)         (None, 4096)              2400258   \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_7 (LoraLayer)         (None, 512)               2393090   \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20660632 (78.81 MB)\n",
      "Trainable params: 1741872 (6.64 MB)\n",
      "Non-trainable params: 18918760 (72.17 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "exp26_lora_vgg16.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ugu_3sPBACK_",
    "outputId": "1a1abe8e-8ae7-4f7b-a46a-add76eb92b22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: input_3\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block1_conv1\n",
      "  Trainable parameters: 9712\n",
      "  Non-trainable parameters: 1794\n",
      "Layer: block1_conv2\n",
      "  Trainable parameters: 18496\n",
      "  Non-trainable parameters: 36930\n",
      "Layer: block1_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block2_conv1\n",
      "  Trainable parameters: 27776\n",
      "  Non-trainable parameters: 73858\n",
      "Layer: block2_conv2\n",
      "  Trainable parameters: 36992\n",
      "  Non-trainable parameters: 147586\n",
      "Layer: block2_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block3_conv1\n",
      "  Trainable parameters: 55552\n",
      "  Non-trainable parameters: 295170\n",
      "Layer: block3_conv2\n",
      "  Trainable parameters: 73984\n",
      "  Non-trainable parameters: 590082\n",
      "Layer: block3_conv3\n",
      "  Trainable parameters: 73984\n",
      "  Non-trainable parameters: 590082\n",
      "Layer: block3_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block4_conv1\n",
      "  Trainable parameters: 111104\n",
      "  Non-trainable parameters: 1180162\n",
      "Layer: block4_conv2\n",
      "  Trainable parameters: 147968\n",
      "  Non-trainable parameters: 2359810\n",
      "Layer: block4_conv3\n",
      "  Trainable parameters: 147968\n",
      "  Non-trainable parameters: 2359810\n",
      "Layer: block4_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: block5_conv1\n",
      "  Trainable parameters: 147968\n",
      "  Non-trainable parameters: 2359810\n",
      "Layer: block5_conv2\n",
      "  Trainable parameters: 147968\n",
      "  Non-trainable parameters: 2359810\n",
      "Layer: block5_conv3\n",
      "  Trainable parameters: 147968\n",
      "  Non-trainable parameters: 2359810\n",
      "Layer: block5_pool\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: flatten_2\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: dense_6\n",
      "  Trainable parameters: 299008\n",
      "  Non-trainable parameters: 2101250\n",
      "Layer: dropout_4\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: dense_7\n",
      "  Trainable parameters: 295424\n",
      "  Non-trainable parameters: 2097666\n",
      "Layer: dropout_5\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 0.0\n",
      "Layer: dense_8\n",
      "  Trainable parameters: 0.0\n",
      "  Non-trainable parameters: 5130\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#모델의 각 레이어를 순회하며 파라미터 수를 계산\n",
    "for layer in exp26_lora_vgg16.layers:\n",
    "    trainable_count = np.sum([tf.size(w).numpy() for w in layer.trainable_weights])\n",
    "    non_trainable_count = np.sum([tf.size(w).numpy() for w in layer.non_trainable_weights])\n",
    "\n",
    "    print(f\"Layer: {layer.name}\")\n",
    "    print(f\"  Trainable parameters: {trainable_count}\")\n",
    "    print(f\"  Non-trainable parameters: {non_trainable_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "QIRBl6_HACLA"
   },
   "outputs": [],
   "source": [
    "lora_layers = []  # lora 레이어들을 저장할 리스트\n",
    "\n",
    "for layer in exp26_lora_vgg16.layers:\n",
    "    if isinstance(layer, LoraLayer) or isinstance(layer, ConvLoRALayer00_cdn2):\n",
    "        lora_layers.append(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "rQLsY-jCACLA"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "class PrintCurrentStepCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, lora_layers):\n",
    "        super().__init__()\n",
    "        self.lora_layers = lora_layers\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print()\n",
    "        for i, lora_layer in enumerate(self.lora_layers):\n",
    "            current_step = lora_layer.current_step.value()\n",
    "            decay_factor = lora_layer.decay_factor.value()\n",
    "            print(f\"End of epoch {epoch + 1}, LoraLayer {i}: {current_step} Step\")\n",
    "            print(f\"End of epoch {epoch + 1}, LoraLayer {i}: Decay factor: {decay_factor}\")\n",
    "\n",
    "class TestCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, test_data):\n",
    "        self.test_data = test_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        x, y = self.test_data\n",
    "        loss, acc = self.model.evaluate(x, y, verbose=0)\n",
    "        print('\\nTesting loss: {}, acc: {}\\n'.format(loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "i5VDOlyHACLA"
   },
   "outputs": [],
   "source": [
    "# 콜백 생성 시 lora_layers 딕셔너리의 값만 사용\n",
    "print_step_callback = PrintCurrentStepCallback(lora_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "id": "mYnT-xGlACLA"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# 모델 컴파일\n",
    "exp26_lora_vgg16.compile(optimizer=Adam(learning_rate=0.00005),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sU49T8ueACLA"
   },
   "source": [
    "### 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vw3LrTXoACLA",
    "outputId": "4c8bc6cb-79de-4456-d641-301e2d97ded9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.1181 - accuracy: 0.9630\n",
      "End of epoch 1, LoraLayer 0: 1667 Step\n",
      "End of epoch 1, LoraLayer 0: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 1: 1667 Step\n",
      "End of epoch 1, LoraLayer 1: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 2: 1667 Step\n",
      "End of epoch 1, LoraLayer 2: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 3: 1667 Step\n",
      "End of epoch 1, LoraLayer 3: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 4: 1667 Step\n",
      "End of epoch 1, LoraLayer 4: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 5: 1667 Step\n",
      "End of epoch 1, LoraLayer 5: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 6: 1667 Step\n",
      "End of epoch 1, LoraLayer 6: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 7: 1667 Step\n",
      "End of epoch 1, LoraLayer 7: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 8: 1667 Step\n",
      "End of epoch 1, LoraLayer 8: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 9: 1667 Step\n",
      "End of epoch 1, LoraLayer 9: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 10: 1667 Step\n",
      "End of epoch 1, LoraLayer 10: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 11: 1667 Step\n",
      "End of epoch 1, LoraLayer 11: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 12: 1667 Step\n",
      "End of epoch 1, LoraLayer 12: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 13: 1667 Step\n",
      "End of epoch 1, LoraLayer 13: Decay factor: 1.0\n",
      "End of epoch 1, LoraLayer 14: 1667 Step\n",
      "End of epoch 1, LoraLayer 14: Decay factor: 1.0\n",
      "\n",
      "Testing loss: 2.30263090133667, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 67s 35ms/step - loss: 0.1181 - accuracy: 0.9630 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 2/20\n",
      "1665/1667 [============================>.] - ETA: 0s - loss: 0.0872 - accuracy: 0.9722\n",
      "End of epoch 2, LoraLayer 0: 3334 Step\n",
      "End of epoch 2, LoraLayer 0: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 1: 3334 Step\n",
      "End of epoch 2, LoraLayer 1: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 2: 3334 Step\n",
      "End of epoch 2, LoraLayer 2: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 3: 3334 Step\n",
      "End of epoch 2, LoraLayer 3: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 4: 3334 Step\n",
      "End of epoch 2, LoraLayer 4: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 5: 3334 Step\n",
      "End of epoch 2, LoraLayer 5: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 6: 3334 Step\n",
      "End of epoch 2, LoraLayer 6: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 7: 3334 Step\n",
      "End of epoch 2, LoraLayer 7: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 8: 3334 Step\n",
      "End of epoch 2, LoraLayer 8: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 9: 3334 Step\n",
      "End of epoch 2, LoraLayer 9: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 10: 3334 Step\n",
      "End of epoch 2, LoraLayer 10: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 11: 3334 Step\n",
      "End of epoch 2, LoraLayer 11: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 12: 3334 Step\n",
      "End of epoch 2, LoraLayer 12: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 13: 3334 Step\n",
      "End of epoch 2, LoraLayer 13: Decay factor: 0.9999625086784363\n",
      "End of epoch 2, LoraLayer 14: 3334 Step\n",
      "End of epoch 2, LoraLayer 14: Decay factor: 0.9999625086784363\n",
      "\n",
      "Testing loss: 2.3026416301727295, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 53s 32ms/step - loss: 0.0873 - accuracy: 0.9722 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 3/20\n",
      "1665/1667 [============================>.] - ETA: 0s - loss: 0.0801 - accuracy: 0.9739\n",
      "End of epoch 3, LoraLayer 0: 5001 Step\n",
      "End of epoch 3, LoraLayer 0: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 1: 5001 Step\n",
      "End of epoch 3, LoraLayer 1: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 2: 5001 Step\n",
      "End of epoch 3, LoraLayer 2: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 3: 5001 Step\n",
      "End of epoch 3, LoraLayer 3: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 4: 5001 Step\n",
      "End of epoch 3, LoraLayer 4: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 5: 5001 Step\n",
      "End of epoch 3, LoraLayer 5: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 6: 5001 Step\n",
      "End of epoch 3, LoraLayer 6: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 7: 5001 Step\n",
      "End of epoch 3, LoraLayer 7: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 8: 5001 Step\n",
      "End of epoch 3, LoraLayer 8: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 9: 5001 Step\n",
      "End of epoch 3, LoraLayer 9: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 10: 5001 Step\n",
      "End of epoch 3, LoraLayer 10: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 11: 5001 Step\n",
      "End of epoch 3, LoraLayer 11: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 12: 5001 Step\n",
      "End of epoch 3, LoraLayer 12: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 13: 5001 Step\n",
      "End of epoch 3, LoraLayer 13: Decay factor: 0.9374484419822693\n",
      "End of epoch 3, LoraLayer 14: 5001 Step\n",
      "End of epoch 3, LoraLayer 14: Decay factor: 0.9374484419822693\n",
      "\n",
      "Testing loss: 2.3026821613311768, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 50s 30ms/step - loss: 0.0801 - accuracy: 0.9739 - val_loss: 2.3027 - val_accuracy: 0.1000\n",
      "Epoch 4/20\n",
      "1665/1667 [============================>.] - ETA: 0s - loss: 0.0782 - accuracy: 0.9743\n",
      "End of epoch 4, LoraLayer 0: 6668 Step\n",
      "End of epoch 4, LoraLayer 0: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 1: 6668 Step\n",
      "End of epoch 4, LoraLayer 1: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 2: 6668 Step\n",
      "End of epoch 4, LoraLayer 2: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 3: 6668 Step\n",
      "End of epoch 4, LoraLayer 3: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 4: 6668 Step\n",
      "End of epoch 4, LoraLayer 4: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 5: 6668 Step\n",
      "End of epoch 4, LoraLayer 5: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 6: 6668 Step\n",
      "End of epoch 4, LoraLayer 6: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 7: 6668 Step\n",
      "End of epoch 4, LoraLayer 7: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 8: 6668 Step\n",
      "End of epoch 4, LoraLayer 8: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 9: 6668 Step\n",
      "End of epoch 4, LoraLayer 9: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 10: 6668 Step\n",
      "End of epoch 4, LoraLayer 10: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 11: 6668 Step\n",
      "End of epoch 4, LoraLayer 11: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 12: 6668 Step\n",
      "End of epoch 4, LoraLayer 12: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 13: 6668 Step\n",
      "End of epoch 4, LoraLayer 13: Decay factor: 0.8749343752861023\n",
      "End of epoch 4, LoraLayer 14: 6668 Step\n",
      "End of epoch 4, LoraLayer 14: Decay factor: 0.8749343752861023\n",
      "\n",
      "Testing loss: 2.30275559425354, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 53s 31ms/step - loss: 0.0782 - accuracy: 0.9743 - val_loss: 2.3028 - val_accuracy: 0.1000\n",
      "Epoch 5/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.0787 - accuracy: 0.9737\n",
      "End of epoch 5, LoraLayer 0: 8335 Step\n",
      "End of epoch 5, LoraLayer 0: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 1: 8335 Step\n",
      "End of epoch 5, LoraLayer 1: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 2: 8335 Step\n",
      "End of epoch 5, LoraLayer 2: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 3: 8335 Step\n",
      "End of epoch 5, LoraLayer 3: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 4: 8335 Step\n",
      "End of epoch 5, LoraLayer 4: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 5: 8335 Step\n",
      "End of epoch 5, LoraLayer 5: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 6: 8335 Step\n",
      "End of epoch 5, LoraLayer 6: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 7: 8335 Step\n",
      "End of epoch 5, LoraLayer 7: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 8: 8335 Step\n",
      "End of epoch 5, LoraLayer 8: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 9: 8335 Step\n",
      "End of epoch 5, LoraLayer 9: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 10: 8335 Step\n",
      "End of epoch 5, LoraLayer 10: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 11: 8335 Step\n",
      "End of epoch 5, LoraLayer 11: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 12: 8335 Step\n",
      "End of epoch 5, LoraLayer 12: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 13: 8335 Step\n",
      "End of epoch 5, LoraLayer 13: Decay factor: 0.8124203085899353\n",
      "End of epoch 5, LoraLayer 14: 8335 Step\n",
      "End of epoch 5, LoraLayer 14: Decay factor: 0.8124203085899353\n",
      "\n",
      "Testing loss: 2.30290150642395, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 50s 30ms/step - loss: 0.0788 - accuracy: 0.9737 - val_loss: 2.3029 - val_accuracy: 0.1000\n",
      "Epoch 6/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.0845 - accuracy: 0.9718\n",
      "End of epoch 6, LoraLayer 0: 10002 Step\n",
      "End of epoch 6, LoraLayer 0: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 1: 10002 Step\n",
      "End of epoch 6, LoraLayer 1: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 2: 10002 Step\n",
      "End of epoch 6, LoraLayer 2: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 3: 10002 Step\n",
      "End of epoch 6, LoraLayer 3: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 4: 10002 Step\n",
      "End of epoch 6, LoraLayer 4: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 5: 10002 Step\n",
      "End of epoch 6, LoraLayer 5: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 6: 10002 Step\n",
      "End of epoch 6, LoraLayer 6: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 7: 10002 Step\n",
      "End of epoch 6, LoraLayer 7: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 8: 10002 Step\n",
      "End of epoch 6, LoraLayer 8: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 9: 10002 Step\n",
      "End of epoch 6, LoraLayer 9: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 10: 10002 Step\n",
      "End of epoch 6, LoraLayer 10: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 11: 10002 Step\n",
      "End of epoch 6, LoraLayer 11: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 12: 10002 Step\n",
      "End of epoch 6, LoraLayer 12: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 13: 10002 Step\n",
      "End of epoch 6, LoraLayer 13: Decay factor: 0.7499062418937683\n",
      "End of epoch 6, LoraLayer 14: 10002 Step\n",
      "End of epoch 6, LoraLayer 14: Decay factor: 0.7499062418937683\n",
      "\n",
      "Testing loss: 2.3034744262695312, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 51s 31ms/step - loss: 0.0845 - accuracy: 0.9718 - val_loss: 2.3035 - val_accuracy: 0.1000\n",
      "Epoch 7/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.0977 - accuracy: 0.9669\n",
      "End of epoch 7, LoraLayer 0: 11669 Step\n",
      "End of epoch 7, LoraLayer 0: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 1: 11669 Step\n",
      "End of epoch 7, LoraLayer 1: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 2: 11669 Step\n",
      "End of epoch 7, LoraLayer 2: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 3: 11669 Step\n",
      "End of epoch 7, LoraLayer 3: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 4: 11669 Step\n",
      "End of epoch 7, LoraLayer 4: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 5: 11669 Step\n",
      "End of epoch 7, LoraLayer 5: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 6: 11669 Step\n",
      "End of epoch 7, LoraLayer 6: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 7: 11669 Step\n",
      "End of epoch 7, LoraLayer 7: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 8: 11669 Step\n",
      "End of epoch 7, LoraLayer 8: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 9: 11669 Step\n",
      "End of epoch 7, LoraLayer 9: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 10: 11669 Step\n",
      "End of epoch 7, LoraLayer 10: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 11: 11669 Step\n",
      "End of epoch 7, LoraLayer 11: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 12: 11669 Step\n",
      "End of epoch 7, LoraLayer 12: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 13: 11669 Step\n",
      "End of epoch 7, LoraLayer 13: Decay factor: 0.6873922348022461\n",
      "End of epoch 7, LoraLayer 14: 11669 Step\n",
      "End of epoch 7, LoraLayer 14: Decay factor: 0.6873922348022461\n",
      "\n",
      "Testing loss: 2.304462194442749, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 50s 30ms/step - loss: 0.0976 - accuracy: 0.9669 - val_loss: 2.3045 - val_accuracy: 0.1000\n",
      "Epoch 8/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.1123 - accuracy: 0.9609\n",
      "End of epoch 8, LoraLayer 0: 13336 Step\n",
      "End of epoch 8, LoraLayer 0: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 1: 13336 Step\n",
      "End of epoch 8, LoraLayer 1: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 2: 13336 Step\n",
      "End of epoch 8, LoraLayer 2: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 3: 13336 Step\n",
      "End of epoch 8, LoraLayer 3: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 4: 13336 Step\n",
      "End of epoch 8, LoraLayer 4: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 5: 13336 Step\n",
      "End of epoch 8, LoraLayer 5: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 6: 13336 Step\n",
      "End of epoch 8, LoraLayer 6: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 7: 13336 Step\n",
      "End of epoch 8, LoraLayer 7: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 8: 13336 Step\n",
      "End of epoch 8, LoraLayer 8: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 9: 13336 Step\n",
      "End of epoch 8, LoraLayer 9: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 10: 13336 Step\n",
      "End of epoch 8, LoraLayer 10: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 11: 13336 Step\n",
      "End of epoch 8, LoraLayer 11: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 12: 13336 Step\n",
      "End of epoch 8, LoraLayer 12: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 13: 13336 Step\n",
      "End of epoch 8, LoraLayer 13: Decay factor: 0.6248781681060791\n",
      "End of epoch 8, LoraLayer 14: 13336 Step\n",
      "End of epoch 8, LoraLayer 14: Decay factor: 0.6248781681060791\n",
      "\n",
      "Testing loss: 2.306586265563965, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 50s 30ms/step - loss: 0.1124 - accuracy: 0.9609 - val_loss: 2.3066 - val_accuracy: 0.1000\n",
      "Epoch 9/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.1334 - accuracy: 0.9539\n",
      "End of epoch 9, LoraLayer 0: 15003 Step\n",
      "End of epoch 9, LoraLayer 0: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 1: 15003 Step\n",
      "End of epoch 9, LoraLayer 1: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 2: 15003 Step\n",
      "End of epoch 9, LoraLayer 2: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 3: 15003 Step\n",
      "End of epoch 9, LoraLayer 3: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 4: 15003 Step\n",
      "End of epoch 9, LoraLayer 4: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 5: 15003 Step\n",
      "End of epoch 9, LoraLayer 5: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 6: 15003 Step\n",
      "End of epoch 9, LoraLayer 6: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 7: 15003 Step\n",
      "End of epoch 9, LoraLayer 7: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 8: 15003 Step\n",
      "End of epoch 9, LoraLayer 8: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 9: 15003 Step\n",
      "End of epoch 9, LoraLayer 9: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 10: 15003 Step\n",
      "End of epoch 9, LoraLayer 10: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 11: 15003 Step\n",
      "End of epoch 9, LoraLayer 11: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 12: 15003 Step\n",
      "End of epoch 9, LoraLayer 12: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 13: 15003 Step\n",
      "End of epoch 9, LoraLayer 13: Decay factor: 0.5623641014099121\n",
      "End of epoch 9, LoraLayer 14: 15003 Step\n",
      "End of epoch 9, LoraLayer 14: Decay factor: 0.5623641014099121\n",
      "\n",
      "Testing loss: 2.309565782546997, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 50s 30ms/step - loss: 0.1334 - accuracy: 0.9539 - val_loss: 2.3096 - val_accuracy: 0.1000\n",
      "Epoch 10/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.1605 - accuracy: 0.9436\n",
      "End of epoch 10, LoraLayer 0: 16670 Step\n",
      "End of epoch 10, LoraLayer 0: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 1: 16670 Step\n",
      "End of epoch 10, LoraLayer 1: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 2: 16670 Step\n",
      "End of epoch 10, LoraLayer 2: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 3: 16670 Step\n",
      "End of epoch 10, LoraLayer 3: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 4: 16670 Step\n",
      "End of epoch 10, LoraLayer 4: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 5: 16670 Step\n",
      "End of epoch 10, LoraLayer 5: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 6: 16670 Step\n",
      "End of epoch 10, LoraLayer 6: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 7: 16670 Step\n",
      "End of epoch 10, LoraLayer 7: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 8: 16670 Step\n",
      "End of epoch 10, LoraLayer 8: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 9: 16670 Step\n",
      "End of epoch 10, LoraLayer 9: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 10: 16670 Step\n",
      "End of epoch 10, LoraLayer 10: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 11: 16670 Step\n",
      "End of epoch 10, LoraLayer 11: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 12: 16670 Step\n",
      "End of epoch 10, LoraLayer 12: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 13: 16670 Step\n",
      "End of epoch 10, LoraLayer 13: Decay factor: 0.4998500347137451\n",
      "End of epoch 10, LoraLayer 14: 16670 Step\n",
      "End of epoch 10, LoraLayer 14: Decay factor: 0.4998500347137451\n",
      "\n",
      "Testing loss: 2.3191027641296387, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 51s 30ms/step - loss: 0.1605 - accuracy: 0.9436 - val_loss: 2.3191 - val_accuracy: 0.1000\n",
      "Epoch 11/20\n",
      "1665/1667 [============================>.] - ETA: 0s - loss: 0.1919 - accuracy: 0.9323\n",
      "End of epoch 11, LoraLayer 0: 18337 Step\n",
      "End of epoch 11, LoraLayer 0: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 1: 18337 Step\n",
      "End of epoch 11, LoraLayer 1: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 2: 18337 Step\n",
      "End of epoch 11, LoraLayer 2: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 3: 18337 Step\n",
      "End of epoch 11, LoraLayer 3: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 4: 18337 Step\n",
      "End of epoch 11, LoraLayer 4: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 5: 18337 Step\n",
      "End of epoch 11, LoraLayer 5: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 6: 18337 Step\n",
      "End of epoch 11, LoraLayer 6: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 7: 18337 Step\n",
      "End of epoch 11, LoraLayer 7: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 8: 18337 Step\n",
      "End of epoch 11, LoraLayer 8: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 9: 18337 Step\n",
      "End of epoch 11, LoraLayer 9: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 10: 18337 Step\n",
      "End of epoch 11, LoraLayer 10: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 11: 18337 Step\n",
      "End of epoch 11, LoraLayer 11: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 12: 18337 Step\n",
      "End of epoch 11, LoraLayer 12: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 13: 18337 Step\n",
      "End of epoch 11, LoraLayer 13: Decay factor: 0.4373359680175781\n",
      "End of epoch 11, LoraLayer 14: 18337 Step\n",
      "End of epoch 11, LoraLayer 14: Decay factor: 0.4373359680175781\n",
      "\n",
      "Testing loss: 2.3527004718780518, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 49s 29ms/step - loss: 0.1919 - accuracy: 0.9323 - val_loss: 2.3527 - val_accuracy: 0.1000\n",
      "Epoch 12/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.2407 - accuracy: 0.9150\n",
      "End of epoch 12, LoraLayer 0: 20004 Step\n",
      "End of epoch 12, LoraLayer 0: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 1: 20004 Step\n",
      "End of epoch 12, LoraLayer 1: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 2: 20004 Step\n",
      "End of epoch 12, LoraLayer 2: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 3: 20004 Step\n",
      "End of epoch 12, LoraLayer 3: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 4: 20004 Step\n",
      "End of epoch 12, LoraLayer 4: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 5: 20004 Step\n",
      "End of epoch 12, LoraLayer 5: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 6: 20004 Step\n",
      "End of epoch 12, LoraLayer 6: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 7: 20004 Step\n",
      "End of epoch 12, LoraLayer 7: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 8: 20004 Step\n",
      "End of epoch 12, LoraLayer 8: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 9: 20004 Step\n",
      "End of epoch 12, LoraLayer 9: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 10: 20004 Step\n",
      "End of epoch 12, LoraLayer 10: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 11: 20004 Step\n",
      "End of epoch 12, LoraLayer 11: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 12: 20004 Step\n",
      "End of epoch 12, LoraLayer 12: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 13: 20004 Step\n",
      "End of epoch 12, LoraLayer 13: Decay factor: 0.37482190132141113\n",
      "End of epoch 12, LoraLayer 14: 20004 Step\n",
      "End of epoch 12, LoraLayer 14: Decay factor: 0.37482190132141113\n",
      "\n",
      "Testing loss: 2.399817943572998, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 53s 31ms/step - loss: 0.2407 - accuracy: 0.9150 - val_loss: 2.3998 - val_accuracy: 0.1000\n",
      "Epoch 13/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.2946 - accuracy: 0.8964\n",
      "End of epoch 13, LoraLayer 0: 21671 Step\n",
      "End of epoch 13, LoraLayer 0: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 1: 21671 Step\n",
      "End of epoch 13, LoraLayer 1: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 2: 21671 Step\n",
      "End of epoch 13, LoraLayer 2: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 3: 21671 Step\n",
      "End of epoch 13, LoraLayer 3: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 4: 21671 Step\n",
      "End of epoch 13, LoraLayer 4: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 5: 21671 Step\n",
      "End of epoch 13, LoraLayer 5: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 6: 21671 Step\n",
      "End of epoch 13, LoraLayer 6: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 7: 21671 Step\n",
      "End of epoch 13, LoraLayer 7: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 8: 21671 Step\n",
      "End of epoch 13, LoraLayer 8: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 9: 21671 Step\n",
      "End of epoch 13, LoraLayer 9: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 10: 21671 Step\n",
      "End of epoch 13, LoraLayer 10: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 11: 21671 Step\n",
      "End of epoch 13, LoraLayer 11: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 12: 21671 Step\n",
      "End of epoch 13, LoraLayer 12: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 13: 21671 Step\n",
      "End of epoch 13, LoraLayer 13: Decay factor: 0.31230783462524414\n",
      "End of epoch 13, LoraLayer 14: 21671 Step\n",
      "End of epoch 13, LoraLayer 14: Decay factor: 0.31230783462524414\n",
      "\n",
      "Testing loss: 2.5156848430633545, acc: 0.09989999979734421\n",
      "\n",
      "1667/1667 [==============================] - 55s 33ms/step - loss: 0.2946 - accuracy: 0.8964 - val_loss: 2.5157 - val_accuracy: 0.0998\n",
      "Epoch 14/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.3576 - accuracy: 0.8756\n",
      "End of epoch 14, LoraLayer 0: 23338 Step\n",
      "End of epoch 14, LoraLayer 0: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 1: 23338 Step\n",
      "End of epoch 14, LoraLayer 1: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 2: 23338 Step\n",
      "End of epoch 14, LoraLayer 2: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 3: 23338 Step\n",
      "End of epoch 14, LoraLayer 3: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 4: 23338 Step\n",
      "End of epoch 14, LoraLayer 4: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 5: 23338 Step\n",
      "End of epoch 14, LoraLayer 5: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 6: 23338 Step\n",
      "End of epoch 14, LoraLayer 6: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 7: 23338 Step\n",
      "End of epoch 14, LoraLayer 7: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 8: 23338 Step\n",
      "End of epoch 14, LoraLayer 8: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 9: 23338 Step\n",
      "End of epoch 14, LoraLayer 9: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 10: 23338 Step\n",
      "End of epoch 14, LoraLayer 10: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 11: 23338 Step\n",
      "End of epoch 14, LoraLayer 11: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 12: 23338 Step\n",
      "End of epoch 14, LoraLayer 12: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 13: 23338 Step\n",
      "End of epoch 14, LoraLayer 13: Decay factor: 0.24979376792907715\n",
      "End of epoch 14, LoraLayer 14: 23338 Step\n",
      "End of epoch 14, LoraLayer 14: Decay factor: 0.24979376792907715\n",
      "\n",
      "Testing loss: 2.680372953414917, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 53s 32ms/step - loss: 0.3576 - accuracy: 0.8756 - val_loss: 2.6804 - val_accuracy: 0.1000\n",
      "Epoch 15/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.4350 - accuracy: 0.8484\n",
      "End of epoch 15, LoraLayer 0: 25005 Step\n",
      "End of epoch 15, LoraLayer 0: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 1: 25005 Step\n",
      "End of epoch 15, LoraLayer 1: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 2: 25005 Step\n",
      "End of epoch 15, LoraLayer 2: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 3: 25005 Step\n",
      "End of epoch 15, LoraLayer 3: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 4: 25005 Step\n",
      "End of epoch 15, LoraLayer 4: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 5: 25005 Step\n",
      "End of epoch 15, LoraLayer 5: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 6: 25005 Step\n",
      "End of epoch 15, LoraLayer 6: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 7: 25005 Step\n",
      "End of epoch 15, LoraLayer 7: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 8: 25005 Step\n",
      "End of epoch 15, LoraLayer 8: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 9: 25005 Step\n",
      "End of epoch 15, LoraLayer 9: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 10: 25005 Step\n",
      "End of epoch 15, LoraLayer 10: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 11: 25005 Step\n",
      "End of epoch 15, LoraLayer 11: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 12: 25005 Step\n",
      "End of epoch 15, LoraLayer 12: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 13: 25005 Step\n",
      "End of epoch 15, LoraLayer 13: Decay factor: 0.18727970123291016\n",
      "End of epoch 15, LoraLayer 14: 25005 Step\n",
      "End of epoch 15, LoraLayer 14: Decay factor: 0.18727970123291016\n",
      "\n",
      "Testing loss: 2.6999385356903076, acc: 0.10000000149011612\n",
      "\n",
      "1667/1667 [==============================] - 51s 31ms/step - loss: 0.4350 - accuracy: 0.8484 - val_loss: 2.6999 - val_accuracy: 0.1000\n",
      "Epoch 16/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.5216 - accuracy: 0.8198\n",
      "End of epoch 16, LoraLayer 0: 26672 Step\n",
      "End of epoch 16, LoraLayer 0: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 1: 26672 Step\n",
      "End of epoch 16, LoraLayer 1: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 2: 26672 Step\n",
      "End of epoch 16, LoraLayer 2: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 3: 26672 Step\n",
      "End of epoch 16, LoraLayer 3: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 4: 26672 Step\n",
      "End of epoch 16, LoraLayer 4: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 5: 26672 Step\n",
      "End of epoch 16, LoraLayer 5: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 6: 26672 Step\n",
      "End of epoch 16, LoraLayer 6: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 7: 26672 Step\n",
      "End of epoch 16, LoraLayer 7: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 8: 26672 Step\n",
      "End of epoch 16, LoraLayer 8: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 9: 26672 Step\n",
      "End of epoch 16, LoraLayer 9: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 10: 26672 Step\n",
      "End of epoch 16, LoraLayer 10: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 11: 26672 Step\n",
      "End of epoch 16, LoraLayer 11: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 12: 26672 Step\n",
      "End of epoch 16, LoraLayer 12: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 13: 26672 Step\n",
      "End of epoch 16, LoraLayer 13: Decay factor: 0.12476563453674316\n",
      "End of epoch 16, LoraLayer 14: 26672 Step\n",
      "End of epoch 16, LoraLayer 14: Decay factor: 0.12476563453674316\n",
      "\n",
      "Testing loss: 2.6502864360809326, acc: 0.10040000081062317\n",
      "\n",
      "1667/1667 [==============================] - 52s 31ms/step - loss: 0.5215 - accuracy: 0.8199 - val_loss: 2.6503 - val_accuracy: 0.1004\n",
      "Epoch 17/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.6066 - accuracy: 0.7919\n",
      "End of epoch 17, LoraLayer 0: 28339 Step\n",
      "End of epoch 17, LoraLayer 0: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 1: 28339 Step\n",
      "End of epoch 17, LoraLayer 1: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 2: 28339 Step\n",
      "End of epoch 17, LoraLayer 2: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 3: 28339 Step\n",
      "End of epoch 17, LoraLayer 3: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 4: 28339 Step\n",
      "End of epoch 17, LoraLayer 4: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 5: 28339 Step\n",
      "End of epoch 17, LoraLayer 5: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 6: 28339 Step\n",
      "End of epoch 17, LoraLayer 6: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 7: 28339 Step\n",
      "End of epoch 17, LoraLayer 7: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 8: 28339 Step\n",
      "End of epoch 17, LoraLayer 8: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 9: 28339 Step\n",
      "End of epoch 17, LoraLayer 9: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 10: 28339 Step\n",
      "End of epoch 17, LoraLayer 10: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 11: 28339 Step\n",
      "End of epoch 17, LoraLayer 11: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 12: 28339 Step\n",
      "End of epoch 17, LoraLayer 12: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 13: 28339 Step\n",
      "End of epoch 17, LoraLayer 13: Decay factor: 0.06225156784057617\n",
      "End of epoch 17, LoraLayer 14: 28339 Step\n",
      "End of epoch 17, LoraLayer 14: Decay factor: 0.06225156784057617\n",
      "\n",
      "Testing loss: 2.515119791030884, acc: 0.19280000030994415\n",
      "\n",
      "1667/1667 [==============================] - 51s 31ms/step - loss: 0.6066 - accuracy: 0.7918 - val_loss: 2.5150 - val_accuracy: 0.1926\n",
      "Epoch 18/20\n",
      "1667/1667 [==============================] - ETA: 0s - loss: 0.6873 - accuracy: 0.7656\n",
      "End of epoch 18, LoraLayer 0: 30006 Step\n",
      "End of epoch 18, LoraLayer 0: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 1: 30006 Step\n",
      "End of epoch 18, LoraLayer 1: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 2: 30006 Step\n",
      "End of epoch 18, LoraLayer 2: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 3: 30006 Step\n",
      "End of epoch 18, LoraLayer 3: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 4: 30006 Step\n",
      "End of epoch 18, LoraLayer 4: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 5: 30006 Step\n",
      "End of epoch 18, LoraLayer 5: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 6: 30006 Step\n",
      "End of epoch 18, LoraLayer 6: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 7: 30006 Step\n",
      "End of epoch 18, LoraLayer 7: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 8: 30006 Step\n",
      "End of epoch 18, LoraLayer 8: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 9: 30006 Step\n",
      "End of epoch 18, LoraLayer 9: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 10: 30006 Step\n",
      "End of epoch 18, LoraLayer 10: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 11: 30006 Step\n",
      "End of epoch 18, LoraLayer 11: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 12: 30006 Step\n",
      "End of epoch 18, LoraLayer 12: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 13: 30006 Step\n",
      "End of epoch 18, LoraLayer 13: Decay factor: 0.0\n",
      "End of epoch 18, LoraLayer 14: 30006 Step\n",
      "End of epoch 18, LoraLayer 14: Decay factor: 0.0\n",
      "\n",
      "Testing loss: 0.7957940101623535, acc: 0.7287999987602234\n",
      "\n",
      "1667/1667 [==============================] - 49s 30ms/step - loss: 0.6873 - accuracy: 0.7656 - val_loss: 0.7958 - val_accuracy: 0.7288\n",
      "Epoch 19/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.6490 - accuracy: 0.7799\n",
      "End of epoch 19, LoraLayer 0: 31673 Step\n",
      "End of epoch 19, LoraLayer 0: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 1: 31673 Step\n",
      "End of epoch 19, LoraLayer 1: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 2: 31673 Step\n",
      "End of epoch 19, LoraLayer 2: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 3: 31673 Step\n",
      "End of epoch 19, LoraLayer 3: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 4: 31673 Step\n",
      "End of epoch 19, LoraLayer 4: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 5: 31673 Step\n",
      "End of epoch 19, LoraLayer 5: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 6: 31673 Step\n",
      "End of epoch 19, LoraLayer 6: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 7: 31673 Step\n",
      "End of epoch 19, LoraLayer 7: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 8: 31673 Step\n",
      "End of epoch 19, LoraLayer 8: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 9: 31673 Step\n",
      "End of epoch 19, LoraLayer 9: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 10: 31673 Step\n",
      "End of epoch 19, LoraLayer 10: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 11: 31673 Step\n",
      "End of epoch 19, LoraLayer 11: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 12: 31673 Step\n",
      "End of epoch 19, LoraLayer 12: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 13: 31673 Step\n",
      "End of epoch 19, LoraLayer 13: Decay factor: 0.0\n",
      "End of epoch 19, LoraLayer 14: 31673 Step\n",
      "End of epoch 19, LoraLayer 14: Decay factor: 0.0\n",
      "\n",
      "Testing loss: 0.7255613207817078, acc: 0.755299985408783\n",
      "\n",
      "1667/1667 [==============================] - 53s 32ms/step - loss: 0.6489 - accuracy: 0.7799 - val_loss: 0.7255 - val_accuracy: 0.7553\n",
      "Epoch 20/20\n",
      "1666/1667 [============================>.] - ETA: 0s - loss: 0.5724 - accuracy: 0.8048\n",
      "End of epoch 20, LoraLayer 0: 33340 Step\n",
      "End of epoch 20, LoraLayer 0: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 1: 33340 Step\n",
      "End of epoch 20, LoraLayer 1: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 2: 33340 Step\n",
      "End of epoch 20, LoraLayer 2: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 3: 33340 Step\n",
      "End of epoch 20, LoraLayer 3: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 4: 33340 Step\n",
      "End of epoch 20, LoraLayer 4: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 5: 33340 Step\n",
      "End of epoch 20, LoraLayer 5: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 6: 33340 Step\n",
      "End of epoch 20, LoraLayer 6: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 7: 33340 Step\n",
      "End of epoch 20, LoraLayer 7: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 8: 33340 Step\n",
      "End of epoch 20, LoraLayer 8: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 9: 33340 Step\n",
      "End of epoch 20, LoraLayer 9: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 10: 33340 Step\n",
      "End of epoch 20, LoraLayer 10: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 11: 33340 Step\n",
      "End of epoch 20, LoraLayer 11: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 12: 33340 Step\n",
      "End of epoch 20, LoraLayer 12: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 13: 33340 Step\n",
      "End of epoch 20, LoraLayer 13: Decay factor: 0.0\n",
      "End of epoch 20, LoraLayer 14: 33340 Step\n",
      "End of epoch 20, LoraLayer 14: Decay factor: 0.0\n",
      "\n",
      "Testing loss: 0.7579485774040222, acc: 0.7416999936103821\n",
      "\n",
      "1667/1667 [==============================] - 50s 30ms/step - loss: 0.5723 - accuracy: 0.8049 - val_loss: 0.7579 - val_accuracy: 0.7419\n"
     ]
    }
   ],
   "source": [
    "history_exp26 = exp26_lora_vgg16.fit(x_train, y_train, batch_size=30, epochs=20, validation_data=(x_test, y_test), callbacks=[print_step_callback, TestCallback((x_test, y_test))] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p2oLoaoZACLB",
    "outputId": "7913fd46-3d5e-43c3-a21b-37e64438053b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 3s 10ms/step - loss: 0.7579 - accuracy: 0.7417\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7579485774040222, 0.7416999936103821]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 평가\n",
    "exp26_lora_vgg16.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "id": "7lmu1SLyACLB",
    "outputId": "42b5f2b2-e3fe-4e09-81b9-84460b7ed130"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA+kAAAHWCAYAAAALjsguAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAACydUlEQVR4nOzdeZiN9f/H8eeZ7cy+YJgZxj52xi5ki5psZakk+9aGkpSkhIpfoYTKt7KkEhFShCFkK6SRsm8zlrGbGbNv5/fHMYcxM8ww48zyelzXfZ1zPvf2PmcO537fn81gMplMiIiIiIiIiIjV2Vg7ABERERERERExU5IuIiIiIiIikk8oSRcRERERERHJJ5Ski4iIiIiIiOQTStJFRERERERE8gkl6SIiIiIiIiL5hJJ0ERERERERkXxCSbqIiIiIiIhIPqEkXURERERERCSfUJIu+Ur//v0pX778Xe07fvx4DAZD7gaUz5w8eRKDwcD8+fPv+7kNBgPjx4+3vJ4/fz4Gg4GTJ0/ecd/y5cvTv3//XI3nXr4rIiJSOOi64fZ03XCDrhukIFGSLtliMBiytWzatMnaoRZ5L730EgaDgaNHj2a5zdixYzEYDPzzzz/3MbKcO3v2LOPHjyckJMTaoWTqwIEDGAwGHB0diYiIsHY4IiL5hq4bCg5dN+SttBslU6dOtXYoUoDYWTsAKRi++eabdK8XLFhAcHBwhvLq1avf03m+/PJLUlNT72rft956izfeeOOezl8Y9OrVi5kzZ7Jw4ULGjRuX6Tbff/89tWvXpk6dOnd9nj59+vD0009jNBrv+hh3cvbsWSZMmED58uWpW7duunX38l3JLd9++y0+Pj5cvXqVpUuXMnjwYKvGIyKSX+i6oeDQdYNI/qMkXbKld+/e6V7/8ccfBAcHZyi/VWxsLM7Oztk+j729/V3FB2BnZ4ednb7STZo0oXLlynz//feZ/tju2LGDEydO8H//93/3dB5bW1tsbW3v6Rj34l6+K7nBZDKxcOFCnnnmGU6cOMF3332Xb5P0mJgYXFxcrB2GiBQhum4oOHTdIJL/qLm75JrWrVtTq1Yt/vrrL1q2bImzszNvvvkmAD/99BMdO3bEz88Po9FIpUqVePfdd0lJSUl3jFv7C93cROiLL76gUqVKGI1GGjVqxK5du9Ltm1nfMoPBwLBhw1ixYgW1atXCaDRSs2ZN1qxZkyH+TZs20bBhQxwdHalUqRL/+9//st1fbcuWLTz55JOULVsWo9GIv78/r7zyCnFxcRnen6urK2fOnKFLly64urri7e3NqFGjMnwWERER9O/fHw8PDzw9PenXr1+2m1T36tWLgwcPsmfPngzrFi5ciMFgoGfPniQmJjJu3DgaNGiAh4cHLi4utGjRgo0bN97xHJn1LTOZTLz33nuUKVMGZ2dn2rRpw3///Zdh3ytXrjBq1Chq166Nq6sr7u7utG/fnr1791q22bRpE40aNQJgwIABlqaRaf3qMutbFhMTw6uvvoq/vz9Go5GqVasydepUTCZTuu1y8r3IyrZt2zh58iRPP/00Tz/9NL///junT5/OsF1qaiqffPIJtWvXxtHREW9vbx599FF2796dbrtvv/2Wxo0b4+zsjJeXFy1btmTdunXpYr65b1+aW/vtpf1dNm/ezIsvvkjJkiUpU6YMAKGhobz44otUrVoVJycnihcvzpNPPplp/8CIiAheeeUVypcvj9FopEyZMvTt25dLly4RHR2Ni4sLL7/8cob9Tp8+ja2tLZMnT87mJykiRZWuG3TdUJSuG+7kwoULDBo0iFKlSuHo6EhgYCBff/11hu0WLVpEgwYNcHNzw93dndq1a/PJJ59Y1iclJTFhwgQCAgJwdHSkePHiPPjggwQHB+darJL3dPtQctXly5dp3749Tz/9NL1796ZUqVKA+T9mV1dXRo4ciaurK7/99hvjxo0jKiqKKVOm3PG4Cxcu5Nq1azz33HMYDAY+/PBDunXrxvHjx+94Z3Tr1q0sW7aMF198ETc3N2bMmEH37t0JCwujePHiAPz99988+uij+Pr6MmHCBFJSUpg4cSLe3t7Zet9LliwhNjaWF154geLFi7Nz505mzpzJ6dOnWbJkSbptU1JSCAoKokmTJkydOpX169czbdo0KlWqxAsvvACYf7Qef/xxtm7dyvPPP0/16tVZvnw5/fr1y1Y8vXr1YsKECSxcuJD69eunO/cPP/xAixYtKFu2LJcuXeKrr76iZ8+eDBkyhGvXrjFnzhyCgoLYuXNnhqZidzJu3Djee+89OnToQIcOHdizZw+PPPIIiYmJ6bY7fvw4K1as4Mknn6RChQqcP3+e//3vf7Rq1Yr9+/fj5+dH9erVmThxIuPGjePZZ5+lRYsWADRr1izTc5tMJh577DE2btzIoEGDqFu3LmvXruW1117jzJkzfPzxx+m2z8734na+++47KlWqRKNGjahVqxbOzs58//33vPbaa+m2GzRoEPPnz6d9+/YMHjyY5ORktmzZwh9//EHDhg0BmDBhAuPHj6dZs2ZMnDgRBwcH/vzzT3777TceeeSRbH/+N3vxxRfx9vZm3LhxxMTEALBr1y62b9/O008/TZkyZTh58iSff/45rVu3Zv/+/Zbaq+joaFq0aMGBAwcYOHAg9evX59KlS6xcuZLTp09Tt25dunbtyuLFi/noo4/S1Yx8//33mEwmevXqdVdxi0jRousGXTcUleuG24mLi6N169YcPXqUYcOGUaFCBZYsWUL//v2JiIiw3BQPDg6mZ8+etG3blg8++AAwj4+zbds2yzbjx49n8uTJDB48mMaNGxMVFcXu3bvZs2cPDz/88D3FKfeRSeQuDB061HTr16dVq1YmwDR79uwM28fGxmYoe+6550zOzs6m+Ph4S1m/fv1M5cqVs7w+ceKECTAVL17cdOXKFUv5Tz/9ZAJMP//8s6XsnXfeyRATYHJwcDAdPXrUUrZ3714TYJo5c6alrHPnziZnZ2fTmTNnLGVHjhwx2dnZZThmZjJ7f5MnTzYZDAZTaGhouvcHmCZOnJhu23r16pkaNGhgeb1ixQoTYPrwww8tZcnJyaYWLVqYANO8efPuGFOjRo1MZcqUMaWkpFjK1qxZYwJM//vf/yzHTEhISLff1atXTaVKlTINHDgwXTlgeueddyyv582bZwJMJ06cMJlMJtOFCxdMDg4Opo4dO5pSU1Mt27355psmwNSvXz9LWXx8fLq4TCbz39poNKb7bHbt2pXl+731u5L2mb333nvptnviiSdMBoMh3Xcgu9+LrCQmJpqKFy9uGjt2rKXsmWeeMQUGBqbb7rfffjMBppdeeinDMdI+oyNHjphsbGxMXbt2zfCZ3Pw53vr5pylXrly6zzbt7/Lggw+akpOT022b2fd0x44dJsC0YMECS9m4ceNMgGnZsmVZxr127VoTYPr111/Tra9Tp46pVatWGfYTkaJN1w13fn+6bjArbNcNad/JKVOmZLnN9OnTTYDp22+/tZQlJiaamjZtanJ1dTVFRUWZTCaT6eWXXza5u7tn+H2/WWBgoKljx463jUnyPzV3l1xlNBoZMGBAhnInJyfL82vXrnHp0iVatGhBbGwsBw8evONxe/TogZeXl+V12t3R48eP33Hfdu3aUalSJcvrOnXq4O7ubtk3JSWF9evX06VLF/z8/CzbVa5cmfbt29/x+JD+/cXExHDp0iWaNWuGyWTi77//zrD9888/n+51ixYt0r2X1atXY2dnZ7lDDua+XMOHD89WPGDuD3j69Gl+//13S9nChQtxcHDgySeftBzTwcEBMDfLvnLlCsnJyTRs2DDTJm+3s379ehITExk+fHi6pn4jRozIsK3RaMTGxvzfT0pKCpcvX8bV1ZWqVavm+LxpVq9eja2tLS+99FK68ldffRWTycSvv/6arvxO34vb+fXXX7l8+TI9e/a0lPXs2ZO9e/ema6b3448/YjAYeOeddzIcI+0zWrFiBampqYwbN87ymdy6zd0YMmRIhr5/N39Pk5KSuHz5MpUrV8bT0zPd5/7jjz8SGBhI165ds4y7Xbt2+Pn58d1331nW/fvvv/zzzz937HMqIpJG1w26bigK1w3ZicXHxyfddYW9vT0vvfQS0dHRbN68GQBPT09iYmJu23Td09OT//77jyNHjtxzXGI9StIlV5UuXdryn/fN/vvvP7p27YqHhwfu7u54e3tbLuQjIyPveNyyZcume532w3v16tUc75u2f9q+Fy5cIC4ujsqVK2fYLrOyzISFhdG/f3+KFStm6S/WqlUrIOP7S+uXnFU8YO477Ovri6ura7rtqlatmq14AJ5++mlsbW1ZuHAhAPHx8Sxfvpz27dunu3D5+uuvqVOnjqXfkre3N6tWrcrW3+VmoaGhAAQEBKQr9/b2Tnc+MP+wf/zxxwQEBGA0GilRogTe3t78888/OT7vzef38/PDzc0tXXnayMFp8aW50/fidr799lsqVKiA0Wjk6NGjHD16lEqVKuHs7JwuaT127Bh+fn4UK1Ysy2MdO3YMGxsbatSoccfz5kSFChUylMXFxTFu3DhL37u0zz0iIiLd537s2DFq1ap12+Pb2NjQq1cvVqxYQWxsLGDuAuDo6Gi5mBMRuRNdN+i6oShcN2QnloCAgAw362+N5cUXX6RKlSq0b9+eMmXKMHDgwAz94idOnEhERARVqlShdu3avPbaa/l+6jzJSEm65Kqb7wyniYiIoFWrVuzdu5eJEyfy888/ExwcbOlLk53pMLIaDdR0y8Aeub1vdqSkpPDwww+zatUqRo8ezYoVKwgODrYMVHLr+7tfI5uWLFmShx9+mB9//JGkpCR+/vlnrl27lq6v8Lfffkv//v2pVKkSc+bMYc2aNQQHB/PQQw/l6TQlkyZNYuTIkbRs2ZJvv/2WtWvXEhwcTM2aNe/b9Ch3+72Iiori559/5sSJEwQEBFiWGjVqEBsby8KFC3Ptu5Udtw4clCazf4vDhw/n/fff56mnnuKHH35g3bp1BAcHU7x48bv63Pv27Ut0dDQrVqywjHbfqVMnPDw8cnwsESmadN2g64bsKMjXDbmpZMmShISEsHLlSkt/+vbt26cbe6Bly5YcO3aMuXPnUqtWLb766ivq16/PV199dd/ilHungeMkz23atInLly+zbNkyWrZsaSk/ceKEFaO6oWTJkjg6OnL06NEM6zIru9W+ffs4fPgwX3/9NX379rWU38somuXKlWPDhg1ER0enuyt+6NChHB2nV69erFmzhl9//ZWFCxfi7u5O586dLeuXLl1KxYoVWbZsWbqmZpk1z85OzABHjhyhYsWKlvKLFy9muMu8dOlS2rRpw5w5c9KVR0REUKJECcvrnDT3LleuHOvXr+fatWvp7oqnNYtMi+9eLVu2jPj4eD7//PN0sYL57/PWW2+xbds2HnzwQSpVqsTatWu5cuVKlrXplSpVIjU1lf379992wB0vL68Mo/QmJiYSHh6e7diXLl1Kv379mDZtmqUsPj4+w3ErVarEv//+e8fj1apVi3r16vHdd99RpkwZwsLCmDlzZrbjERHJjK4bck7XDWb58bohu7H8888/pKampqtNzywWBwcHOnfuTOfOnUlNTeXFF1/kf//7H2+//balJUexYsUYMGAAAwYMIDo6mpYtWzJ+/Ph8O1WsZKSadMlzaXceb77TmJiYyGeffWatkNKxtbWlXbt2rFixgrNnz1rKjx49mqE/Ulb7Q/r3ZzKZ0k2HkVMdOnQgOTmZzz//3FKWkpKS4wSoS5cuODs789lnn/Hrr7/SrVs3HB0dbxv7n3/+yY4dO3Icc7t27bC3t2fmzJnpjjd9+vQM29ra2ma487xkyRLOnDmTrixtbu/sTCHToUMHUlJSmDVrVrryjz/+GIPBkO1+gnfy7bffUrFiRZ5//nmeeOKJdMuoUaNwdXW1NHnv3r07JpOJCRMmZDhO2vvv0qULNjY2TJw4MUNtwM2fUaVKldL1EwT44osvsqxJz0xmn/vMmTMzHKN79+7s3buX5cuXZxl3mj59+rBu3TqmT59O8eLFc+1zFpGiS9cNOafrBrP8eN2QHR06dODcuXMsXrzYUpacnMzMmTNxdXW1dIW4fPlyuv1sbGyoU6cOAAkJCZlu4+rqSuXKlS3rpWBQTbrkuWbNmuHl5UW/fv146aWXMBgMfPPNN/e1edCdjB8/nnXr1tG8eXNeeOEFy3/atWrVIiQk5Lb7VqtWjUqVKjFq1CjOnDmDu7s7P/744z31UercuTPNmzfnjTfe4OTJk9SoUYNly5bluN+Vq6srXbp0sfQvu3VarE6dOrFs2TK6du1Kx44dOXHiBLNnz6ZGjRpER0fn6Fxp87ZOnjyZTp060aFDB/7++29+/fXXDDXOnTp1YuLEiQwYMIBmzZqxb98+vvvuu3R30sGcmHp6ejJ79mzc3NxwcXGhSZMmmfa37ty5M23atGHs2LGcPHmSwMBA1q1bx08//cSIESPSDfZyt86ePcvGjRszDDKTxmg0EhQUxJIlS5gxYwZt2rShT58+zJgxgyNHjvDoo4+SmprKli1baNOmDcOGDaNy5cqMHTuWd999lxYtWtCtWzeMRiO7du3Cz8/PMt/44MGDef755+nevTsPP/wwe/fuZe3atRk+29vp1KkT33zzDR4eHtSoUYMdO3awfv36DFPHvPbaayxdupQnn3ySgQMH0qBBA65cucLKlSuZPXs2gYGBlm2feeYZXn/9dZYvX84LL7xwx6mNRETuRNcNOafrBrP8dt1wsw0bNhAfH5+hvEuXLjz77LP873//o3///vz111+UL1+epUuXsm3bNqZPn26p6R88eDBXrlzhoYceokyZMoSGhjJz5kzq1q1r6b9eo0YNWrduTYMGDShWrBi7d+9m6dKlDBs2LFffj+Sx+zCCvBRCWU2lUrNmzUy337Ztm+mBBx4wOTk5mfz8/Eyvv/66ZQqnjRs3WrbLaiqVzKat4JapPbKaSmXo0KEZ9r112iqTyWTasGGDqV69eiYHBwdTpUqVTF999ZXp1VdfNTk6OmbxKdywf/9+U7t27Uyurq6mEiVKmIYMGWKZmuPmaUD69etncnFxybB/ZrFfvnzZ1KdPH5O7u7vJw8PD1KdPH9Pff/+d7alU0qxatcoEmHx9fTOd4mvSpEmmcuXKmYxGo6levXqmX375JcPfwWS681QqJpPJlJKSYpowYYLJ19fX5OTkZGrdurXp33//zfB5x8fHm1599VXLds2bNzft2LHD1KpVqwzTd/3000+mGjVqWKa1SXvvmcV47do10yuvvGLy8/Mz2dvbmwICAkxTpkxJN7VL2nvJ7vfiZtOmTTMBpg0bNmS5zfz5802A6aeffjKZTObpaqZMmWKqVq2aycHBweTt7W1q37696a+//kq339y5c0316tUzGY1Gk5eXl6lVq1am4OBgy/qUlBTT6NGjTSVKlDA5OzubgoKCTEePHs1yCrZdu3ZliO3q1aumAQMGmEqUKGFydXU1BQUFmQ4ePJjp+758+bJp2LBhptKlS5scHBxMZcqUMfXr18906dKlDMft0KGDCTBt3749y89FRIo2XTekp+sGs8J+3WAy3fhOZrV88803JpPJZDp//rzlN9rBwcFUu3btDH+3pUuXmh555BFTyZIlTQ4ODqayZcuannvuOVN4eLhlm/fee8/UuHFjk6enp8nJyclUrVo10/vvv29KTEy8bZySvxhMpnx0W1Ikn+nSpYumsRC5g65du7Jv375s9cUUESnMdN0gIrlBfdJFrouLi0v3+siRI6xevZrWrVtbJyCRAiA8PJxVq1bRp08fa4ciInJf6bpBRPKKatJFrvP19aV///5UrFiR0NBQPv/8cxISEvj7778zzOEpUtSdOHGCbdu28dVXX7Fr1y6OHTuGj4+PtcMSEblvdN0gInlFA8eJXPfoo4/y/fffc+7cOYxGI02bNmXSpEn6oRXJxObNmxkwYABly5bl66+/VoIuIkWOrhtEJK+oJl1EREREREQkn1CfdBEREREREZF8Qkm6iIiIiIiISD5R5Pqkp6amcvbsWdzc3DAYDNYOR0REBJPJxLVr1/Dz88PGRvfPc4N+70VEJD/JyW99kUvSz549i7+/v7XDEBERyeDUqVOUKVPG2mEUCvq9FxGR/Cg7v/VFLkl3c3MDzB+Ou7u7laMRERGBqKgo/P39Lb9Rcu/0ey8iIvlJTn7ri1ySntbkzd3dXT/aIiKSr6hZdu7R772IiORH2fmtt2rHt99//53OnTvj5+eHwWBgxYoVd9xn06ZN1K9fH6PRSOXKlZk/f36exykiIiIiIiJyP1g1SY+JiSEwMJBPP/00W9ufOHGCjh070qZNG0JCQhgxYgSDBw9m7dq1eRypiIiIiIiISN6zanP39u3b0759+2xvP3v2bCpUqMC0adMAqF69Olu3buXjjz8mKCgor8IUERERERERuS8KVJ/0HTt20K5du3RlQUFBjBgxIst9EhISSEhIsLyOiorKq/BERESkgDCZTCQnJ5OSkmLtUKSQsbW1xc7OTmNMiMhdK1BJ+rlz5yhVqlS6slKlShEVFUVcXBxOTk4Z9pk8eTITJky4XyGKiIhIPpeYmEh4eDixsbHWDkUKKWdnZ3x9fXFwcLB2KCJSABWoJP1ujBkzhpEjR1pepw19LyIiIkVPamoqJ06cwNbWFj8/PxwcHFTjKbnGZDKRmJjIxYsXOXHiBAEBAdjYWHUIKBEpgApUku7j48P58+fTlZ0/fx53d/dMa9EBjEYjRqPxfoQnIiIi+VxiYiKpqan4+/vj7Oxs7XCkEHJycsLe3p7Q0FASExNxdHS0dkgiUsAUqFt7TZs2ZcOGDenKgoODadq0qZUiEhERkYJItZuSl/T9EpF7YdX/QaKjowkJCSEkJAQwT7EWEhJCWFgYYG6q3rdvX8v2zz//PMePH+f111/n4MGDfPbZZ/zwww+88sor1ghfREREREREJFdZNUnfvXs39erVo169egCMHDmSevXqMW7cOADCw8MtCTtAhQoVWLVqFcHBwQQGBjJt2jS++uorTb8mIiIiIiIihYJV+6S3bt0ak8mU5fr58+dnus/ff/+dh1GJiIiIFA3ly5dnxIgRt53O9mabNm2iTZs2XL16FU9PzzyNTUSkqFKHGREREZF8zmAw3HYZP378XR13165dPPvss9nevlmzZoSHh+Ph4XFX58uuTZs2YTAYiIiIyNPziIjkRwVqdHcRERGRoig8PNzyfPHixYwbN45Dhw5ZylxdXS3PTSYTKSkp2Nnd+TLP29s7R3E4ODjg4+OTo31ERCRnlKTnQ6mpJuKTU0hISk33GJ+USkJSCvHJqcQnpZCQ9njT8/ikVBKub5u2TXJqKqmpkGoykWoy/3inPU81mTCZ0tbdvD6T7a8fA8DGYMDWxoCNjQEbA9gazM9tbyq3NYCtjfkO/63ladvaGAzY2RpwtLfFyd4WR3sb83MHWxztzI/mcvO69K/Nz+1tDZrjVkRE7prJZCIuKcUq53ayt83Wb9jNibGHhwcGg8FSltYEffXq1bz11lvs27ePdevW4e/vz8iRI/njjz+IiYmhevXqTJ48mXbt2lmOdWtzd4PBwJdffsmqVatYu3YtpUuXZtq0aTz22GPpzpXW3H3+/PmMGDGCxYsXM2LECE6dOsWDDz7IvHnz8PX1BSA5OZmRI0eyYMECbG1tGTx4MOfOnSMyMpIVK1bc1ed29epVXn75ZX7++WcSEhJo1aoVM2bMICAgAIDQ0FCGDRvG1q1bSUxMpHz58kyZMoUOHTpw9epVhg0bxrp164iOjqZMmTK8+eabDBgw4K5iEcmxlCSIPA0RoXA19KbHMHAtCdU7Q5VHwcnT2pGKlShJt6Jr8UkcOneNA+FRHLj+eOR8NNEJydYOrUCxtTHgaGdO4NMS92IuDpR0d8Tb1UhJdyMl3Yx4uxkp6eaIt5sRL2d7JfYiIgJAXFIKNcattcq5908Mwtkhdy7H3njjDaZOnUrFihXx8vLi1KlTdOjQgffffx+j0ciCBQvo3Lkzhw4domzZslkeZ8KECXz44YdMmTKFmTNn0qtXL0JDQylWrFim28fGxjJ16lS++eYbbGxs6N27N6NGjeK7774D4IMPPuC7775j3rx5VK9enU8++YQVK1bQpk2bu36v/fv358iRI6xcuRJ3d3dGjx5Nhw4d2L9/P/b29gwdOpTExER+//13XFxc2L9/v6W1wdtvv83+/fv59ddfKVGiBEePHiUuLu6uYxHJwGSC6PO3JOAnzY9XQyHqDJhuc2Pw4C9gYw+V2kD1x6BaR3DO/N+fFE5K0u+D1FQToVdiOXhTMn7wXBSnrtz5B8He1oCjnS1GexuM1x8d7cy1yka7GzXPRjsbS+2y0c4Go/2NbRyu1zTbGAwYDGBjwPLaxsBN5TfKDDets7FJvz1ASqq5Bj4l1USKyUTq9cesylNTzbXyKanmWvm09SYTJKWkWmr+45NSiLM8phKfePPrFEtrgdjEZFJNN2KJSUwhJjH7tSD2tga8Xc2Ju7ebIyXdjZaE3vzoSEk3IyVcjTjYaegGERHJ/yZOnMjDDz9seV2sWDECAwMtr999912WL1/OypUrGTZsWJbH6d+/Pz179gRg0qRJzJgxg507d/Loo49mun1SUhKzZ8+mUqVKAAwbNoyJEyda1s+cOZMxY8bQtWtXAGbNmsXq1avv+n2mJefbtm2jWbNmAHz33Xf4+/uzYsUKnnzyScLCwujevTu1a9cGoGLFipb9w8LCqFevHg0bNgTMrQlE7sqV43Bu3y3J+PUa8eT42+9rawTPsuBVDjzLmR89/OHiQdj/k/nxyDrz8vPLUKEl1HgcqnUC15x1U5GCR0l6LrsWn8TBc9c4GB7F/nBzQn74/DVis0ggfT0cqebjRnVfd6r5ulPdx43irkZLgm1ro9rezJhMJpJSzM0TE64n8HFpCXxCMpdiErkQFc/F6AQuRiVw4VoCF68lcOFaPFdjk0hKMXE2Mp6zkfFA5G3P5eVsj4+HE6U9HSnt6YSfpxOlvZwo7WleSrgasdHfSUSkwHKyt2X/ROtM5+pkb5trx0pLOtNER0czfvx4Vq1aRXh4OMnJycTFxaWb3jYzderUsTx3cXHB3d2dCxcuZLm9s7OzJUEH8PX1tWwfGRnJ+fPnady4sWW9ra0tDRo0IDU1NUfvL82BAwews7OjSZMmlrLixYtTtWpVDhw4AMBLL73ECy+8wLp162jXrh3du3e3vK8XXniB7t27s2fPHh555BG6dOliSfZFsm3XHFj1KpDFTFUGG3AvfSMB9ywHXuVvPHctBTZZVAS1eRMuHoL9K80J+/l9cHyjeVk1Eso1Nyfs1TuDm8aIKIyUpN+D01dj2Xc6Ml1z9dNXM68dN9rZUKWUG9V93ajm425Oyn3c8HJxuM9RFw4GgwEHO4O5ltvJPkf7Jiancik6feJ+ISqBi9EJlseL1xP8pBQTV2OTuBqbxIHwqEyP52Brg+/NCfwtSbyvpyNGu9y7CBMRkdxlMBhyrcm5Nbm4uKR7PWrUKIKDg5k6dSqVK1fGycmJJ554gsTExNsex94+/e+qwWC4bUKd2fa3m2L3fhg8eDBBQUGsWrWKdevWMXnyZKZNm8bw4cNp3749oaGhrF69muDgYNq2bcvQoUOZOnWqVWOWAuTfZTcSdJ86UCLglmS8HLiXAbt7uM73rgqtXjMvl4+Zk/X9P0F4CJzcYl5WvwZlm95I2D1K59Y7FCsr+L9IVjRjwxF+2H06Q7mvh6MlCa/u6051XzfKF3fBzlbNpvMDBzsb/K4n1LeTmmoiMi6J89fiCY+M58zVOM5GxHEm4vrj1TjORcWTmJJK6OVYQi/HZnksbzcjfp5OlLkpgS9X3JlyxV0o4+WEvb4bIiKSy7Zt20b//v0tzcyjo6M5efLkfY3Bw8ODUqVKsWvXLlq2bAlASkoKe/bsoW7dund1zOrVq5OcnMyff/5pqQG/fPkyhw4dokaNGpbt/P39ef7553n++ecZM2YMX375JcOHDwfMo9r369ePfv360aJFC1577TUl6ZI9RzfAsmcBEzQcCB0/grwe56h4JWgx0rxcPWmuYT+wEk7vgrDt5mXNaCjT6HrC/pj5RoEUWErS70GgvycHz1270Vzdx5yQezqrdrwwsLEx4OXigJeLA9V83DPdJikllfNR1xP4SHPifiYijjMR8Zy5GsuZiDjik1K5eL3Wfu+piAzHsLUx4OfpSLliLtcTd3PyXq64M2WLOReK2h0REbn/AgICWLZsGZ07d8ZgMPD222/fdRPzezF8+HAmT55M5cqVqVatGjNnzuTq1avZGsB13759uLm5WV4bDAYCAwN5/PHHGTJkCP/73/9wc3PjjTfeoHTp0jz++OMAjBgxgvbt21OlShWuXr3Kxo0bqV69OgDjxo2jQYMG1KxZk4SEBH755RfLOpHbOr0bFveG1CSo2RU6TM37BP1WXuWh+UvmJfI0HPjZXMMe9oc5aT+9C9a9BX71zAl7zW5K2AsgXf3fg15NytGrib70RZm9rQ1lvJwp4+Wc6XqTydxc/kbybk7kT12NJexyLKFXYohPSuXUlThOXYlj69GMxyjpZryesLtQvrgzZYs7U/56Eq8bQiIikpWPPvqIgQMH0qxZM0qUKMHo0aOJisq861ZeGj16NOfOnaNv377Y2try7LPPEhQUhK3tnbuCpdW+p7G1tSU5OZl58+bx8ssv06lTJxITE2nZsiWrV6+2NL1PSUlh6NChnD59Gnd3dx599FE+/vhjwDzX+5gxYzh58iROTk60aNGCRYsW5f4bl8LlwkH47glIioWKbaDrF2Bj5e6MHmXggRfMS1S4eVT4/T9B6DY4+7d5+e196PIZ1HnKurFKjhhM1u40dJ9FRUXh4eFBZGQk7u6Z146K3C8mk4kL1xIIvRzLycsx1xP3WEIvxxB6OZbIuKTb7u/uaEf5Ei5UKOFCLT8PapfxoKafO26OOeunLyLWpd+m3JfVZxofH8+JEyeoUKECjo6OVoyw6EpNTaV69eo89dRTvPvuu9YOJ0/oe1bIRITBnCC4dhZKN4C+K8Hoau2oshZ90Zyw/7MYwnaYy9pNgOYv3/+af7HIyW+9atJFrMhgMFDK3ZFS7o40rpBx/suI2ERzf/crsYRdjuHkZXMN/MnLMVy4lkBUfDL/nI7kn9OR/BRy9voxoWIJF+qU8aR26RuJu5rNi4iINYSGhrJu3TpatWpFQkICs2bN4sSJEzzzzDPWDk3kzmIuwTddzQl6iarQa2n+TtDBPEVbwwFQvx8Evw07ZsH6d+BaOARNsn4LALkjXbWL5GOezg54OjsQ6O+ZYV1cYgphV8wJ+9EL0ew7Hcm+M5GciYjj2MUYjl2MYfnfZwCwMUDlkq7ULu1JnTLmxL2GrzuOuTj1j4iISGZsbGyYP38+o0aNwmQyUatWLdavX69+4JL/JVyDb7vD5aPmOcz7LAfnjJUq+ZaNDQS9D26+sG4s/DnbnKh3/QLs1cIjP1OSLlJAOTnYUtXHjao+bgTVvFF+KTqBfWci2Xe9hn3fmQjORyVw+Hw0h89H8+Me84wEtjYGqpRyo8712vY6ZTyo6uOm6eJERCRX+fv7s23bNmuHIZIzSfGw6BnzlGfOxc0JekGd4qzZMPN86sufN/dZj7kMT38HTp7WjkyyoCRdpJAp4WqkTdWStKla0lJ2PirenLSfiWTf6Qj2nYnkUnQiB8KjOBAexeLdpwCwtzVQzced2mU8qOvvSYNyXlQs4ZKtEXhFRERECoXUFFg2GE78Dg6u0PtH81zoBVntJ8DF2zw6fehWmPuo+X0V1BsPhZySdJEioJS7I6VqONKuRinAPGBdeGQ8/5yO5N8z5uT9n9MRRMQmmWvhz0Sy8M8wADyd7al3PWGvX9aLQH9PXIz6r0NEREQKIZMJfhlhntrM1gGeXmiezqwwqNgKBqyGb5+AiwdgzsPmRL2kup7kN7rSFimCDAYDfp5O+Hk68WgtH8CcuJ++Gse+M5HsPRXBnrCr/HM6kojYJDYeusjGQxcBc//2qj7uNCjnSf2y5sS9XHFn1baLiIhIwbdhAuxZAAYb6D7HnNgWJj61YXCwua/9pcMwNwh6LoJyzawdmdxESbqIAObE3b+YM/7FnOlQ2xeAxORU9odHsSf0KnvCrvJ3WARnIuIszeS//cNc217cxYF6Zb2ofz1xDyzjiZOD+raLiIhIAbJ9Jmz92Py803So8ZhVw8kznmVh4Fr4/mk49Scs6ALdvoCaXawdmVynJF1EsuRgZ0Ndf0/q+nsykAoAnIuMZ0/YVUvi/u+ZKC7HJLL+wHnWHzgPgJ2Ngeq+7tQv60n9683ky3g5qbZdRERE8qeQhbDuLfPztu9Ag37WjSevOReDvj/Bj4PNc6ov6Q/RH0CT56wdmaAkXURyyMfDkQ61fS217QnJKfx7Joq/w8xJ+1+hVzkflWDp2/71jlDAPHd7p0A/OtfxJaCUmzXfgoiIiMgNB1fDT8PMz5sOgwdfsW4894u9Ezy1AFa/BrvnwK+vQ9RZaDceVLFiVUrSReSeGO1saVDOiwblvABz3/azkfGWmvY9YRH8dyaS45dimLHhCDM2HKGajxud6vjSqY4f5Uu4WPkdiIgUHa1bt6Zu3bpMnz4dgPLlyzNixAhGjBiR5T4Gg4Hly5fTpUuXezp3bh1HJFed3GauRTalQOAz8Mh7RStBtbGFjtPA3Rd+ew+2TYdr5+CxmWDnYO3oiiwl6SKSqwwGA6U9nSjt6UTnQD8AohOSWb//PD/vPcvvRy5y8Nw1Dp67xtR1h6lTxoNOdXzpWMeP0p5OVo5eRCR/6ty5M0lJSaxZsybDui1bttCyZUv27t1LnTp1cnTcXbt24eKSuzdLx48fz4oVKwgJCUlXHh4ejpeXV66e61bz589nxIgRRERE5Ol5pJAI/8fcLzslAap2MCemRSlBT2MwQMvXwM0PVg6HfxZBzAVzLbtRrR+tQUm6iOQ5V6MdXeqVpku90kTGJrH2v3P8/M9Zth+7zD+nI/nndCSTVh+kYTkvOtXxpUMdX0q6OVo7bBGRfGPQoEF0796d06dPU6ZMmXTr5s2bR8OGDXOcoAN4e3vnVoh35OPjc9/OJXJHl4/Bt90gIQrKNYcn5oJtEU+N6vUC15LwQz849hvM6wC9loJbKWtHVuTYWDsAESlaPJzteaqRP98MasKfb7bl3S61aFKhGAYD7A69yvif9/PApA088+UfLPwzjKsxidYOWUQKO5MJEmOss5hM2QqxU6dOeHt7M3/+/HTl0dHRLFmyhEGDBnH58mV69uxJ6dKlcXZ2pnbt2nz//fe3PW758uUtTd8Bjhw5QsuWLXF0dKRGjRoEBwdn2Gf06NFUqVIFZ2dnKlasyNtvv01SUhJgrsmeMGECe/fuxWAwYDAYLDEbDAZWrFhhOc6+fft46KGHcHJyonjx4jz77LNER0db1vfv358uXbowdepUfH19KV68OEOHDrWc626EhYXx+OOP4+rqiru7O0899RTnz5+3rN+7dy9t2rTBzc0Nd3d3GjRowO7duwEIDQ2lc+fOeHl54eLiQs2aNVm9evVdxyJWFBUO33SBmItQqjb0/N7cP1sg4GHo/zM4l4Bz/5jnUr901NpRFTlF/HaRiFhTCVcjfR4oR58HynEuMp5V+8L55Z+z/B0WwfZjl9l+7DLjfvqXBwNK0KmOH4/ULIW7o721wxaRwiYpFib5Wefcb54Fhzs3N7ezs6Nv377Mnz+fsWPHWmbLWLJkCSkpKfTs2ZPo6GgaNGjA6NGjcXd3Z9WqVfTp04dKlSrRuHHjO54jNTWVbt26UapUKf78808iIyMz7avu5ubG/Pnz8fPzY9++fQwZMgQ3Nzdef/11evTowb///suaNWtYv349AB4eHhmOERMTQ1BQEE2bNmXXrl1cuHCBwYMHM2zYsHQ3IjZu3Iivry8bN27k6NGj9OjRg7p16zJkyJA7vp/M3l9agr5582aSk5MZOnQoPXr0YNOmTQD06tWLevXq8fnnn2Nra0tISAj29ubfnaFDh5KYmMjvv/+Oi4sL+/fvx9XVNcdxiJXFXTXXoEeEQbGK0GcZOGb8jhZppRuY51L/phtcPQFzH4FnfoAyDa0dWZGhJF1E8gUfD0cGPViBQQ9W4NSVWH75x5yw/3c2ik2HLrLp0EUcltnQqqo3nQP9aFe9JM4O+i9MRIqOgQMHMmXKFDZv3kzr1q0Bc1P37t274+HhgYeHB6NGjbJsP3z4cNauXcsPP/yQrSR9/fr1HDx4kLVr1+LnZ75pMWnSJNq3b59uu7feesvyvHz58owaNYpFixbx+uuv4+TkhKurK3Z2drdt3r5w4ULi4+NZsGCBpU/8rFmz6Ny5Mx988AGlSpmb13p5eTFr1ixsbW2pVq0aHTt2ZMOGDXeVpG/YsIF9+/Zx4sQJ/P39AViwYAE1a9Zk165dNGrUiLCwMF577TWqVasGQEBAgGX/sLAwunfvTu3atQGoWLFijmMQK0uMhYU94MJ+cPWBPsvNzbslo2IVYVAwLHwSzv4N8zvBk/Oh6qPWjqxI0BWuiOQ7/sWceaF1JV5oXYljF6P5ZW84P/9zlqMXognef57g/edxdrDl0Vo+dK9fhgcqFsfWpggO9CJSAEyePJlly5Zx8OBBnJycaNasGR988AFVq1bNcp/58+czYMCAdGVGo5H4+Pi8CdLe2VyjbQ32ztnetFq1ajRr1oy5c+fSunVrjh49ypYtW5g4cSIAKSkpTJo0iR9++IEzZ86QmJhIQkICzs7ZO8eBAwfw9/e3JOgATZs2zbDd4sWLmTFjBseOHSM6Oprk5GTc3d2z/T7SzhUYGJhu0LrmzZuTmprKoUOHLEl6zZo1sbW1tWzj6+vLvn37cnSum8/p7+9vSdABatSogaenJwcOHKBRo0aMHDmSwYMH880339CuXTuefPJJKlWqBMBLL73ECy+8wLp162jXrh3du3e/q3EAxEpSkuCHvnDqT3PNeZ9l4FXe2lHlb67e0O/6HOpHg2FRT+g0vfDPIZ8PqE+6iORrlbxdebldAMGvtGTNiBYMa1OZssWciU1MYdmeM/T66k8e/OA3PlxzkKMXou98QBG5rzZv3szQoUP5448/CA4OJikpiUceeYSYmJjb7ufu7k54eLhlCQ0NzbsgDQZzk3NrLDkcSXrQoEH8+OOPXLt2jXnz5lGpUiVatWoFwJQpU/jkk08YPXo0GzduJCQkhKCgIBITc29sjx07dtCrVy86dOjAL7/8wt9//83YsWNz9Rw3S2tqnsZgMJCampon5wLzyPT//fcfHTt25LfffqNGjRosX74cgMGDB3P8+HH69OnDvn37aNiwITNnzsyzWCQXRV8wN3E/Ggx2TvDMEihV09pRFQxGV3Of/bq9wZQKP78ExzdbO6pCT0m6iBQIBoOBaj7ujAqqyubXWvPjC814pklZ3B3tCI+M57NNx2j30WYen7WVr7ef5IoGnBPJF9asWUP//v2pWbMmgYGBzJ8/n7CwMP7666/b7mcwGPDx8bEsaTWrRd1TTz2FjY0NCxcuZMGCBQwcONDSP33btm08/vjj9O7dm8DAQCpWrMjhw4ezfezq1atz6tQpwsPDLWV//PFHum22b99OuXLlGDt2LA0bNiQgICDDDRQHBwdSUlLueK69e/emu1mzbds2bGxsbtvK4l6kvb9Tp05Zyvbv309ERAQ1atSwlFWpUoVXXnmFdevW0a1bN+bNm2dZ5+/vz/PPP8+yZct49dVX+fLLL/MkVslFodthdgs48bu55UqPb6BsE2tHVbDY2sPjs6DK9a4vp3daN54iQEm6iBQ4BoOBBuW8mNS1NjvHtuOzXvVpV70ktjYG9p6O5J2V/9Fk0nqeXbCbNf+eIzE572pdRCRnIiMjAShWrNhtt4uOjqZcuXL4+/vz+OOP899//912+4SEBKKiotIthZGrqys9evRgzJgxhIeH079/f8u6gIAAgoOD2b59OwcOHOC5555LN3L5nbRr144qVarQr18/9u7dy5YtWxg7dmy6bQICAggLC2PRokUcO3aMGTNmWGqa05QvX54TJ04QEhLCpUuXSEhIyHCuXr164ejoSL9+/fj333/ZuHEjw4cPp0+fPvd8QyYlJYWQkJB0y4EDB2jXrh21a9emV69e7Nmzh507d9K3b19atWpFw4YNiYuLY9iwYWzatInQ0FC2bdvGrl27qF69OgAjRoxg7dq1nDhxgj179rBx40bLOsmHTCbYNsPclzr6HJSoCkM2mkcvl5wzGKDk9e979EXrxlIEKEkXkQLN0d6WDrV9+apfI/58sy3jOtWgVml3klJMrNt/nue//YvGk9Yz7qd/CTkVgSmb0x2JSO5LTU1lxIgRNG/enFq1amW5XdWqVZk7dy4//fQT3377LampqTRr1ozTp09nuc/kyZMtg6d5eHik63dc2AwaNIirV68SFBSUrv/4W2+9Rf369QkKCqJ169b4+PjQpUuXbB/XxsaG5cuXExcXR+PGjRk8eDDvv/9+um0ee+wxXnnlFYYNG0bdunXZvn07b7/9drptunfvzqOPPkqbNm3w9vbOdBo4Z2dn1q5dy5UrV2jUqBFPPPEEbdu2ZdasWTn7MDIRHR1NvXr10i2dO3fGYDDw008/4eXlRcuWLWnXrh0VK1Zk8eLFANja2nL58mX69u1LlSpVeOqpp2jfvj0TJkwAzMn/0KFDqV69Oo8++ihVqlThs88+u+d4JQ/ERcDi3hD8NphSoPaTMOQ3KFnN2pEVbC7e5scYJel5zWAqYlesUVFReHh4EBkZmeNBTkSk4Dh07hrL9pxm+d9nuHDtRi1ORW8XutcvQ5d6pSntqTlRJX8oKr9NL7zwAr/++itbt26lTJky2d4vKSmJ6tWr07NnT959991Mt0lISEhXYxsVFYW/v3+GzzQ+Pp4TJ05QoUIFHB0d7/7NiNyGvmdWdDYElvSDqyfB1gEe/T9oODDH4z9IJvYthR8HQfkW0P8Xa0dT4OTkt16ju4tIoVTVx40xHarz+qPV2Hb0Esv2nGbNf+c4fjGGKWsPMXXdIZpWLE63+mVoX8sHF6P+OxTJS8OGDeOXX37h999/z1GCDubBw+rVq8fRo0ez3MZoNGI0Gu81TBEpqEwm2PM1rH4dUhLAsyw8+TWUrm/tyAqPtJr06AvWjaMI0FWpiBRqtjYGWlbxpmUVb67FJ/Hrv+dYtuc0fxy/wvZjl9l+7DJvr/iX9rV86PVAWeqX9bIMwiQi985kMjF8+HCWL1/Opk2bqFChQo6PkZKSwr59++jQoUMeRCgiBV5iDKx6FfZe71pR5VHo8jk4337sC8khNXe/b5Ski0iR4eZoz1MN/XmqoT+nr8ay4u8z/LjnDCcuxbDs7zMs+/sM1X3d6f1AWbrULa3adZFcMHToUBYuXMhPP/2Em5sb586dA8DDwwMnJ3OXk759+1K6dGkmT54MwMSJE3nggQeoXLkyERERTJkyhdDQUAYPHmy19yEi+dSlI7C4D1w8AAYbeOhtaD4CbDT0Vq5zLWl+jLtinnfe1v7228td0xWoiBRJZbycGfZQAEPbVGZPWASLdoaxcu9ZDoRHMXb5v0xefZDu9UvT+4FyBJRys3a4IgXW559/DkDr1q3Tlc+bN88yMnlYWBg2N11QX716lSFDhnDu3Dm8vLxo0KAB27dvTzdNlogI//4IK1+CxGhwKQlPzIUKLawdVeHl5GW+EWJKhdjL4OZj7YgKLQ0cJyJyXURsIkv/Os13f4Zx4tKNuXubVChGn6bleKSGDw52ujMvuU+/Tbkvq880bUCv8uXLW2ryRXJbXFwcJ0+e1MBxeSU5Eda9BTv/Z35d7kF4Yo6SxvthSgDEXIDntoBvHWtHU6Bo4DgRkbvg6ezA4BYVGdi8AtuOXeLbP0IJ3n+eP09c4c8TVyjhaqRnY396Ni6Ln0aGFymQ7O3NzTNjY2OVpEueiY2NBW583yQXRZyCJf3hzG7z6wdHQpuxYKu05r5w8TYn6eqXnqf0bRYRuYWNjYEWAd60CPAmPDKO73ee4vudYVy8lsDM347y6cajtK1eij4PlOPByiWwsdFAcyIFha2tLZ6enly4YB6d2NnZWYNFSq4xmUzExsZy4cIFPD09sbW1tXZIhcuRYFg2BOKugqMHdP0Cqj5q7aiKFldvuICS9DymJF1E5DZ8PZwY+XAVhj9UmXX/nefbP0LZcfwywfvPE7z/POWLO9OrSTmeaFAGLxcHa4crItng42NuEpuWqIvkNk9PT8v3THJBagpsmgy/TzG/9q0LT30NXuWtGVXR5HJ98Dgl6XlKSbqISDbY29rQsY4vHev4cuT8Nb77M4wf/zrNycuxvL/6AFPXHaJTHT/6NC1HYBkP1cyJ5GMGgwFfX19KlixJUlKStcORQsbe3l416Lkp+iL8OBBO/G5+3WgwBE0CO6N14yqqNFf6faEkXUQkhwJKuTH+sZq8FlSVlXvP8s2OUPaHR/HjntP8uOc0tUt70OeBcjxezw+jnS7URPIrW1tbJVMi+VnoDlg6AK6Fg70LdP4E6jxp7aiKNlfNlX4/KEkXEblLLkY7ejYuy9ON/Pn7VATf7gjll3/C2Xcmktd//IeZG4/w6sNVeSzQT/3WRUREsiMlCUK3wf6V8Nd8MKVAiarQ4xvwrmrt6MRFSfr9oCRdROQeGQwG6pf1on5ZL97qVIMfdp9i7tYTnLoSx4jFIfzv9+OMfrQqrap4qxm8iIjIrRKuwdENcHAVHFkL8ZE31tV+Cjp9DEZX68UnN6T1SVdz9zylJF1EJBcVc3Hg+VaV6Ne0PHO3nWD25mMcCI+i/7xdNK1YnDfaVyPQ39PaYYqIiFjXtfNw+FdzYn58E6Qk3ljnXAKqtocaXaByW9AN7vzDpYT5MeaSdeMo5JSki4jkAScHW4a2qcwzjcvy2aajfL3dPCr8459uo0NtH0Y9UpWK3qoVEBGRIuTSETj4CxxcDad3AaYb64pVhGodoVonKNMIbDReRL7ketPo7iaTbqDkESXpIiJ5yMvFgbEda9C/eQU+Dj7Mj3tOs3rfOdb+d54ejfwZ0TaAku6O1g5TREQk96Wmwpm/zIn5odVw6XD69X71byTm3lWV8BUEztdr0lOTID4CnLysGk5hpSRdROQ+KO3pxNQnAxncogJT1hxiw8ELLPwzjOV7zjDowQo826oi7o721g5TRETk3iQnmKdLO/gLHPoVos/fWGdjDxVaQrUOULUDuPtZL065O/aOYPSAhEjz9HhK0vOEknQRkfuomo87c/o3YueJK/zfrwfYExbBrI1H+e7PUIa2qUzvB8rhaK8mfiIiUkCkpsLlo3B6JxwJhqPrITH6xnoHNwh42FxjHvAwOHpYL1bJHa7e5iQ95iJ4V7F2NIWSknQREStoXKEYP77QjOD95/lw7SGOXojmvVUHmLftJCMfrkKXeqWx1bRtIiKS38RHwZndcHo3nNpp7lseH5F+Gzdf88Bv1TpC+RZgZ7RKqJJHXLzNN2ZiNMJ7XlGSLiJiJQaDgUdq+vBQtZL8uOc0Hwcf4UxEHK8u2csXvx9ndPuqtKlaUtO2iYiIdaSmwuUjN5Lx07vgwgHSDfgGYOcIfvWgXDOo2tH83MbGKiHLfZA2V3q05krPK0rSRUSszM7Whh6NyvJ43dLM336SzzYe5dD5awycv5vGFYrxRvtq1C+rPl8iIpLH4iPNNeSnd5ubr5/elX7O8jSeZaFMY/BvbB6JvVQtsHO4//GKdaQl6TFK0vOKknQRkXzC0d6W51tVomejsny2+Sjzt51k54krdPtsO4/UKMXo9tWopGnbREQkN6SmmkdbT0vGT+2CiwfJWEvuZK4Z929kTszLNAK3UlYJWfIJyzRsau6eV5Ski4jkMx7O9oxpX53+zcozPfgIS/46xbr95/nt4AV6P1COl9sG4OWiGgsRkSIvOdFc021ZIm55fZvyuAhISch4TK/y5kS8TGNzYl6qFthq9hG5icv1adhiLlk3jkJMSbqISD7l6+HEB0/UYXCLCnyw5iDrD1xg/vaTLNtzmpfaBtC3aXkc7NTnT0TkrphM15fUG0tqMphSIDXF/Dzt8a7KkiEl6abHpExeJ2csT0nMYl0iJFxLn2gnxd7bZ2DvbJ6rvEzDG03X02pJRbLicv07Eq2a9LyiJF1EJJ8LKOXGV/0ase3oJd79ZT8Hz13jvVUH+PaPUMZ0qM4jNUppcDmRvHJqF6x5I5cParrzJjk+ZFbHvM25stwnk/0ybHvr+uzsb8r4eLt1lkfusD4tyTbdeMy0/PqStq4wMbqbpzZz9ABHz5ue32Fx91MtueScpbm7+qTnFSXpIiIFRPPKJVj1UguW7D7F1HWHOXk5lue++YsHKhbjrY41qFVac8+K5LqESPN0U1I02diBwdb8aGN7fclBmY092KY92pvLbO0zKb/ddmmvHcDRPWOibXQ3n0/kftHAcXlOSbqISAFia2Pg6cZl6RTox+ebjvLllhP8cfwKnWdt5Yn6ZXgtqCol3R2tHaZI4eETCD0X5cGB86D1S5Ytam5zrtu2wjHc9mXG9bdukNl6Q8bH262zPJJFuc315abnaetvLbesu3Wfm45lY3cj0basE5F00pL0xGhIjAUHZ+vGUwgpSRcRKYBcjXa8FlSNno3L8uGaQ6zce5Ylf51m1b5wnm9ViSEtKuLkoJoVkXvm6g1V21s7ChGR/MPoBrZG88CDMRfBoZy1Iyp0NOKQiEgBVsbLmRk967HsxWbUK+tJbGIKHwUf5qFpm1j+92lSU/Og76uIiIgUXQaD+qXnMSXpIiKFQP2yXix7oRkzetajtKcT4ZHxvLJ4L10/28buk1esHZ6IiIgUJpZp2JSk5wUl6SIihYTBYOCxQD82vNqK14Kq4uJgy97TkTwxewdDv9vDqSv3OFWPiIiICGgatjymJF1EpJBxtLdlaJvKbHytNU838sdggFX7wmk7bTOTfz1AVHyStUMUERGRgkwjvOcpJekiIoVUSTdH/q97HVYNb0HzysVJTEnlf5uP02bKJr79I5TklEI2T7CIiIjcH65K0vOSknQRkUKuhp873w5qwpx+Dano7cLlmETeWvEvHWZsYeOhC5hMGlxOREREcsBFA8flJSXpIiJFgMFgoG31Uqwd0ZLxnWvg6WzP4fPRDJi3iz5zdvLf2UhrhygiIiIFRVpzd/VJzxNK0kVEihB7Wxv6N6/A5lFtGNKiAg62Nmw9eolOM7cyaslezkXGWztEERERye/U3D1PKUkXESmCPJztGduxButHtqJTHV9MJlj612laT93ItHWHiE5ItnaIIiIikl9p4Lg8pSRdRKQIK1vcmVnP1GfZi81oWM6L+KRUZv52lNZTNrHwzzANLiciIiIZpfVJj70CKbqxn9usnqR/+umnlC9fHkdHR5o0acLOnTtvu/306dOpWrUqTk5O+Pv788orrxAfr+aZIiL3on5ZL5Y835TZvetTvrgzl6ITeHP5Ptp/soWNBzW4XGYiY5M4fVVzz4uISBHkXAwMNoAJYi9bO5pCx6pJ+uLFixk5ciTvvPMOe/bsITAwkKCgIC5cyHwAgoULF/LGG2/wzjvvcODAAebMmcPixYt5880373PkIiKFj8Fg4NFavqx7pRXjOpkHlztyIZoB83fRe86fGlzuuqsxiUxbd4gHP/iN8Sv/s3Y4IiIi95+NLTgXNz+P0eBxuc2qSfpHH33EkCFDGDBgADVq1GD27Nk4Ozszd+7cTLffvn07zZs355lnnqF8+fI88sgj9OzZ84617yIikn0OdjYMfNA8uNyzLSviYGvDtqOXLYPLhUfGWTtEq7gSk8iHaw7y4Ae/MfO3o1xLSOb01ThiE9XMT0REiiBNw5ZnrJakJyYm8tdff9GuXbsbwdjY0K5dO3bs2JHpPs2aNeOvv/6yJOXHjx9n9erVdOjQIcvzJCQkEBUVlW4REZE783C2580O1dnwais6B/pZBpdrM3VTkRpc7lJ0ApN/PcCDH/zGZ5uOEZOYQg1fd2b3bsDql1rg7GBn7RBFRETuP5cS5sdoJem5zWpXFpcuXSIlJYVSpUqlKy9VqhQHDx7MdJ9nnnmGS5cu8eCDD2IymUhOTub555+/bXP3yZMnM2HChFyNXUSkKPEv5szMnvUY2Lw87686wO7Qq8z87Sjf7wzjlYer0KOhP3a2Vh/iJNdduBbPF5uP8+2focQnmQfQq1XanZfbVqFd9ZIYDAYrRygiImJFrqpJzysF6qpq06ZNTJo0ic8++4w9e/awbNkyVq1axbvvvpvlPmPGjCEyMtKynDp16j5GLCJSeNTLMLhcImOX/1voBpc7HxXPhJ//o8UHG/lq6wnik1IJLOPB3P4N+XnYgzxco5QSdBEREcs0bOqTntusVpNeokQJbG1tOX/+fLry8+fP4+Pjk+k+b7/9Nn369GHw4MEA1K5dm5iYGJ599lnGjh2LjU3Gew5GoxGj0Zj7b0BEpAhKG1zuoWql+O7PUD7ZcMQyuFyzSsV5Lagq9cp6WTvMuxIeGcfsTcf4ftcpEpPNNef1ynryctsAWlXxVmIuIiJys7QkXc3dc53VatIdHBxo0KABGzZssJSlpqayYcMGmjZtmuk+sbGxGRJxW1tbgEJTgyMiUhA42NkwoHkFNr/WhueuDy63/dhlun62ncFf72L/2YIz/seZiDjeWrGPVh9u4usdoSQmp9KwnBffDGrMshea0bqqmraLiIhkYKlJV5Ke26w62s3IkSPp168fDRs2pHHjxkyfPp2YmBgGDBgAQN++fSldujSTJ08GoHPnznz00UfUq1ePJk2acPToUd5++206d+5sSdZFROT+8XCyZ0yH6vR+oBwzNhzhxz2nWX/gAusPXKBjHV9eaVeFyiVdrR1mpk5dieWzTcdY+tcpklLMN3obVyjGiLYBNK1UXIm5iIjI7Vj6pKu5e26zapLeo0cPLl68yLhx4zh37hx169ZlzZo1lsHkwsLC0tWcv/XWWxgMBt566y3OnDmDt7c3nTt35v3337fWWxAREcyDy015MpDnW1fi4+DD/PJPOKv+CefXfeF0q1+Gl9sG4F/M2dphAhB2OZZPNx7lxz2nSU41J+dNKxbn5XYBPFCxuJWjExERKSDSRnePuWTdOAohg6mItROPiorCw8ODyMhI3N3drR2OiEihdCA8imnrDrP+gHncEXtbAz0a+TP8oQBKuTtaJaaTl2KYtfEoy/8+Q8r15PzByiV4qW0AjSsUs0pMafTblPv0mYqI5LGIUzC9Ftg6wFsXQC3Qbisnv0ua3FVERHJddV93vurXkL/DrjJt3WG2Hr3Et3+EsWT3afo2LcfzrSpR3DVvB/VMSTWx70wkmw5dYPPhi4SciiDttnSrKt681LYyDcpZNzkXEREpsNL6pKckQnwkOHlaNZzCREm6iIjkmXplvfh2cBN2HLvM1HWH+Cv0Kl9uOcHCP8MY+GAFBreoiIeTfa6d7+K1BLYcucjmwxf5/fBFrsYmpVvfpqo3L7UNKLAj0IuIiOQb9o5gdIeEKPPgcUrSc42SdBERyXNNKxVn6fNN2XT4ItPWHeLfM1HM/O0oX28/yXOtKtG/WXlcjDn/SUpOSeXvUxFsPmROzPediUy33s1ox4MBJWhd1ZuWVbzx9XDKrbckIiIiLt43kvQSAdaOptBQki4iIveFwWCgTdWStK7izZp/z/FR8GGOXIhmytpDzNt2ghdaV6ZXk7I42t9+to5zkfH8fvgimw5fYMuRS1yLT063vlZpd1pV8aZVlZLUK+uJva3VZhsVEREp3Fy84coxiNYI77lJSbqIiNxXBoOB9rV9eaSmDyv3nuHj4COEXYnl3V/289WW4wx/KIAnG5axJNeJyansDr3C5sMX2XzoIgfPXUt3PE9ne1oEeNO6ijctqpSgpJt1BqYTEREpclw1V3peUJIuIiJWYWtjoGu9MnSq48fSv04zY8MRwiPjeXP5PmZvPsaTDcrwz5lIth+9RExiimU/gwECy3jSqoo3rat6U6eMJ7Y2GlFWRETkvnNRkp4XlKSLiIhV2dva0LNxWbrWK83CP8P4bNNRwq7EMi34sGWbEq4OtAzwplVVb1oEeFPMxcGKEYuIiAgALiXNj2runquUpIuISL7gaG/LwAcr8HRjf77eHspfoVep6+9B66olqeHrjo1qy0VERPIXlxLmR9Wk5yol6SIikq84O9jxQutK1g5DRERE7sT1ek26kvRcpSFvRUSk8DOZIOosJCdaOxIREZHCw0VJel5QTbqIiBRuodthw0QI2wH2LlC+OVRsbV5K1jCPRCciIiI5lzZwXLSS9NykJF1ERAqn8L2w4V04GnyjLCkGjqwzL2C+uEhL2Cu0Ak9/a0QqIiJSMKVNwZZ4DZLiwN7JuvEUEkrSRUSkcLl4GDa+D/tXmF/b2EG9PtByFMRegeOb4MRmOLnN3Dxv3xLzAlCs0k1Jewtw8rLOexARESkIjO5g6wApiebfVM+y1o6oUFCfdBERKRwiwuCnofBZk+sJugFqPwVDd0Ln6eBRBnzrQPOXoPeP8EYo9F8FLV+DMo3AYANXjsHuOfBDH/iwInzRBtZPgOObISneym+wYJo8eTKNGjXCzc2NkiVL0qVLFw4dOnTH/ZYsWUK1atVwdHSkdu3arF69+j5EKyIiOWIw3DQNm5q85xbVpIuISMEWfQG2TIPdc8138gGqdoA2Y8GnVtb72Rmh/IPm5aG3ID4STm41J+THN8GlQ3B2j3nZ+hHYOULZpjdq2n3qgI3udd/J5s2bGTp0KI0aNSI5OZk333yTRx55hP379+Pi4pLpPtu3b6dnz55MnjyZTp06sXDhQrp06cKePXuoVes2f1MREbn/XEpA1GkNHpeLDCaTyWTtIO6nqKgoPDw8iIyMxN3d3drhiIjI3YqLgO0z4Y/PzX3NAcq3gLbvgH+jez9+1NkbCfvxTRB9Lv16Jy9zst59DtjY3tupitBv08WLFylZsiSbN2+mZcuWmW7To0cPYmJi+OWXXyxlDzzwAHXr1mX27NnZOk9R+kxFRKzquyfNY708NhPq97V2NPlWTn6XVJMuIiIFS2IM/Pk/2DbdXPsNULoBtB1nTppzi7sf1O1pXkwmuHT4RsJ+YgvEXYWrJ+85QS9qIiPNf7NixYpluc2OHTsYOXJkurKgoCBWrFiR5T4JCQkkJCRYXkdFRd1boCIikj2ahi3XKUkXEZGCITkR/poPv0+BmAvmMu/q5qbq1Trm7VRqBgN4VzUvTZ6DlGRzM/ik2Lw7ZyGUmprKiBEjaN68+W2brZ87d45SpUqlKytVqhTnzp3LYg9z3/cJEybkWqwiIpJNLiXMj+qTnmuUpIuISP6WmgL/LIZNk82DwwF4loM2b0LtJ61Tk21rB/6N7/95C7ihQ4fy77//snXr1lw/9pgxY9LVvkdFReHvryn1RETynKtq0nObknQREcmfTCY48DP89p55EDcAVx9o9RrU6wt2DtaNT3Jk2LBh/PLLL/z++++UKVPmttv6+Phw/vz5dGXnz5/Hx8cny32MRiNGozFXYhURkRxwuT5XelorN7lnGpZWRETyn3P74IvW5qnQLh0CR09oNwFe+hsaDVaCXoCYTCaGDRvG8uXL+e2336hQocId92natCkbNmxIVxYcHEzTpk3zKkwREblbaUm6mrvnGtWki4hI/vPbexAeAvYu0HQoNBsGjh7WjkruwtChQ1m4cCE//fQTbm5uln7lHh4eODk5AdC3b19Kly7N5MmTAXj55Zdp1aoV06ZNo2PHjixatIjdu3fzxRdfWO19iIhIFiw16UrSc4tq0kVEJP+JOmN+fGIOPDRWCXoB9vnnnxMZGUnr1q3x9fW1LIsXL7ZsExYWRnh4uOV1s2bNWLhwIV988QWBgYEsXbqUFStWaI50EZH8KK1Peuxl88Cqcs9Uky4iIvlP9PV+be6lrRuH3DOTyXTHbTZt2pSh7Mknn+TJJ5/Mg4hERCRXORUDDIAJ4q7cSNrlrqkmXURE8pfUlBtN5lxL3X5bERERsS5bO3Aubn4ercHjcoOSdBERyV9iL4MpFQw2N+ZeFRERkfxL07DlKiXpIiKSv0Rfn3rLuYR15kAXERGRnEm7qa4kPVcoSRcRkfzl2vUkXU3dRURECgYX1aTnJiXpIiKSv6TVpGvgGRERkYLBMle6+qTnBiXpIiKSv0SrJl1ERKRAcdVc6blJSbqIiOQvaXfhVZMuIiJSMLgoSc9NStJFRCR/SatJd/OxbhwiIiKSPWl90tXcPVcoSRcRkfxFNekiIiIFi6W5+yXrxlFIKEkXEZH8RX3SRUREChZLc/cLYDJZN5ZCQEm6iIjkL5aadCXpIiIiBUJakp6SCAlR1o2lEFCSLiIi+UdSHCREmp+rubuIiEjBYO8EDm7m59EaPO5eKUkXEZH8I62pu50jGN2tG4uIiIhkn6ZhyzVK0kVEJP+4edA4g8G6sYiIiEj23dwvXe6JknQREck/NGiciIhIwZSWpGsatnumJF1ERPIPJekiIiIFk4umYcstStJFRCT/0MjuIiIiBVPagK9q7n7PlKSLiEj+oZp0ERGRgslFA8flFiXpIiKSf9w8cJyIiIgUHJY+6UrS75WSdBERyT+unTM/qiZdRESkYLE0d1eSfq+UpIuISP6hPukiIiIFk5q75xol6SIikj+YTDf1SVdzdxERkQIlLUlPiIKkeOvGUsApSRcRkfwh7iqkJpmfK0kXEREpWBw9wNbB/Fy16fdESbqIiOQPaU3dnbzAzmjdWERERCRnDIabmrxrGrZ7oSRdRETyB02/JiIiUrBZkvRL1o2jgFOSLiIi+YOmXxMRESnYLNOwqSb9XihJFxGR/EE16SIiIgWbpmHLFUrSRUQkf4jWHOkiIiIFmksJ86OS9HuiJF1ERPIHNXcXEREp2FxUk54blKSLiEj+oObuIiIiBZv6pOcKJekiIpI/qCZdRESkYHNNG91dNen3Qkm6iIjkD5aadB/rxiEiIiJ3x0VJem5Qki4iItaXkgSxl83P1dxdRESkYErrkx57GVJTrBtLAaYkXURErC/tjruNHTh5WTcWERERuTvOxQEDmFIh9oq1oymwlKSLiIj1Xbs+/ZpLSbDRT5OIiEiBZGsHzsXMz2M0eNzd0pWQiIhYnwaNExERKRw0Dds9U5IuIiLWp+nXRERECgeXEubHaCXpd0tJuoiIWJ9q0kVERAoHV9Wk3ysl6SIiYn2qSRcRESkcLNOwqU/63VKSLiIi1peWpLtpjnQREZECLS1JV3P3u6YkXURErE/N3UVERAoHNXe/Z0rSRUTE+tTcXUREpHBQc/d7piRdRESsy2S6KUlXTbqIiEiBZpmC7ZJ14yjAlKSLiIh1JUZDUqz5uYuSdBERkQLNMgXbBfONeMkxJekiImJdaf3RHVzB6GrdWEREROTepDV3T0mAhGvWjaWAsnqS/umnn1K+fHkcHR1p0qQJO3fuvO32ERERDB06FF9fX4xGI1WqVGH16tX3KVoREcl1auouIiJSeDg4m2+8gwaPu0s5TtLLly/PxIkTCQsLu+eTL168mJEjR/LOO++wZ88eAgMDCQoK4sKFzAcZSExM5OGHH+bkyZMsXbqUQ4cO8eWXX1K6dOl7jkVERKxEg8aJiIgULpbB45Sk340cJ+kjRoxg2bJlVKxYkYcffphFixaRkJBwVyf/6KOPGDJkCAMGDKBGjRrMnj0bZ2dn5s6dm+n2c+fO5cqVK6xYsYLmzZtTvnx5WrVqRWBg4F2dX0RE8gHL9GtK0kVERAoFy1zpGuH9btxVkh4SEsLOnTupXr06w4cPx9fXl2HDhrFnz55sHycxMZG//vqLdu3a3QjGxoZ27dqxY8eOTPdZuXIlTZs2ZejQoZQqVYpatWoxadIkUlJSsjxPQkICUVFR6RYREclHVJMuIiJSuFjmSleSfjfuuk96/fr1mTFjBmfPnuWdd97hq6++olGjRtStW5e5c+diusNIfpcuXSIlJYVSpdJflJUqVYpz585lus/x48dZunQpKSkprF69mrfffptp06bx3nvvZXmeyZMn4+HhYVn8/f1z/mZFRCTvqE+6iIhI4WJp7q5p2O6G3d3umJSUxPLly5k3bx7BwcE88MADDBo0iNOnT/Pmm2+yfv16Fi5cmJuxkpqaSsmSJfniiy+wtbWlQYMGnDlzhilTpvDOO+9kus+YMWMYOXKk5XVUVFS2EvWUlBSSkpJyLXaR/MLe3h5bW1trhyFywzXVpIuIiBQqau5+T3KcpO/Zs4d58+bx/fffY2NjQ9++ffn444+pVq2aZZuuXbvSqFGj2x6nRIkS2Nracv78+XTl58+fx8fHJ9N9fH19MyQY1atX59y5cyQmJuLg4JBhH6PRiNFozPb7M5lMnDt3joiIiGzvI1LQeHp64uPjg8FgsHYoImruLiIiUthYmrtr4Li7keMkvVGjRjz88MN8/vnndOnSBXt7+wzbVKhQgaeffvq2x3FwcKBBgwZs2LCBLl26AOaa8g0bNjBs2LBM92nevDkLFy4kNTUVGxtzS/3Dhw/j6+ubaYJ+N9IS9JIlS+Ls7KwkRgoVk8lEbGysZQYFX19fK0ckwk0Dx6m5u4iISKHgUsL8qCT9ruQ4ST9+/DjlypW77TYuLi7MmzfvjscaOXIk/fr1o2HDhjRu3Jjp06cTExPDgAEDAOjbty+lS5dm8uTJALzwwgvMmjWLl19+meHDh3PkyBEmTZrESy+9lNO3kamUlBRLgl68ePFcOaZIfuPk5ATAhQsXKFmypJq+i3Wlptz4AVdNuoiISOHgopr0e5HjJP3ChQucO3eOJk2apCv/888/sbW1pWHDhtk+Vo8ePbh48SLjxo3j3Llz1K1blzVr1lgGkwsLC7PUmAP4+/uzdu1aXnnlFerUqUPp0qV5+eWXGT16dE7fRqbS+qA7OzvnyvFE8qu073hSUpKSdLGu2CtgSgEMN/qviYiISMFm6ZOuJP1u5DhJHzp0KK+//nqGJP3MmTN88MEH/Pnnnzk63rBhw7Js3r5p06YMZU2bNuWPP/7I0TlySk3cpbDTd1zyjbT+6C4lwPauxzIVERGR/MT1epKeEAnJCWCX/THC5C6mYNu/fz/169fPUF6vXj3279+fK0GJiEgRoUHjCr3ff/+dzp074+fnh8FgYMWKFbfdftOmTRgMhgxLVtOziohIPuToCTbXxy5Tk/ccy3GSbjQaM4zIDhAeHo6dnWpBCpPy5cszffr0bG+fdmGlkfFFJNs0aFyhFxMTQ2BgIJ9++mmO9jt06BDh4eGWpWRJfUdERAoMg0HTsN2DHGfVjzzyCGPGjOGnn37Cw8MDgIiICN58800efvjhXA9Q7uxOTZffeecdxo8fn+Pj7tq1CxcXl2xv36xZM8LDwy3fi/uhWrVqnDhxgtDQ0Cyn7hORfCz6eu2oatILrfbt29O+ffsc71eyZEk8PT2zvX1CQgIJCQmW11FRUTk+p4iI5CJXb7h2FmIuWTuSAifHNelTp07l1KlTlCtXjjZt2tCmTRsqVKjAuXPnmDZtWl7EKHdwc03D9OnTcXd3T1c2atQoy7Ymk4nk5ORsHdfb2ztHg+g5ODjc17m3t27dSlxcHE888QRff/31fTnn7aQNPCgiOaCadMlC3bp18fX15eGHH2bbtm133H7y5Ml4eHhYFn9///sQpYiIZCmtJj1GNek5leMkvXTp0vzzzz98+OGH1KhRgwYNGvDJJ5+wb98+/SBaiY+Pj2Xx8PDAYDBYXh88eBA3Nzd+/fVXGjRogNFoZOvWrRw7dozHH3+cUqVK4erqSqNGjVi/fn26497a3N1gMPDVV1/RtWtXnJ2dCQgIYOXKlZb1tzZ3nz9/Pp6enqxdu5bq1avj6urKo48+Snh4uGWf5ORkXnrpJTw9PSlevDijR4+mX79+dOnS5Y7ve86cOTzzzDP06dOHuXPnZlh/+vRpevbsSbFixXBxcaFhw4bpBjb8+eefadSoEY6OjpQoUYKuXbume6+39pv09PRk/vz5AJw8eRKDwcDixYtp1aoVjo6OfPfdd1y+fJmePXtSunRpnJ2dqV27Nt9//32646SmpvLhhx9SuXJljEYjZcuW5f333wfgoYceyjCQ4sWLF3FwcGDDhg13/ExEChz1SZdb+Pr6Mnv2bH788Ud+/PFH/P39ad26NXv27LntfmPGjCEyMtKynDp16j5FLCIimdI0bHftrjqRu7i48Oyzz+Z2LPmSyWQiLinFKud2srfNtVrpN954g6lTp1KxYkW8vLw4deoUHTp04P3338doNLJgwQI6d+7MoUOHKFu2bJbHmTBhAh9++CFTpkxh5syZ9OrVi9DQUIoVK5bp9rGxsUydOpVvvvkGGxsbevfuzahRo/juu+8A+OCDD/juu++YN28e1atX55NPPmHFihW0adPmtu/n2rVrLFmyhD///JNq1aoRGRnJli1baNGiBQDR0dG0atWK0qVLs3LlSnx8fNizZw+pqakArFq1iq5duzJ27FgWLFhAYmIiq1evvqvPddq0adSrVw9HR0fi4+Np0KABo0ePxt3dnVWrVtGnTx8qVapE48aNAfOF5JdffsnHH3/Mgw8+SHh4OAcPHgRg8ODBDBs2jGnTpmE0mkfB/PbbbyldujQPPfRQjuMTyfcsNelK0sWsatWqVK1a1fK6WbNmHDt2jI8//phvvvkmy/2MRqPl/00REckHXEqYHzUNW47d9Uhv+/fvJywsjMTExHTljz322D0HlZ/EJaVQY9xaq5x7/8QgnB1yZzC+iRMnphszoFixYgQGBlpev/vuuyxfvpyVK1dmOSUeQP/+/enZsycAkyZNYsaMGezcuZNHH3000+2TkpKYPXs2lSpVAsxT7k2cONGyfubMmYwZM8ZSiz1r1qxsJcuLFi0iICCAmjVrAvD0008zZ84cS5K+cOFCLl68yK5duyw3ECpXrmzZ//333+fpp59mwoQJlrKbP4/sGjFiBN26dUtXdnP3guHDh7N27Vp++OEHGjduzLVr1/jkk0+YNWsW/fr1A6BSpUo8+OCDAHTr1o1hw4bx008/8dRTTwHmFgn9+/fXtGlSOFlq0tXcXbLWuHFjtm7dau0wREQkJ1xVk363cpwBHj9+nK5du7Jv3z4MBgMmkwm4MXhZSop1ap3l9ho2bJjudXR0NOPHj2fVqlWEh4eTnJxMXFwcYWFhtz1OnTp1LM9dXFxwd3fnwoWs+5k4OztbEnQwN2NM2z4yMpLz589bapgBbG1tadCggaXGOytz586ld+/elte9e/emVatWzJw5Ezc3N0JCQqhXr16WNfwhISEMGTLktufIjls/15SUFCZNmsQPP/zAmTNnSExMJCEhwdK3/8CBAyQkJNC2bdtMj+fo6Ghpvv/UU0+xZ88e/v3333TdCkQKFUuSroEf86NTp05hMBgoU6YMADt37mThwoXUqFHjvraoCwkJwdfX976dT0REcoH6pN+1HCfpL7/8MhUqVGDDhg1UqFCBnTt3cvnyZV599VWmTp2aFzFalZO9LfsnBlnt3Lnl1lHaR40aRXBwMFOnTqVy5co4OTnxxBNPZGgZcSt7e/t0rw0Gw20T6sy2T7uxc7f279/PH3/8wc6dOxk9erSlPCUlhUWLFjFkyBCcnJxue4w7rc8szswGhrv1c50yZQqffPIJ06dPp3bt2ri4uDBixAjL53qn84K5yXvdunU5ffo08+bN46GHHqJcuXJ33E+kwEmKh/hI83PVpOdLzzzzDM8++yx9+vTh3LlzPPzww9SsWZPvvvuOc+fOMW7cuDseIzo6mqNHj1penzhxgpCQEIoVK0bZsmUZM2YMZ86cYcGCBQBMnz6dChUqULNmTeLj4/nqq6/47bffWLduXZ69TxERyQOWKdhUk55TOR44bseOHUycOJESJUpgY2ODjY0NDz74IJMnT+all17KixitymAw4OxgZ5UlL5s3b9u2jf79+9O1a1dq166Nj48PJ0+ezLPzZcbDw4NSpUqxa9cuS1lKSsodBweaM2cOLVu2ZO/evYSEhFiWkSNHMmfOHMBc4x8SEsKVK1cyPUadOnVuOxCbt7d3ugHujhw5Qmxs7B3f07Zt23j88cfp3bs3gYGBVKxYkcOHD1vWBwQE4OTkdNtz165dm4YNG/Lll1+ycOFCBg4ceMfzihRIaXfWbY3geP+mbpTs+/fffy2tnX744Qdq1arF9u3b+e677ywDad7J7t27qVevHvXq1QNg5MiR1KtXz5Lgh4eHp2vFlZiYyKuvvkrt2rVp1aoVe/fuZf369Vm2QBIRkXxKzd3vWo5r0lNSUnBzcwOgRIkSnD17lqpVq1KuXDkOHTqU6wFK3ggICGDZsmV07twZg8HA22+/fccm5nlh+PDhTJ48mcqVK1OtWjVmzpzJ1atXs7xBkZSUxDfffMPEiROpVatWunWDBw/mo48+4r///qNnz55MmjSJLl26MHnyZHx9ffn777/x8/OjadOmvPPOO7Rt25ZKlSrx9NNPk5yczOrVqy018w899BCzZs2iadOmpKSkMHr06AytAjITEBDA0qVL2b59O15eXnz00UecP3+eGjVqAObm7KNHj+b111/HwcGB5s2bc/HiRf777z8GDRqU7r0MGzYMFxeXdKPOixQq124a2V1jLuRLSUlJlsHY1q9fbxl3plq1auluZN5O69atb9uC6tZk//XXX+f111+/u4BFRCT/SKtJj70EqSlgk3uthAu7HNek16pVi7179wLQpEkTPvzwQ7Zt28bEiROpWLFirgcoeeOjjz7Cy8uLZs2a0blzZ4KCgqhfv/59j2P06NH07NmTvn370rRpU1xdXQkKCsLR0THT7VeuXMnly5czTVyrV69O9erVmTNnDg4ODqxbt46SJUvSoUMHateuzf/93/9ha2v+z6F169YsWbKElStXUrduXR566CF27txpOda0adPw9/enRYsWPPPMM4waNSpbc8a/9dZb1K9fn6CgIFq3bo2Pj0+G6eTefvttXn31VcaNG0f16tXp0aNHhn79PXv2xM7Ojp49e2b5WYgUeBo0Lt+rWbMms2fPZsuWLQQHB1sGCT179izFixe3cnQiIpKvOV8f3d2UCnFXrRtLAWMw5bCD8Nq1a4mJiaFbt24cPXqUTp06cfjwYYoXL87ixYvz/TRRUVFReHh4EBkZibu7e7p18fHxnDhxggoVKigxspLU1FSqV6/OU089xbvvvmvtcKzm5MmTVKpUiV27duXJzRN91yVf2DUHVo2Eqh2h50JrR2NVt/ttsqZNmzbRtWtXoqKi6NevH3PnzgXgzTff5ODBgyxbtszKEWYtv36mIiJFygcVIO4KvLADStWwdjRWlZPfpRw3dw8KujGIWuXKlTl48CBXrlzBy8tLU0RJjoWGhrJu3TpatWpFQkICs2bN4sSJEzzzzDPWDs0qkpKSuHz5Mm+99RYPPPCAVVo3iNw3ljnSVZOeX7Vu3ZpLly4RFRWFl5eXpfzZZ5/NVusiEREp4lxLmpN09UvPkRw1d09KSsLOzo5///03XXmxYsWUoMtdsbGxYf78+TRq1IjmzZuzb98+1q9fT/Xq1a0dmlVs27YNX19fdu3axezZs60djkjeir6pT7rkS3FxcSQkJFgS9NDQUKZPn86hQ4coWVI3V0RE5A4s07ApSc+JHNWk29vbU7ZsWc2FLrnG39+fbdu2WTuMfONOAyyJFCppNeluStLzq8cff5xu3brx/PPPExERQZMmTbC3t+fSpUt89NFHvPDCC9YOUURE8jMl6XclxwPHjR07ljfffDPLqa1ERESyRTXp+d6ePXto0aIFAEuXLqVUqVKEhoayYMECZsyYYeXoREQk30vr0hZ94fbbSTo57pM+a9Ysjh49ip+fH+XKlcPFxSXd+jvNcS0iIgLc1CddSXp+FRsba5l2dd26dXTr1g0bGxseeOABQkNDrRydiIjkey7XR3iPUZKeEzlO0m+dTkpERCTHTCaIPmd+roHj8q3KlSuzYsUKunbtytq1a3nllVcAuHDhgkZMFxGRO3O5/hsfc8m6cRQwOU7S33nnnbyIQ0REipL4CEhJND93UZKeX40bN45nnnmGV155hYceeoimTZsC5lr1evXqWTk6ERHJ99L6pKu5e47kOEkXERG5Z2k/1o4eYO9o3VgkS0888QQPPvgg4eHhBAYGWsrbtm1L165drRiZiIgUCK6qSb8bOU7SbWxsbjvdmkZ+FxGRO9KgcQWGj48PPj4+nD59GoAyZcrQuHFjK0clIiIFws190k0m0LTd2ZLj0d2XL1/OsmXLLMvixYt544038PX15YsvvsiLGOU+ad26NSNGjLC8Ll++PNOnT7/tPgaDgRUrVtzzuXPrOCJSQGjQuAIhNTWViRMn4uHhQbly5ShXrhyenp68++67pKamWjs8ERHJ79KauyfHQ2K0dWMpQHJck/74449nKHviiSeoWbMmixcvZtCgQbkSmGRf586dSUpKYs2aNRnWbdmyhZYtW7J3717q1KmTo+Pu2rUrw+j992r8+PGsWLGCkJCQdOXh4eF4eXnl6rmyEhcXR+nSpbGxseHMmTMYjcb7cl4RuYlq0guEsWPHMmfOHP7v//6P5s2bA7B161bGjx9PfHw877//vpUjFBGRfM3BBexdICnGfIPe6GbtiAqEHNekZ+WBBx5gw4YNuXU4yYFBgwYRHBxsaYp4s3nz5tGwYcMcJ+gA3t7eODs750aId+Tj43PfkuUff/yRmjVrUq1aNavX3ptMJpKTk60ag4hVKEkvEL7++mu++uorXnjhBerUqUOdOnV48cUX+fLLL5k/f761wxMRkYLA9XptuvqlZ1uuJOlxcXHMmDGD0qVL58bhJIc6deqEt7d3hgum6OholixZwqBBg7h8+TI9e/akdOnSODs7U7t2bb7//vvbHvfW5u5HjhyhZcuWODo6UqNGDYKDgzPsM3r0aKpUqYKzszMVK1bk7bffJikpCYD58+czYcIE9u7di8FgwGAwWGK+tbn7vn37eOihh3BycqJ48eI8++yzREffaCLTv39/unTpwtSpU/H19aV48eIMHTrUcq7bmTNnDr1796Z3797MmTMnw/r//vuPTp064e7ujpubGy1atODYsWOW9XPnzqVmzZoYjUZ8fX0ZNmwYACdPnsRgMKRrJRAREYHBYGDTpk0AbNq0CYPBwK+//kqDBg0wGo1s3bqVY8eO8fjjj1OqVClcXV1p1KgR69evTxdXQkICo0ePxt/fH6PRSOXKlZkzZw4mk4nKlSszderUdNuHhIRgMBg4evToHT8TkfvuWlqSrpHd87MrV65QrVq1DOXVqlXjypUrVohIREQKHMs0bBrhPbty3Nzdy8sr3cBxJpOJa9eu4ezszLfffpurweULJhMkxVrn3PbO2Rpcwc7Ojr59+zJ//nzGjh1r+fssWbKElJQUevbsSXR0NA0aNGD06NG4u7uzatUq+vTpQ6VKlbI1AFBqairdunWjVKlS/Pnnn0RGRqbrv57Gzc2N+fPn4+fnx759+xgyZAhubm68/vrr9OjRg3///Zc1a9ZYElAPD48Mx4iJiSEoKIimTZuya9cuLly4wODBgxk2bFi6GxEbN27E19eXjRs3cvToUXr06EHdunUZMmRIlu/j2LFj7Nixg2XLlmEymXjllVcIDQ2lXLlyAJw5c4aWLVvSunVrfvvtN9zd3dm2bZultvvzzz9n5MiR/N///R/t27cnMjKSbdu23fHzu9Ubb7zB1KlTqVixIl5eXpw6dYoOHTrw/vvvYzQaWbBgAZ07d+bQoUOULVsWgL59+7Jjxw5mzJhBYGAgJ06c4NKlSxgMBgYOHMi8efMYNWqU5Rzz5s2jZcuWVK5cOcfxieQ51aQXCIGBgcyaNYsZM2akK581a9ZdtdASEZEiSNOw5ViOk/SPP/44XZJuY2ODt7c3TZo0uW99iu+rpFiY5Gedc7951tyPIxsGDhzIlClT2Lx5M61btwbMSVr37t3x8PDAw8MjXQI3fPhw1q5dyw8//JCtJH39+vUcPHiQtWvX4udn/jwmTZpE+/bt02331ltvWZ6XL1+eUaNGsWjRIl5//XWcnJxwdXXFzs4OHx+fLM+1cOFC4uPjWbBggaVP/KxZs+jcuTMffPABpUqZL+q9vLyYNWsWtra2VKtWjY4dO7Jhw4bbJulz586lffv2lu9qUFAQ8+bNY/z48QB8+umneHh4sGjRIuzt7QGoUqWKZf/33nuPV199lZdfftlS1qhRozt+freaOHEiDz/8sOV1sWLF0k1v9O6777J8+XJWrlzJsGHDOHz4MD/88APBwcG0a9cOgIoVK1q279+/P+PGjWPnzp00btyYpKQkFi5cmKF2XSTfsAwcp5r0/OzDDz+kY8eOrF+/3jJH+o4dOzh16hSrV6+2cnQiIlIgqLl7juU4Se/fv38ehCH3qlq1ajRr1oy5c+fSunVrjh49ypYtW5g4cSJgnhpv0qRJ/PDDD5w5c4bExEQSEhKy3ef8wIED+Pv7WxJ0wHLBdrPFixczY8YMjh07RnR0NMnJybi7u+fovRw4cIDAwMB0g9Y1b96c1NRUDh06ZEnSa9asia2trWUbX19f9u3bl+VxU1JS+Prrr/nkk08sZb1792bUqFGMGzcOGxsbQkJCaNGihSVBv9mFCxc4e/Ysbdu2zdH7yUzDhg3TvY6Ojmb8+PGsWrWK8PBwkpOTiYuLIywsDDA3Xbe1taVVq1aZHs/Pz4+OHTsyd+5cGjduzM8//0xCQgJPPvnkPccqkidUk14gtGrVisOHD/Ppp59y8OBBALp168azzz7Le++9R4sWLawcoYiI5HtpNelq7p5tOU7S582bh6ura4aL/yVLlhAbG0u/fv1yLbh8wd7ZXKNtrXPnwKBBgxg+fDiffvop8+bNo1KlSpakbsqUKXzyySdMnz6d2rVr4+LiwogRI0hMTMy1cHfs2EGvXr2YMGECQUFBlhrpadOm5do5bnZrIm0wGG47JdDatWs5c+YMPXr0SFeekpLChg0bePjhh3Fycspy/9utA3OrEjB3AUmTVR/5W0fNHzVqFMHBwUydOpXKlSvj5OTEE088Yfn73OncAIMHD6ZPnz58/PHHzJs3jx49ety3gf9EciQlCWIvm58rSc/3/Pz8MozivnfvXubMmaOpV0VE5M4sfdIvWjeOAiTHA8dNnjyZEiVKZCgvWbIkkyZNypWg8hWDwdzk3BpLNvqj3+ypp57CxsaGhQsXsmDBAgYOHGjpmrBt2zYef/xxevfuTWBgIBUrVuTw4cPZPnb16tU5deoU4eHhlrI//vgj3Tbbt2+nXLlyjB07loYNGxIQEEBoaGi6bRwcHEhJSbnjufbu3UtMTIylbNu2bdjY2FC1atVsx3yrOXPm8PTTTxMSEpJuefrppy0DyNWpU4ctW7Zkmly7ublRvnz5LGcx8PY23yW8+TO6daq5rGzbto3+/fvTtWtXateujY+PDydPnrSsr127NqmpqWzevDnLY3To0AEXFxc+//xz1qxZw8CBA7N1bpH7LuYSYAKDLTgXt3Y0IiIikpdcrueO0UrSsyvHSXpYWBgVKlTIUF6uXDlL01yxDldXV3r06MGYMWMIDw9P1zUhICCA4OBgtm/fzoEDB3juuec4f/58to/drl07qlSpQr9+/di7dy9btmxh7Nix6bYJCAggLCyMRYsWcezYMWbMmMHy5cvTbVO+fHlOnDhBSEgIly5dIiEhIcO5evXqhaOjI/369ePff/9l48aNDB8+nD59+liauufUxYsX+fnnn+nXrx+1atVKt/Tt25cVK1Zw5coVhg0bRlRUFE8//TS7d+/myJEjfPPNNxw6dAgwz/M+bdo0ZsyYwZEjR9izZw8zZ84EzLXdDzzwAP/3f//HgQMH2Lx5c7o++rcTEBDAsmXLCAkJYe/evTzzzDPpWgWUL1+efv36MXDgQFasWMGJEyfYtGkTP/zwg2UbW1tb+vfvz5gxYwgICMi0O4JIvhB908juNrk2E6iIiIjkR66qSc+pHF8dlSxZkn/++SdD+d69eyleXDUi1jZo0CCuXr1KUFBQuv7jb731FvXr1ycoKIjWrVvj4+NDly5dsn1cGxsbli9fTlxcHI0bN2bw4MEZmj8+9thjvPLKKwwbNoy6deuyfft23n777XTbdO/enUcffZQ2bdrg7e2d6TRwzs7OrF27litXrtCoUSOeeOIJ2rZty6xZs3L2YdwkbRC6zPqTt23bFicnJ7799luKFy/Ob7/9RnR0NK1ataJBgwZ8+eWXlqb1/fr1Y/r06Xz22WfUrFmTTp06ceTIEcux5s6dS3JyMg0aNGDEiBG899572Yrvo48+wsvLi2bNmtG5c2eCgoKoX79+um0+//xznnjiCV588UWqVavGkCFD0rU2APPfPzExkQEDBuT0IxK5fzRonIiISNGhKdhyzGC6uQNtNowePZrFixdbpncC2Lx5MwMHDuSJJ57I96NJR0VF4eHhQWRkZIYBzeLj4zlx4gQVKlTA0dHRShGK3L0tW7bQtm1bTp06ddtWB/qui1XtWQArh0PAI9BribWjyRdu99tkDd26dbvt+oiICDZv3nzH7kvWlN8+UxGRIiv2Cnx4vSX2WxfBzsG68VhJTn6Xcjxw3LvvvsvJkydp27Ytdnbm3VNTU+nbt2/h7JMuUgAkJCRw8eJFxo8fz5NPPnnX3QJE7oubm7tLvuTh4XHH9X379r1P0YiISIHm5AU2dpCabG7y7lHa2hHlezlO0h0cHFi8eDHvvfceISEhODk5Ubt2bcqVK5cX8YlINnz//fcMGjSIunXrsmDBAmuHI3J7lubuupmUX82bN8/aIYiISGFhMJinYbsWbm7yriT9jnKcpKcJCAggICAgN2MRkbvUv3//dAMFiuRrmiNdRESkaLEk6ZesHUmBkOOB47p3784HH3yQofzDDz/MMHe6iIhIBho4TkREpGhxMU9VbLkGkNvKcZL++++/06FDhwzl7du35/fff8+VoKwth2PpiRQ4+o6LVakmXUREpGjRNGw5kuMkPTo6GgeHjCPy2dvbExUVlStBWUvaNFuxsbFWjkQkb6V9x9O+8yL3lfqki4iIFC0uJcyPStKzJcd90mvXrs3ixYsZN25cuvJFixZRo0aNXAvMGmxtbfH09OTCBfMFpLOzMwaDwcpRieQek8lEbGwsFy5cwNPTE1tbW2uHJEVNQjQkRpufK0kXEREpGlxUk54TOU7S3377bbp168axY8d46KGHANiwYQMLFy5k6dKluR7g/ebj4wNgSdRFCiNPT0/Ld13kvkpr6m7vAkZX68YiIiIi90dac3f1Sc+WHCfpnTt3ZsWKFUyaNImlS5fi5OREYGAgv/32G8WKFcuLGO8rg8GAr68vJUuWJCkpydrhiOQ6e3t71aCL9WjQOBERkaLH0txdo7tnx11NwdaxY0c6duwIQFRUFN9//z2jRo3ir7/+IiUlJVcDtBZbW1slMiIiuU2DxomIiBQ9lubuqknPjhwPHJfm999/p1+/fvj5+TFt2jQeeugh/vjjj9yMTUREChvVpIuIiBQ9aVOwxVyC1FTrxlIA5Kgm/dy5c8yfP585c+YQFRXFU089RUJCAitWrCjwg8aJiMh9oJp0ERGRoietubspBeKugktx68aTz2W7Jr1z585UrVqVf/75h+nTp3P27FlmzpyZl7GJiEhhoyRdRESk6LG1Bycv83M1eb+jbNek//rrr7z00ku88MILBAQE5GVMIiJSWKU1d3dTki4iIlKkuJQ016LHXASqWzuafC3bNelbt27l2rVrNGjQgCZNmjBr1iwuXdLofCIikgOqSRcRESmaNA1btmU7SX/ggQf48ssvCQ8P57nnnmPRokX4+fmRmppKcHAw165dy8s4RUSkMLAk6Ro4TkREpEjRNGzZluPR3V1cXBg4cCBbt25l3759vPrqq/zf//0fJUuW5LHHHsuLGEVEpDBITb1pdHfVpIuIiBQpmoYt2+56CjaAqlWr8uGHH3L69Gm+//773IpJREQKo7gr5lFd4cZULCIiIlI0pP32q7n7Hd1Tkp7G1taWLl26sHLlytw4nIiIFEZpTd2di5tHeRUREZGiw/WmudLltnIlSRcREbkjDRonIiJSdKXVpKu5+x0pSRcRkfvD0h9dg8aJiIgUOZY+6RetG0cBoCRdRETuD0tNuo914xAREZH7L2109+iLYDJZN5Z8Tkm6iIjcH6pJFxERKbrSfv+T4yAxxrqx5HNK0kVE5P64ds78qD7pIiIiRY+DC9i7mJ+rX/ptKUkXEZH7QwPHiYiIFG1pTd41wvttKUkXEZH7Q83dRUREira0awDNlX5bStJFROT+UE26iIhI0aZp2LJFSbqIiOS95ASIjzA/V026iIhI0WRJ0tXc/XaUpIuISN5La9Zm6wBOXtaNRURERKwjLUlXc/fbUpIuIiJ5z9IfvRQYDNaNRURERKwjrTVdzEXrxpHPKUkXEZG8F502/ZqauouIiBRZltHdlaTfjpJ0ERHJexo0TkRERFxUk54dStJFRCTvafq1Iuv333+nc+fO+Pn5YTAYWLFixR332bRpE/Xr18doNFK5cmXmz5+f53GKiMh9oCnYskVJuoiI5D3VpBdZMTExBAYG8umnn2Zr+xMnTtCxY0fatGlDSEgII0aMYPDgwaxduzaPIxURkTyXNnBcfAQkJ1o1lPzMztoBiIhIEaCa9CKrffv2tG/fPtvbz549mwoVKjBt2jQAqlevztatW/n4448JCgrKqzBFROR+cPQEGztITYbYS+DuZ+2I8qV8UZP+6aefUr58eRwdHWnSpAk7d+7M1n6LFi3CYDDQpUuXvA1QRETujWrSJZt27NhBu3bt0pUFBQWxY8eO2+6XkJBAVFRUukVERPIZGxtwvj54nJq8Z8nqSfrixYsZOXIk77zzDnv27CEwMJCgoCAuXLj9H+3kyZOMGjWKFi1a3KdIRUTkrilJl2w6d+4cpUql/56UKlWKqKgo4uListxv8uTJeHh4WBZ/f/+8DlVERO6G6/Um7zGXrBtHPmb1JP2jjz5iyJAhDBgwgBo1ajB79mycnZ2ZO3dulvukpKTQq1cvJkyYQMWKFe9jtCIikmMmU/p50kXywJgxY4iMjLQsp06dsnZIIiKSmbR+6TGqSc+KVZP0xMRE/vrrr3TN2mxsbGjXrt1tm7VNnDiRkiVLMmjQoDueQ83fRESsLD4SkuPNz9UnXe7Ax8eH8+fPpys7f/487u7uODk5Zbmf0WjE3d093SIiIvmQpmG7I6sm6ZcuXSIlJSXTZm3nzp3LdJ+tW7cyZ84cvvzyy2ydQ83fRESsLK0W3egB9lknWSIATZs2ZcOGDenKgoODadq0qZUiEhGRXJXW3F190rNk9ebuOXHt2jX69OnDl19+SYkSJbK1j5q/iYhYmaU/umrRi6Lo6GhCQkIICQkBzFOshYSEEBYWBph/p/v27WvZ/vnnn+f48eO8/vrrHDx4kM8++4wffviBV155xRrhi4hIbnNRn/Q7seoUbCVKlMDW1jbTZm0+Pj4Ztj927BgnT56kc+fOlrLU1FQA7OzsOHToEJUqVUq3j9FoxGg05kH0IiKSLRo0rkjbvXs3bdq0sbweOXIkAP369WP+/PmEh4dbEnaAChUqsGrVKl555RU++eQTypQpw1dffaXp10RECgtLc3fVpGfFqkm6g4MDDRo0YMOGDZZp1FJTU9mwYQPDhg3LsH21atXYt29furK33nqLa9eu8cknn6gpu4hIfqQ50ou01q1bYzKZslw/f/78TPf5+++/8zAqERGxGktNuvqkZ8WqSTqY76j369ePhg0b0rhxY6ZPn05MTAwDBgwAoG/fvpQuXZrJkyfj6OhIrVq10u3v6ekJkKFcRETyCdWki4iISBpLn3Ql6VmxepLeo0cPLl68yLhx4zh37hx169ZlzZo1lsHkwsLCsLEpUF3nRUTkZqpJFxERkTQ316SnpoJyvQysnqQDDBs2LNPm7QCbNm267b6ZNZMTEZF8JK0m3S3jWCMiIiJSxDhfHwDclALxEeBczKrh5Ee6bSEiInlLo7uLiIhIGjsHcPQ0P9c0bJlSki4iInlLfdJFRETkZmk37jV4XKaUpIuISN5JSb4xD6qSdBEREQFNw3YHStJFRCTvxF4CTGCwAefi1o5GRERE8gOX6/3S027kSzpK0kVEJO+kNXV38QYbW+vGIiIiIvlDWnN39UnPlJJ0ERHJO5p+TURERG5lmYZNSXpmlKSLiEje0aBxIiIicitLkq7m7plRki4iInnHkqRrjnQRERG5Li1JV3P3TClJFxGRvHNNc6SLiIjILTQF220pSRcRkbyj5u4iIiJyK0tzdyXpmVGSLiIieUcDx4mIiMit0pL0pFhIjLFuLPmQknQREck7qkkXERGRWxldwd7Z/Fz90jNQki4iInnHUpOuJF1ERERu4lLC/KgR3jNQki4iInkjMQYSr5mfq7m7iIiI3MwlbfA41aTfSkm6iIjkjbRadDsnMLpZNxYRERHJXzQNW5aUpIuISN5I+9F1KwUGg3VjERERkfzFNW2EdzV3v5WSdBERyRvR58yP6o8uIiIit0qrSb90CFJTrRtLPqMkXURE8oamXxMREZGseJY1P+5bAp82gr++huQE68aUTyhJFxGRvKHp10RERCQrdZ6Glq+BowdcPgo/vwTTa8PWjyE+0trRWZWSdBERyRtK0kVERCQr9o7w0Fvwyn8QNAncS5uvHdaPh49qwrq3ISrc2lFahZJ0ERHJG2ruLiIiIndidIOmQ+GlEOjyOXhXM0/hun0GfFIHfhoGl45YO8r7Skm6iIjkDdWki4iISHbZOUDdZ+CFHdBzMZRtCimJ8Pc3MKsRLOoFp3bdn1jio+D4JtgyDX4cAibT/TnvdXb39WwiIlJ0qCZdREREcsrGBqo+al7C/oRt0+HQajj4i3kp1xyavwwBj+TOFK8pyXDhPzi9G87sgTO74eIh4KbEvM2bUKzCvZ8rm5Ski4hI7ktNvakm3ce6sYiIiEjBVLYJlP3enDRvmwH/LIbQbealZA1zsl6rO9jaZ+94JhNEnrqekP9lXs6GQHJcxm09ykKZBlC6ATi45urbuhMl6SIikvvirkJqsvl52jyoIiIiInfDuyp0+dRco/3n57B7PlzYD8ufgw3vmvu01+8LxluS6bgIOPu3uXb89PWkPOZCxuMbPaB0PSjdEMo0NCfmVmwJqCRdRERyX1otulMxcx8zERERkXvlURoeeQ9ajILdc+GPzyHqNKwdA5s/gMbPglup6wn5brh0OOMxbOygVK0byXjphlC8srmZfT6hJF1ERHKfBo0TERGRvOLkCS1GwgMvwt7vzSPBXzkOv3+YcVvPctcT8utJuW8dsHe67yHnhJJ0ERHJfRo0TkRERPKavSM0HGBu6n7wF9g9zzyY3M3N1l1KWDvKHFOSLiIiuU816SIiInK/2NhCjcfNSyGQfxrei4hI4WFJ0lWTLiIiIpITStJFRCT3WZq7qyZdREREMhcZm0RySqq1w8h31NxdRERyX/T/t3fn8U1Vef/APzdpk3RNN7qXlq3sLQxLLegwYKUgM9IZFeTHCDKKo4KP2vH3IKNQ1MdB1EFe4zCgM4Iz4wIyj6IjClMKuGARZAdpWYRCadMNmrQpTdrkPH+kDYQuUEh7b9LP+/XKK3c59/Z7cpKefHPvPdfgeA7iPdKJiIjIVaGhBq9uKcTWY2UI0vpgVK8wpPcOR3qfcAyMCYZaJckdoqyYpBMRkftx4DgiIiK6SvHFOryeewIf7S+GEI5lNZZGbCsox7YCx3cHvZ8vRl+RtPePCoKqmyXtTNKJiMj9OHAcERERNamqteDP20/ivV1nYW06vX3ykGhk35GM+gY78n+sRP6pKuw5cxHGSw3I/aEMuT84vkuEBWiQ1isM6X3Ckd47HH0jAyFJ3p20M0knIiL3arQAly46ppmkExERdVu1lkb87esf8devfoTZagMAjOkTjgWTBiA1IcRZbmi8Hg//tA8abXYcPm9E/o9VyD9Vhe/PXMQFsxVfHDHgiyOOS+kiArW4pfflpL1XRIDXJe1M0omIyL3MFY5nlS+gC5E1FCIiIup6lkYb3v/uLP687SSqzFYAwJC4YCyYNAC39evR5nY+ahWG9wzF8J6heOxnfWFttONQcTXyT1Vh12lH0l5Za8Fnh0rx2aFSAEB0sM6ZtI/pE4GEMP8uqWNnYpJORETudeXt11S8iQgREVF3YbMLbNx/Hstzj+N89SUAQK+IADw9sT8mD4nu8LXlGh8VRiaFYWRSGB5HP1gabThwttp5pH3/2WoYTPXYeKAEGw+UAACGxunxZEY/TBgQ6bFH2JmkExGRe3HQOCIiom5FCIG8Y+V4dUshCstqAACRQVo8mZGMe0fGw1ftnh/ttT5qpPUOR1rvcDyZAdQ32LCv6KIzaT9wrhqHzxvx4N+/R2q8Hk/ekYyfJffwuGSdSToREbkXB40jIiLqNvacuYBlXxTg+yLHeDTBOh88+rO+eGBMEvw06k792zpfNcb0jcCYvhEAgAtmK9766kf8/dszOFhsxJy1ezAsIQTZdyTjtn4RHpOsM0knIiL3qmGSTkRE5O2OlZrw2pZC5DXdOk3ro8Kcsb3w6Lg+0Pv7yhJTWIAGz0wegIdu64W3vvoR/8g/gwPnqjFrzW6MSAxF9h3JGNMnXPHJOpN0IiJyLx5JJyIi8lrnLtRhee5xbDxwHkIAapWE6aMS8F8T+iFar5M7PACOEeB/f+dAPHRbL7z55Y94d1cR9hZdxMy/fYfRvcLwVEYy0vuEyx1mm5ikExGRe105cBwRERF5hcpaC/687STe+64IDTYBAJiSEoPf3ZGM3j0CZY6udZFBOiz6+SD89qe98Zcdp/D+7rPYffoCZvx1F9J7h+OpO5IxuleY3GG2wCSdiIjcyzlwHI+kExERebr6BhvW7jyDldtPotbSCAC4rV8E/jtzAIbG62WO7vpEBuuw5K7BeGRcH/xlx0ms233OMdjcm/m4tW8EnrqjH0YkKidZZ5JORETuxdPdiYiIPJ4QAv8+VIplXxQ4b6c2JC4YCycPxNimgdo8TbRehxemDsFvx/XByu0nseH7c/jmZCW+OVmJnyb3wFMZ/TC8Z6jcYTJJJyIiNxKCt2AjIiLycHuLLuJ/Nv2A/WerAQDRwTr896T+yBoW1+F7nStRXIgf/vDLoXi0KVn/195ifHW8Al8dr8D4/j3w1B3JSIkPkS0+JulEROQ+lhqg0fFrO5N0IiIiz3LuQh2WbS7AZ4dKAQD+GjUeGdcHc2/r3em3U5NDQpg/Xr47BY/9rC/e2HYCH+0/j+2FFdheWIGMgZF4MiMZQ+K6/pR+JulEROQ+zUfRNUGAJkDeWIiIiOi6mOob8Jftp7Bm52lYG+2QJGDaiAT8bmIyIoOVMWJ7Z+oZ7o9X703FvPF98adtJ7Bx/3lsPVaOrcfKMXFQFJ6dMhCJ4V33vYZJOhERuU+twfEcxOvRiYiIlK7RZscHe87h9dzjuGC2AgDG9AnHs1MGYnCsZwwK505JEQFYPm0Y5o3vizfyTuCTgyXYVlCORT8f1KVxMEknIiL34aBxREREiieEwI7CCrz0+TGcLK8FAPTuEYBn7xyICQMiIUmef935zejTIxAr7huO+RP64vszF5EQ5t+lf59JOhERuQ8HjSMiIlK0AoMJL206hq9PVAIAQv198WRGMv5fWk/4qlUyR6csfSOD0DcyqMv/LpN0IiJyHx5JJyIiUqTymnq8nnsc6/ecg10AvmoJc8b2wrzxfaH385U7PLoCk3QiInIfHkknIiJSlPoGG97+5jT+sv0kzFYbAODOodFYMGlAlw6GRtePSToREbkPj6QTEREpgt0u8OnBEryyuQAlxnoAQGq8Hs/9fBBGJYXJHB21hxcdEBGR+zBJpzasXLkSSUlJ0Ol0SEtLw+7du9ss+84770CSJJeHTuf9twAiInKXvUUX8ctV3+LJ9QdQYqxHrF6HFdOH4ePHxjJB9wA8kk5ERO7D092pFevXr0d2djZWr16NtLQ0rFixApmZmSgsLERkZOvvleDgYBQWFjrnu/tIw0RE1+N89SUs+6IAnx4sAQAEaNR4bHxfPHhrL+h81TJHR9eLSToREbmH3QaYKxzTgdHyxkKKsnz5csydOxdz5swBAKxevRqbNm3CmjVr8Mwzz7S6jSRJiI7m+4iI6HrUWRuxescpvPnVj7A02iFJwL0j4vH0xP6IDOaZSJ6GSToREbmHuRIQdkBSAQERckdDCmG1WrF3714sXLjQuUylUiEjIwP5+fltbldbW4vExETY7Xb85Cc/wR/+8AcMHjy4zfIWiwUWi8U5bzKZ3FMBIiIFs9sFNh44j2WbC1BmcvwPHN0rDIt/PghD4vQyR0c3ikk6ERG5R/P16P4RgIqn1JFDZWUlbDYboqJcxymIiopCQUFBq9v0798fa9asQUpKCoxGI1577TWMGTMGR48eRXx8fKvbLF26FM8//7zb4yciUqq9RRfwwr9/wMFiIwAgPtQPz945EJOGRPMSIQ/HJJ2IiNzDeT06B42jm5Oeno709HTn/JgxYzBw4EC8+eabePHFF1vdZuHChcjOznbOm0wmJCQkdHqsRERdrbXrzudN6IvfjOV1596CSToREbmHc2R3DhpHl0VERECtVqOsrMxleVlZ2XVfc+7r64vhw4fj5MmTbZbRarXQarU3FSsRkZK1dt35tBEJ+F1mMiKDeN25N+Et2IiIyD14+zVqhUajwYgRI5CXl+dcZrfbkZeX53K0vD02mw2HDx9GTExMZ4VJRKRYdrvAR/uKMf61HfjTtpOwNNoxulcY/j3/Viy7J4UJuhfikXQiInIP3n6N2pCdnY3Zs2dj5MiRGD16NFasWAGz2ewc7X3WrFmIi4vD0qVLAQAvvPACbrnlFvTt2xfV1dV49dVXUVRUhIceekjOahARdbmrrztPCPPD7yfzunNvxySdiIjco/lIehBvm0Wupk+fjoqKCixevBgGgwHDhg3D5s2bnYPJnT17FirV5ZP7Ll68iLlz58JgMCA0NBQjRozAt99+i0GDBslVBSKiLtXadefzJ/TDnLFJvO68G5CEEELuILqSyWSCXq+H0WhEcHCw3OEQEXmPtXcCRTuBe9YAQ+6WOxqPwr7J/fiaEpEn4nXn3qsj/RKPpBMRkXvwmnQiIqIbYrMLbNx/Hq9suXy/87ReYVjE+513S0zSiYjIPXgLNiIiog6x2wU2HS7Fiq3HcarCDMBx3fmzdw5E5mBed95dMUknIqKbZ60DLCbHNAeOIyIiapcQAluOGvB67gkUltUAAEL8ffHIuD54YAyvO+/umKQTEdHNMzcdRffRAVpe/0tERNQaIQS2FZRjee5xHC1x/LgdpPPB3Nt6Y87YJATpfGWOkJSASToREd28K2+/xlPziIiIXAgh8NWJSizPPY6D56oBOEZs/82tvfDQrb2h92dyTpeprl2k861cuRJJSUnQ6XRIS0vD7t272yz717/+FbfddhtCQ0MRGhqKjIyMdssTEVEX4KBxRERErfr2ZCXuXZ2P2Wt24+C5avj5qvHIuD74esEE/G5ifybo1ILsR9LXr1+P7OxsrF69GmlpaVixYgUyMzNRWFiIyMiW1zXu2LEDM2bMwJgxY6DT6bBs2TJMnDgRR48eRVxcnAw1ICIi1Bgcz0zSiYiIAAB7zlzAH/9TiF0/XgAAaH1U+PUtiXhkXB/0CNLKHB0pmexJ+vLlyzF37lzMmTMHALB69Wps2rQJa9aswTPPPNOi/Hvvvecy/7e//Q3/+7//i7y8PMyaNatLYiYioqtwZHciIiIAwP6zF7E89zi+PlEJANCoVZgxOgGPje+LqGDe65yuTdYk3Wq1Yu/evVi4cKFzmUqlQkZGBvLz869rH3V1dWhoaEBYWFir6y0WCywWi3PeZDLdXNBERNQST3cnIqJu7nCxEa9vPY5tBY4frn1UEqaNSsC88X0RF+Inc3TkSWRN0isrK2Gz2RAV5fqlLioqCgUFBde1jwULFiA2NhYZGRmtrl+6dCmef/75m46ViIjaceXAcURERN3IsVITXs89jv/84PjBWq2S8Kvhcfiv2/shIcxf5ujIE8l+uvvNePnll7Fu3Trs2LEDOl3rp44sXLgQ2dnZznmTyYSEhISuCpGIqHvgkXQiIupmTpTVYMXWE9h0uBSA4+YmWcMcyXmviACZoyNPJmuSHhERAbVajbKyMpflZWVliI6Obnfb1157DS+//DK2bt2KlJSUNstptVpotRyYgYio0wgB1Di+oDBJJyIib2azC2wvKMc/dxXhy+MVzuVTUmLwVEY/9I0MkjE68hayJukajQYjRoxAXl4esrKyAAB2ux15eXmYP39+m9u98soreOmll7BlyxaMHDmyi6IlIqJWFWxyJOk+fkB4H7mjISIicruqWgvWf38O7+06i/PVl5zLMwdH4ak7kjEgOljG6MjbyH66e3Z2NmbPno2RI0di9OjRWLFiBcxms3O091mzZiEuLg5Lly4FACxbtgyLFy/G+++/j6SkJBgMjtv+BAYGIjAwULZ6EBF1S7YGYGuOYzp9HuAXIms4RERE7iKEwP5z1fhnfhE2HSqF1WYHAOj9fDFtZDx+fUsiEsN5Wju5n+xJ+vTp01FRUYHFixfDYDBg2LBh2Lx5s3MwubNnz0KlUjnLr1q1ClarFffcc4/LfnJycrBkyZKuDJ2IiPb9A6g6CfiHA2OfkDsaIiKim3bJasOnB8/jH/lFOFpy+c5QQ+P0uD89EXelxkLnq5YxQvJ2khBCyB1EVzKZTNDr9TAajQgO5mkpREQ3zFID/Gk4YK4AJr8KpD0sd0Qei32T+/E1JaKOOl1pxru7irDh+3Mw1TcCADQ+KvwiJRaz0hORmhAib4Dk0TrSL8l+JJ2IiDzUt284EvSwPsDIOXJHQ0RE1GE2u0DesTL8c1cRvj5R6VyeEOaHX6cl4t6RCQgL0MgYIXVHTNKJiKjjagyOJB0AMnIAta+88RAREXVAZa0F6/ecw/vfXR4ITpKA8f0jcf8tiRiX3AMqlSRzlNRdMUknIqKO27EUaKgD4kcBA++SOxoiIqJrEkJg39mL+Ed+ET4/XIoGm+Oq31B/X0wblYCZoxPRM9xf5iiJmKQTEVFHlRc4BowDgIn/4zj0QEREpFAVNRb8+2AJNuwtxrHSywPBDUsIwf23JGJKSgwHgiNFYZJOREQds3UJIOzAgJ8DPW+ROxoiIqIWai2N+M9RAz7efx47T1bC3jRUttZHhbtSYzErPQlD4/XyBknUBibpRER0/c58Axz/ApDUQMYSuaMhIiJyarDZ8dXxCmw8UILcHwyob7A71w1LCEHWsFhkDY9DiD8HgiNlY5JORETXx24H/rPIMT3iASCin6zhEBERCSGwt+giNh44j02HSnGxrsG5rldEAKYOi8XUYXHoFREgY5REHcMknYiIrs8PHwMl+wBNIPCzZ+SOhoiIurGT5TXYuL8Enxw8j3MXLjmXRwRq8YvUGGQNi0NKvB4Sx00hD8QknYiIrq3RAmx93jE99gkgMFLeeIiIqNspM9Xj0wMl2HjgPI6WXB4ALkCjRuaQaGQNi8OYPuHwUatkjJLo5jFJJyKia9vzNlBdBARGAenz5I6GiIi6CVN9AzYfMWDj/vPI/7EKomkAOB+VhHHJPTB1eBzuGBgFPw1HZyfvwSSdiIjad6ka+OoVx/T43wMaXtdHRESdp87aiC8LK/DvQyXYeqwc1sbLA8CNTAzF1OFxmDI0BmEBHACOvBOTdCIiat83rwOXLgI9BgDDfi13NERE5IWMlxqwraAMXxw24MvjFbBckZj3iwxE1vA43JUai4QwfxmjJOoaTNKJiKht1eeAXasc0xnPA2p2G0RE5B5VtRb854cybD5iwLenKtFgE851CWF+mDwkBlOHxWJQTDAHgKNuhd+2iIiobdtfAmwWIPFWIDlT7miIiMjDlRovYcsRA744YsCeMxdgv5yXo19kICYPiUbmkGgm5tStMUknIqLWlR4CDq5zTE98AeCXJSIiugFnKs3YfNSAzUcMOHCu2mXd0Dg9Jg2JRubgaPSNDJQnQCKFYZJORESt25oDQABD7gbiRsgdDREReQghBI6X1eKLI6XYfMSAAkONc50kOQZ/yxwcjUlDohEfymvMia7GJJ2IiFo6mQec2gaofIEJi+SOhoiIFE4IgUPFRucR89OVZuc6tUpCeu9wTBoSjYmDohAZrJMxUiLlY5JORESu7DYgN8cxPXouENZL3niIiEhxKmstOFRcjYPnjDhUXI1DxUZUma3O9RofFX7aLwKZg6Nxx6AohPjzdmlE14tJOhERuTr0IVB2GNDqgZ/+f7mjISIimZnqG3Ck2IiDxZcT8vPVl1qU89eoMX5AJCYNjsb4AZEI1DLVILoR/OQQEdFlDZeAbf/jmL4tG/APkzceIiLqUvUNNhwtMTmT8YPF1fixwtyinCQBfXoEIiVej9T4EKTE6zEwJhg6X7UMURN5FybpRER02XerAVMxEBwPpP1W7miIiKgTNdjsOF5Wg0NNR8gPnjOisKwGtivvi9YkPtTPmYynxIdgSFwwgnS+MkRN5P2YpBMRkYO5Cvh6uWN6wnOAr5+88RARkdsY6xpQYDChsKwGx0prUGAw4YcSEyyN9hZlIwK1SG1KxlMS9EiJ0yM8UCtD1ETdE5N0IiJy+Po1wGICooYCKdPljoaIiG5Ag82O05VmHCs1ocBQg0JDDQpKTSgx1rdaPkjn4zw63pyYx+h1kCSpiyMnomZM0omICLjwI7D7r47piS8AKpW88RARUbuEEKiosaDA4DgqXlBag2OGGpwqr4XV1vLoOADEhfhhQHQQBsQEoX90MIbEBiMpPAAqFRNyIiVhkk5EREDei4C9AegzwfEgIiLFMF5qwJlKMwrLalDQdKp6gaEGF6645dmVAjRqDIgJRv/oIAyMDsKAmGAkRwVB78dryIk8AZN0IqLurngvcPQjABJwxwtyR0NE1O0IIVBltqKoyoyiqjqcqarD2SozzlTVoajKjIt1Da1up5KApIgADIwOxoDoIEdSHhOMuBA/Hh0n8mBM0omIujMhgNxFjunUGUD0UHnjIVKY+gYbDp83YnBsMPw1/NpEN85uFzCY6lHUlHifqarD2QtmnKmsw9kLdai1NLa7fUSgBslRQRgQHYwBMUEYGB2MflGBvOUZkRdib0NE1J0d3wwU7QTUWmDCs3JHQ6Q4R0tMuHd1PlQS0Dcy0DHadbweQ+N4T2hyZW20o7LWgvIaC8pN9S4JeVFVHYou1MHaykjqzSQJiNX7oWeYP5Ii/JEYHoDEMMdzz3B/BGr5tZ2ou+CnnYiou7I1Ark5julbHgX08fLGQ6RAF81WRAVrUWay4HhZLY6X1eJfe4sBAD4qCclRQY6kPV6P1PgQJEcFQePDgRe9idnS6Ey8K2otKDc1JeI19aioccxX1FravD78Sj4qCQlh/o5EPLwpEW96jg/1448+RASASToRUfe1/59AZSHgFwbc+pTc0RApUsagKGQMikKZqR6Hi404dN6Iw8XVOFRsRJXZih9KTfih1IR1e84BADRqFQbGBGFovB4pcSEYGq9Hv8hA+KiZuCuJpdGGC2YrKmusqDRbUFVrdSTczYl3jaUpAa+H2Wq77v36qiX0CNSiR7AOUUFaJIb7o2d4AJLC/ZEUHoAYvY7vBSK6JibpRETdkaUW2LHUMT3uvwG/EFnDIVK6qGAdogbpkDEoCoBjoK9SYz0ONSXsh88bcajYCOOlBhwsNuJgsRHAWQCAzleFwbGOU+Qd96PWIzE8AL5M1tzGbhcwXmpAldmCylorqmqtjukaCyrNVlTVWpqWWVFZa0FNffvXf1/NX6NGZJAWkUE69AjWOqcjg7ToEaRFZLBjPsTPlwO2EdFNY5JOROSthACEvfXHzj8BtWVAaBIw8kG5IyXyOJIkITbED7Ehfpg0JAaAI3E/d+ESDp2vdhx1LzbiyHkjaiyN2Ft0EXuLLrrsQ+OjQoBGDX+NDwK0l58DND4I0PrAX6O+/KzxgX/TOpflTc9+vmqoVZLzoZIk+DRNS5KyksZGmx31jXZcstpQ3+B4XGqwOeZbW95gQ/0V65qXmS41oLLWkXRfMFths4sOxeGjkhAeqEF4gBbhgZqmI+CXk+/IIC0ig3XoEaTl9eBE1KX4H+dm7HgZ+H6t3FEQdV/OL55S0/SVz20tb+dZCACiA89oZ727iVaS7ivnbS0T8etx+2LAR9MJ8RJ1P5IkoWe4P3qG++PnKbEAHEd4z1SZcagpaT98vhpHzptwqcEGa6Md1kZ7m7fXcl9cjoRUJTUl8ZIEtdrxrFJJLut8VI5lakmCgHD8m4HjBwjnv7wr5h3rBexN/3KuXn55e8cp5vUNNjTYOuN/pEOwzgcRgVpEBDoS7+YkPCJQg/ArlkcEaBHs56O4HzCIiAAm6TfHUgPUGuSOgojoxvS9Axj8K7mjIPJqKpWE3j0C0btHILKGxwEAbHaBmvoGmK021Fkanc+1lkbUWW0wWxtRZ2l6ttpgtlz13Mp6SzujhguBpsS485LjG6XzVcHPVw2dr9r5rPNVwU9z5bxj2k+jhs5HBZ1GDZ2PGno/X0QEaREeoEFEoBZhARoO2kdEXoFJ+s1Inw+k3id3FETd0w0d3b7Gc4uj66rrOAKPdta7maRq5SG1sVwFSOp21kuAj9b9MRLRNalVEkL8NQjxd98+hRCw2QUa7QL2pmnn44p5ux1otNthF46yVy+zNU/bm/59AE3/6iTnvCRdOe0ooJKalqN5O8n5b7B5XtuUkDcn31ofFa/fJiJqBZP0mxEc43gQERERyUiSJPioJfjwDl5ERB6P5wQRERERERERKQSTdCIiIiIiIiKFYJJOREREREREpBBM0omIiIiIiIgUgkk6ERERdbqVK1ciKSkJOp0OaWlp2L17d7vlN2zYgAEDBkCn02Ho0KH4/PPPuyhSIiIieTFJJyIiok61fv16ZGdnIycnB/v27UNqaioyMzNRXl7eavlvv/0WM2bMwIMPPoj9+/cjKysLWVlZOHLkSBdHTkRE1PUkIYSQO4iuZDKZoNfrYTQaERwcLHc4REREXt83paWlYdSoUfjzn/8MALDb7UhISMDjjz+OZ555pkX56dOnw2w247PPPnMuu+WWWzBs2DCsXr36uv6mt7+mRETkWTrSL/FIOhEREXUaq9WKvXv3IiMjw7lMpVIhIyMD+fn5rW6Tn5/vUh4AMjMz2ywPABaLBSaTyeVBRETkiZikExERUaeprKyEzWZDVFSUy/KoqCgYDIZWtzEYDB0qDwBLly6FXq93PhISEm4+eCIiIhkwSSciIiKPt3DhQhiNRufj3LlzcodERER0Q3zkDoCIiIi8V0REBNRqNcrKylyWl5WVITo6utVtoqOjO1QeALRaLbRa7c0HTEREJDMeSSciIqJOo9FoMGLECOTl5TmX2e125OXlIT09vdVt0tPTXcoDQG5ubpvliYiIvAmPpBMREVGnys7OxuzZszFy5EiMHj0aK1asgNlsxpw5cwAAs2bNQlxcHJYuXQoAeOKJJzBu3Dj88Y9/xJQpU7Bu3Tp8//33eOutt+SsBhERUZdgkk5ERESdavr06aioqMDixYthMBgwbNgwbN682Tk43NmzZ6FSXT65b8yYMXj//ffx3HPP4fe//z369euHjRs3YsiQIXJVgYiIqMt0u/ukG41GhISE4Ny5c7xvKhERKYLJZEJCQgKqq6uh1+vlDscrsL8nIiIl6Uhf3+2OpNfU1AAAb81CRESKU1NTwyTdTdjfExGREl1PX9/tjqTb7XaUlJQgKCgIkiTd1L6afw3xhl/pWRfl8ZZ6AN5TF2+pB+A9dfGWegghUFNTg9jYWJfTvunGsb9vyVvqAXhPXbylHgDrokTeUg/AO+rSkb6+2x1JV6lUiI+Pd+s+g4ODPfbNcjXWRXm8pR6A99TFW+oBeE9dvKEePILuXuzv2+Yt9QC8py7eUg+AdVEib6kH4Pl1ud6+nj/XExERERERESkEk3QiIiIiIiIihWCSfhO0Wi1ycnKg1WrlDuWmsS7K4y31ALynLt5SD8B76uIt9SBl85b3mbfUA/CeunhLPQDWRYm8pR6Ad9XlenS7geOIiIiIiIiIlIpH0omIiIiIiIgUgkk6ERERERERkUIwSSciIiIiIiJSCCbpRERERERERArBJP0aVq5ciaSkJOh0OqSlpWH37t3tlt+wYQMGDBgAnU6HoUOH4vPPP++iSNu2dOlSjBo1CkFBQYiMjERWVhYKCwvb3eadd96BJEkuD51O10URt23JkiUt4howYEC72yixTZKSklrUQ5IkzJs3r9XySmqPr776Cr/4xS8QGxsLSZKwceNGl/VCCCxevBgxMTHw8/NDRkYGTpw4cc39dvSz5g7t1aWhoQELFizA0KFDERAQgNjYWMyaNQslJSXt7vNG3qOdWQ8AeOCBB1rENGnSpGvuV2ltAqDVz40kSXj11Vfb3KccbUKex9P7e/b1ymqPZp7a37OvZ1/fmdjXXxuT9HasX78e2dnZyMnJwb59+5CamorMzEyUl5e3Wv7bb7/FjBkz8OCDD2L//v3IyspCVlYWjhw50sWRu/ryyy8xb9487Nq1C7m5uWhoaMDEiRNhNpvb3S44OBilpaXOR1FRURdF3L7Bgwe7xPXNN9+0WVapbbJnzx6XOuTm5gIA7r333ja3UUp7mM1mpKamYuXKla2uf+WVV/CnP/0Jq1evxnfffYeAgABkZmaivr6+zX129LPmLu3Vpa6uDvv27cOiRYuwb98+fPTRRygsLMRdd911zf125D3qDtdqEwCYNGmSS0wffPBBu/tUYpsAcKlDaWkp1qxZA0mScPfdd7e7365uE/Is3tDfs69XVns089T+nn09+/rOxL7+Oghq0+jRo8W8efOc8zabTcTGxoqlS5e2Wn7atGliypQpLsvS0tLEb3/7206Ns6PKy8sFAPHll1+2WWbt2rVCr9d3XVDXKScnR6Smpl53eU9pkyeeeEL06dNH2O32VtcrtT0AiI8//tg5b7fbRXR0tHj11Vedy6qrq4VWqxUffPBBm/vp6GetM1xdl9bs3r1bABBFRUVtlunoe9TdWqvH7NmzxdSpUzu0H09pk6lTp4oJEya0W0buNiHl88b+nn29stqjmSf29+zrW5K7X2Ff35LcbeJuPJLeBqvVir179yIjI8O5TKVSISMjA/n5+a1uk5+f71IeADIzM9ssLxej0QgACAsLa7dcbW0tEhMTkZCQgKlTp+Lo0aNdEd41nThxArGxsejduzdmzpyJs2fPtlnWE9rEarXi3XffxW9+8xtIktRmOaW2x5VOnz4Ng8Hg8prr9XqkpaW1+ZrfyGdNLkajEZIkISQkpN1yHXmPdpUdO3YgMjIS/fv3x6OPPoqqqqo2y3pKm5SVlWHTpk148MEHr1lWiW1CyuCt/T37emW1B+A9/T37egcl9ivs65XXJjeKSXobKisrYbPZEBUV5bI8KioKBoOh1W0MBkOHysvBbrfjySefxNixYzFkyJA2y/Xv3x9r1qzBJ598gnfffRd2ux1jxoxBcXFxF0bbUlpaGt555x1s3rwZq1atwunTp3Hbbbehpqam1fKe0CYbN25EdXU1HnjggTbLKLU9rtb8unbkNb+Rz5oc6uvrsWDBAsyYMQPBwcFtluvoe7QrTJo0Cf/4xz+Ql5eHZcuW4csvv8TkyZNhs9laLe8pbfL3v/8dQUFB+NWvftVuOSW2CSmHN/b37OuV1R7NvKW/Z1+vzH6Ffb3y2uRm+MgdAHWtefPm4ciRI9e8RiM9PR3p6enO+TFjxmDgwIF488038eKLL3Z2mG2aPHmyczolJQVpaWlITEzEhx9+eF2/sCnR22+/jcmTJyM2NrbNMkptj+6ioaEB06ZNgxACq1ataresEt+j9913n3N66NChSElJQZ8+fbBjxw7cfvvtssTkDmvWrMHMmTOvOaiSEtuEqDOxr1cm9vfKxr5embprX88j6W2IiIiAWq1GWVmZy/KysjJER0e3uk10dHSHyne1+fPn47PPPsP27dsRHx/foW19fX0xfPhwnDx5spOiuzEhISFITk5uMy6lt0lRURG2bt2Khx56qEPbKbU9ml/XjrzmN/JZ60rNnXZRURFyc3Pb/WW9Ndd6j8qhd+/eiIiIaDMmpbcJAHz99dcoLCzs8GcHUGabkHy8rb9nX++glPZo5k39Pfv6lpTYr7CvV16bdAST9DZoNBqMGDECeXl5zmV2ux15eXkuv3BeKT093aU8AOTm5rZZvqsIITB//nx8/PHH2LZtG3r16tXhfdhsNhw+fBgxMTGdEOGNq62txalTp9qMS6lt0mzt2rWIjIzElClTOrSdUtujV69eiI6OdnnNTSYTvvvuuzZf8xv5rHWV5k77xIkT2Lp1K8LDwzu8j2u9R+VQXFyMqqqqNmNScps0e/vttzFixAikpqZ2eFsltgnJx1v6e/b1ymqPq3lTf8++viUl9ivs65XXJh0i77h1yrZu3Tqh1WrFO++8I3744Qfx8MMPi5CQEGEwGIQQQtx///3imWeecZbfuXOn8PHxEa+99po4duyYyMnJEb6+vuLw4cNyVUEIIcSjjz4q9Hq92LFjhygtLXU+6urqnGWursvzzz8vtmzZIk6dOiX27t0r7rvvPqHT6cTRo0flqILT7373O7Fjxw5x+vRpsXPnTpGRkSEiIiJEeXm5EMJz2kQIxwiaPXv2FAsWLGixTsntUVNTI/bv3y/2798vAIjly5eL/fv3O0dBffnll0VISIj45JNPxKFDh8TUqVNFr169xKVLl5z7mDBhgnjjjTec89f6rMlRF6vVKu666y4RHx8vDhw44PLZsVgsbdblWu/Rrq5HTU2NePrpp0V+fr44ffq02Lp1q/jJT34i+vXrJ+rr69ushxLbpJnRaBT+/v5i1apVre5DCW1CnsUb+nv29cpqjyt5Yn/Pvp59fWdiX39tTNKv4Y033hA9e/YUGo1GjB49Wuzatcu5bty4cWL27Nku5T/88EORnJwsNBqNGDx4sNi0aVMXR9wSgFYfa9eudZa5ui5PPvmks95RUVHizjvvFPv27ev64K8yffp0ERMTIzQajYiLixPTp08XJ0+edK73lDYRQogtW7YIAKKwsLDFOiW3x/bt21t9PzXHa7fbxaJFi0RUVJTQarXi9ttvb1HHxMREkZOT47Ksvc+aHHU5ffp0m5+d7du3t1mXa71Hu7oedXV1YuLEiaJHjx7C19dXJCYmirlz57bogD2hTZq9+eabws/PT1RXV7e6DyW0CXkeT+/v2dcrqz2u5In9Pft69vVy1aVZd+/rJSGEuNGj8ERERERERETkPrwmnYiIiIiIiEghmKQTERERERERKQSTdCIiIiIiIiKFYJJOREREREREpBBM0omIiIiIiIgUgkk6ERERERERkUIwSSciIiIiIiJSCCbpRERERERERArBJJ2IupwkSdi4caPcYRAREVEnYV9PdOOYpBN1Mw888AAkSWrxmDRpktyhERERkRuwryfybD5yB0BEXW/SpElYu3atyzKtVitTNERERORu7OuJPBePpBN1Q1qtFtHR0S6P0NBQAI7T01atWoXJkyfDz88PvXv3xr/+9S+X7Q8fPowJEybAz88P4eHhePjhh1FbW+tSZs2aNRg8eDC0Wi1iYmIwf/58l/WVlZX45S9/CX9/f/Tr1w+ffvpp51aaiIioG2FfT+S5mKQTUQuLFi3C3XffjYMHD2LmzJm47777cOzYMQCA2WxGZmYmQkNDsWfPHmzYsAFbt2516ZhXrVqFefPm4eGHH8bhw4fx6aefom/fvi5/4/nnn8e0adNw6NAh3HnnnZg5cyYuXLjQpfUkIiLqrtjXEymYIKJuZfbs2UKtVouAgACXx0svvSSEEAKAeOSRR1y2SUtLE48++qgQQoi33npLhIaGitraWuf6TZs2CZVKJQwGgxBCiNjYWPHss8+2GQMA8dxzzznna2trBQDxxRdfuK2eRERE3RX7eiLPxmvSibqh8ePHY9WqVS7LwsLCnNPp6eku69LT03HgwAEAwLFjx5CamoqAgADn+rFjx8Jut6OwsBCSJKGkpAS33357uzGkpKQ4pwMCAhAcHIzy8vIbrRIRERFdgX09kedikk7UDQUEBLQ4Jc1d/Pz8rqucr6+vy7wkSbDb7Z0REhERUbfDvp7Ic/GadCJqYdeuXS3mBw4cCAAYOHAgDh48CLPZ7Fy/c+dOqFQq9O/fH0FBQUhKSkJeXl6XxkxERETXj309kXLxSDpRN2SxWGAwGFyW+fj4ICIiAgCwYcMGjBw5Erfeeivee+897N69G2+//TYAYObMmcjJycHs2bOxZMkSVFRU4PHHH8f999+PqKgoAMCSJUvwyCOPIDIyEpMnT0ZNTQ127tyJxx9/vGsrSkRE1E2xryfyXEzSibqhzZs3IyYmxmVZ//79UVBQAMAxGuu6devw2GOPISYmBh988AEGDRoEAPD398eWLVvwxBNPYNSoUfD398fdd9+N5cuXO/c1e/Zs1NfX4/XXX8fTTz+NiIgI3HPPPV1XQSIiom6OfT2R55KEEELuIIhIOSRJwscff4ysrCy5QyEiIqJOwL6eSNl4TToRERERERGRQjBJJyIiIiIiIlIInu5OREREREREpBA8kk5ERERERESkEEzSiYiIiIiIiBSCSToRERERERGRQjBJJyIiIiIiIlIIJulERERERERECsEknYiIiIiIiEghmKQTERERERERKQSTdCIiIiIiIiKF+D/t/RwQn+tUVwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 학습 및 검증 정확도 시각화\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# 정확도 그래프\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_exp26.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history_exp26.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# 손실 그래프\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_exp26.history['loss'], label='Training Loss')\n",
    "plt.plot(history_exp26.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 3s 9ms/step - loss: 0.7834 - accuracy: 0.7410\n",
      "(16,-): [0.7834004163742065, 0.7409999966621399]\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 0.6972 - accuracy: 0.7735\n",
      "(32,-): [0.6971790194511414, 0.7735000252723694]\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.7337 - accuracy: 0.7639\n",
      "(32,32): [0.7337435483932495, 0.7638999819755554]\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.7479 - accuracy: 0.7608\n",
      "(32,16): [0.7479420900344849, 0.7608000040054321]\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.6961 - accuracy: 0.7763\n",
      "(32,64): [0.6960694789886475, 0.7763000130653381]\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 0.7478 - accuracy: 0.7600\n",
      "(16,16): [0.7477818131446838, 0.7599999904632568]\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.7575 - accuracy: 0.7555\n",
      "(16,32): [0.7575260996818542, 0.7555000185966492]\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.7579 - accuracy: 0.7417\n",
      "(16,64): [0.7579485774040222, 0.7416999936103821]\n"
     ]
    }
   ],
   "source": [
    "print(\"(16,-):\", exp11_lora_vgg16.evaluate(x_test, y_test))\n",
    "print(\"(32,-):\", exp12_lora_vgg16.evaluate(x_test, y_test))\n",
    "print(\"(32,32):\", exp21_lora_vgg16.evaluate(x_test, y_test))\n",
    "print(\"(32,16):\", exp22_lora_vgg16.evaluate(x_test, y_test))\n",
    "print(\"(32,64):\", exp23_lora_vgg16.evaluate(x_test, y_test))\n",
    "print(\"(16,16):\", exp24_lora_vgg16.evaluate(x_test, y_test))\n",
    "print(\"(16,32):\", exp25_lora_vgg16.evaluate(x_test, y_test))\n",
    "print(\"(16,64):\", exp26_lora_vgg16.evaluate(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
